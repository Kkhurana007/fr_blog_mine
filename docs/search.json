[
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "Homepage",
    "section": "",
    "text": "See all/Voir tout"
  },
  {
    "objectID": "test.html#recent-posts-posts-recentes",
    "href": "test.html#recent-posts-posts-recentes",
    "title": "Homepage",
    "section": "",
    "text": "See all/Voir tout"
  },
  {
    "objectID": "test.html#posts-in-english",
    "href": "test.html#posts-in-english",
    "title": "Homepage",
    "section": "Posts in English",
    "text": "Posts in English\n\n\n\n See all posts in English"
  },
  {
    "objectID": "test.html#posts-en-français",
    "href": "test.html#posts-en-français",
    "title": "Homepage",
    "section": "Posts en français",
    "text": "Posts en français\n\n\n\n Voir posts en français"
  },
  {
    "objectID": "posts/when_to_water/index.html",
    "href": "posts/when_to_water/index.html",
    "title": "Can AI decide when to water crops?",
    "section": "",
    "text": "In Texas, 2022 was one of the driest years on record. The article in general wishes to tackle the water scarcity problems in agriculture. For that, Texas A&M AgriLife Research and IBM are collaborating to streamline a new low-cost app, similar to Liquid prep, to help farmers determine the optimal time to water their crops. The app, called “When to Water,” combines data from various sources, including soil sensors, crop types, and weather forecasts, to provide customized irrigation recommendations to farmers. The collaboration aims to improve the accuracy and speed of data collection, analysis, and transmission, ultimately helping farmers make more informed decisions about irrigation management."
  },
  {
    "objectID": "posts/semainedu20mai2024/knn-algorithm.html",
    "href": "posts/semainedu20mai2024/knn-algorithm.html",
    "title": "knn",
    "section": "",
    "text": "import numpy as np\n\n\n\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\n\ncmap = ListedColormap(['#FF0000','#00FF00','#0000FF'])\n\niris = datasets.load_iris()\nX, y = iris.data, iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n\nplt.figure()\nplt.scatter(X[:,2],X[:,3], c=y, cmap=cmap, edgecolor='k', s=20)\nplt.show()\n\n\n\n\n\nfrom collections import Counter\n\ndef euclidean_distance(x1, x2):\n    np.sqrt(np.sum((x1-x2)**2))\n\nclass KNN:\n    def __init__(self, k=3):\n        self.k = k\n    \n    def fit(self, X, y):\n        self.X_train = X\n        self.y_train = y\n    \n    def predict(self, X):\n        predictions = [self._predict(x) for x in X]\n        return predictions\n    \n    def _predict(self, x):\n        # conmpute the distance\n        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n        \n        # get the closest k\n        k_indices = np.argsort(distances)[:self.k]\n        k_nearest_labels = [self.y_train[i] for i in k_indices]\n        return predictions\n    \n        # majority vote \n        most_common = Counter(k_nearest_labels).most_common()\n        return most_common[0][0]\n    \n        \n\n\nfrom collections import Counter\nimport numpy as np\n\ndef euclidean_distance(x1, x2):\n    return np.sqrt(np.sum((x1 - x2) ** 2))\n\nclass KNN:\n    def __init__(self, k=3):\n        self.k = k\n    \n    def fit(self, X, y):\n        self.X_train = X\n        self.y_train = y\n    \n    def predict(self, X):\n        predictions = [self._predict(x) for x in X]\n        return predictions\n    \n    def _predict(self, x):\n        # Compute the distance\n        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n        \n        # Get the closest k indices\n        k_indices = np.argsort(distances)[:self.k]\n        \n        # Get the labels of the k nearest neighbors\n        k_nearest_labels = [self.y_train[i] for i in k_indices]\n        \n        # Majority vote, most common class label\n        most_common = Counter(k_nearest_labels).most_common()\n        return most_common  # Return the most common label\n\n\nclf = KNN(k=5)\nclf.fit(X_train, y_train)\npredictions = clf.predict(X_test)\n\nprint(predictions)\n\n[[(1, 4), (2, 1)], [(2, 3), (1, 2)], [(2, 5)], [(0, 5)], [(1, 5)], [(0, 5)], [(0, 5)], [(0, 5)], [(1, 5)], [(2, 5)], [(1, 5)], [(0, 5)], [(2, 5)], [(1, 5)], [(0, 5)], [(1, 5)], [(2, 5)], [(0, 5)], [(2, 5)], [(1, 5)], [(1, 5)], [(1, 5)], [(1, 5)], [(1, 5)], [(2, 5)], [(0, 5)], [(2, 4), (1, 1)], [(1, 5)], [(2, 5)], [(0, 5)]]\n\n\n\nfrom collections import Counter\nimport numpy as np\n\ndef euclidean_distance(x1, x2):\n    return np.sqrt(np.sum((x1 - x2) ** 2))\n\nclass KNN:\n    def __init__(self, k=3):\n        self.k = k\n    \n    def fit(self, X, y):\n        self.X_train = X\n        self.y_train = y\n    \n    def predict(self, X):\n        predictions = [self._predict(x) for x in X]\n        return predictions\n    \n    def _predict(self, x):\n        # Compute the distance\n        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n        \n        # Get the closest k indices\n        k_indices = np.argsort(distances)[:self.k]\n        \n        # Get the labels of the k nearest neighbors\n        k_nearest_labels = [self.y_train[i] for i in k_indices]\n        \n        # Majority vote, most common class label\n        # refining the class to get the first label\n        most_common = Counter(k_nearest_labels).most_common()\n        return most_common[0][0]  # Return the most common label\n\n\nclf = KNN(k=5)\nclf.fit(X_train, y_train)\npredictions = clf.predict(X_test)\n\nprint(predictions)\n\n[1, 2, 2, 0, 1, 0, 0, 0, 1, 2, 1, 0, 2, 1, 0, 1, 2, 0, 2, 1, 1, 1, 1, 1, 2, 0, 2, 1, 2, 0]\n\n\n\n# calculating the accuracy\n\nacc = np.sum(predictions == y_test) / len(y_test)\n\nprint(acc)\n\n0.9666666666666667\n\n\nResources:-\n\nhttps://medium.com/@Khuranasoils/linear-regression-is-a-fundamental-statistical-method-used-for-modelling-the-relationship-between-a-e0544296fe56\nhttps://www.youtube.com/watch?v=rTEtEy5o3X0"
  },
  {
    "objectID": "posts/qmd_titanic/titanic.html",
    "href": "posts/qmd_titanic/titanic.html",
    "title": "Titanic dataset analysis using Pandas and Numpy",
    "section": "",
    "text": "This notebook follows the fastai style conventions."
  },
  {
    "objectID": "posts/qmd_titanic/titanic.html#importnig-packages",
    "href": "posts/qmd_titanic/titanic.html#importnig-packages",
    "title": "Titanic dataset analysis using Pandas and Numpy",
    "section": "Importnig packages",
    "text": "Importnig packages\n\nimport pandas as pd\n\n\nimport numpy as np\n\n\nfrom scipy import stats, integrate\nimport scipy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(color_codes=True)\n%pylab inline\n\nPopulating the interactive namespace from numpy and matplotlib"
  },
  {
    "objectID": "posts/qmd_titanic/titanic.html#problem-statement",
    "href": "posts/qmd_titanic/titanic.html#problem-statement",
    "title": "Titanic dataset analysis using Pandas and Numpy",
    "section": "Problem Statement",
    "text": "Problem Statement\n\nWhat is the dependent variable and what are the factors in this data? Who had more chances of survival, what are the factors?\nData exploration section will investigate the dependent variable ‘Survived’ and understand the relationship of factors such as being a female, or child, or being in a certain class, or having sibling/spouse, parent/child affect the survival rate. We will also come up with a hypothesis and test it.\n\ndata = pd.read_csv('Titanic.csv')\n\n\ndata.head()\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS"
  },
  {
    "objectID": "posts/qmd_titanic/titanic.html#data-wrangling",
    "href": "posts/qmd_titanic/titanic.html#data-wrangling",
    "title": "Titanic dataset analysis using Pandas and Numpy",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\nPassengerId    891 non-null int64\nSurvived       891 non-null int64\nPclass         891 non-null int64\nName           891 non-null object\nSex            891 non-null object\nAge            714 non-null float64\nSibSp          891 non-null int64\nParch          891 non-null int64\nTicket         891 non-null object\nFare           891 non-null float64\nCabin          204 non-null object\nEmbarked       889 non-null object\ndtypes: float64(2), int64(5), object(5)\nmemory usage: 66.2+ KB\n\n\nThe titanic data given has 891 rows, most of the columns have 891 rows except Age, Cabin and Embarked.\n\nprint(data['Cabin'].describe())\nprint(data['Embarked'].describe())\nprint(data['Age'].describe())\n\ncount         204\nunique        147\ntop       B96 B98\nfreq            4\nName: Cabin, dtype: object\ncount     889\nunique      3\ntop         S\nfreq      644\nName: Embarked, dtype: object\ncount    714.000000\nmean      29.699118\nstd       14.526497\nmin        0.420000\n25%       20.125000\n50%       28.000000\n75%       38.000000\nmax       80.000000\nName: Age, dtype: float64\n\n\n\ndata['Cabin'].value_counts().plot(kind ='bar', figsize= (15,3))\nsns.plt.title('Frequency/Counts by Cabin')\n\n&lt;matplotlib.text.Text&gt;\n\n\n\n\n\nCabin has 147 unique values for 204 rows, Max freq is 4. It is difficult to draw conclusion on this data and since it has just 22.8% of rows, I will be dropping this column from any further analysis. Also PassengerId does not give me any useful information, so I will drop that column as well\n\ndel data['Cabin']\ndel data['PassengerId']\n\nLet us also drop the rows with missing values for Age and Embarked now\n\ndata.dropna(subset = ['Embarked', 'Age'], inplace = True)\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 712 entries, 0 to 890\nData columns (total 10 columns):\nSurvived    712 non-null int64\nPclass      712 non-null int64\nName        712 non-null object\nSex         712 non-null object\nAge         712 non-null float64\nSibSp       712 non-null int64\nParch       712 non-null int64\nTicket      712 non-null object\nFare        712 non-null float64\nEmbarked    712 non-null object\ndtypes: float64(2), int64(4), object(4)\nmemory usage: 50.1+ KB\n\n\nPclass should not be numeric, so let us update it to upper, middle and lower class. For that, we need to look at its relationship with Fare\n\nsns.barplot(x=\"Pclass\", y=\"Fare\", data=data);\nsns.plt.title('Pclass by Mean Fare')\n\n&lt;matplotlib.text.Text&gt;\n\n\n\n\n\nMean Fare of Pclass 1 was 88 dollars, Pclass 2 was 21.47 dollars and Pclass 3 was 13.22 dollars, so let us update the values of Pclass to ‘Upper’ for Class 1, ‘Middle’ for Class 2 and ‘Lower’ for Class 3\n\ndata.loc[data['Pclass'] == 1, 'Pclass'] = 'Upper'\ndata.loc[data['Pclass'] == 2, 'Pclass'] = 'Middle'\ndata.loc[data['Pclass'] == 3, 'Pclass'] = 'Lower'"
  },
  {
    "objectID": "posts/qmd_titanic/titanic.html#data-exploration",
    "href": "posts/qmd_titanic/titanic.html#data-exploration",
    "title": "Titanic dataset analysis using Pandas and Numpy",
    "section": "Data Exploration",
    "text": "Data Exploration\n\n# Distribution of numeric variables\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(ncols=4, figsize = (15,3))\ndata['Age'].plot(kind ='hist', bins = 25, ax=ax1)\nax1.set_title('Age')\ndata['Fare'].plot(kind = 'hist', bins= 25, ax=ax2)\nax2.set_title('Fare')\ndata['Parch'].plot(kind = 'hist', ax=ax3)\nax3.set_title('Parch')\ndata['SibSp'].plot(kind = 'hist', ax=ax4)\nax4.set_title('SibSp')\n\n&lt;matplotlib.text.Text&gt;\n\n\n\n\n\n\n# Distribution of categorical variables\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(ncols=4, figsize = (12,3))\ndata['Sex'].value_counts().plot(kind ='bar', ax=ax1)\nax1.set_title('Sex')\ndata['Survived'].value_counts().plot(kind = 'bar', ax=ax2)\nax2.set_title('Survived')\nax2.set_xticklabels(['Perished', 'Survived'])\ndata['Pclass'].value_counts().plot(kind = 'bar', ax=ax3)\nax3.set_title('Pclass')\ndata['Embarked'].value_counts().plot(kind = 'bar', ax=ax4)\nax4.set_title('Embarked')\n\n&lt;matplotlib.text.Text&gt;\n\n\n\n\n\nThe above plots show the distributions of numerical and categorical columns in our data. Age ranges from 0 to 80 years with mean and mode around 25-30 years, Fare ranges from 0 to over 500 dollars, Parch and SibSp has its mode at 0 meaning most people did not travel with any parent/child or sibling/spouse, There were around 453 males and 289 females onboard, 424 perished and 288 survived. Most of the passengers were in Lower Pclass and embarked at station S.\n\nUnderstanding the dependencies of dependent and independent variables\nSince for the given data, more than 50% of the passengers perished, We will investigate the factors that survival of the passengers depend on and would like to answer questions like did females have more chance of surviving, how does age or fare affect the survival, does having a parent or child, or sibling or spouse influence survival and how does Pclass affect survival. Dependent variable is ‘Survived’ which gives 0 for rows for passengers who perished and 1 for passengers that survived. Independent variables are Sex, Pclass, Embarked, Age, Fare etc.\nThere could be other factors or variables like location of cabins or location/state(sleep or awake) of passengers at the time of the accident etc which we had limited data for and hence ave been omitted from the analysis. We also omitted rows that had missing values for ‘Age’ and ‘Embarked’ so that will also skew the statistical analysis a bit.\n\n#Function to create grouped data by factors\ndef grouped_by_factors(df,factor):\n    mean_by_factor = df.groupby(factor).describe()\n    return mean_by_factor\n\nSome understanding of mean/max/std/count would be helpful for our analysis so I created a function to display statistics using groupby function. We will also be creating plots to help visualize the data.\n\n\nUnderstanding Dependent variable ‘Survived’ by numerical columns\n\n‘Survived’ by Age and Fare\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize =(12,3))\nfig1 = sns.regplot(x=\"Age\", y=\"Survived\", data=data, ax = ax1)\nfig2 = sns.regplot(x=\"Fare\", y=\"Survived\", data=data, ax = ax2)\nplt.suptitle(\"Perished vs. Survived by Age and Fare\", size=12)\nfig1.set(ylabel='Survival Rate'), fig2.set(ylabel='Survival Rate')\n\n([&lt;matplotlib.text.Text&gt;], [&lt;matplotlib.text.Text&gt;])\n\n\n\n\n\n\n\n‘Survived’ by SibSp and Parch\n\ng = sns.PairGrid(data, y_vars=[\"Survived\"], x_vars=[\"SibSp\", \"Parch\"], size=4)\ng.map(sns.barplot, color=\".4\")\ng.set(ylabel='Survival Rate')\nplt.suptitle(\"Perished vs. Survived by SibSp and Parch\", size=12)\n\n&lt;matplotlib.text.Text&gt;\n\n\n\n\n\n\ngrouped_by_factors(data,'Survived')\n\n\n\n\n\n\n\n\nAge\nFare\nParch\nSibSp\n\n\nSurvived\n\n\n\n\n\n\n\n\n\n0\ncount\n424.000000\n424.000000\n424.000000\n424.000000\n\n\nmean\n30.626179\n22.965456\n0.365566\n0.525943\n\n\nstd\n14.172110\n31.448825\n0.878341\n1.044760\n\n\nmin\n1.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n21.000000\n7.895800\n0.000000\n0.000000\n\n\n50%\n28.000000\n11.887500\n0.000000\n0.000000\n\n\n75%\n39.000000\n26.550000\n0.000000\n1.000000\n\n\nmax\n74.000000\n263.000000\n6.000000\n5.000000\n\n\n1\ncount\n288.000000\n288.000000\n288.000000\n288.000000\n\n\nmean\n28.193299\n51.647672\n0.531250\n0.496528\n\n\nstd\n14.859146\n70.664499\n0.808747\n0.732512\n\n\nmin\n0.420000\n0.000000\n0.000000\n0.000000\n\n\n25%\n19.000000\n13.000000\n0.000000\n0.000000\n\n\n50%\n28.000000\n26.250000\n0.000000\n0.000000\n\n\n75%\n36.000000\n65.000000\n1.000000\n1.000000\n\n\nmax\n80.000000\n512.329200\n5.000000\n4.000000\n\n\n\n\n\n\n\nThe data shows 424 passengers did not survive and 288 did.\nAverage age of passengers that survived was 28.2(std=14.8) years as compared to 30.62(14.17) for those who did not survive. On average, passengers who survived paid higher fare(mean=51.6 dollars) as compared to who did not(mean=22.9 dollars).\nFrom the barchart, the survival rate for those travelling with 1/2 sibling or spouse and 1/2/3 parent or children was higher than the ones that did not. The relationship of survival is not linear with the number of sibsp/parch which could be due to lack of data.\nFrom the correlation plot, Survival rate is positively correlated to Fare and negatively correlated to Age which means younger people and those who paid more had higher chances of surviving\n\n\n\nUnderstanding Dependent variable ‘Survived’ by Categorical columns\n\n‘Survived’ by Pclass\n\ngrouped_by_factors(data,'Pclass')\n\n\n\n\n\n\n\n\nAge\nFare\nParch\nSibSp\nSurvived\n\n\nPclass\n\n\n\n\n\n\n\n\n\n\nLower\ncount\n355.000000\n355.000000\n355.000000\n355.000000\n355.000000\n\n\nmean\n25.140620\n13.229435\n0.456338\n0.585915\n0.239437\n\n\nstd\n12.495398\n10.043158\n0.971447\n1.157303\n0.427342\n\n\nmin\n0.420000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n18.000000\n7.775000\n0.000000\n0.000000\n0.000000\n\n\n50%\n24.000000\n8.050000\n0.000000\n0.000000\n0.000000\n\n\n75%\n32.000000\n15.741700\n1.000000\n1.000000\n0.000000\n\n\nmax\n74.000000\n56.495800\n6.000000\n5.000000\n1.000000\n\n\nMiddle\ncount\n173.000000\n173.000000\n173.000000\n173.000000\n173.000000\n\n\nmean\n29.877630\n21.471556\n0.404624\n0.427746\n0.479769\n\n\nstd\n14.001077\n13.187429\n0.705775\n0.611645\n0.501041\n\n\nmin\n0.670000\n10.500000\n0.000000\n0.000000\n0.000000\n\n\n25%\n23.000000\n13.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n29.000000\n15.045800\n0.000000\n0.000000\n0.000000\n\n\n75%\n36.000000\n26.000000\n1.000000\n1.000000\n1.000000\n\n\nmax\n70.000000\n73.500000\n3.000000\n3.000000\n1.000000\n\n\nUpper\ncount\n184.000000\n184.000000\n184.000000\n184.000000\n184.000000\n\n\nmean\n38.105543\n88.048121\n0.413043\n0.456522\n0.652174\n\n\nstd\n14.778904\n81.293524\n0.734061\n0.634406\n0.477580\n\n\nmin\n0.920000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n27.000000\n33.890600\n0.000000\n0.000000\n0.000000\n\n\n50%\n37.000000\n67.950000\n0.000000\n0.000000\n1.000000\n\n\n75%\n49.000000\n107.043750\n1.000000\n1.000000\n1.000000\n\n\nmax\n80.000000\n512.329200\n4.000000\n3.000000\n1.000000\n\n\n\n\n\n\n\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize =(12,3))\nfig1 = sns.countplot(x=\"Pclass\", data=data, hue='Survived', palette=\"Greens_d\", ax=ax1);\nplt.legend([\"Perished\", \"Survived\"])\nfig2 = sns.barplot(x=\"Pclass\", y=\"Survived\", data=data, ax=ax2);\nplt.suptitle(\"Survival rate by Pclass\", size=12)\n\n&lt;matplotlib.text.Text&gt;\n\n\n\n\n\nMean Fare of Upper Class was 88 dollars, Middle Class was 21.47 dollars and Lower Class was 13.22 dollars. Most survivors were from upper class(mean survival = 0.65), followed by middle(mean survival = 0.48) and then lower(mean survival = 0.24). Most of the passengers who did not survive belonged to the lower class Pclass shows linear relation to survival probability. There could be several reasons for that. People in upper classes could have boarded lifeboats before the lower classes, it also fits well with the correlation to fare in the prev plot.\n\n\n‘Survived’ by Embarked\n\nsns.countplot(x=\"Embarked\", data=data, hue='Survived', palette=\"Greens_d\");\nsns.countplot(x=\"Embarked\", data=data, hue='Pclass', palette=\"Reds_d\");\nplt.suptitle(\"Valuecounts of Survivors by Pclass\", size=12)\nplt.suptitle(\"Valuecounts of Female vs Male survivors\", size=12)\nlabel = [\"Perished\", \"Survived\", \"Lower\", \"Upper\", \"Middle\"]\nplt.legend(label, loc='upper center')\n\n&lt;matplotlib.legend.Legend&gt;\n\n\n\n\n\nMost of the passengers were in lower Pclass and embarked from ‘S’ followed by ‘C’ and ‘Q’. Does not show much relationship to survival rate\n\n\n‘Survived’ by Sex\n\ndata.groupby('Sex').describe()\n\n\n\n\n\n\n\n\nAge\nFare\nParch\nSibSp\nSurvived\n\n\nSex\n\n\n\n\n\n\n\n\n\n\nfemale\ncount\n259.000000\n259.000000\n259.000000\n259.000000\n259.000000\n\n\nmean\n27.745174\n47.332433\n0.714286\n0.644788\n0.752896\n\n\nstd\n13.989760\n61.517487\n1.069045\n0.930367\n0.432163\n\n\nmin\n0.750000\n6.750000\n0.000000\n0.000000\n0.000000\n\n\n25%\n18.000000\n13.000000\n0.000000\n0.000000\n1.000000\n\n\n50%\n27.000000\n26.000000\n0.000000\n0.000000\n1.000000\n\n\n75%\n36.000000\n56.964600\n1.000000\n1.000000\n1.000000\n\n\nmax\n63.000000\n512.329200\n6.000000\n5.000000\n1.000000\n\n\nmale\ncount\n453.000000\n453.000000\n453.000000\n453.000000\n453.000000\n\n\nmean\n30.726645\n27.268836\n0.271523\n0.439294\n0.205298\n\n\nstd\n14.678201\n45.841889\n0.651076\n0.923609\n0.404366\n\n\nmin\n0.420000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n21.000000\n7.895800\n0.000000\n0.000000\n0.000000\n\n\n50%\n29.000000\n13.000000\n0.000000\n0.000000\n0.000000\n\n\n75%\n39.000000\n28.500000\n0.000000\n1.000000\n0.000000\n\n\nmax\n80.000000\n512.329200\n5.000000\n5.000000\n1.000000\n\n\n\n\n\n\n\n\nsns.countplot(x=\"Sex\", data=data, hue='Survived', palette=\"Greens_d\");\nplt.suptitle(\"Valuecounts of Female vs Male survivors\", size=12)\nlabel = [\"Perished\", \"Survived\"]\nplt.legend(label, loc='upper center')\n\n&lt;matplotlib.legend.Legend&gt;\n\n\n\n\n\nMean age of females who boarded the ship was 27-28 years and males was 30-31 years. There were 259 females and 453 males, more number of females(mean survival = 0.75) survived than males(mean survival = 0.20)\nFor the purpose of this analysis, I will pick Sex, Pclass and Age as major factors and investigate them further. The reason why I am picking them is because they show correlation with survival rate. Survival showed correlation to Fare as well but since the fare is represented by Pclass, I picked Pclass over Fare. Although other factors also affect survival, but I will focus on these three for this exercise\n\n\n\nUnderstanding Pclass and Sex as a factor\n\nsns.barplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=data);\nplt.suptitle(\"Mean Survival rate of Female vs Male survivors by Pclass\", size=12)\n\n&lt;matplotlib.text.Text&gt;\n\n\n\n\n\nThere were 314 females and 577 males, mean for female survivors(mean=0.74,std= 0.44) is more than males(mean=0.19,std= 0.39) across all Pclasses, Survival has linear relationship with class. Females had high probability of survival in both Upper and Middle class. Only upper class males had high probability of survival, which was lower than low class female passengers however\n\n\nUnderstanding Age as a factor\n\nprint(grouped_by_factors(data,'Age').head())\nprint(grouped_by_factors(data,'Age').tail())\n\n              Fare  Parch  SibSp  Survived\nAge                                       \n0.42 count  1.0000    1.0    1.0       1.0\n     mean   8.5167    1.0    0.0       1.0\n     std       NaN    NaN    NaN       NaN\n     min    8.5167    1.0    0.0       1.0\n     25%    8.5167    1.0    0.0       1.0\n          Fare  Parch  SibSp  Survived\nAge                                   \n80.0 min  30.0    0.0    0.0       1.0\n     25%  30.0    0.0    0.0       1.0\n     50%  30.0    0.0    0.0       1.0\n     75%  30.0    0.0    0.0       1.0\n     max  30.0    0.0    0.0       1.0\n\n\n\nsurv_age = data[data['Survived'] == 1]\ng = surv_age['Age'].plot(kind='hist', figsize=[12,6], alpha=.8)\nnotsurv_age = data[data['Survived'] == 0]\nnotsurv_age['Age'].plot(kind='hist', figsize=[12,6], alpha=.4)\nplt.legend(label)\ng.set(xlabel='Age')\nplt.suptitle(\"Distribution of Age for Perished and Survived\", size=12)\n\n&lt;matplotlib.text.Text&gt;\n\n\n\n\n\nAge of the passengers ranged from 0 to 80 years. Green bar is for passengers who did not survive and the blue is for those who survived. The distribution is almost normal distribution with similar shape and mode around 20 years. Below is the correlation for Age vs mean survived, it shows slight negative correlation with pearson’r value of -0.082\n\nCorrelation of ‘Survived’ with Age\n\nsns.set(style=\"darkgrid\", color_codes=True)\ng = sns.jointplot(\"Age\", \"Survived\", data=data, kind=\"reg\",color=\"g\", size=7)\nplt.subplots_adjust(top=0.95)\nplt.suptitle(\"Distribution of Age for Perished and Survived\", size=12)\n\n&lt;matplotlib.text.Text&gt;\n\n\n\n\n\n\n\nCorrelation of ‘Survived’ with Age and Sex\n\ng = sns.lmplot(x=\"Age\", y=\"Survived\", col=\"Sex\", hue=\"Sex\", data=data, y_jitter=.02, logistic=True)\nplt.subplots_adjust(top=0.9)\nplt.suptitle(\"Correlation of Age with Survival Rate\", size=12)\n\n&lt;matplotlib.text.Text&gt;\n\n\n\n\n\nSurvival probability was higher for Younger Men and Older Women, Side by side comparison of males and females by age further supports that\n\ng = sns.factorplot(x=\"Survived\", y=\"Age\", hue=\"Sex\", data=data, size=6, kind=\"bar\", palette=\"muted\")\ng.despine(left=True)\nplt.subplots_adjust(top=0.95)\nplt.suptitle(\"Survived vs Age for males and females\", size=12)\ng.set_xticklabels(['Perished', 'Survived'])"
  },
  {
    "objectID": "posts/qmd_titanic/titanic.html#hypothesis-testing",
    "href": "posts/qmd_titanic/titanic.html#hypothesis-testing",
    "title": "Titanic dataset analysis using Pandas and Numpy",
    "section": "Hypothesis testing",
    "text": "Hypothesis testing\nI have a hypothesis that passengers that are lower in age(&lt;15 years) had greater chance of survival than females.\nNull Hypothesis would be that the difference in chances of survival of passengers greater or lower than 15 years is not significant and alternate would be that it is significant.\n                        H0: µchild = µfemale at α = 0.05, \n                        HA: µchild ≠ µfemale at α = 0.05, where α is the t-critical at which the probability is .05 and µchild and µfemale are population means for the two groups.\n\n#Children under 15yrs of age\ndata_children = data[data['Age'] &lt;= 15]\n\n\n#Females of age greater than 15 years\ndata_female = data[(data['Sex'] == 'female') & (data['Age'] &gt; 15)]\n\n\nscipy.stats.ttest_ind(data_children['Survived'], data_female['Survived'], axis=0, equal_var=False, nan_policy='propagate')\n\nTtest_indResult(statistic=-2.978953154108325, pvalue=0.0034528377861817636)\n\n\n\nSince p value is low, the difference in mean survival is significant for females vs. children. Negative t-statistic shows that the mean survival of females is more than that of children"
  },
  {
    "objectID": "posts/qmd_titanic/titanic.html#conclusions",
    "href": "posts/qmd_titanic/titanic.html#conclusions",
    "title": "Titanic dataset analysis using Pandas and Numpy",
    "section": "Conclusions",
    "text": "Conclusions\nIn Conclusion with the given dataset, Most contributing factors are ‘Sex’ and Pclass. Women had the most probability of survival in general. Survival rate is positively correlated to Fare and negatively correlated to Age which means younger people and those who paid more had higher chances of surviving. Females had positive correlation of survival with age and Males had negative correlation. Most survivors were from Upper Pclass followed by medium and lower class passengers. Most of the passengers in lower class perished. Passengers with any parent/child/sibling or spouse had higher chance at survival than the ones that did not. The analysis has following limitations: Omitted rows with missing values for ‘Age’ and ‘emabarked’ Did not draw conclusions based on ‘Name’ column dropped ‘Cabin’ and ‘PassengerId’ during data wrangling phase The data set is limited, the complete dataset should contain data for 1500 passengers"
  },
  {
    "objectID": "posts/pythonic_thinking/Coding_Rules.html",
    "href": "posts/pythonic_thinking/Coding_Rules.html",
    "title": "Python Coding Rules",
    "section": "",
    "text": "reversing order gives traceback\ndifficult to read the code\nusing same value multiple times in tuple (repeat it in the right side)\ndictionary formats\n\n\n\n\n\nUse if/else conditional to reduce visual noise\nMoreover, if/else expression provides a more readable alternative over the boolean or/and in expressions.\n\n\n\n\n\nuse special syntax to unpack multiple values and keys in a single statement.\n\n\n\n\n\nrange (built-in funciton) is useful for loops\nprefer enumerate instead of looping over a range\n\n\n# example of enumeration with list- \nflavor_list = ['vanilla', 'chocolate', 'pecan', 'strawberry']\nfor flavor in flavor_list:\n    print(f'{flavor} is delicious')\n\nvanilla is delicious\nchocolate is delicious\npecan is delicious\nstrawberry is delicious\n\n\n\n\n\n\nnames = ['Kunal', 'Xives', 'pricila']\ncounts = [len(n) for n in names]\nprint(counts)\n\n[5, 5, 7]\n\n\n\n# iterating over lenght of lists\nlongest_name = None\nmax_count = 0\n\nfor i in range(len(names)):\n    count = counts[i]\n    if count &gt; max_count:\n        longest_name = names[i]\n        max_count = count\n        \nprint(longest_name)\n\npricila\n\n\n\n# we see that the above code is a bit noisy. \n# to imporve it, we'll use the enumerate method\n\nfor i, name in enumerate(names):\n    count = counts[i]\n    if count &gt; max_count:\n        longest_name = name\n        max_count = count\nprint(longest_name)\n\npricila\n\n\n\n# to improve it further, we'll use the inbuilt zip function\n\nfor name, count in zip(names, counts):\n    if count &gt; max_count:\n        longest_name = name\n        max_count = count\n\nprint(longest_name)\n\npricila\n\n\n\n# zip's behavior is different if counts are not updated\n\nnames.append('Rosy')\nfor name, count in zip(names, counts):\n    print(name)\n\nKunal\nXives\npricila\n\n\n\n# so, be careful when using iterators of different lenght. \n\n# consider using zip_longest function from itertools instead\n\nimport itertools\nfor name, count in itertools.zip_longest (names, counts):\n    print (f'{name}: {count}')\n\nKunal: 5\nXives: 5\npricila: 7\nRosy: None\n\n\n\n\n\n\n# for loops first\n\nfor i in range(3):\n    print('Loop', i)\nelse:\n    print('Else block!')\n\nLoop 0\nLoop 1\nLoop 2\nElse block!\n\n\n\n# using break in the code\n\nfor i in range(3):\n    print('Loop', i)\n    if i == 1:\n        break\n        \nelse:\n    print('Else block!')\n\nLoop 0\nLoop 1\n\n\n\n# else runs immediately if looped over an empty sequence\n\nfor x in []:\n    print('Never runs')\nelse:\n    print('For else block!')\n\nFor else block!\n\n\n\n# else also runs when while loops are initially false\nwhile False:\n    print('Never runs')\nelse:\n    print('While else block!')\n\nWhile else block!\n\n\n\n## finding coprimes (having common divisor i.e. 1)\n\na = 11\nb = 9\n\nfor i in range(2, min(a, b) + 1):\n    print ('Testing', i)\n    if a% i == 0 and b%i == 0:\n        print('Not coprime')\n        break\nelse:\n    print('coprime')\n\nTesting 2\nTesting 3\nTesting 4\nTesting 5\nTesting 6\nTesting 7\nTesting 8\nTesting 9\ncoprime\n\n\n\n\n\n\n# Without the walrus operator\neven_numbers_without_walrus = []\ncount = 0\nwhile count &lt; 5:\n    number = count * 2\n    if number % 2 == 0:\n        even_numbers_without_walrus.append(number)\n        count += 1\n\nprint(even_numbers_without_walrus)\n\n[0, 2, 4, 6, 8]\n\n\n\n# With the walrus operator\neven_numbers_with_walrus = []\ncount = 0\nwhile count &lt; 5:\n    if (number := count * 2) % 2 == 0:\n        even_numbers_with_walrus.append(number)\n        count += 1\n\nprint(even_numbers_with_walrus)\n\n[0, 2, 4, 6, 8]"
  },
  {
    "objectID": "posts/pythonic_thinking/Coding_Rules.html#general",
    "href": "posts/pythonic_thinking/Coding_Rules.html#general",
    "title": "Python Coding Rules",
    "section": "",
    "text": "reversing order gives traceback\ndifficult to read the code\nusing same value multiple times in tuple (repeat it in the right side)\ndictionary formats\n\n\n\n\n\nUse if/else conditional to reduce visual noise\nMoreover, if/else expression provides a more readable alternative over the boolean or/and in expressions.\n\n\n\n\n\nuse special syntax to unpack multiple values and keys in a single statement.\n\n\n\n\n\nrange (built-in funciton) is useful for loops\nprefer enumerate instead of looping over a range\n\n\n# example of enumeration with list- \nflavor_list = ['vanilla', 'chocolate', 'pecan', 'strawberry']\nfor flavor in flavor_list:\n    print(f'{flavor} is delicious')\n\nvanilla is delicious\nchocolate is delicious\npecan is delicious\nstrawberry is delicious\n\n\n\n\n\n\nnames = ['Kunal', 'Xives', 'pricila']\ncounts = [len(n) for n in names]\nprint(counts)\n\n[5, 5, 7]\n\n\n\n# iterating over lenght of lists\nlongest_name = None\nmax_count = 0\n\nfor i in range(len(names)):\n    count = counts[i]\n    if count &gt; max_count:\n        longest_name = names[i]\n        max_count = count\n        \nprint(longest_name)\n\npricila\n\n\n\n# we see that the above code is a bit noisy. \n# to imporve it, we'll use the enumerate method\n\nfor i, name in enumerate(names):\n    count = counts[i]\n    if count &gt; max_count:\n        longest_name = name\n        max_count = count\nprint(longest_name)\n\npricila\n\n\n\n# to improve it further, we'll use the inbuilt zip function\n\nfor name, count in zip(names, counts):\n    if count &gt; max_count:\n        longest_name = name\n        max_count = count\n\nprint(longest_name)\n\npricila\n\n\n\n# zip's behavior is different if counts are not updated\n\nnames.append('Rosy')\nfor name, count in zip(names, counts):\n    print(name)\n\nKunal\nXives\npricila\n\n\n\n# so, be careful when using iterators of different lenght. \n\n# consider using zip_longest function from itertools instead\n\nimport itertools\nfor name, count in itertools.zip_longest (names, counts):\n    print (f'{name}: {count}')\n\nKunal: 5\nXives: 5\npricila: 7\nRosy: None\n\n\n\n\n\n\n# for loops first\n\nfor i in range(3):\n    print('Loop', i)\nelse:\n    print('Else block!')\n\nLoop 0\nLoop 1\nLoop 2\nElse block!\n\n\n\n# using break in the code\n\nfor i in range(3):\n    print('Loop', i)\n    if i == 1:\n        break\n        \nelse:\n    print('Else block!')\n\nLoop 0\nLoop 1\n\n\n\n# else runs immediately if looped over an empty sequence\n\nfor x in []:\n    print('Never runs')\nelse:\n    print('For else block!')\n\nFor else block!\n\n\n\n# else also runs when while loops are initially false\nwhile False:\n    print('Never runs')\nelse:\n    print('While else block!')\n\nWhile else block!\n\n\n\n## finding coprimes (having common divisor i.e. 1)\n\na = 11\nb = 9\n\nfor i in range(2, min(a, b) + 1):\n    print ('Testing', i)\n    if a% i == 0 and b%i == 0:\n        print('Not coprime')\n        break\nelse:\n    print('coprime')\n\nTesting 2\nTesting 3\nTesting 4\nTesting 5\nTesting 6\nTesting 7\nTesting 8\nTesting 9\ncoprime\n\n\n\n\n\n\n# Without the walrus operator\neven_numbers_without_walrus = []\ncount = 0\nwhile count &lt; 5:\n    number = count * 2\n    if number % 2 == 0:\n        even_numbers_without_walrus.append(number)\n        count += 1\n\nprint(even_numbers_without_walrus)\n\n[0, 2, 4, 6, 8]\n\n\n\n# With the walrus operator\neven_numbers_with_walrus = []\ncount = 0\nwhile count &lt; 5:\n    if (number := count * 2) % 2 == 0:\n        even_numbers_with_walrus.append(number)\n        count += 1\n\nprint(even_numbers_with_walrus)\n\n[0, 2, 4, 6, 8]"
  },
  {
    "objectID": "posts/pythonic_thinking/Coding_Rules.html#lists-and-dictionaries",
    "href": "posts/pythonic_thinking/Coding_Rules.html#lists-and-dictionaries",
    "title": "Python Coding Rules",
    "section": "Lists and dictionaries",
    "text": "Lists and dictionaries\n\nKnow how to slice sequneces\n\n#somelist [start:end]\na = ['a', 'b', 'c', 'd', 'e', 'f']\nprint ('Middle two: ', a[2:4])\n\nMiddle two:  ['c', 'd']\n\n\n\n\nAvoid striding and slicing in a single expression\n\nb = [1, 2, 3, 4, 5, 6]\nodds = b[::2]\nevens = b[1::2]\nprint(odds)\nprint(evens)\n\n[1, 3, 5]\n[2, 4, 6]\n\n\n\n# stride syntax that can introduce bugs ; Avoid\nc = b'rouge'\nd = c[::-1]\n\nprint(d)\n\nb'eguor'\n\n\n\nx = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\nprint(x[2::2])     # ['c', 'e', 'g']\nprint(x[-2::-2])   # ['g', 'e', 'c', 'a']\nprint(x[-2:2:-2])  # ['g', 'e']  #[start: stop : step]\nprint(x[2:2:-2])   # []\n\n['c', 'e', 'g']\n['g', 'e', 'c', 'a']\n['g', 'e']\n[]\n\n\n\n\nPerfect Catch-‘All Unpacking Over Slicing’\n\nUnpacking - extracting individual elements from a sequence (like a list or tuple) and assigning them to variables.\nSlicing - selecting a subset of elements from a sequence.\n\n\n# Example sequence\nnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n# Using slicing to get a portion of the sequence\nsubset = numbers[2:8]\n\n# Using unpacking to assign values to variables\nfirst, *middle, last = subset   # *used for extended unpacking\n\n# Print the results\nprint(\"Subset:\", subset)\nprint(\"First element:\", first)\nprint(\"Middle elements:\", middle)\nprint(\"Last element:\", last)\n\nSubset: [3, 4, 5, 6, 7, 8]\nFirst element: 3\nMiddle elements: [4, 5, 6, 7]\nLast element: 8\n\n\n\n\nSort by Complex Criteria using the ‘key’ parameter\n\nsort method works for all built-in types (strings, floats, etc.), but it doesn’t work for the classes, including a repr method for instance.\n\n\nclass Tool:\n    def __init__(self, name, weight):\n        self.name = name\n        self.weight = weight\n\n    def __repr__(self):\n        return f'Tool({self.name}, {self.weight})'\n\n# Example usage of the Tool class\ntools = [\n    Tool('level', 3.5),\n    Tool('hammer', 1.25),\n    Tool('screwdriver', 0.5),\n    Tool('chisel', 0.25),\n]\n\n# tools.sort()   #this will give us a traceback\n \n# Display the unsorted list of tools\nprint('Unsorted:')\nfor tool in tools:\n    print(repr(tool))\n\n# Sort the tools based on their names\ntools.sort(key=lambda x: x.name)\n\n# Display the sorted list of tools\nprint('\\nSorted:')\nfor tool in tools:\n    print(tool)\n\nUnsorted:\nTool(level, 3.5)\nTool(hammer, 1.25)\nTool(screwdriver, 0.5)\nTool(chisel, 0.25)\n\nSorted:\nTool(chisel, 0.25)\nTool(hammer, 1.25)\nTool(level, 3.5)\nTool(screwdriver, 0.5)\n\n\n\n\nDictionaries : insertion ordering, dict types, dict values\n\n# cutest baby animal\n\nvotes = {\n    'otter': 1281,\n    'polar bear': 587,\n    'fox': 863,\n}\n\n# save the rank to an empty dictionary\ndef populate_ranks(votes, ranks):  #takes votes and ranks dictionary\n    names = list(votes.keys())\n    names.sort(key=votes.get, reverse=True)\n    for i, name in enumerate(names, 1):\n        ranks[name] = i\n    \n# function that returs the animal with hightest rank\ndef get_winner(ranks):\n    return next(iter(ranks))\n\n# results\nranks = {}\npopulate_ranks(votes, ranks)\nprint(ranks)\nwinner = get_winner(ranks)\nprint(winner)\n\n{'otter': 1, 'fox': 2, 'polar bear': 3}\notter\n\n\n\n\nPrefer ‘get’ Over ‘in’ and ‘KeyError’ to handle missing dictionary keys\n\naccessing and assigning\nfor maintaining dictionaries, consider Counter class from the collections built-in module\nsetdefault is another shortened method other than get method, but readability is not clear. so, avoid it\n\n\nexample 1\n\nbread = {\n    '14grain': 4,\n    'multigrain' : 2\n}\n\n#1) 'in' method\nkey = 'wheat'\n\nif key in bread:\n    count = bread[key]\nelse:\n    count = 0\n\nbread[key] = count + 1  #incrementing the count by 1 for 'wheat' key\n\n#2) ‘KeyError’ method key = ‘wheat’\ntry: count = bread[key] except KeyError: count = 0\nbread[key] = count + 1\n\n#3) 'get' method - best one (shortest and clearest)\n\nkey = 'oats'\n\ncount = bread.get(key,0)\nbread[key] = count + 1\n\n\nbread\n\n{'14grain': 4, 'multigrain': 2, 'wheat': 2, 'oats': 1}\n\n\n\n\nexample 2\n\n# more complex dictionary, to know who voted for which type of bread\n\nvotes = {\n    '14grain' : ['Bob', 'Ashley', 'Suzan', 'Susan'],\n    'multigrain' : ['Dikshita', 'Kavya'],\n    'wheat' : ['Bhavna', 'Shristi'],\n    'oats' : ['Nikumbh']\n}\n\nkey = 'kinoa' \nwho = 'Raph'\n\nif key in votes: \n    names = votes[key]\nelse:\n    votes[key] = names = []\n    \nnames.append(who)\nprint (votes)\n\n{'14grain': ['Bob', 'Ashley', 'Suzan', 'Susan'], 'multigrain': ['Dikshita', 'Kavya'], 'wheat': ['Bhavna', 'Shristi'], 'oats': ['Nikumbh'], 'kinoa': ['Raph']}\n\n\n\n# try except\n\ntry:\n    names = votes[key]\nexcept KeyError:\n    votes[key] = names =[]\n\nnames.append(who)\nprint(votes)\n\n{'14grain': ['Bob', 'Ashley', 'Suzan', 'Susan'], 'multigrain': ['Dikshita', 'Kavya'], 'wheat': ['Bhavna', 'Shristi'], 'oats': ['Nikumbh'], 'kinoa': ['Raph', 'Raph', 'Raph']}\n\n\n\n# get method\nnames = votes.get(key)\nif names is None:\n    votes[key] = names = []\n    \nnames.append(who)\n\nprint(votes)\n\n{'14grain': ['Bob', 'Ashley', 'Suzan', 'Susan'], 'multigrain': ['Dikshita', 'Kavya'], 'wheat': ['Bhavna', 'Shristi'], 'oats': ['Nikumbh', 'Raph', 'Raph', 'Raph', 'Raph'], 'kinoa': ['Raph', 'Raph', 'Raph']}\n\n\n\n# prevent repetition\nif (names := votes.get(key)) is None:\n    votes[key] = names = []\nnames.append(who)\n\nprint(votes)\n\n{'14grain': ['Bob', 'Ashley', 'Suzan', 'Susan'], 'multigrain': ['Dikshita', 'Kavya'], 'wheat': ['Bhavna', 'Shristi'], 'oats': ['Nikumbh', 'Raph', 'Raph', 'Raph', 'Raph', 'Raph', 'Raph'], 'kinoa': ['Raph', 'Raph', 'Raph']}\n\n\n\n\n\nPrefer ‘defaultdict’ Over ‘Setdefault’ to handle missing items\n\n# list of countires and cities visited\nvisits = {\n    'India' : {'Punjab', 'Rajastan', 'Goa', 'Himachal Pardesh', 'Haryana'},\n    'UAE' : {'Dubai'},\n    'Nepal' : {'Kathmandu'},\n    'Canada' : {'Québec', 'Ontario'},\n}\n\n\n# using setdefalut method to add to the list (method 1)\n\nvisits.setdefault('France', set()).add('Remi')  #short\n\nif (japan := visits.get('Japan')) is None:      #long\n    visits['Japan'] = japan = set()\njapan.add('Kyoto')\n\nprint(visits)\n\n{'India': {'Rajastan', 'Haryana', 'Punjab', 'Himachal Pardesh', 'Goa'}, 'UAE': {'Dubai'}, 'Nepal': {'Kathmandu'}, 'Canada': {'Ontario', 'Québec'}, 'France': {'Remi'}, 'Japan': {'Kyoto'}}\n\n\n\n# how about i create a class then add places\n\nfrom collections import defaultdict\n\nclass Visits:\n    def __init__(self):\n       self.data = defaultdict(set)\n\n    def add(self, country, city):\n       self.data[country].add(city)\n\nvisits = Visits()\nvisits.add('England', 'Bath')\nvisits.add('England', 'London')\nprint(visits.data)\n\ndefaultdict(&lt;class 'set'&gt;, {'England': {'Bath', 'London'}})"
  },
  {
    "objectID": "posts/pythonic_thinking/Coding_Rules.html#functions",
    "href": "posts/pythonic_thinking/Coding_Rules.html#functions",
    "title": "Python Coding Rules",
    "section": "Functions",
    "text": "Functions\n\nNever Unpack more than 3 variables when fucntions return multiple vaues\n\n# Function returning multiple values\ndef get_person_details():\n    name = \"John\"\n    age = 30\n    city = \"Montréal\"\n    gender = \"Male\"\n    return name, age, city, gender\n\n# Unpacking with three variables\nname, age, city = get_person_details() #return 3 \nvariables\n\n# Displaying the results\nprint(\"Name:\", name)\nprint(\"Age:\", age)\nprint(\"City:\", city)\n\nValueError: too many values to unpack (expected 3)\n\n\n\n# Function returning multiple values\ndef get_person_details():\n    name = \"John\"\n    age = 30\n    city = \"Montréal\"\n    #gender = \"Male\"\n    return name, age, city\n\n# Unpacking with three variables\nname, age, city = get_person_details() #3 return variables\n\n# Displaying the results\nprint(\"Name:\", name)\nprint(\"Age:\", age)\nprint(\"City:\", city)\n\nName: John\nAge: 30\nCity: Montréal\n\n\n\n\nPrefer raising exceptions to returning None\n\n# Function that returns None on failure\ndef divide_numbers(a, b):\n    if b == 0:\n        return None  # Indicating failure by returning None\n    else:\n        return a / b\n\n# Using the function and checking for failure with None\nresult = divide_numbers(10, 2)\n\nif result is not None:\n    print(\"Result:\", result)\nelse:\n    print(\"Error: Cannot divide by zero.\")\n\n# Using the function and checking for failure with None\nresult = divide_numbers(10, 0)\n\nif result is not None:\n    print(\"Result:\", result)\nelse:\n    print(\"Error: Cannot divide by zero.\")\n\nResult: 5.0\nError: Cannot divide by zero.\n\n\n\n# Function that raises an exception on failure\ndef divide_numbers(a, b):\n    if b == 0:\n        raise ValueError(\"Cannot divide by zero\")\n    else:\n        return a / b\n\n# Using the function and handling the exception\ntry:\n    result = divide_numbers(10, 2)\n    print(\"Result:\", result)\nexcept ValueError as e:\n    print(\"Error:\", e)\n\n# Using the function and handling the exception\ntry:\n    result = divide_numbers(10, 0)\n    print(\"Result:\", result)\nexcept ValueError as e:\n    print(\"Error:\", e)\n\nResult: 5.0\nError: Cannot divide by zero\n\n\n\n\nKnow how Closures Interact with Variable Scope\n\nIt is better to write a helper class compared to non-local or helper function.\nused specifically when we want to priortise certain groups in a function.\n\n\nclass Sorter:\n    def __init__(self, group):\n        self.group = group\n        self.found = False\n\n    def __call__(self, x):\n        if x in self.group:\n            self.found = True\n            return (0, x)\n        else:\n            return (1, x)\n\n# Example usage\ngroup = {2, 4, 6}\nnumbers = [5, 3, 2, 1, 4]\n\nsorter = Sorter(group)\nnumbers.sort(key=sorter)\n\n# Display the sorted list\nprint(\"Sorted List:\", numbers)\n\n# Check if any item from the group is found during sorting\nassert sorter.found is True\n\nSorted List: [2, 4, 1, 3, 5]\n\n\n\n\nReduce Visual Noise with Variable Positional Arguments\n*args is not suggested for two reasons-\n1) Optional positional arguments are always turned into a tuple before they are passed to a function. Uses a lot of memory and could crash a function.\n\n2) Doesn't provide value inclusive of the new argument. Hence, no use of adding an additional argument. \n\n# Original function with *args\ndef example_function(*args):\n    # Existing functionality using args\n    total = sum(args)\n    return total\n\n# Example usage\nresult = example_function(1, 2, 3)\nprint(\"Result:\", result)\n\n# Attempt to add a new positional argument\n# This would break existing callers\ndef updated_function(new_arg, *args):\n     total = sum(args) + new_arg\n     return total\n\nresult2 = updated_function(4,5)\nprint('Result2:', result2)\n\nResult: 6\nResult2: 9\n\n\n\n\nProvide Optional Behavior with Keyword Arguments\n\ndef calculate_rectangle_area(length, width):\n    return length * width\n\n\ndef calculate_rectangle_area(length, width=None):\n    if width is not None:\n        return length * width\n    else:\n        # If width is not provided, assume it's a square (width = length)\n        return length * length\n\n\narea1 = calculate_rectangle_area(5, 3)  # Calculates area of a rectangle\narea2 = calculate_rectangle_area(4)     # Assumes it's a square with side length 4\n\nprint(area1)\nprint(area2)\n\n15\n16\n\n\n\n\nUse None and Docstrings to Specify Dynamic Default Arguments\n\nfrom datetime import datetime\n\ndef log_message(message, timestamp=None):\n    \"\"\"\n    Log a message with an optional timestamp.\n\n    Parameters:\n    - message (str): The message to be logged.\n    - timestamp (datetime, optional): The timestamp for the log message.\n      Defaults to the current time if not provided.\n    \"\"\"\n    if timestamp is None:\n        timestamp = datetime.now()\n\n    print(f\"{timestamp}: {message}\")\n\n# Example usage\nlog_message(\"Error occurred\")  # Logs the message with the current timestamp\nlog_message(\"Warning\", timestamp=datetime(2023, 1, 1))  # Logs the message with a specific timestamp\n\n2023-12-05 18:49:25.657917: Error occurred\n2023-01-01 00:00:00: Warning\n\n\n\n\nDefine Function Decorators with funtools.wraps\n\nDecorator in Python is a function that takes another function as input and extends or modifies the behavior of the latter function.\nIn this case, the trace decorator is designed to print information about the function calls.\n\n\ndef trace(func):\n    def wrapper(*args, **kwargs):\n        result = func(*args, **kwargs)\n        print(f'{func.__name__}({args!r}, {kwargs!r}) '\n              f'-&gt; {result!r}')\n        return result\n    return wrapper\n\n\n@trace\ndef example_function(x, y):\n    return x * y\n\nresult = example_function(3, 4)\n\nexample_function((3, 4), {}) -&gt; 12\n\n\n\nhelp(example_function)\n\nHelp on function wrapper in module __main__:\n\nwrapper(*args, **kwargs)"
  },
  {
    "objectID": "posts/python/p_9.html",
    "href": "posts/python/p_9.html",
    "title": "Generating data (data visualization, representations, etc.)",
    "section": "",
    "text": "Generate data sets and create visualizations\nCreate simple plots with Matplotlib and use a scatter plot to explore random walks\nCreate a histogram with Plotly and use a histogram to explore the results of rolling dice of different sizes"
  },
  {
    "objectID": "posts/python/p_9.html#plotting-tools-used",
    "href": "posts/python/p_9.html#plotting-tools-used",
    "title": "Generating data (data visualization, representations, etc.)",
    "section": "Plotting tools used",
    "text": "Plotting tools used\n\nMatplotlib- mathematical plotting library\nPlotly- visualizations which work with digital devices.\n\n\nPlotting a line graph\n\nimport matplotlib.pyplot as plt\n\nsquares = [1, 4, 9, 16, 25]\n\n# Create a figure and axis\nfig, ax = plt.subplots()\n\n# Plot the squares with a blue line\nax.plot(squares, linewidth=3, marker='o', linestyle='--')\n\n# Customize the plot\nax.set_title('Square Numbers')\nax.set_xlabel('Index')\nax.set_ylabel('Value')\nax.grid(True)\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\nCorrecting the plot\n\nimport matplotlib.pyplot as plt\n\ninput_values = [1,2,3,4,5]   #adding this would fix it\nsquares = [1, 4, 9, 16, 25]\n\n# Create a figure and axis\nfig, ax = plt.subplots()\n\n# Plot the squares with a blue line\nax.plot(input_values, squares, linewidth=3, marker='o', linestyle='--')\n\n# Customize the plot\nax.set_title('Square Numbers')\nax.set_xlabel('Index')\nax.set_ylabel('Value')\nax.grid(True)\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\nUsing built-in Styles\n\nimport matplotlib.pyplot as plt\nplt.style.available\n\n['Solarize_Light2',\n '_classic_test_patch',\n '_mpl-gallery',\n '_mpl-gallery-nogrid',\n 'bmh',\n 'classic',\n 'dark_background',\n 'fast',\n 'fivethirtyeight',\n 'ggplot',\n 'grayscale',\n 'seaborn-v0_8',\n 'seaborn-v0_8-bright',\n 'seaborn-v0_8-colorblind',\n 'seaborn-v0_8-dark',\n 'seaborn-v0_8-dark-palette',\n 'seaborn-v0_8-darkgrid',\n 'seaborn-v0_8-deep',\n 'seaborn-v0_8-muted',\n 'seaborn-v0_8-notebook',\n 'seaborn-v0_8-paper',\n 'seaborn-v0_8-pastel',\n 'seaborn-v0_8-poster',\n 'seaborn-v0_8-talk',\n 'seaborn-v0_8-ticks',\n 'seaborn-v0_8-white',\n 'seaborn-v0_8-whitegrid',\n 'tableau-colorblind10']\n\n\n\n# using style\nimport matplotlib.pyplot as plt\n\ninput_values = [1,2,3,4,5]   #adding this would fix it\nsquares = [1, 4, 9, 16, 25]\n\n#use style\nplt.style.use('fast')\n\n# Create a figure and axis\nfig, ax = plt.subplots()\n\n\n# Plot the squares with a blue line\nax.plot(input_values, squares, linewidth=3, marker='o', linestyle='--')\n\n# Customize the plot\nax.set_title('Square Numbers')\nax.set_xlabel('Index')\nax.set_ylabel('Value')\nax.grid(True)\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\nPlotting and Styling Individual Points with scatter()\n\n# using style\nimport matplotlib.pyplot as plt\n\ninput_values = [1,2,3,4,5]   #adding this would fix it\nsquares = [1, 4, 9, 16, 25]\n\n#use style\nplt.style.use('fast')\n\n# Create a figure and axis\nfig, ax = plt.subplots()\n\n\n# Plot the squares with a blue line\nax.scatter(2,4,s=200)\n\n# Customize the plot\nax.set_title('Square Numbers')\nax.set_xlabel('Index')\nax.set_ylabel('Value')\nax.grid(True)\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\nCaluculating data automatically\n\nx_values = range(1,1001)\ny_values = [x**2 for x in x_values]\n\nplt.style.use('fast')\nfig, ax = plt.subplots()\nax.scatter(x_values, y_values, s= 10)\n\n# Customize the plot\nax.set_title('Square Numbers')\nax.set_xlabel('Index')\nax.set_ylabel('Value')\nax.grid(False)\n\n#Set the range for each axis\nax.axis([0, 1100, 0, 1100000])\n\nplt.show()\n\n\n\n\n\n\nUsing a Colormap\n\nimport matplotlib.pyplot as plt\n\nx_values = range(1, 1000)\ny_values = [x**2 for x in x_values]\nfig, ax = plt.subplots()\nax.scatter(x_values, y_values, s= 10)\n\nax.scatter(x_values, y_values, c= y_values, cmap= plt.cm.Blues, s=10)\n\n\n# Customize the plot\nax.set_title('Square Numbers')\nax.set_xlabel('Index')\nax.set_ylabel('Value')\nax.grid(False)\n\nplt.show()\n\n\n\n\n\n\nSaving the plots automatically\n\nplt.savefig('squares_plot.png', bbox_inches= 'tight')   #second argument trims extra white space\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\nExample\n1. plot for first five cubic numbers.\n2. plot for first 5000 cubic numbers.\n\nimport matplotlib.pyplot as plt\n\n# Function to calculate the cube of a number\ndef cube(x):\n    return x**3\n\n# Generate the first five cubic numbers\nfirst_five_cubic = [cube(x) for x in range(1, 6)]\n\n# Generate the first 5000 cubic numbers\nfirst_5000_cubic = [cube(x) for x in range(1, 5001)]\n\n# Plot the first five cubic numbers\nplt.figure(1)\nplt.plot(range(1, 6), first_five_cubic, marker='o', linestyle='-', color='b')\nplt.title(\"First Five Cubic Numbers\")\nplt.xlabel(\"Number\")\nplt.ylabel(\"Cubic Value\")\n\n# Plot the first 5000 cubic numbers\nplt.figure(2)\nplt.plot(range(1, 5001), first_5000_cubic, color='r')\nplt.title(\"First 5000 Cubic Numbers\")\nplt.xlabel(\"Number\")\nplt.ylabel(\"Cubic Value\")\n\n# Show the plots\nplt.show()\n\n\n\n\n\n\n\n\n\nRandom walks (creating and plotting)\n\nCreating\n\nimport random\n\nclass RandomWalk:\n    def __init__(self, num_points=5000):\n        self.num_points = num_points\n        self.x_values = [0]\n        self.y_values = [0]\n\n    def fill_walk(self):\n        while len(self.x_values) &lt; self.num_points:\n            x_step = random.choice([-1, 1]) * random.choice([0, 1, 2, 3, 4])\n            y_step = random.choice([-1, 1]) * random.choice([0, 1, 2, 3, 4])\n\n            if x_step == 0 and y_step == 0:\n                continue\n\n            x = self.x_values[-1] + x_step\n            y = self.y_values[-1] + y_step\n\n            self.x_values.append(x)\n            self.y_values.append(y)\n\n\n\nPlotting\n\nrw = RandomWalk()\nrw.fill_walk()\n\nplt.style.use('fast')\nfig, ax = plt.subplots()\n\nax.scatter(rw.x_values, rw.y_values, s=15)\nplt.show()\n\n\n\n\n\n\n\nGenerating Multiple Random Walks\n\n# just by wrapping the above code in a while loop\n\nwhile True:\n    rw = RandomWalk()\n    rw.fill_walk()\n\n    plt.style.use('fast')\n    fig, ax = plt.subplots()\n\n    ax.scatter(rw.x_values, rw.y_values, s=15)\n    plt.show()\n    \n    keep_running = input(\"Make another walk? (y/n): \")\n    if keep_running == 'n':\n        break\n        \n\n\n\n\nMake another walk? (y/n): y\nMake another walk? (y/n): n\n\n\n\n\n\n\n\nStyling the walk\n\nafter generating the list using range() function, we stored them in point_numbers()\nthen passing the point_numbers to c argument, we used  colormap\nfinally, pass edgecolors = ‘none’ to get rid of black outline.\n\n\nwhile True:\n    rw = RandomWalk()\n    rw.fill_walk()\n\n    plt.style.use('fast')\n    fig, ax = plt.subplots()\n    point_numbers = range(rw.num_points)   # added here to style\n\n    ax.scatter(rw.x_values, rw.y_values, c= point_numbers, cmap= plt.cm.Blues, edgecolors= 'none', s=15)\n    plt.show()\n    \n    keep_running = input(\"Make another walk? (y/n): \")\n    if keep_running == 'n':\n        break\n\n\n\n\nMake another walk? (y/n): y\nMake another walk? (y/n): n\n\n\n\n\n\n\n\nPlotting the starting and ending points\n\nto see where the walk begins and where it ends (we add first and last points)\n\n\nwhile True:\n    rw = RandomWalk()\n    rw.fill_walk()\n\n    plt.style.use('fast')\n    fig, ax = plt.subplots()\n    \n    point_numbers = range(rw.num_points)   # added here to style\n\n    ax.scatter(rw.x_values, rw.y_values, c= point_numbers, cmap= plt.cm.Blues, edgecolors= 'none', s=15)\n    plt.show()\n    \n     # Emphasize the first and last points.\n    ax.scatter(0, 0, c='green', edgecolors='none', s=100)\n    ax.scatter(rw.x_values[-1], rw.y_values[-1], c='red', edgecolors='none',\n        s=100)\n    \n    keep_running = input(\"Make another walk? (y/n): \")\n    if keep_running == 'n':\n        break\n\n\n\nRemoving the Axes\n\nwhile True:\n    rw = RandomWalk()\n    rw.fill_walk()\n\n    plt.style.use('fast')\n    fig, ax = plt.subplots()\n    \n    point_numbers = range(rw.num_points)   # added here to style\n\n    ax.scatter(rw.x_values, rw.y_values, c= point_numbers, cmap= plt.cm.Blues, edgecolors= 'none', s=15)\n    plt.show()\n    \n     # Remove the axes..\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n   \n    \n    keep_running = input(\"Make another walk? (y/n): \")\n    if keep_running == 'n':\n        break\n\n\n\n\nMake another walk? (y/n): n\n\n\n\n\nAltering the size to fit screen\n\nwhile True:\n    rw = RandomWalk(50_000)\n    rw.fill_walk()\n\n    plt.style.use('fast')\n    fig, ax = plt.subplots(figsize=(15,9), dpi=128)    #here size and if pixels are know too!\n    \n    point_numbers = range(rw.num_points)   # added here to style\n\n    ax.scatter(rw.x_values, rw.y_values, c= point_numbers, cmap= plt.cm.Blues, edgecolors= 'none', s=15)\n    plt.show()\n    \n     # Remove the axes..\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n   \n    \n    keep_running = input(\"Make another walk? (y/n): \")\n    if keep_running == 'n':\n        break\n\n\n\n\nMake another walk? (y/n): n"
  },
  {
    "objectID": "posts/python/p_9.html#rolling-dice-with-plotly",
    "href": "posts/python/p_9.html#rolling-dice-with-plotly",
    "title": "Generating data (data visualization, representations, etc.)",
    "section": "Rolling dice with Plotly",
    "text": "Rolling dice with Plotly\n\nfrom random import randint\n\nclass Die:\n    \"defining method\"\n    \n    def __init__(self, num_sides=6):\n        self.num_sides = num_sides\n    \n    def roll(self):\n        return randint(1, self.num_sides)\n\n\ndie = Die()\n\n# make some rolls and store results in the list\nresults = []\nfor roll_num in range(100):\n    result = die.roll()\n    results.append(result)\n\n\nprint(results)\n\n[2, 6, 3, 3, 5, 6, 2, 2, 4, 1, 3, 4, 3, 1, 5, 2, 3, 5, 1, 6, 3, 1, 2, 6, 1, 1, 3, 4, 3, 2, 1, 6, 3, 2, 6, 3, 2, 2, 3, 4, 5, 5, 1, 3, 6, 3, 5, 5, 1, 3, 3, 6, 3, 4, 2, 1, 2, 1, 5, 3, 5, 6, 3, 4, 5, 3, 6, 3, 6, 3, 3, 2, 2, 4, 2, 6, 1, 6, 3, 2, 1, 6, 1, 4, 3, 2, 5, 5, 5, 1, 3, 5, 4, 2, 3, 5, 6, 5, 3, 3]\n\n\n\nAnalyzing the results\n\nfrequencies = []\nfor value in range(1, die.num_sides+1):\n    frequency = results.count(value)\n    frequencies.append(frequency)\n\nprint(frequencies)\n\n[20, 11, 24, 15, 17, 13]\n\n\n\n# printing frequencies for 1000 rolls\nfor roll_num in range(1000):\n    result = die.roll()\n    results.append(result)\n\nfrequencies= []\nfor value in range(1, die.num_sides+1):\n    frequency = results.count(value)\n    frequencies.append(frequency)\n\nprint(frequencies)\n\n[351, 314, 348, 355, 366, 366]\n\n\n\n\nHistogram\n\nfrom plotly.graph_objs import Bar, Layout\nfrom plotly import offline\n\nx_values = list(range(1, die.num_sides+1))\ndata = [Bar(x=x_values, y=frequencies)]\n\nx_axis_config = {'title': 'Result'}\ny_axis_config = {'title': 'Frequency of Result'}\n\nmy_layout = Layout(title='Results of rolling 1000 times',\n                  xaxis = x_axis_config, yaxis= y_axis_config)\n\noffline.plot({'data': data, 'layout': my_layout}, filename = 'd6.html')\n\n'd6.html'"
  },
  {
    "objectID": "posts/python/p_9.html#rolling-two-die",
    "href": "posts/python/p_9.html#rolling-two-die",
    "title": "Generating data (data visualization, representations, etc.)",
    "section": "Rolling two die",
    "text": "Rolling two die\n\nfrom plotly.graph_objs import Bar, Layout\nfrom plotly import offline\n\n# creating\ndie_1 = Die()\ndie_2 = Die()\n\nresults_2= []\nfor roll_num in range(1000):\n    result = die_1.roll()  + die_2.roll()\n    results_2.append(result)\n    \n# analyzing\nfrequencies_2 = []\nmax_result = die_1.num_sides + die_2.num_sides  #here aswell\nfor value in range(2, max_result+1):\n    frequency = results_2.count(value)\n    frequencies_2.append(frequency)\n\n# Visualizing\nx_values = list(range(2, max_result+1))           #changed here\ndata = [Bar(x= x_values, y = frequencies_2)]\n\nx_axis_config = {'title': 'Result', 'dtick' : 1}    #changed here compared to one die\ny_axis_config = {'title': 'Frequency of Result'}\n\nmy_layout = Layout(title='Results of rolling two D6 dies 1000 times',\n                  xaxis = x_axis_config, yaxis= y_axis_config)\n\noffline.plot({'data': data, 'layout': my_layout}, filename = 'd6_d6.html')\n\n'd6_d6.html'"
  },
  {
    "objectID": "posts/python/p_9.html#rolling-two-die-of-different-sizes",
    "href": "posts/python/p_9.html#rolling-two-die-of-different-sizes",
    "title": "Generating data (data visualization, representations, etc.)",
    "section": "Rolling two die of different sizes",
    "text": "Rolling two die of different sizes\n\nfrom plotly.graph_objs import Bar, Layout\nfrom plotly import offline\n\n# creating\ndie_1 = Die()\ndie_2 = Die(10)   #change here\n\nresults_2= []\nfor roll_num in range(1000):\n    result = die_1.roll()  + die_2.roll()\n    results_2.append(result)\n    \n# analyzing\nfrequencies_2 = []\nmax_result = die_1.num_sides + die_2.num_sides  #here aswell\nfor value in range(2, max_result+1):\n    frequency = results_2.count(value)\n    frequencies_2.append(frequency)\n\n# Visualizing\nx_values = list(range(2, max_result+1))           #changed here\ndata = [Bar(x= x_values, y = frequencies_2)]\n\nx_axis_config = {'title': 'Result', 'dtick' : 1}    #changed here compared to one die\ny_axis_config = {'title': 'Frequency of Result'}\n\nmy_layout = Layout(title='Results of rolling two D6 dies 1000 times',\n                  xaxis = x_axis_config, yaxis= y_axis_config)\n\noffline.plot({'data': data, 'layout': my_layout}, filename = 'd6_d10.html')\n\n'd6_d10.html'"
  },
  {
    "objectID": "posts/python/p_9.html#rolling-three-dice",
    "href": "posts/python/p_9.html#rolling-three-dice",
    "title": "Generating data (data visualization, representations, etc.)",
    "section": "Rolling three dice",
    "text": "Rolling three dice\n\nfrom plotly.graph_objs import Bar, Layout\nfrom plotly import offline\n\n# Creating\ndie_1 = Die()\ndie_2 = Die()\ndie_3 = Die() #change here\n\nresults_3= []\nfor roll_num in range(1000):\n    result = die_1.roll()  + die_2.roll() + die_3.roll()   #die added\n    results_3.append(result)\n    \n# Analyzing\nfrequencies_3 = []\nmax_result = die_1.num_sides + die_2.num_sides + die_3.num_sides #here aswell\nfor value in range(2, max_result+1):\n    frequency = results_3.count(value)\n    frequencies_3.append(frequency)\n\n# Visualizing\nx_values = list(range(3, max_result+1))           #range changed \ndata = [Bar(x= x_values, y = frequencies_3)]\n\nx_axis_config = {'title': 'Result', 'dtick' : 1}    #changed here compared to one die\ny_axis_config = {'title': 'Frequency of Result'}\n\nmy_layout = Layout(title='Results of rolling three D6 dies 1000 times',\n                  xaxis = x_axis_config, yaxis= y_axis_config)\n\noffline.plot({'data': data, 'layout': my_layout}, filename = 'd6_d6_d6.html')\n\n'd6_d6_d6.html'"
  },
  {
    "objectID": "posts/python/p_6.html",
    "href": "posts/python/p_6.html",
    "title": "Classes",
    "section": "",
    "text": "a brief introduction to object-oriented programming\nwriting and storing information in classes\ninit() method\nintro to python’s standard library and random module"
  },
  {
    "objectID": "posts/python/p_6.html#example-below",
    "href": "posts/python/p_6.html#example-below",
    "title": "Classes",
    "section": "Example below",
    "text": "Example below\n\ncreating a Restraunt class\n\nclass Restaurant:\n    \n    def __init__ (self, restaurant_name, cuisine_type):\n        self.restaurant_name = restaurant_name\n        self.cuisine_type = cuisine_type\n        \n    def describe_restaurant(self):\n        print(f\"\\nThe restaurant's name is {self.restaurant_name}.\")   #add self to access instance variables correctly\n        print(f\"The cuisine type offered is {self.cuisine_type}.\")\n    \n    def open_restaurant(self):\n        print(f\"The {self.restaurant_name} is now open!\")\n\n\n\nMaking an instance from a class\n\nrestaurant = Restaurant('Desi Dhabha', 'Vegetarian')\n\n\n\nPrinting individual attributes\n\nprint(f\"Restaurant Name: {restaurant.restaurant_name}\")\nprint(f\"Cuisine Type: {restaurant.cuisine_type}\")\n\nRestaurant Name: Desi Dhabha\nCuisine Type: Vegetarian\n\n\n\n\nCalling methods\n\nrestaurant.describe_restaurant()\nrestaurant.open_restaurant()\n\n\nThe restaurant's name is Desi Dhabha.\nThe cuisine type offered is Vegetarian.\nThe Desi Dhabha is now open!\n\n\n\n\nWorking with classes and instances\n\nclass Car:\n    \n    def __init__(self, make, model, year):\n        self.make = make\n        self.model = model\n        self.year = year\n    def get_descriptive_name (self):\n        long_name = f\"{self.year} {self.make} {self.model}\"\n        return long_name.title()\n    \nmy_new_car = Car('audi', 'a4', 2023)\nprint(my_new_car.get_descriptive_name())\n\n2023 Audi A4\n\n\n\n# adding an attribute that changes over time\nclass Car:\n    \n    def __init__(self, make, model, year):\n        self.make = make\n        self.model = model\n        self.year = year\n        self.odometer_reading = 0\n    def get_descriptive_name (self):\n        long_name = f\"{self.year} {self.make} {self.model}\"\n        return long_name.title()\n    def read_odometer(self):\n        print(f\"This car has {self.odometer_reading} miles on it.\")\n    \nmy_new_car = Car('audi', 'a4', 2023)\nprint(my_new_car.get_descriptive_name())\nmy_new_car.read_odometer()\n\n2023 Audi A4\nThis car has 0 miles on it.\n\n\n\nThe simplest way to modify the value of an attribute is to access the attribute directly through an instance.\nHere we set the odometer reading to 23 directly\n\nmy_new_car.odometer_reading = 45\nmy_new_car.read_odometer()\n\nThis car has 45 miles on it.\n\n\n\n\n\nModifying an Attribute’s Value Through a Method\n\n# rest of the code same\nclass Car:\n    \n    def __init__(self, make, model, year, odometer_reading):\n        self.make = make\n        self.model = model\n        self.year = year\n        self.odometer_reading = odometer_reading\n    def get_descriptive_name (self):\n        long_name = f\"{self.year} {self.make} {self.model}\"\n        return long_name.title()\n    def update_odometer(self, odometer_reading):\n        mileage = self.odometer_reading\n        return mileage\n    \nmy_new_car.update_odometer(52)\nmy_new_car.read_odometer()\n\nAttributeError: 'Car' object has no attribute 'update_odometer'\n\n\n\nclass Car:\n    \n    def __init__(self, make, model, year, odometer_reading):\n        self.make = make\n        self.model = model\n        self.year = year\n        self.odometer_reading = odometer_reading\n\n    def get_descriptive_name(self):\n        long_name = f\"{self.year} {self.make} {self.model}\"\n        return long_name.title()\n\n    def update_odometer(self, odometer_reading):\n        self.odometer_reading = odometer_reading  # Update the odometer reading\n        print(f\"Odometer reading set to {self.odometer_reading}\")\n\n# Create an instance of the Car class\nmy_new_car = Car(\"honda\", \"Accord\", 2023, 150)\n\n# Update the odometer reading\nmy_new_car.update_odometer(52)\n\n# Get the descriptive name of the car\ncar_name = my_new_car.get_descriptive_name()\nprint(f\"Car name: {car_name}\")\n\n# Get the current odometer reading\nprint(f\"Odometer reading: {my_new_car.odometer_reading}\")\n\nOdometer reading set to 52\nCar name: 2023 Honda Accord\nOdometer reading: 52\n\n\n\n\nInheritance\n\nclass Car:\n    def __init__(self,make, model, year):\n        self.make = make\n        self.model = model\n        self.year = year\n        self.odometer_reading = 0\n        \n    def get_descriptive_name (self):\n        long_name = f\"{self.year}{self.make}{self.model}\"\n        return long_name.title()\n    \n    def read_odometer(self):\n        print(f\"This car has {self.odometer_reading} miles on it.\")\n        \n    def update_odometer(self, mileage):\n        if mileage &gt;= self.odometer_reading:\n            self.odometer_reading = mileage\n        else:\n            print(\"You can't roll back an odometer!\")\n    \n    def increment_odometer(self, miles):\n        self.odometer_reading += miles\n    \nclass ElectricCar(Car): #specific to electric vehicles\n    \n    def __init__(self, make, model, year):\n        super().__init__(make, model, year)      #initializing attributes of parent class with super function\n        \nmy_tesla = ElectricCar(' tesla', ' model s', 2019)\nprint(my_tesla.get_descriptive_name())\n\n2019 Tesla Model S\n\n\n\n\nDefinig attributes and methods for child class\n\nclass Car:\n    def __init__(self,make, model, year):\n        self.make = make\n        self.model = model\n        self.year = year\n        self.odometer_reading = 0\n        \n    def get_descriptive_name (self):\n        long_name = f\"{self.year}{self.make}{self.model}\"\n        return long_name.title()\n    \n    def read_odometer(self):\n        print(f\"This car has {self.odometer_reading} miles on it.\")\n        \n    def update_odometer(self, mileage):\n        if mileage &gt;= self.odometer_reading:\n            self.odometer_reading = mileage\n        else:\n            print(\"You can't roll back an odometer!\")\n    \n    def increment_odometer(self, miles):\n        self.odometer_reading += miles\n    \nclass ElectricCar(Car): #specific to electric vehicles\n    \n    def __init__(self, make, model, year):\n        super().__init__(make, model, year)      #initializing attributes of parent class with super function\n        self.battery = Battery()\n        \nclass Battery:\n    def __init__(self, battery_size= 75):\n        self.battery_size = battery_size       #initializaing attributes\n        \n    def describe_battery(self):\n        print(f\"The car has a {self.battery_size}-kWh battery.\")\n    \n        \nmy_tesla = ElectricCar(' tesla', ' model s', 2019)\nprint(my_tesla.get_descriptive_name())\nmy_tesla.battery.describe_battery()\n\n2019 Tesla Model S\nThe car has a 75-kWh battery.\n\n\n\n\nExtending battery class\n\nclass Car:\n    def __init__(self,make, model, year):\n        self.make = make\n        self.model = model\n        self.year = year\n        self.odometer_reading = 0\n        \n    def get_descriptive_name (self):\n        long_name = f\"{self.year}{self.make}{self.model}\"\n        return long_name.title()\n    \n    def read_odometer(self):\n        print(f\"This car has {self.odometer_reading} miles on it.\")\n        \n    def update_odometer(self, mileage):\n        if mileage &gt;= self.odometer_reading:\n            self.odometer_reading = mileage\n        else:\n            print(\"You can't roll back an odometer!\")\n    \n    def increment_odometer(self, miles):\n        self.odometer_reading += miles\n    \nclass ElectricCar(Car): #specific to electric vehicles\n    \n    def __init__(self, make, model, year):\n        super().__init__(make, model, year)      #initializing attributes of parent class with super function\n        self.battery = Battery()\n        \nclass Battery:\n    def __init__(self, battery_size= 100):\n        self.battery_size = battery_size       #initializaing attributes\n        \n    def describe_battery(self):\n        print(f\"The car has a {self.battery_size}-kWh battery.\")\n        \n    def get_range(self):\n        if self.battery_size == 75:\n            range = 260\n        elif self.battery_size == 100:\n            range = 315\n        \n        print(f\"This car can go about {range} miles on a full charge.\")\n    \n        \nmy_tesla = ElectricCar(' tesla', ' model s', 2019)\nprint(my_tesla.get_descriptive_name())\nmy_tesla.battery.describe_battery()\nmy_tesla.battery.get_range()\n\n2019 Tesla Model S\nThe car has a 100-kWh battery.\nThis car can go about 315 miles on a full charge."
  },
  {
    "objectID": "posts/python/p_6.html#importing-classes",
    "href": "posts/python/p_6.html#importing-classes",
    "title": "Classes",
    "section": "Importing classes",
    "text": "Importing classes\n\nfrom car import Car\n\nmy_new_car = Car('audi', 'a4', 2019)\nprint(my_new_car.get_descriptive_name())\n\nmy_new_car.odometer_reading = 23\nmy_new_car.read_odometer()\n\nNameError: name 'null' is not defined\n\n\n\nfrom car import Car\nfrom electric_car import ElectricCar\n\n\nImporting from python’s standard library\nbefore we learned, how to create a class and import stuff from there\n\nfrom random import randint\nrandint(1,6)\n\n\nfrom random import choice\nplayers = ['charles', 'martina', 'michael', 'florence', 'eli']\nfirst_up = choice(players)\nfirst_up\n\n\nfirst_up\n\n\nfrom random import randint\nrandint(1,7)\n\n\n\nRolling a die 10 times and storing it in class Die\n\n\nimport random\n\nclass Die:\n    def __init__(self, sides = 6):\n        self.sides = sides\n        \n    def roll_die(self):\n        result = random.randint(1, self.sides)\n        return result\n# create a 6-sided die\nsix_sided_die = Die()\n\n# Roll the die 10 times\nfor _ in range(10):\n    roll_result = six_sided_die.roll_die()\n    print(f\"The die rolled: {roll_result}\")\n        \n\n\n\nLottery analysis\n\n\nprint four numbers or letters from a list of 10 numbers and 5 letters.\n-print if the person has won the lottery or not. \n\nimport random\n\n# Create a list with numbers and letters\nticket_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 'A', 'B', 'C', 'D', 'E']\n\n# Randomly select four elements from the list\nwinning_combination = random.sample(ticket_list, 4)\n\n# Print the winning combination\nprint(\"Winning Combination:\", winning_combination)\n\n# Check if the winning combination matches\nif all(item in winning_combination for item in ['A', 'B', 'C', 'D']):\n    print(\"Congratulations! You've won a prize!\")\nelse:\n    print(\"Sorry, your ticket did not match the winning combination.\")\n\nWinning Combination: ['E', 10, 9, 1]\nSorry, your ticket did not match the winning combination."
  },
  {
    "objectID": "posts/python/p_4.html",
    "href": "posts/python/p_4.html",
    "title": "User input and while loops",
    "section": "",
    "text": "how the input() function works\nwhile loops (text and numerical inputs)\nusing while loop with lists and dictionaries\ncontrol the flow of a while loop by setting an active flag, using the break statement, and using the continue statement\nusing a while loop to move items from one list to another and to remove all instances of a value from a list.\n\n\n\n\nmessage = input(\"Please enter your full name: \")\nprint(message)\n\n\nprint(f\"\\nHello, {message}!\")\n\n\n\n\n\nage = input(\"May i know your age, please?\")\nage = int(age)\n\nif age &lt;=12 or age &gt;= 65 :\n    print(\"\\nYou can enter the zoo for free\")\n\nelse:\n    print(\"\\nYou'll have to pay 45CAD for a 90 minutes visit\")\n    \n\nMay i know your age, please?6556\n\nYou can enter the zoo for free\n\n\n\n\n\nprovides the remainder after division\n\n4%3\n\n1\n\n\n\n5 % 2\n\n1\n\n\n\n\n\n\n# counting (1 to 5)\ncurrent_number = 1\nwhile current_number &lt;=5:\n    print(current_number)\n    current_number += 1\n    \n\n1\n2\n3\n4\n5\n\n\n\n# infinite loop which stops with quit message\nprompt = \"\\nTell me something and I will repeat it back to you:\"\nprompt += \"\\n Enter 'quit' to end the program. \"\nmessage = \"\"\nwhile message != 'quit':\n    message = input(prompt)\n    print(message)\n\n\nTell me something and I will repeat it back to you:\n Enter 'quit' to end the program. I'm doing great!\nI'm doing great!\n\nTell me something and I will repeat it back to you:\n Enter 'quit' to end the program. keep on playing with this game until you get tired\nkeep on playing with this game until you get tired\n\nTell me something and I will repeat it back to you:\n Enter 'quit' to end the program. quit\nquit\n\n\n\n\n\n\ndescription- when flag conditions are true, the program continues to run. Else, it stops.\nbenefits over while loop- can be used to execute several conditions. Contrary to while loop, which uses only one condition\n\n\nprompt = \"\\n Tell me something, and I will repeat it back to you:\"\nprompt += \"\\n Enter 'quit' to end the program. \"\n\nactive = True\nwhile active :\n    message = input(prompt)\n    \n    if message == 'quit':\n        active = False\n    else:\n        print(message)\n\n\n Tell me something, and I will repeat it back to you:\n Enter 'quit' to end the program. I'm doing good today!\nI'm doing good today!\n\n Tell me something, and I will repeat it back to you:\n Enter 'quit' to end the program. quit\n\n\n\nprompt = \"\\nPlease enter the name of a city you have visited:\"\nprompt += \"\\n(Enter 'quit' when you are finished.)\"\n\nwhile True:\n    city = input(prompt)\n    \n    if city == \"quit\":\n        break\n    else:\n        print(f\"I'd like to visit {city.title()}!\")\n\n\nPlease enter the name of a city you have visited:\n(Enter 'quit' when you are finished.)Dubai\nI'd like to visit this Dubai!\n\nPlease enter the name of a city you have visited:\n(Enter 'quit' when you are finished.)Thailand\nI'd like to visit this Thailand!\n\nPlease enter the name of a city you have visited:\n(Enter 'quit' when you are finished.)Switzerland\nI'd like to visit this Switzerland!\n\nPlease enter the name of a city you have visited:\n(Enter 'quit' when you are finished.)Mexico\nI'd like to visit this Mexico!\n\nPlease enter the name of a city you have visited:\n(Enter 'quit' when you are finished.)Thailand\nI'd like to visit this Thailand!\n\nPlease enter the name of a city you have visited:\n(Enter 'quit' when you are finished.)Pataya\nI'd like to visit this Pataya!\n\nPlease enter the name of a city you have visited:\n(Enter 'quit' when you are finished.)Cuba\nI'd like to visit this Cuba!\n\nPlease enter the name of a city you have visited:\n(Enter 'quit' when you are finished.)Vancouver\nI'd like to visit this Vancouver!\n\nPlease enter the name of a city you have visited:\n(Enter 'quit' when you are finished.)quit\n\n\n\n\n\n\n# print list of odd numbers upto 10\ncurrent_number = 0\nwhile current_number &lt; 10:\n    current_number += 1\n    if current_number % 2== 0:\n        continue\n    print(current_number)\n\n1\n3\n5\n7\n9\n\n\n\n# print list of even numbers upto 20\ncurrent_even_n = 0\nwhile current_even_n &lt; 21:      #upto 20 gets printed\n    current_even_n += 1\n    if current_even_n % 2 == 1:\n        continue\n    print(current_even_n)\n\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n\n\n\nnumber = 1\n\nwhile number &lt;= 10:\n    square = number * number\n    print(f\"The square of {number} is {square}\")\n    number += 1\n\n\nx = 1\n\nwhile x &lt;= 10:\n    square = x * x\n    print(f\"The square of {x} is {square}\")\n    x += 1  # prevents infinite loop\n\n    \n\nThe square of 1 is 1\nThe square of 2 is 4\nThe square of 3 is 9\nThe square of 4 is 16\nThe square of 5 is 25\nThe square of 6 is 36\nThe square of 7 is 49\nThe square of 8 is 64\nThe square of 9 is 81\nThe square of 10 is 100\n\n\n\n\n\n\nunconfirmed_users = ['raghav', 'britany', 'solance', 'aisha']\nconfirmed_users = []\n\nwhile unconfirmed_users :\n    current_user = unconfirmed_users.pop()\n    \n    #moving\n    \n    print(f\"Verifying user: {current_user.title()}\")\n    confirmed_users.append(current_user)\n    \n    #displaying\n    \n    print(\"\\nThe following users have been confirmed:\")\n    for confirmed_user in confirmed_users:\n        print(confirmed_user.title())\n\nVerifying user: Aisha\n\nThe following users have been confirmed:\nAisha\nVerifying user: Solance\n\nThe following users have been confirmed:\nAisha\nSolance\nVerifying user: Britany\n\nThe following users have been confirmed:\nAisha\nSolance\nBritany\nVerifying user: Raghav\n\nThe following users have been confirmed:\nAisha\nSolance\nBritany\nRaghav\n\n\n\n\n\n\n\npets = ['dog', 'cat', 'cheetah', 'beer', 'rabbit', 'lion']\nprint(pets)\n\nwhile 'cat' in pets:\n    pets.remove('cat')    #remove method\n\nprint(pets)\n\n['dog', 'cat', 'cheetah', 'beer', 'rabbit', 'lion']\n['dog', 'cheetah', 'beer', 'rabbit', 'lion']\n\n\n\n\n\n\nresponses = {}         #initializing empty dictionary\n\n\npolling_active = True  # setting flag indicator to True\n\nwhile polling_active:\n    name = input(\"\\nWhat is your name?\")\n    response = input(\"Which mountain would you like to climb someday?\")\n    \n    #storing the response in a dictionary\n    responses[name] = response  #where name is the key, and response is the value\n    \n    #finding out if we want to store more keys and variables in responses\n    repeat = input(\"Would you like to let another person respond? (yes/no)\")\n    if repeat == 'no':\n        polling_active = False\n        \n#polling complete; show the results\nprint(\"\\n_______Poll Results_____________\")\nfor name,response in responses.items():              #for loop iterates for key, value pairs in responses dictionary\n    print(f\"{name.title()} would like to climb {response.title()}.\")\n\n\nWhat is your name?sunita\nWhich mountain would you like to climb someday?valdavid\nWould you like to let another person respond? (yes/no)no\n\n_______Poll Results_____________\nSunita would like to climb Valdavid.\n\n\n\n# callling the dictionary\nprint(responses)\n\n{'sunita': 'valdavid'}\n\n\n\n\n\n\nnew dictionary is responses_new\n\n\nresponses_new = {}         #initializing empty dictionary\n\n\n\n\nwhile True:\n    name = input(\"\\nWhat is your name?\")\n    response = input(\"Which mountain would you like to climb someday?\")\n    \n    #storing the response in a dictionary\n    responses_new[name] = response  #where name is the key, and response is the value\n    \n    #finding out if we want to store more keys and variables in responses\n    repeat = input(\"Would you like to let another person respond? (yes/no)\")\n    if repeat.lower() != 'yes':\n        break  #exits the loop if response in not 'yes'\n        \n#polling complete; show the results\nprint(\"\\n_______Poll Results_____________\")\nfor name,response in responses_new.items():              #for loop iterates for key, value pairs in responses dictionary\n    print(f\"{name.title()} would like to climb {response.title()}.\")\n\n\nWhat is your name?kk\nWhich mountain would you like to climb someday?tatata\nWould you like to let another person respond? (yes/no)yes\n\nWhat is your name?paula\nWhich mountain would you like to climb someday?tatal\nWould you like to let another person respond? (yes/no)no\n\n_______Poll Results_____________\nKk would like to climb Tatata.\nPaula would like to climb Tatal.\n\n\n\nprint(responses_new)\n\n{'kunal': 'kanchunjunga', 'sushil': 'peu importe'}\n\n\n\n# Dream vacation\n\nvacation = {}\n\nwhile True:\n    key = input(\"\\nWhat is your name?\")\n    value = input(\"If I ask you to visit one place in the world, where would you go?\")\n    \n    vacation[key]= value\n    \n    repeat = input(\"Next person in line, if there is one? ('yes/no')\")\n    if repeat.lower() != 'yes':\n        break\n\n#polling complete, show results\nprint(\"______Poll_results\")\nfor key,value in vacation.items():\n    print(f\"{key.title()} would like to go {value.title()}.\")\n\n\nWhat is your name?kunal\nIf I ask you to visit one place in the world, where would you go?dubai\nNext person in line, if there is one? ('yes/no')yes\n\nWhat is your name?martina\nIf I ask you to visit one place in the world, where would you go?new brunswick\nNext person in line, if there is one? ('yes/no')yes\n\nWhat is your name?kathy\nIf I ask you to visit one place in the world, where would you go?québec city\nNext person in line, if there is one? ('yes/no')no\n______Poll_results\nKunal would like to go Dubai.\nMartina would like to go New Brunswick.\nKathy would like to go Québec City."
  },
  {
    "objectID": "posts/python/p_4.html#learning-outcomes",
    "href": "posts/python/p_4.html#learning-outcomes",
    "title": "User input and while loops",
    "section": "",
    "text": "how the input() function works\nwhile loops (text and numerical inputs)\nusing while loop with lists and dictionaries\ncontrol the flow of a while loop by setting an active flag, using the break statement, and using the continue statement\nusing a while loop to move items from one list to another and to remove all instances of a value from a list.\n\n\n\n\nmessage = input(\"Please enter your full name: \")\nprint(message)\n\n\nprint(f\"\\nHello, {message}!\")\n\n\n\n\n\nage = input(\"May i know your age, please?\")\nage = int(age)\n\nif age &lt;=12 or age &gt;= 65 :\n    print(\"\\nYou can enter the zoo for free\")\n\nelse:\n    print(\"\\nYou'll have to pay 45CAD for a 90 minutes visit\")\n    \n\nMay i know your age, please?6556\n\nYou can enter the zoo for free\n\n\n\n\n\nprovides the remainder after division\n\n4%3\n\n1\n\n\n\n5 % 2\n\n1\n\n\n\n\n\n\n# counting (1 to 5)\ncurrent_number = 1\nwhile current_number &lt;=5:\n    print(current_number)\n    current_number += 1\n    \n\n1\n2\n3\n4\n5\n\n\n\n# infinite loop which stops with quit message\nprompt = \"\\nTell me something and I will repeat it back to you:\"\nprompt += \"\\n Enter 'quit' to end the program. \"\nmessage = \"\"\nwhile message != 'quit':\n    message = input(prompt)\n    print(message)\n\n\nTell me something and I will repeat it back to you:\n Enter 'quit' to end the program. I'm doing great!\nI'm doing great!\n\nTell me something and I will repeat it back to you:\n Enter 'quit' to end the program. keep on playing with this game until you get tired\nkeep on playing with this game until you get tired\n\nTell me something and I will repeat it back to you:\n Enter 'quit' to end the program. quit\nquit\n\n\n\n\n\n\ndescription- when flag conditions are true, the program continues to run. Else, it stops.\nbenefits over while loop- can be used to execute several conditions. Contrary to while loop, which uses only one condition\n\n\nprompt = \"\\n Tell me something, and I will repeat it back to you:\"\nprompt += \"\\n Enter 'quit' to end the program. \"\n\nactive = True\nwhile active :\n    message = input(prompt)\n    \n    if message == 'quit':\n        active = False\n    else:\n        print(message)\n\n\n Tell me something, and I will repeat it back to you:\n Enter 'quit' to end the program. I'm doing good today!\nI'm doing good today!\n\n Tell me something, and I will repeat it back to you:\n Enter 'quit' to end the program. quit\n\n\n\nprompt = \"\\nPlease enter the name of a city you have visited:\"\nprompt += \"\\n(Enter 'quit' when you are finished.)\"\n\nwhile True:\n    city = input(prompt)\n    \n    if city == \"quit\":\n        break\n    else:\n        print(f\"I'd like to visit {city.title()}!\")\n\n\nPlease enter the name of a city you have visited:\n(Enter 'quit' when you are finished.)Dubai\nI'd like to visit this Dubai!\n\nPlease enter the name of a city you have visited:\n(Enter 'quit' when you are finished.)Thailand\nI'd like to visit this Thailand!\n\nPlease enter the name of a city you have visited:\n(Enter 'quit' when you are finished.)Switzerland\nI'd like to visit this Switzerland!\n\nPlease enter the name of a city you have visited:\n(Enter 'quit' when you are finished.)Mexico\nI'd like to visit this Mexico!\n\nPlease enter the name of a city you have visited:\n(Enter 'quit' when you are finished.)Thailand\nI'd like to visit this Thailand!\n\nPlease enter the name of a city you have visited:\n(Enter 'quit' when you are finished.)Pataya\nI'd like to visit this Pataya!\n\nPlease enter the name of a city you have visited:\n(Enter 'quit' when you are finished.)Cuba\nI'd like to visit this Cuba!\n\nPlease enter the name of a city you have visited:\n(Enter 'quit' when you are finished.)Vancouver\nI'd like to visit this Vancouver!\n\nPlease enter the name of a city you have visited:\n(Enter 'quit' when you are finished.)quit\n\n\n\n\n\n\n# print list of odd numbers upto 10\ncurrent_number = 0\nwhile current_number &lt; 10:\n    current_number += 1\n    if current_number % 2== 0:\n        continue\n    print(current_number)\n\n1\n3\n5\n7\n9\n\n\n\n# print list of even numbers upto 20\ncurrent_even_n = 0\nwhile current_even_n &lt; 21:      #upto 20 gets printed\n    current_even_n += 1\n    if current_even_n % 2 == 1:\n        continue\n    print(current_even_n)\n\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n\n\n\nnumber = 1\n\nwhile number &lt;= 10:\n    square = number * number\n    print(f\"The square of {number} is {square}\")\n    number += 1\n\n\nx = 1\n\nwhile x &lt;= 10:\n    square = x * x\n    print(f\"The square of {x} is {square}\")\n    x += 1  # prevents infinite loop\n\n    \n\nThe square of 1 is 1\nThe square of 2 is 4\nThe square of 3 is 9\nThe square of 4 is 16\nThe square of 5 is 25\nThe square of 6 is 36\nThe square of 7 is 49\nThe square of 8 is 64\nThe square of 9 is 81\nThe square of 10 is 100\n\n\n\n\n\n\nunconfirmed_users = ['raghav', 'britany', 'solance', 'aisha']\nconfirmed_users = []\n\nwhile unconfirmed_users :\n    current_user = unconfirmed_users.pop()\n    \n    #moving\n    \n    print(f\"Verifying user: {current_user.title()}\")\n    confirmed_users.append(current_user)\n    \n    #displaying\n    \n    print(\"\\nThe following users have been confirmed:\")\n    for confirmed_user in confirmed_users:\n        print(confirmed_user.title())\n\nVerifying user: Aisha\n\nThe following users have been confirmed:\nAisha\nVerifying user: Solance\n\nThe following users have been confirmed:\nAisha\nSolance\nVerifying user: Britany\n\nThe following users have been confirmed:\nAisha\nSolance\nBritany\nVerifying user: Raghav\n\nThe following users have been confirmed:\nAisha\nSolance\nBritany\nRaghav\n\n\n\n\n\n\n\npets = ['dog', 'cat', 'cheetah', 'beer', 'rabbit', 'lion']\nprint(pets)\n\nwhile 'cat' in pets:\n    pets.remove('cat')    #remove method\n\nprint(pets)\n\n['dog', 'cat', 'cheetah', 'beer', 'rabbit', 'lion']\n['dog', 'cheetah', 'beer', 'rabbit', 'lion']\n\n\n\n\n\n\nresponses = {}         #initializing empty dictionary\n\n\npolling_active = True  # setting flag indicator to True\n\nwhile polling_active:\n    name = input(\"\\nWhat is your name?\")\n    response = input(\"Which mountain would you like to climb someday?\")\n    \n    #storing the response in a dictionary\n    responses[name] = response  #where name is the key, and response is the value\n    \n    #finding out if we want to store more keys and variables in responses\n    repeat = input(\"Would you like to let another person respond? (yes/no)\")\n    if repeat == 'no':\n        polling_active = False\n        \n#polling complete; show the results\nprint(\"\\n_______Poll Results_____________\")\nfor name,response in responses.items():              #for loop iterates for key, value pairs in responses dictionary\n    print(f\"{name.title()} would like to climb {response.title()}.\")\n\n\nWhat is your name?sunita\nWhich mountain would you like to climb someday?valdavid\nWould you like to let another person respond? (yes/no)no\n\n_______Poll Results_____________\nSunita would like to climb Valdavid.\n\n\n\n# callling the dictionary\nprint(responses)\n\n{'sunita': 'valdavid'}\n\n\n\n\n\n\nnew dictionary is responses_new\n\n\nresponses_new = {}         #initializing empty dictionary\n\n\n\n\nwhile True:\n    name = input(\"\\nWhat is your name?\")\n    response = input(\"Which mountain would you like to climb someday?\")\n    \n    #storing the response in a dictionary\n    responses_new[name] = response  #where name is the key, and response is the value\n    \n    #finding out if we want to store more keys and variables in responses\n    repeat = input(\"Would you like to let another person respond? (yes/no)\")\n    if repeat.lower() != 'yes':\n        break  #exits the loop if response in not 'yes'\n        \n#polling complete; show the results\nprint(\"\\n_______Poll Results_____________\")\nfor name,response in responses_new.items():              #for loop iterates for key, value pairs in responses dictionary\n    print(f\"{name.title()} would like to climb {response.title()}.\")\n\n\nWhat is your name?kk\nWhich mountain would you like to climb someday?tatata\nWould you like to let another person respond? (yes/no)yes\n\nWhat is your name?paula\nWhich mountain would you like to climb someday?tatal\nWould you like to let another person respond? (yes/no)no\n\n_______Poll Results_____________\nKk would like to climb Tatata.\nPaula would like to climb Tatal.\n\n\n\nprint(responses_new)\n\n{'kunal': 'kanchunjunga', 'sushil': 'peu importe'}\n\n\n\n# Dream vacation\n\nvacation = {}\n\nwhile True:\n    key = input(\"\\nWhat is your name?\")\n    value = input(\"If I ask you to visit one place in the world, where would you go?\")\n    \n    vacation[key]= value\n    \n    repeat = input(\"Next person in line, if there is one? ('yes/no')\")\n    if repeat.lower() != 'yes':\n        break\n\n#polling complete, show results\nprint(\"______Poll_results\")\nfor key,value in vacation.items():\n    print(f\"{key.title()} would like to go {value.title()}.\")\n\n\nWhat is your name?kunal\nIf I ask you to visit one place in the world, where would you go?dubai\nNext person in line, if there is one? ('yes/no')yes\n\nWhat is your name?martina\nIf I ask you to visit one place in the world, where would you go?new brunswick\nNext person in line, if there is one? ('yes/no')yes\n\nWhat is your name?kathy\nIf I ask you to visit one place in the world, where would you go?québec city\nNext person in line, if there is one? ('yes/no')no\n______Poll_results\nKunal would like to go Dubai.\nMartina would like to go New Brunswick.\nKathy would like to go Québec City."
  },
  {
    "objectID": "posts/python/p_2.html",
    "href": "posts/python/p_2.html",
    "title": "IF statements",
    "section": "",
    "text": "write conditional tests, which always evaluate to True or False.\nwrite simple if statements, if-else chains, and if-elif-else chains.\nuse these structures to identify particular conditions you needed to test and to know when those conditions have been met in your programs.\nlearn to handle certain items in a list differently than all other items while continuing to utilize the efficiency of a for loop.\n\n\n\n\n#  let's print bmw in upper case and the other cars in the list as title case\ncars = ['honda', 'toyota', 'volksvogen', 'ferrari', 'bmw', 'benz']\n\nfor car in cars:\n    if car == 'bmw':\n        print(car.upper())\n    else:\n        print(car.title())\n    \n\nHonda\nToyota\nVolksvogen\nFerrari\nBMW\nBenz\n\n\n\n\n\n\nbanned_users = ['raghav', 'sunita', 'carlos']\nuser = 'dave'\n\nif user not in banned_users:\n    print(f\"{user.title()}, you can post a comment if you wish.\")\n\nDave, you can post a comment if you wish.\n\n\n\n\n\n\nage = 19\n\nif age &gt;= 18:\n    print('You are old enough to vote!')\n    print('Are you registered to vote?')\nelse:\n    print(\"Sorry, you are too young to vote.\")\n    print(\"PLease register to vote as soon as you turn 18!\")\n    \n\nYou are old enough to vote!\nAre you registered to vote?\n\n\n\n\n\n\nage = 12\n\nif age &lt; 4:\n    price = 0\nelif age &lt; 18:\n    price = 25\nelif age &lt;= 65:\n    price = 40\nelse:\n    price = 20\n    \nprint(f\"Your admission price is {price}.\")\n\nYour admission price is 25.\n\n\n\nage = 66\n\nif age &lt; 4:\n    price = 0\nelif age &lt; 18:\n    price = 25\nelif age &lt;= 65:\n    price = 40\nelse:\n    price = 20\n    \nprint(f\"Your admission price is {price}.\")\n\nYour admission price is 20.\n\n\n\n\n\n\nreq_toppings = ['mushrooms', 'green peppers', 'belpeppers', 'onions', 'lettuce']\n\nfor req_topping in req_toppings:\n    if req_topping == 'onions':\n        print(\"Sorry, we are out of onions right now.\")\n    else:\n        print(f'Adding {req_topping}.')\n    \nprint('\\nFinished making your pizza!')\n\nAdding mushrooms.\nAdding green peppers.\nAdding belpeppers.\nSorry, we are out of onions right now.\nAdding lettuce.\n\nFinished making your pizza!\n\n\n\nreq_toppings = []\n\nif req_toppings:\n    for req_topping in req_toppings:\n         print(f'Adding {req_topping}.')\n    print('\\nFinished making your pizza!')\nelse:\n    print(\"Are you sure you want a plain pizza?\")\n\nAre you sure you want a plain pizza?\n\n\n\navailable_toppings = ['mushrooms', 'mayo', 'south-west', 'green peppers', 'belpeppers', 'lettuce']\nreq_toppings = ['mushrooms', 'green peppers', 'broccoli']\n\nfor req_topping in req_toppings:  #we iterate over req_toppings\n    if req_topping in available_toppings:\n        print(f\"Adding {req_topping}\")\n    else:\n        print(f\"Sorry, we don't have {req_topping}.\")\n        \nprint(\"\\nFinished making your pizza!\")\n\nAdding mushrooms\nAdding green peppers\nSorry, we don't have broccoli.\n\nFinished making your pizza!"
  },
  {
    "objectID": "posts/python/p_2.html#learning-outcmoes",
    "href": "posts/python/p_2.html#learning-outcmoes",
    "title": "IF statements",
    "section": "",
    "text": "write conditional tests, which always evaluate to True or False.\nwrite simple if statements, if-else chains, and if-elif-else chains.\nuse these structures to identify particular conditions you needed to test and to know when those conditions have been met in your programs.\nlearn to handle certain items in a list differently than all other items while continuing to utilize the efficiency of a for loop.\n\n\n\n\n#  let's print bmw in upper case and the other cars in the list as title case\ncars = ['honda', 'toyota', 'volksvogen', 'ferrari', 'bmw', 'benz']\n\nfor car in cars:\n    if car == 'bmw':\n        print(car.upper())\n    else:\n        print(car.title())\n    \n\nHonda\nToyota\nVolksvogen\nFerrari\nBMW\nBenz\n\n\n\n\n\n\nbanned_users = ['raghav', 'sunita', 'carlos']\nuser = 'dave'\n\nif user not in banned_users:\n    print(f\"{user.title()}, you can post a comment if you wish.\")\n\nDave, you can post a comment if you wish.\n\n\n\n\n\n\nage = 19\n\nif age &gt;= 18:\n    print('You are old enough to vote!')\n    print('Are you registered to vote?')\nelse:\n    print(\"Sorry, you are too young to vote.\")\n    print(\"PLease register to vote as soon as you turn 18!\")\n    \n\nYou are old enough to vote!\nAre you registered to vote?\n\n\n\n\n\n\nage = 12\n\nif age &lt; 4:\n    price = 0\nelif age &lt; 18:\n    price = 25\nelif age &lt;= 65:\n    price = 40\nelse:\n    price = 20\n    \nprint(f\"Your admission price is {price}.\")\n\nYour admission price is 25.\n\n\n\nage = 66\n\nif age &lt; 4:\n    price = 0\nelif age &lt; 18:\n    price = 25\nelif age &lt;= 65:\n    price = 40\nelse:\n    price = 20\n    \nprint(f\"Your admission price is {price}.\")\n\nYour admission price is 20.\n\n\n\n\n\n\nreq_toppings = ['mushrooms', 'green peppers', 'belpeppers', 'onions', 'lettuce']\n\nfor req_topping in req_toppings:\n    if req_topping == 'onions':\n        print(\"Sorry, we are out of onions right now.\")\n    else:\n        print(f'Adding {req_topping}.')\n    \nprint('\\nFinished making your pizza!')\n\nAdding mushrooms.\nAdding green peppers.\nAdding belpeppers.\nSorry, we are out of onions right now.\nAdding lettuce.\n\nFinished making your pizza!\n\n\n\nreq_toppings = []\n\nif req_toppings:\n    for req_topping in req_toppings:\n         print(f'Adding {req_topping}.')\n    print('\\nFinished making your pizza!')\nelse:\n    print(\"Are you sure you want a plain pizza?\")\n\nAre you sure you want a plain pizza?\n\n\n\navailable_toppings = ['mushrooms', 'mayo', 'south-west', 'green peppers', 'belpeppers', 'lettuce']\nreq_toppings = ['mushrooms', 'green peppers', 'broccoli']\n\nfor req_topping in req_toppings:  #we iterate over req_toppings\n    if req_topping in available_toppings:\n        print(f\"Adding {req_topping}\")\n    else:\n        print(f\"Sorry, we don't have {req_topping}.\")\n        \nprint(\"\\nFinished making your pizza!\")\n\nAdding mushrooms\nAdding green peppers\nSorry, we don't have broccoli.\n\nFinished making your pizza!"
  },
  {
    "objectID": "posts/python/p_1.html",
    "href": "posts/python/p_1.html",
    "title": "Lists, for loops",
    "section": "",
    "text": "How to work through a list using a for loop, how Python uses indentation to structure a program, and how to avoid some common indentation errors.\nNumerical lists, as well as a few operations you can perform on numerical lists.\nSlice a list to work with a subset of items and how to copy lists properly using a slice\nTuples (which provide a degree of protection to a set of values that shouldn’t change) and how to style your increasingly complex code to make it easy to read.\n\n\n# looping through list\n\ncourses = ['Agriculture', 'Soil Science', 'Francisation', 'génie civil']\nfor course in courses:\n    print(course)\n\nAgriculture\nSoil Science\nFrancisation\ngénie civil\n\n\n\n\n\ndon’t forget indentation in second line\ndon’t forget to include two dots\nwith for loop, we associated course with courses, The output gets printed in seperate lines of code\ngeneric- for item in list_of_items\n\n\nfor course in courses:\n    print(f\"{course.title()}, was awesome!\")\n\nAgriculture, was awesome!\nSoil Science, was awesome!\nFrancisation, was awesome!\nGénie Civil, was awesome!\n\n\n\n# adding a line in front\nfor course in courses:\n    print (f\"I look forward to becoming more smart in {course.title()}.\")\n\nI look forward to becoming more smart in Agriculture.\nI look forward to becoming more smart in Soil Science.\nI look forward to becoming more smart in Francisation.\nI look forward to becoming more smart in Génie Civil.\n\n\n\n# adding an empty line after\nfor course in courses:\n    print (f\"I will add empty line after {course.title()}.\\n\")\n\nI will add empty line after Agriculture.\n\nI will add empty line after Soil Science.\n\nI will add empty line after Francisation.\n\nI will add empty line after Génie Civil.\n\n\n\n\n# way to write multiple lines of code using for loop\nfor course in courses:\n    print(f\"{course.title()} was a great subject\")\n    print(f\"I got to learn a lot about {course.title()}.\")\n    print(\"Thanks awesome people who came to my life as mentors and changed it.\\n\")\n\nAgriculture was a great subject\nI got to learn a lot about Agriculture.\nThanks awesome people who came to my life as mentors and changed it.\n\nSoil Science was a great subject\nI got to learn a lot about Soil Science.\nThanks awesome people who came to my life as mentors and changed it.\n\nFrancisation was a great subject\nI got to learn a lot about Francisation.\nThanks awesome people who came to my life as mentors and changed it.\n\nGénie Civil was a great subject\nI got to learn a lot about Génie Civil.\nThanks awesome people who came to my life as mentors and changed it.\n\n\n\n\n\n\n\nwe use range function for that\n\n\nfor value in range(1,6):\n    print(value)\n\n1\n2\n3\n4\n5\n\n\n\n# printing a list of numbers\nfor value in range(1,6):\n    print(list[value])\n\nlist[1]\nlist[2]\nlist[3]\nlist[4]\nlist[5]\n\n\n\n# printing list of numbers using list function\nnumbers = list(range(1,6))\nprint(numbers)\n\n[1, 2, 3, 4, 5]\n\n\n\n# printing even numbers\neven_numbers = list(range(0,11,2))\nprint(even_numbers)\n\n[0, 2, 4, 6, 8, 10]\n\n\n\nsquares = []\nfor value in range(1,11):\n    square = value ** 2\n    squares.append(square)\n    print(squares)\nprint (squares)                       #difference between this and previous, wow!\n\n# indentation is important\n\n[1]\n[1, 4]\n[1, 4, 9]\n[1, 4, 9, 16]\n[1, 4, 9, 16, 25]\n[1, 4, 9, 16, 25, 36]\n[1, 4, 9, 16, 25, 36, 49]\n[1, 4, 9, 16, 25, 36, 49, 64]\n[1, 4, 9, 16, 25, 36, 49, 64, 81]\n[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n\n\n\nprint(squares)\n\n[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n\n\n\n# precise way\n\nsq_numbers = []\nfor a in range(1,11):\n    sq_numbers.append(a**2)\nprint(sq_numbers)\n\n[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n\n\n\n\n\n\nextended form of a for loop (result we need in front, followed by for loop)\ncombines for loop and creates new elements in one line\n\n\nsq = [value**2 for value in range(1, 11)]\nprint(sq)\n\n[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n\n\n\ncubes = [value**3 for value in range(1,11)]\nprint(cubes)\n\n[1, 8, 27, 64, 125, 216, 343, 512, 729, 1000]\n\n\n\n\n\nuniversities = ['punjabi', 'pau', 'laval', 'Mcgill']\nprint(\"here are the list of universities, I studied:\")\nfor university in universities:\n    print(university.title())\n    \n\nhere are the list of universities, I studied:\nPunjabi\nPau\nLaval\nMcgill\n\n\n\n\n\n\nthey are immutable lists\nuses parenthesis instead of square brackets\nTo define a tupel with one element, you need a comma behind\n\n\nBuffet= ('Aloo gobhi', 'matar paneer', 'parantha', 'raita', 'roti',)\nprint(Buffet)\n\n('Aloo gobhi', 'matar paneer', 'parantha', 'raita', 'roti')\n\n\n\nBuffet[1]= ('pappad')\n\n\nprint(\"Buffet:\")\nfor item in Buffet:\n    print(item)\n    \n\nBuffet:\nAloo gobhi\nmatar paneer\nparantha\nraita\nroti\n\n\n\nNew_buffet = ('raita', 'parantha', 'butternan', 'allogobhi', 'Kasaurimethi')\nprint('New_buffet:')\nfor item in New_buffet:\n    print(item)\n\nNew_buffet:\nraita\nparantha\nbutternan\nallogobhi\nKasaurimethi\n\n\nRespecting community; some guidelines 1. Indentation- PEP 8 2. line length- 79 characters long 3. Blank lines - 5 lines of code follwoed by a blank line is suggested"
  },
  {
    "objectID": "posts/python/p_1.html#list-comprehensions",
    "href": "posts/python/p_1.html#list-comprehensions",
    "title": "Lists, for loops",
    "section": "",
    "text": "extended form of a for loop (result we need in front, followed by for loop)\ncombines for loop and creates new elements in one line\n\n\nsq = [value**2 for value in range(1, 11)]\nprint(sq)\n\n[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n\n\n\ncubes = [value**3 for value in range(1,11)]\nprint(cubes)\n\n[1, 8, 27, 64, 125, 216, 343, 512, 729, 1000]\n\n\n\n\n\nuniversities = ['punjabi', 'pau', 'laval', 'Mcgill']\nprint(\"here are the list of universities, I studied:\")\nfor university in universities:\n    print(university.title())\n    \n\nhere are the list of universities, I studied:\nPunjabi\nPau\nLaval\nMcgill\n\n\n\n\n\n\nthey are immutable lists\nuses parenthesis instead of square brackets\nTo define a tupel with one element, you need a comma behind\n\n\nBuffet= ('Aloo gobhi', 'matar paneer', 'parantha', 'raita', 'roti',)\nprint(Buffet)\n\n('Aloo gobhi', 'matar paneer', 'parantha', 'raita', 'roti')\n\n\n\nBuffet[1]= ('pappad')\n\n\nprint(\"Buffet:\")\nfor item in Buffet:\n    print(item)\n    \n\nBuffet:\nAloo gobhi\nmatar paneer\nparantha\nraita\nroti\n\n\n\nNew_buffet = ('raita', 'parantha', 'butternan', 'allogobhi', 'Kasaurimethi')\nprint('New_buffet:')\nfor item in New_buffet:\n    print(item)\n\nNew_buffet:\nraita\nparantha\nbutternan\nallogobhi\nKasaurimethi\n\n\nRespecting community; some guidelines 1. Indentation- PEP 8 2. line length- 79 characters long 3. Blank lines - 5 lines of code follwoed by a blank line is suggested"
  },
  {
    "objectID": "posts/python/Plotting_and_visualization-checkpoint.html",
    "href": "posts/python/Plotting_and_visualization-checkpoint.html",
    "title": "Plotting and Visualization",
    "section": "",
    "text": "Figures and subplots\nadjusting the spacing around subplots\ncolors, markers and line styles\nticks, labels, and legends\nadding legends\nannotation and drawing on a subplot\nsaving plots to file\nmatplotlib configuration"
  },
  {
    "objectID": "posts/python/Plotting_and_visualization-checkpoint.html#matplotlib-api-primer",
    "href": "posts/python/Plotting_and_visualization-checkpoint.html#matplotlib-api-primer",
    "title": "Plotting and Visualization",
    "section": "",
    "text": "Figures and subplots\nadjusting the spacing around subplots\ncolors, markers and line styles\nticks, labels, and legends\nadding legends\nannotation and drawing on a subplot\nsaving plots to file\nmatplotlib configuration"
  },
  {
    "objectID": "posts/python/Plotting_and_visualization-checkpoint.html#plotting-with-pandas-and-seaborn",
    "href": "posts/python/Plotting_and_visualization-checkpoint.html#plotting-with-pandas-and-seaborn",
    "title": "Plotting and Visualization",
    "section": "plotting with pandas and seaborn",
    "text": "plotting with pandas and seaborn\n\nline plots\nBar plots\nHistogram and density plots\nScatter or Point plots\nFacet Grids and Categorical Data\n\n\nInto matplotlib\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n%matplotlib inline\n\n\ndata = np.arange(10)\n\ndata\n\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\nplt.plot(data)\n\n\n\n\n\n\nFigures and subplots\n\nfig = plt.figure()\nplt.show()\n\n\n\n# 2, 2 means 4 sub-plots will be created\n\n# % matplotlib\nax1 = fig.add_subplot(2, 2, 1)\n\n\nax2 = fig.add_subplot(2, 2, 2)\nax3 = fig.add_subplot(2, 2, 3)\n\n\nfig = plt.figure()\nax1 = fig.add_subplot(2, 2, 1)\nax2 = fig.add_subplot(2, 2, 2)\nax3 = fig.add_subplot(2, 2, 3)\n\n\n\n\n\n%matplotlib notebook\nax3.plot(np.random.standard_normal(50).cumsum(),\n        color = 'black', linestyle= 'dashed')\n\n\nax3.plot(np.random.standard_normal(50).cumsum(),\n        color = 'black', linestyle = 'dashed');\n\n\n\nhelp(plt)\n\n\nax1.hist(np.random.standard_normal(100), bins = 20, \n        color = 'black', alpha = 0)\nax2.scatter(np.arange(30), np.arange(30) + 3 * np.random.standard_normal(30));\n\n\n! pip install ipympl\n\n\nfig, axes = plt.subplots(2,3)\n\n\n\n\n\naxes\n\narray([[&lt;Axes: &gt;, &lt;Axes: &gt;, &lt;Axes: &gt;],\n       [&lt;Axes: &gt;, &lt;Axes: &gt;, &lt;Axes: &gt;]], dtype=object)\n\n\n\n\nAdjusting spacing around subplots\n\nsubplots_adjust(left= None, bottom= None, \n                 right= None, top= None, \n                 wpace = None, hspace= None);\n\n\nfig, axes = plt.subplots(2, 2, sharex = True, sharey = True)\nfor i in range(2):\n    for j in range(2):\n        axes[1, j].hist(np.random.standard_normal(500),\n                       bins = 50, color = 'black', alpha = 0.5)\n        \nfig.subplots_adjust(wspace= 0, hspace= 0)\n\n\n\n\n\nax.plot(x, y, linestyle = '--', color = 'green')\n\n\nax = fig.add_subplot()\n\nax.plot(np.random.standard_normal(30).cumsum(),\n       color = 'black', linestyle = 'dashed', marker= 'o' )\n\n\nfig = plt.figure()\nax = fig.add_subplot()\n\ndata = np.random.standard_normal(30).cumsum()\n\nax.plot(data, color = 'black', linestyle = 'dashed', \n       label = 'Default')\nax.plot(data, color = 'black', linestyle = 'dashed',\n       drawstyle= 'steps-post', label= 'steps-post')\n\nax.legend()\n\n&lt;matplotlib.legend.Legend at 0x205861b2460&gt;\n\n\n\n\n\n\n\nTicks, Labels, and Legends\n\nax.get_xlim([0, 10])\n\n\nhelp(plt.xlim)\n\nHelp on function xlim in module matplotlib.pyplot:\n\nxlim(*args, **kwargs)\n    Get or set the x limits of the current axes.\n    \n    Call signatures::\n    \n        left, right = xlim()  # return the current xlim\n        xlim((left, right))   # set the xlim to left, right\n        xlim(left, right)     # set the xlim to left, right\n    \n    If you do not specify args, you can pass *left* or *right* as kwargs,\n    i.e.::\n    \n        xlim(right=3)  # adjust the right leaving left unchanged\n        xlim(left=1)  # adjust the left leaving right unchanged\n    \n    Setting limits turns autoscaling off for the x-axis.\n    \n    Returns\n    -------\n    left, right\n        A tuple of the new x-axis limits.\n    \n    Notes\n    -----\n    Calling this function with no arguments (e.g. ``xlim()``) is the pyplot\n    equivalent of calling `~.Axes.get_xlim` on the current axes.\n    Calling this function with arguments is the pyplot equivalent of calling\n    `~.Axes.set_xlim` on the current axes. All arguments are passed though.\n\n\n\n\n\nSetting the title, axis labels, ticks, and tick labels\n\nfix, ax = plt.subplots()\n\nax.plot(np.random.standard_normal(1000).cumsum());\n    \n\n\n\n\n\nticks= ax.set_xticks([0, 250, 500, 750, 1000])\n\nlabels = ax.set_xticklabels(['one', 'two', 'three', \n                            'four', 'five'],\n                           rotation = 30, fontsize=8)\n\n\nax.set_xlabel('Stages')\n\n\n\nText(0.5, 1.0, 'My matplotlib plot')\n\n\n\nax.set_title('My matplotlib plot')\n\nText(0.5, 1.0, 'My matplotlib plot')\n\n\n\nplt.show()\n\n\n\nAdding legends\n\nfig, ax = plt.subplots(3, 4)\n\n\n\n\n\nfig,ax = plt.subplots()\nax.plot(np.random.randn(1000).cumsum(), color = 'black',\n       label = 'one')\nax.plot(np.random.randn(1000).cumsum(), color = 'black',\n       linestyle = 'dashed')\nax.plot(np.random.randn(1000).cumsum(), color= 'black',\n       linestyle = 'dotted', )\n\n\n\n\n\nax.legend()\n\n&lt;matplotlib.legend.Legend at 0x20588a6e700&gt;\n\n\n\n\nAnnotations and Drawing on a Subplot\n\nax.text(x, y, 'Hello world!',\n       family = 'monospace', fontsize= 10)\n\n\nfrom datetime import datetime\nfig, ax = plt.subplots()\n\nrect = plt.Rectangle((0.2, 0.75), 0.4, 0.15, color = 'black', alpha=0.3)\ncirc = plt.Circle((0.7, 0.2),0.15, color = 'blue', alpha= 0.3)\npgon = plt.Polygon([[0.15, 0.15], [0.35, 0.4], [0.2, 0.6]],\n                  color = 'green', alpha =0.5)\n\nax.add_patch(rect)\nax.add_patch(circ)\nax.add_patch(pgon)\n\n&lt;matplotlib.patches.Polygon at 0x20588b71a60&gt;\n\n\n\n\n\n\n\nSaving photos to file\n\nfig.savefig('figpath.svg')\n\n\nfig.savefig('figpath.png', dpi=400)\n\n\n\nMatplotlib configuration\n\nplt.rc('figure', figsize = (10, 10))\n\n\nplt.rc('font', family= 'monospace', weight = 'bold', size= 8)"
  },
  {
    "objectID": "posts/python/Plotting_and_visualization-checkpoint.html#plotting-with-pandas-and-seaborn-1",
    "href": "posts/python/Plotting_and_visualization-checkpoint.html#plotting-with-pandas-and-seaborn-1",
    "title": "Plotting and Visualization",
    "section": "Plotting with pandas and seaborn",
    "text": "Plotting with pandas and seaborn\n\nLine plots\n\ns = pd.Series(np.random.standard_normal(10).cumsum(), \n             index = np.arange(0, 100, 10))\ns.plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n# to know more about plot method types\nhelp(pd.Series.plot)\n\n\ndf = pd.DataFrame(np.random.standard_normal((10,4)).cumsum(0),\n                 columns = ['A', 'B', 'C', 'D'],\n                 index = np.arange(0, 100, 10))\nplt.style.use('grayscale')\ndf.plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\nBar Plots\n\nfig, axes = plt.subplots(2, 1)\n\ndata = pd.Series(np.random.uniform(size=11), \n                 index=list('adafdfdhfdf'))\ndata.plot.bar(ax = axes[0], color= 'black', alpha=0.7)\n\ndata.plot.barh(ax= axes[1], color= 'black', alpha= 0.7)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\ndf2 = pd.DataFrame(np.random.uniform(size=(6,4)),\n                   index = ['one', 'two', 'three', 'four', 'five', 'six'],\n                   columns =pd.Index(['A', 'B', 'C', 'D'], name= 'Kunal'))\n\ndf2\n\n\n\n\n\n\n\nKunal\nA\nB\nC\nD\n\n\n\n\none\n0.260658\n0.536198\n0.754708\n0.788162\n\n\ntwo\n0.558069\n0.885258\n0.726874\n0.747412\n\n\nthree\n0.731644\n0.476325\n0.688620\n0.529862\n\n\nfour\n0.536672\n0.681522\n0.509112\n0.143861\n\n\nfive\n0.188829\n0.099173\n0.050697\n0.323006\n\n\nsix\n0.521520\n0.208836\n0.807558\n0.411876\n\n\n\n\n\n\n\n\ndf2.plot.bar()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\ndf2.plot.barh(stacked=True, alpha=0.5)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\niris = pd.read_csv(r\"E:/pythonfordatanalysis/semainedu26fevrier/iris.csv\")\niris.head()\n\n\n\n\n\n\n\n\nId\nSepal Length (cm)\nSepal Width (cm)\nPetal Length (cm)\nPetal Width (cm)\nSpecies\n\n\n\n\n0\n1\n5.1\n3.5\n1.4\n0.2\nIris-setosa\n\n\n1\n2\n4.9\n3.0\n1.4\n0.2\nIris-setosa\n\n\n2\n3\n4.7\n3.2\n1.3\n0.2\nIris-setosa\n\n\n3\n4\n4.6\n3.1\n1.5\n0.2\nIris-setosa\n\n\n4\n5\n5.0\n3.6\n1.4\n0.2\nIris-setosa\n\n\n\n\n\n\n\n\niris.tail()\n#len_wd = pd.\n\n\n\n\n\n\n\n\nId\nSepal Length (cm)\nSepal Width (cm)\nPetal Length (cm)\nPetal Width (cm)\nSpecies\n\n\n\n\n145\n146\n6.7\n3.0\n5.2\n2.3\nIris-virginica\n\n\n146\n147\n6.3\n2.5\n5.0\n1.9\nIris-virginica\n\n\n147\n148\n6.5\n3.0\n5.2\n2.0\nIris-virginica\n\n\n148\n149\n6.2\n3.4\n5.4\n2.3\nIris-virginica\n\n\n149\n150\n5.9\n3.0\n5.1\n1.8\nIris-virginica\n\n\n\n\n\n\n\n\nprint(iris.columns)\n\nIndex(['Id', 'Sepal Length (cm)', 'Sepal Width (cm)', 'Petal Length (cm)',\n       'Petal Width (cm)', 'Species'],\n      dtype='object')\n\n\n\ncount = pd.crosstab(iris['Sepal Length (cm)'], iris['Sepal Width (cm)'])\n\n\ncount2 = count.reindex(index=['length', 'width', ])\n\n\ncount2\n\n\n\n\n\n\n\nSepal Width (cm)\n2.0\n2.2\n2.3\n2.4\n2.5\n2.6\n2.7\n2.8\n2.9\n3.0\n...\n3.4\n3.5\n3.6\n3.7\n3.8\n3.9\n4.0\n4.1\n4.2\n4.4\n\n\nSepal Length (cm)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlength\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nwidth\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n2 rows × 23 columns\n\n\n\n\n\nHistogram and Density plots\n\niris['Sepal Length (cm)'].plot.hist(bins= 100)\n\n&lt;Axes: ylabel='Frequency'&gt;\n\n\n\n\n\n\niris['Sepal Length (cm)'].plot.density()\n\n&lt;Axes: ylabel='Density'&gt;\n\n\n\n\n\n\n! pip install seaborn\n\n\nimport seaborn as sns\n\n\ncomp1 = np.random.standard_normal(200)\ncomp2 = 10 + 2 * np.random.standard_normal(200)\n\nvalues = pd.Series(np.concatenate([comp1, comp2]))\n\nsns.histplot(values, bins= 100, color = 'black')\n\n&lt;Axes: ylabel='Count'&gt;\n\n\n\n\n\n\n\nScatter or point plots\n\niris.head()\n\n\n\n\n\n\n\n\nId\nSepal Length (cm)\nSepal Width (cm)\nPetal Length (cm)\nPetal Width (cm)\nSpecies\n\n\n\n\n0\n1\n5.1\n3.5\n1.4\n0.2\nIris-setosa\n\n\n1\n2\n4.9\n3.0\n1.4\n0.2\nIris-setosa\n\n\n2\n3\n4.7\n3.2\n1.3\n0.2\nIris-setosa\n\n\n3\n4\n4.6\n3.1\n1.5\n0.2\nIris-setosa\n\n\n4\n5\n5.0\n3.6\n1.4\n0.2\nIris-setosa\n\n\n\n\n\n\n\n\niris2 = iris[['Sepal Length (cm)', 'Sepal Width (cm)', 'Petal Length (cm)',\n       'Petal Width (cm)']]\n\n\ntrans_iris2 = np.log(iris2).diff().dropna()\n\n\ntrans_iris2.tail()\n\n\n\n\n\n\n\n\nSepal Length (cm)\nSepal Width (cm)\nPetal Length (cm)\nPetal Width (cm)\n\n\n\n\n145\n0.000000\n-0.095310\n-0.091808\n-0.083382\n\n\n146\n-0.061558\n-0.182322\n-0.039221\n-0.191055\n\n\n147\n0.031253\n0.182322\n0.039221\n0.051293\n\n\n148\n-0.047253\n0.125163\n0.037740\n0.139762\n\n\n149\n-0.049597\n-0.125163\n-0.057158\n-0.245122\n\n\n\n\n\n\n\n\nusing regplot method to make scatter plots\n\nax = sns.regplot(x= \"Petal Length (cm)\", y = \"Petal Width (cm)\", data= trans_iris2)\n\n#ax.title(\"Change in log (Petal Length (cm)) length versus log (Petal Width (cm)) width \")\n\n\n\n\n\nsns.pairplot(trans_iris2, diag_kind= 'kde', plot_kws={'alpha': 0.2} )\n\n\n\n\n\n\n\nFacet Grids and Categorical Data\n\ncatplots\n\n\nsns.catplot(x = 'Petal Length (cm)', y= 'Petal Width (cm)',\n             data = iris2[iris2.Petal Length (cm) &lt; 0.5])\n\nSyntaxError: invalid syntax (3918704872.py, line 2)\n\n\n\n# renaming columns\n\ndf3 = df.rename(columns={\n    'Sepal Length (cm)' : 'sepal_lengh', \n    'Sepal Width (cm)': 'sepal_width_1',\n    'Petal Length (cm)': 'petal_length',\n    'Petal Width (cm)' : 'petal_width_2',    \n})\n\n\ndf3\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n1.023099\n1.290412\n0.383457\n0.906869\n\n\n10\n1.764172\n1.074479\n1.263072\n0.599487\n\n\n20\n1.213259\n-0.057754\n0.253086\n0.639868\n\n\n30\n0.885836\n1.862965\n1.889980\n-0.241599\n\n\n40\n0.216726\n1.304783\n1.853073\n1.646926\n\n\n50\n0.260248\n1.741218\n1.235233\n0.662542\n\n\n60\n1.000675\n0.951141\n1.243977\n0.876234\n\n\n70\n0.490475\n1.539086\n0.198194\n1.316823\n\n\n80\n1.442176\n0.393618\n1.413128\n2.013297\n\n\n90\n0.825008\n-0.072619\n2.495192\n2.775830\n\n\n\n\n\n\n\n\nsns.catplot(x = 'Sepal Length (cm)', y= 'Sepal Width (cm)',\n            kind = 'box',\n           data = iris2)"
  },
  {
    "objectID": "posts/python/modeling_libraries_python-checkpoint.html#creating-model-descriptions-with-patsy",
    "href": "posts/python/modeling_libraries_python-checkpoint.html#creating-model-descriptions-with-patsy",
    "title": "Modeling libries in python",
    "section": "Creating model descriptions with patsy",
    "text": "Creating model descriptions with patsy\n\nData Transformations in Pasty Formulas\nCategorical data and patsy"
  },
  {
    "objectID": "posts/python/modeling_libraries_python-checkpoint.html#introduction-to-statsmodels",
    "href": "posts/python/modeling_libraries_python-checkpoint.html#introduction-to-statsmodels",
    "title": "Modeling libries in python",
    "section": "Introduction to statsmodels",
    "text": "Introduction to statsmodels\n\nEstimating linear models\nEstimating time series processes"
  },
  {
    "objectID": "posts/python/modeling_libraries_python-checkpoint.html#introduction-to-scitkit-learn",
    "href": "posts/python/modeling_libraries_python-checkpoint.html#introduction-to-scitkit-learn",
    "title": "Modeling libries in python",
    "section": "Introduction to scitkit-learn",
    "text": "Introduction to scitkit-learn"
  },
  {
    "objectID": "posts/python/modeling_libraries_python-checkpoint.html#interface-data-loading-and-cleaning-beforing-model-building",
    "href": "posts/python/modeling_libraries_python-checkpoint.html#interface-data-loading-and-cleaning-beforing-model-building",
    "title": "Modeling libries in python",
    "section": "Interface (data loading and cleaning beforing model building)",
    "text": "Interface (data loading and cleaning beforing model building)\n\nimport pandas as pd\nimport numpy as np\n\nimport patsy\nimport statsmodels\n\n\ndata = pd.DataFrame({\n    \"x\" :[1, 2, 3, 4, 5],\n    \"y\" :[0.1, .2, 0.4, .6, .7],\n    \"z\" :[-1, -3, -.45, -5.6, 4]\n})\n\n\ndata\n\n\ndata.to_numpy()\n\n\n# working with DataFrames\ndf2 = pd.DataFrame(data.to_numpy(),\n                  columns= ['one', 'two', 'three'])\n\n\ndf2\n\n\ndf3 = data.copy()\n\n\ndf3['strings'] = ['a', 'b', 'c', 'd', 'e']\n\ndf3\n\n\ndf3.to_numpy()\n\n\n# to use subset of columns, loc indexing with to_numpy\n\nmodel_cols = [\"x\", \"y\"]\n\ndata.loc[:, model_cols].to_numpy()\n\n\ndata['category'] =pd.Categorical(['a', 'b', 'a','a', 'b'],\n                                categories = ['a', 'b'])\n\ndata\n\n\n# replacing category with dummy variables\ndummies = pd.get_dummies(data.category,  prefix='category')\n\ndata_with_dummies = data.drop('category', axis=1).join(dummies)\n\ndata_with_dummies"
  },
  {
    "objectID": "posts/python/modeling_libraries_python-checkpoint.html#creating-model-descriptions-with-patsy-1",
    "href": "posts/python/modeling_libraries_python-checkpoint.html#creating-model-descriptions-with-patsy-1",
    "title": "Modeling libries in python",
    "section": "Creating model descriptions with Patsy",
    "text": "Creating model descriptions with Patsy\n\ndata\n\n\ndata.drop('category', axis=1)\n\n\na, b = patsy.dmatrices('z ~ x + y', data)\n\n\na\n\nDesignMatrix with shape (5, 1)\n      z\n  -1.00\n  -3.00\n  -0.45\n  -5.60\n   4.00\n  Terms:\n    'z' (column 0)\n\n\n\nb\n\nDesignMatrix with shape (5, 3)\n  Intercept  x    y\n          1  1  0.1\n          1  2  0.2\n          1  3  0.4\n          1  4  0.6\n          1  5  0.7\n  Terms:\n    'Intercept' (column 0)\n    'x' (column 1)\n    'y' (column 2)\n\n\n\nnp.asarray(a)\n\n\nnp.asarray(b)\n\n\n# adding +o to suppress the intercept\npatsy.dmatrices('z ~ x + y + 0', data)[1]\n\n\npatsy.dmatrices('z ~ x + y + 0', data)\n\n\npassing pasty objects directly into algorithms\n-eg. numpy.linalg.lstsq\n\nrcond = -1\ncoef, resid, _, _ = np.linalg.lstsq(a, b)\n\nC:\\Users\\Khurana_Kunal\\AppData\\Local\\Temp\\ipykernel_23924\\735709127.py:2: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\nTo use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n  coef, resid, _, _ = np.linalg.lstsq(a, b)\n\n\n\ncoef\n\narray([[-0.10510315, -0.18675353, -0.02501629]])\n\n\n\ncoef = pd.Series(coef.squeeze(), index=a.design_info.column_names)\n\n\n\nData Transformations in Pastry Formulas\n\na, b = patsy.dmatrices('x ~ y + np.log(np.abs(x) + 1)', data)\n\n\nb\n\nDesignMatrix with shape (5, 3)\n  Intercept    y  np.log(np.abs(x) + 1)\n          1  0.1                0.69315\n          1  0.2                1.09861\n          1  0.4                1.38629\n          1  0.6                1.60944\n          1  0.7                1.79176\n  Terms:\n    'Intercept' (column 0)\n    'y' (column 1)\n    'np.log(np.abs(x) + 1)' (column 2)\n\n\n\na,b = patsy.dmatrices(\"y ~ standardize(x) + center(z)\", data)\n\n\na\n\nDesignMatrix with shape (5, 1)\n    y\n  0.1\n  0.2\n  0.4\n  0.6\n  0.7\n  Terms:\n    'y' (column 0)\n\n\n\nb\n\nDesignMatrix with shape (5, 3)\n  Intercept  standardize(x)  center(z)\n          1        -1.41421       0.21\n          1        -0.70711      -1.79\n          1         0.00000       0.76\n          1         0.70711      -4.39\n          1         1.41421       5.21\n  Terms:\n    'Intercept' (column 0)\n    'standardize(x)' (column 1)\n    'center(z)' (column 2)\n\n\n\n\nCategorical Data and Patsy\n\ndata2 = pd.DataFrame({\n    'key1' : ['a', 'b', 'c', 'a', 'c', 'b'],\n    'key2' : [0, 1, 0, 1, 0, 1],\n    'v1' : [1, 2, 3, 4, 5, 6],\n    'v2' : [-1, 0, -1.5, 4.0, 2.5, -1.7]\n})\n\n\ny, X = patsy.dmatrices('v2 ~ key1', data2)\n\n\nX\n\nDesignMatrix with shape (6, 3)\n  Intercept  key1[T.b]  key1[T.c]\n          1          0          0\n          1          1          0\n          1          0          1\n          1          0          0\n          1          0          1\n          1          1          0\n  Terms:\n    'Intercept' (column 0)\n    'key1' (columns 1:3)\n\n\n\ny, X = patsy.dmatrices('v2 ~ C(key2)', data2)\n\n\nX\n\nDesignMatrix with shape (6, 2)\n  Intercept  C(key2)[T.1]\n          1             0\n          1             1\n          1             0\n          1             1\n          1             0\n          1             1\n  Terms:\n    'Intercept' (column 0)\n    'C(key2)' (column 1)\n\n\n\ndata2['key2'] = data2['key2'].map({0: 'zero', 1:'one'})\n\n\ndata2\n\n\n\n\n\n\n\n\nkey1\nkey2\nv1\nv2\n\n\n\n\n0\na\nzero\n1\n-1.0\n\n\n1\nb\none\n2\n0.0\n\n\n2\nc\nzero\n3\n-1.5\n\n\n3\na\none\n4\n4.0\n\n\n4\nc\nzero\n5\n2.5\n\n\n5\nb\none\n6\n-1.7\n\n\n\n\n\n\n\n\ny, X = patsy.dmatrices('v2 ~ key1 + key2', data2)\n\n\nX\n\nDesignMatrix with shape (6, 4)\n  Intercept  key1[T.b]  key1[T.c]  key2[T.zero]\n          1          0          0             1\n          1          1          0             0\n          1          0          1             1\n          1          0          0             0\n          1          0          1             1\n          1          1          0             0\n  Terms:\n    'Intercept' (column 0)\n    'key1' (columns 1:3)\n    'key2' (column 3)\n\n\n\ny, X = patsy.dmatrices('v2 ~ key1 + key2 + key1:key2', data2)\n\n\nX\n\nDesignMatrix with shape (6, 6)\n  Columns:\n    ['Intercept',\n     'key1[T.b]',\n     'key1[T.c]',\n     'key2[T.zero]',\n     'key1[T.b]:key2[T.zero]',\n     'key1[T.c]:key2[T.zero]']\n  Terms:\n    'Intercept' (column 0)\n    'key1' (columns 1:3)\n    'key2' (column 3)\n    'key1:key2' (columns 4:6)\n  (to view full data, use np.asarray(this_obj))"
  },
  {
    "objectID": "posts/python/modeling_libraries_python-checkpoint.html#statsmodles",
    "href": "posts/python/modeling_libraries_python-checkpoint.html#statsmodles",
    "title": "Modeling libries in python",
    "section": "Statsmodles",
    "text": "Statsmodles\n\nlinear models, generalized linear models, and robust linear models\nlinear mixed effects models\nANOVA\ntime series and state space models\ngeneralized methods of moments\n\n\nEstimating linear models\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n\n# generating a linear model from a random data\nrng = np.random.default_rng(seed = 12345)\n\ndef dnorm(mean, variance, size=1):\n    if isinstance (size, int):\n        size = size, \n    return mean+ np.sqrt(variance) * rng.standard_normal(*size)\n\n\nN = 100\nX = np.c_[dnorm(0, 0.4, size=N),\n         dnorm(0, 0.6, size=N),\n         dnorm(0, 0.2, size= N)]\neps = dnorm(0, 0.1, size=N)\nbeta = [0.1, 0.3, 0.5]\n\ny = np.dot(X, beta) + eps\n\n\nX[:5]\n\narray([[-0.90050602, -0.18942958, -1.0278702 ],\n       [ 0.79925205, -1.54598388, -0.32739708],\n       [-0.55065483, -0.12025429,  0.32935899],\n       [-0.16391555,  0.82403985,  0.20827485],\n       [-0.04765129, -0.21314698, -0.04824364]])\n\n\n\ny[:5]\n\narray([-0.59952668, -0.58845445,  0.18563386, -0.00747657, -0.01537445])\n\n\n\n# fitting a linear model with intercept term\nX_model = sm.add_constant(X)\n\nX_model[:5]\n\narray([[ 1.        , -0.90050602, -0.18942958, -1.0278702 ],\n       [ 1.        ,  0.79925205, -1.54598388, -0.32739708],\n       [ 1.        , -0.55065483, -0.12025429,  0.32935899],\n       [ 1.        , -0.16391555,  0.82403985,  0.20827485],\n       [ 1.        , -0.04765129, -0.21314698, -0.04824364]])\n\n\n\n# filling least squre linear regression with sm.OLS\nmodel = sm.OLS(y, X)\n\n\nresults = model.fit()\n\n\nresults.params\n\narray([0.06681503, 0.26803235, 0.45052319])\n\n\n\n# printing summary\nprint(results.summary())\n\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:                      y   R-squared (uncentered):                   0.469\nModel:                            OLS   Adj. R-squared (uncentered):              0.452\nMethod:                 Least Squares   F-statistic:                              28.51\nDate:                Fri, 01 Mar 2024   Prob (F-statistic):                    2.66e-13\nTime:                        12:55:52   Log-Likelihood:                         -25.611\nNo. Observations:                 100   AIC:                                      57.22\nDf Residuals:                      97   BIC:                                      65.04\nDf Model:                           3                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nx1             0.0668      0.054      1.243      0.217      -0.040       0.174\nx2             0.2680      0.042      6.313      0.000       0.184       0.352\nx3             0.4505      0.068      6.605      0.000       0.315       0.586\n==============================================================================\nOmnibus:                        0.435   Durbin-Watson:                   1.869\nProb(Omnibus):                  0.805   Jarque-Bera (JB):                0.301\nSkew:                           0.134   Prob(JB):                        0.860\nKurtosis:                       2.995   Cond. No.                         1.64\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\ndata = pd.DataFrame(X, columns=['col0', 'col1', 'col2'])\n\ndata['y']=y \ndata[:5]\n\n\n\n\n\n\n\n\ncol0\ncol1\ncol2\ny\n\n\n\n\n0\n-0.900506\n-0.189430\n-1.027870\n-0.599527\n\n\n1\n0.799252\n-1.545984\n-0.327397\n-0.588454\n\n\n2\n-0.550655\n-0.120254\n0.329359\n0.185634\n\n\n3\n-0.163916\n0.824040\n0.208275\n-0.007477\n\n\n4\n-0.047651\n-0.213147\n-0.048244\n-0.015374\n\n\n\n\n\n\n\n\n# using statsmodles formula API and Pastry formuls\nresults = smf.ols('y ~ col0 + col1 + col2', data=data).fit()\n\n\nresults.params\n\nIntercept   -0.020799\ncol0         0.065813\ncol1         0.268970\ncol2         0.449419\ndtype: float64\n\n\n\nresults.tvalues\n\nIntercept   -0.652501\ncol0         1.219768\ncol1         6.312369\ncol2         6.567428\ndtype: float64\n\n\n\n# computing predicted values\nresults.predict(data[:5])\n\n0   -0.592959\n1   -0.531160\n2    0.058636\n3    0.283658\n4   -0.102947\ndtype: float64\n\n\n\n\nEstimating time series processes\n\ninit_x = 4\nvalues = [init_x, init_x]\nN = 1000\nb0 = 0.8\nb1 = -0.4\nnoise = dnorm(0, 0.1, N)\nfor i in range(N):\n    new_x = values[-1] * b0 + values[-2] +b1 + noise[i]\n    values.append(new_x)\n\n\nfrom statsmodels.tsa.ar_model import AutoReg\n\nMAXLAGS = 5\n\nmodel = AutoReg(values, MAXLAGS)\n\nresults = model.fit()\n\nC:\\Users\\Khurana_Kunal\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\statsmodels\\base\\model.py:1529: RuntimeWarning: invalid value encountered in multiply\n  cov_p = self.normalized_cov_params * scale\n\n\n\nresults.params\n\narray([0.        , 0.81652213, 0.5528124 , 0.37427221, 0.25339463,\n       0.17155651])"
  },
  {
    "objectID": "posts/python/modeling_libraries_python-checkpoint.html#scikit-learn",
    "href": "posts/python/modeling_libraries_python-checkpoint.html#scikit-learn",
    "title": "Modeling libries in python",
    "section": "Scikit-learn",
    "text": "Scikit-learn\n\n! pip install scikit-learn\n\n\ntrain = pd.read_csv(r\"E:\\pythonfordatanalysis\\semainedu26fevrier\\train (1).csv\")\n\n\ntest= pd.read_csv(r\"E:\\pythonfordatanalysis\\semainedu26fevrier\\test (1).csv\")\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\n\n# looking for missing data\ntrain.isna().sum()\n\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n\n\n\ntest.isna().sum()\n\nPassengerId      0\nPclass           0\nName             0\nSex              0\nAge             86\nSibSp            0\nParch            0\nTicket           0\nFare             1\nCabin          327\nEmbarked         0\ndtype: int64\n\n\n\n# using 'Age' as a predictor\n# using median of training set to fill missing values\n\nimpute_value = train['Age'].median()\n\ntrain['Age'] = train['Age'].fillna(impute_value)\n\ntest['Age'] = test['Age'].fillna(impute_value)\n\n\n# specying the models\ntrain['isFemale'] = (train['Sex'] == 'female').astype(int)\n\ntest['isFemale'] = (test['Sex'] == 'female').astype(int)\n\n\n# creating NumPy arrays and deciding on some model variables\npredictors = ['Pclass', 'isFemale', 'Age']\n\nX_train = train[predictors].to_numpy()\n\nX_test = test[predictors].to_numpy()\n\ny_train = train['Survived'].to_numpy()\n\nX_train[:5]\n\narray([[ 3.,  0., 22.],\n       [ 1.,  1., 38.],\n       [ 3.,  1., 26.],\n       [ 1.,  1., 35.],\n       [ 3.,  0., 35.]])\n\n\n\ny_train[:5]\n\narray([0, 1, 1, 1, 0], dtype=int64)\n\n\n\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\n\n\nmodel.fit(X_train, y_train)\nLogisticRegression()\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniNot fittedLogisticRegression() \n\n\n\ny_predict = model.predict(X_test)\n\n\ny_predict[:10]\n\narray([0, 0, 0, 0, 1, 0, 1, 0, 1, 0], dtype=int64)\n\n\n\n(y_true == y_predict).mean()\n\n\nTheory\n\nmany models have parameters that can be tuned to avoid overfitting\n\nexample- Cross-validataion (longer to train, but performs better)\n\n\n\nfrom sklearn.linear_model import LogisticRegressionCV\n\n\nmodel_cv = LogisticRegressionCV(Cs=10)\n\nmodel_cv.fit(X_train, y_train)\n\nLogisticRegressionCV()\n\nLogisticRegressionCV()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegressionCV?Documentation for LogisticRegressionCViNot fittedLogisticRegressionCV() \n\n\n\nfrom sklearn.model_selection import cross_val_score\n\nmodel = LogisticRegression (C=10)\n\nscores = cross_val_score(model, X_train, y_train, cv=4)\n\nscores\n\narray([0.77578475, 0.79820628, 0.77578475, 0.78828829])"
  },
  {
    "objectID": "posts/python/Geopandas.html",
    "href": "posts/python/Geopandas.html",
    "title": "GeoPandas",
    "section": "",
    "text": "import geopandas as gpd\nimport pandas as pd\nfrom shapely.geometry import LineString\n\n# for interactive maps\nimport folium\nfrom folium import Choropleth, Circle, Marker\nfrom folium.plugins import HeatMap, MarkerCluster\nimport math\ndata = gpd.read_file(\"C:\\\\Users\\\\Khurana_Kunal\\\\Downloads\\\\DEC_lands 2\\\\DEC_lands\")\ndata.head()\n\n\n\n\n\n\n\n\nOBJECTID\nCATEGORY\nUNIT\nFACILITY\nCLASS\nUMP\nDESCRIPTIO\nREGION\nCOUNTY\nURL\nSOURCE\nUPDATE_\nOFFICE\nACRES\nLANDS_UID\nGREENCERT\nSHAPE_AREA\nSHAPE_LEN\ngeometry\n\n\n\n\n0\n1\nFOR PRES DET PAR\nCFP\nHANCOCK FP DETACHED PARCEL\nWILD FOREST\nNone\nDELAWARE COUNTY DETACHED PARCEL\n4\nDELAWARE\nhttp://www.dec.ny.gov/\nDELAWARE RPP\n5/12\nSTAMFORD\n738.620192\n103\nN\n2.990365e+06\n7927.662385\nPOLYGON ((486093.245 4635308.586, 486787.235 4...\n\n\n1\n2\nFOR PRES DET PAR\nCFP\nHANCOCK FP DETACHED PARCEL\nWILD FOREST\nNone\nDELAWARE COUNTY DETACHED PARCEL\n4\nDELAWARE\nhttp://www.dec.ny.gov/\nDELAWARE RPP\n5/12\nSTAMFORD\n282.553140\n1218\nN\n1.143940e+06\n4776.375600\nPOLYGON ((491931.514 4637416.256, 491305.424 4...\n\n\n2\n3\nFOR PRES DET PAR\nCFP\nHANCOCK FP DETACHED PARCEL\nWILD FOREST\nNone\nDELAWARE COUNTY DETACHED PARCEL\n4\nDELAWARE\nhttp://www.dec.ny.gov/\nDELAWARE RPP\n5/12\nSTAMFORD\n234.291262\n1780\nN\n9.485476e+05\n5783.070364\nPOLYGON ((486000.287 4635834.453, 485007.550 4...\n\n\n3\n4\nFOR PRES DET PAR\nCFP\nGREENE COUNTY FP DETACHED PARCEL\nWILD FOREST\nNone\nNone\n4\nGREENE\nhttp://www.dec.ny.gov/\nGREENE RPP\n5/12\nSTAMFORD\n450.106464\n2060\nN\n1.822293e+06\n7021.644833\nPOLYGON ((541716.775 4675243.268, 541217.579 4...\n\n\n4\n6\nFOREST PRESERVE\nAFP\nSARANAC LAKES WILD FOREST\nWILD FOREST\nSARANAC LAKES\nNone\n5\nESSEX\nhttp://www.dec.ny.gov/lands/22593.html\nDECRP, ESSEX RPP\n12/96\nRAY BROOK\n69.702387\n1517\nN\n2.821959e+05\n2663.909932\nPOLYGON ((583896.043 4909643.187, 583891.200 4...\ndata_selected = data.loc[:, ['CLASS', 'COUNTY', 'geometry']].copy()\ndata_selected.CLASS.value_counts()\n\nCLASS\nWILD FOREST                   965\nINTENSIVE USE                 108\nPRIMITIVE                      60\nWILDERNESS                     52\nADMINISTRATIVE                 17\nUNCLASSIFIED                    7\nHISTORIC                        5\nPRIMITIVE BICYCLE CORRIDOR      4\nCANOE AREA                      1\nName: count, dtype: int64\n# select lands under 'wild forest' or 'wilderness' category\nwild_lands = data_selected.loc[data_selected.CLASS.isin(['WILD FOREST', 'WILDERNESS'])].copy()\nwild_lands.head()\n\n\n\n\n\n\n\n\nCLASS\nCOUNTY\ngeometry\n\n\n\n\n0\nWILD FOREST\nDELAWARE\nPOLYGON ((486093.245 4635308.586, 486787.235 4...\n\n\n1\nWILD FOREST\nDELAWARE\nPOLYGON ((491931.514 4637416.256, 491305.424 4...\n\n\n2\nWILD FOREST\nDELAWARE\nPOLYGON ((486000.287 4635834.453, 485007.550 4...\n\n\n3\nWILD FOREST\nGREENE\nPOLYGON ((541716.775 4675243.268, 541217.579 4...\n\n\n4\nWILD FOREST\nESSEX\nPOLYGON ((583896.043 4909643.187, 583891.200 4...\nwild_lands.plot()\n\n&lt;Axes: &gt;\nwild_lands.geometry.head()\n\n0    POLYGON ((486093.245 4635308.586, 486787.235 4...\n1    POLYGON ((491931.514 4637416.256, 491305.424 4...\n2    POLYGON ((486000.287 4635834.453, 485007.550 4...\n3    POLYGON ((541716.775 4675243.268, 541217.579 4...\n4    POLYGON ((583896.043 4909643.187, 583891.200 4...\nName: geometry, dtype: geometry\nworld_filepath ='geopandas\\\\ne_110m_admin_0_countries\\\\ne_110m_admin_0_countries.shx'\nworld = gpd.read_file(world_filepath)\nworld.head()\n\n\n\n\n\n\n\n\nfeaturecla\nscalerank\nLABELRANK\nSOVEREIGNT\nSOV_A3\nADM0_DIF\nLEVEL\nTYPE\nTLC\nADMIN\n...\nFCLASS_TR\nFCLASS_ID\nFCLASS_PL\nFCLASS_GR\nFCLASS_IT\nFCLASS_NL\nFCLASS_SE\nFCLASS_BD\nFCLASS_UA\ngeometry\n\n\n\n\n0\nAdmin-0 country\n1\n6\nFiji\nFJI\n0\n2\nSovereign country\n1\nFiji\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nMULTIPOLYGON (((180.00000 -16.06713, 180.00000...\n\n\n1\nAdmin-0 country\n1\n3\nUnited Republic of Tanzania\nTZA\n0\n2\nSovereign country\n1\nUnited Republic of Tanzania\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nPOLYGON ((33.90371 -0.95000, 34.07262 -1.05982...\n\n\n2\nAdmin-0 country\n1\n7\nWestern Sahara\nSAH\n0\n2\nIndeterminate\n1\nWestern Sahara\n...\nUnrecognized\nUnrecognized\nUnrecognized\nNone\nNone\nUnrecognized\nNone\nNone\nNone\nPOLYGON ((-8.66559 27.65643, -8.66512 27.58948...\n\n\n3\nAdmin-0 country\n1\n2\nCanada\nCAN\n0\n2\nSovereign country\n1\nCanada\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nMULTIPOLYGON (((-122.84000 49.00000, -122.9742...\n\n\n4\nAdmin-0 country\n1\n2\nUnited States of America\nUS1\n1\n2\nCountry\n1\nUnited States of America\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nMULTIPOLYGON (((-122.84000 49.00000, -120.0000...\n\n\n\n\n5 rows × 169 columns\nax = world.plot()\n#plotting a map with coordinates\nax = world.plot(figsize=(20,20), color='whitesmoke', linestyle=':', edgecolor='black')\nPHL_loans = data.loc[data.COUNTY==\"Philippines\"].copy()\nprint(world.columns)\n\nIndex(['featurecla', 'scalerank', 'LABELRANK', 'SOVEREIGNT', 'SOV_A3',\n       'ADM0_DIF', 'LEVEL', 'TYPE', 'TLC', 'ADMIN',\n       ...\n       'FCLASS_TR', 'FCLASS_ID', 'FCLASS_PL', 'FCLASS_GR', 'FCLASS_IT',\n       'FCLASS_NL', 'FCLASS_SE', 'FCLASS_BD', 'FCLASS_UA', 'geometry'],\n      dtype='object', length=169)\nworld.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nRangeIndex: 177 entries, 0 to 176\nColumns: 169 entries, featurecla to geometry\ndtypes: float64(6), geometry(1), int64(25), object(137)\nmemory usage: 233.8+ KB\nworld.head()\n\n\n\n\n\n\n\n\nfeaturecla\nscalerank\nLABELRANK\nSOVEREIGNT\nSOV_A3\nADM0_DIF\nLEVEL\nTYPE\nTLC\nADMIN\n...\nFCLASS_TR\nFCLASS_ID\nFCLASS_PL\nFCLASS_GR\nFCLASS_IT\nFCLASS_NL\nFCLASS_SE\nFCLASS_BD\nFCLASS_UA\ngeometry\n\n\n\n\n0\nAdmin-0 country\n1\n6\nFiji\nFJI\n0\n2\nSovereign country\n1\nFiji\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nMULTIPOLYGON (((180.00000 -16.06713, 180.00000...\n\n\n1\nAdmin-0 country\n1\n3\nUnited Republic of Tanzania\nTZA\n0\n2\nSovereign country\n1\nUnited Republic of Tanzania\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nPOLYGON ((33.90371 -0.95000, 34.07262 -1.05982...\n\n\n2\nAdmin-0 country\n1\n7\nWestern Sahara\nSAH\n0\n2\nIndeterminate\n1\nWestern Sahara\n...\nUnrecognized\nUnrecognized\nUnrecognized\nNone\nNone\nUnrecognized\nNone\nNone\nNone\nPOLYGON ((-8.66559 27.65643, -8.66512 27.58948...\n\n\n3\nAdmin-0 country\n1\n2\nCanada\nCAN\n0\n2\nSovereign country\n1\nCanada\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nMULTIPOLYGON (((-122.84000 49.00000, -122.9742...\n\n\n4\nAdmin-0 country\n1\n2\nUnited States of America\nUS1\n1\n2\nCountry\n1\nUnited States of America\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nMULTIPOLYGON (((-122.84000 49.00000, -120.0000...\n\n\n\n\n5 rows × 169 columns"
  },
  {
    "objectID": "posts/python/Geopandas.html#coordinate-reference-systems",
    "href": "posts/python/Geopandas.html#coordinate-reference-systems",
    "title": "GeoPandas",
    "section": "Coordinate Reference Systems",
    "text": "Coordinate Reference Systems\n\nshape file imports CRS automatically\nsettings (DataFrame uses EPSG 32630; csv file uses EPSG 4326)\n\n\nfacilities_df = pd.read_csv('geopandas\\health_facilities.csv')\n\n# convert to GeoDataFrame\nfacilities = gpd.GeoDataFrame(facilities_df, geometry = gpd.points_from_xy\n                              (facilities_df.Longitude, facilities_df.Latitude))\n\n# set CRS\nfacilities.crs = ('epsg:4326')\n\n#view first 5 rows\nfacilities.head()\n\n\n\n\n\n\n\n\nRegion\nDistrict\nFacilityName\nType\nTown\nOwnership\nLatitude\nLongitude\ngeometry\n\n\n\n\n0\nAshanti\nOffinso North\nA.M.E Zion Clinic\nClinic\nAfrancho\nCHAG\n7.40801\n-1.96317\nPOINT (-1.96317 7.40801)\n\n\n1\nAshanti\nBekwai Municipal\nAbenkyiman Clinic\nClinic\nAnwiankwanta\nPrivate\n6.46312\n-1.58592\nPOINT (-1.58592 6.46312)\n\n\n2\nAshanti\nAdansi North\nAboabo Health Centre\nHealth Centre\nAboabo No 2\nGovernment\n6.22393\n-1.34982\nPOINT (-1.34982 6.22393)\n\n\n3\nAshanti\nAfigya-Kwabre\nAboabogya Health Centre\nHealth Centre\nAboabogya\nGovernment\n6.84177\n-1.61098\nPOINT (-1.61098 6.84177)\n\n\n4\nAshanti\nKwabre\nAboaso Health Centre\nHealth Centre\nAboaso\nGovernment\n6.84177\n-1.61098\nPOINT (-1.61098 6.84177)\n\n\n\n\n\n\n\n\n# plotting facilities of Ghana on world map\nax = world.plot(figsize=(20,20), color='whitesmoke', linestyle=':', edgecolor='black')\nfacilities.to_crs(epsg=4326).plot(markersize=.25, ax=ax)\n\nC:\\Users\\Khurana_Kunal\\anaconda3\\Lib\\site-packages\\shapely\\measurement.py:103: RuntimeWarning: invalid value encountered in bounds\n  return lib.bounds(geometry_arr, out=out, **kwargs)\n\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n#get the x coordinates of each point\nfacilities.geometry.head().x\n\n0   -1.96317\n1   -1.58592\n2   -1.34982\n3   -1.61098\n4   -1.61098\ndtype: float64\n\n\n\nbirds_df = pd.read_csv(\"data_for_all_courses\\purple_martin.csv\")\nbirds_df.head()\n\n\n\n\n\n\n\n\ntimestamp\nlocation-long\nlocation-lat\ntag-local-identifier\n\n\n\n\n0\n2014-08-15 05:56:00\n-88.146014\n17.513049\n30448\n\n\n1\n2014-09-01 05:59:00\n-85.243501\n13.095782\n30448\n\n\n2\n2014-10-30 23:58:00\n-62.906089\n-7.852436\n30448\n\n\n3\n2014-11-15 04:59:00\n-61.776826\n-11.723898\n30448\n\n\n4\n2014-11-30 09:59:00\n-61.241538\n-11.612237\n30448\n\n\n\n\n\n\n\n\nprint(f\"There are {birds_df['tag-local-identifier'].nunique()} different birds in the dataset.\")\n\nThere are 11 different birds in the dataset.\n\n\n\nbirds = gpd.GeoDataFrame(birds_df,\n                         geometry = gpd.points_from_xy(birds_df[\"location-long\"],\n                                                      birds_df['location-lat']))\nbirds.head()\n\n\n\n\n\n\n\n\ntimestamp\nlocation-long\nlocation-lat\ntag-local-identifier\ngeometry\n\n\n\n\n0\n2014-08-15 05:56:00\n-88.146014\n17.513049\n30448\nPOINT (-88.14601 17.51305)\n\n\n1\n2014-09-01 05:59:00\n-85.243501\n13.095782\n30448\nPOINT (-85.24350 13.09578)\n\n\n2\n2014-10-30 23:58:00\n-62.906089\n-7.852436\n30448\nPOINT (-62.90609 -7.85244)\n\n\n3\n2014-11-15 04:59:00\n-61.776826\n-11.723898\n30448\nPOINT (-61.77683 -11.72390)\n\n\n4\n2014-11-30 09:59:00\n-61.241538\n-11.612237\n30448\nPOINT (-61.24154 -11.61224)\n\n\n\n\n\n\n\n\n# set the CRS\n\nbirds.crs = ('epsg:4326')\n\n\n# plot the data\namericas = world.loc[world['CONTINENT'].isin(['North America', 'South America',])]\namericas.head()\n\n\n\n\n\n\n\n\nfeaturecla\nscalerank\nLABELRANK\nSOVEREIGNT\nSOV_A3\nADM0_DIF\nLEVEL\nTYPE\nTLC\nADMIN\n...\nFCLASS_TR\nFCLASS_ID\nFCLASS_PL\nFCLASS_GR\nFCLASS_IT\nFCLASS_NL\nFCLASS_SE\nFCLASS_BD\nFCLASS_UA\ngeometry\n\n\n\n\n3\nAdmin-0 country\n1\n2\nCanada\nCAN\n0\n2\nSovereign country\n1\nCanada\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nMULTIPOLYGON (((-122.84000 49.00000, -122.9742...\n\n\n4\nAdmin-0 country\n1\n2\nUnited States of America\nUS1\n1\n2\nCountry\n1\nUnited States of America\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nMULTIPOLYGON (((-122.84000 49.00000, -120.0000...\n\n\n9\nAdmin-0 country\n1\n2\nArgentina\nARG\n0\n2\nSovereign country\n1\nArgentina\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nMULTIPOLYGON (((-68.63401 -52.63637, -68.25000...\n\n\n10\nAdmin-0 country\n1\n2\nChile\nCHL\n0\n2\nSovereign country\n1\nChile\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nMULTIPOLYGON (((-68.63401 -52.63637, -68.63335...\n\n\n16\nAdmin-0 country\n1\n5\nHaiti\nHTI\n0\n2\nSovereign country\n1\nHaiti\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nPOLYGON ((-71.71236 19.71446, -71.62487 19.169...\n\n\n\n\n5 rows × 169 columns\n\n\n\n\nworld.head()\n\n\n\n\n\n\n\n\nfeaturecla\nscalerank\nLABELRANK\nSOVEREIGNT\nSOV_A3\nADM0_DIF\nLEVEL\nTYPE\nTLC\nADMIN\n...\nFCLASS_TR\nFCLASS_ID\nFCLASS_PL\nFCLASS_GR\nFCLASS_IT\nFCLASS_NL\nFCLASS_SE\nFCLASS_BD\nFCLASS_UA\ngeometry\n\n\n\n\n0\nAdmin-0 country\n1\n6\nFiji\nFJI\n0\n2\nSovereign country\n1\nFiji\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nMULTIPOLYGON (((180.00000 -16.06713, 180.00000...\n\n\n1\nAdmin-0 country\n1\n3\nUnited Republic of Tanzania\nTZA\n0\n2\nSovereign country\n1\nUnited Republic of Tanzania\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nPOLYGON ((33.90371 -0.95000, 34.07262 -1.05982...\n\n\n2\nAdmin-0 country\n1\n7\nWestern Sahara\nSAH\n0\n2\nIndeterminate\n1\nWestern Sahara\n...\nUnrecognized\nUnrecognized\nUnrecognized\nNone\nNone\nUnrecognized\nNone\nNone\nNone\nPOLYGON ((-8.66559 27.65643, -8.66512 27.58948...\n\n\n3\nAdmin-0 country\n1\n2\nCanada\nCAN\n0\n2\nSovereign country\n1\nCanada\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nMULTIPOLYGON (((-122.84000 49.00000, -122.9742...\n\n\n4\nAdmin-0 country\n1\n2\nUnited States of America\nUS1\n1\n2\nCountry\n1\nUnited States of America\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nMULTIPOLYGON (((-122.84000 49.00000, -120.0000...\n\n\n\n\n5 rows × 169 columns\n\n\n\n\n# checking for all the columns in a data frame with for loop\nfor column in world.columns:\n    print(column)\n\nfeaturecla\nscalerank\nLABELRANK\nSOVEREIGNT\nSOV_A3\nADM0_DIF\nLEVEL\nTYPE\nTLC\nADMIN\nADM0_A3\nGEOU_DIF\nGEOUNIT\nGU_A3\nSU_DIF\nSUBUNIT\nSU_A3\nBRK_DIFF\nNAME\nNAME_LONG\nBRK_A3\nBRK_NAME\nBRK_GROUP\nABBREV\nPOSTAL\nFORMAL_EN\nFORMAL_FR\nNAME_CIAWF\nNOTE_ADM0\nNOTE_BRK\nNAME_SORT\nNAME_ALT\nMAPCOLOR7\nMAPCOLOR8\nMAPCOLOR9\nMAPCOLOR13\nPOP_EST\nPOP_RANK\nPOP_YEAR\nGDP_MD\nGDP_YEAR\nECONOMY\nINCOME_GRP\nFIPS_10\nISO_A2\nISO_A2_EH\nISO_A3\nISO_A3_EH\nISO_N3\nISO_N3_EH\nUN_A3\nWB_A2\nWB_A3\nWOE_ID\nWOE_ID_EH\nWOE_NOTE\nADM0_ISO\nADM0_DIFF\nADM0_TLC\nADM0_A3_US\nADM0_A3_FR\nADM0_A3_RU\nADM0_A3_ES\nADM0_A3_CN\nADM0_A3_TW\nADM0_A3_IN\nADM0_A3_NP\nADM0_A3_PK\nADM0_A3_DE\nADM0_A3_GB\nADM0_A3_BR\nADM0_A3_IL\nADM0_A3_PS\nADM0_A3_SA\nADM0_A3_EG\nADM0_A3_MA\nADM0_A3_PT\nADM0_A3_AR\nADM0_A3_JP\nADM0_A3_KO\nADM0_A3_VN\nADM0_A3_TR\nADM0_A3_ID\nADM0_A3_PL\nADM0_A3_GR\nADM0_A3_IT\nADM0_A3_NL\nADM0_A3_SE\nADM0_A3_BD\nADM0_A3_UA\nADM0_A3_UN\nADM0_A3_WB\nCONTINENT\nREGION_UN\nSUBREGION\nREGION_WB\nNAME_LEN\nLONG_LEN\nABBREV_LEN\nTINY\nHOMEPART\nMIN_ZOOM\nMIN_LABEL\nMAX_LABEL\nLABEL_X\nLABEL_Y\nNE_ID\nWIKIDATAID\nNAME_AR\nNAME_BN\nNAME_DE\nNAME_EN\nNAME_ES\nNAME_FA\nNAME_FR\nNAME_EL\nNAME_HE\nNAME_HI\nNAME_HU\nNAME_ID\nNAME_IT\nNAME_JA\nNAME_KO\nNAME_NL\nNAME_PL\nNAME_PT\nNAME_RU\nNAME_SV\nNAME_TR\nNAME_UK\nNAME_UR\nNAME_VI\nNAME_ZH\nNAME_ZHT\nFCLASS_ISO\nTLC_DIFF\nFCLASS_TLC\nFCLASS_US\nFCLASS_FR\nFCLASS_RU\nFCLASS_ES\nFCLASS_CN\nFCLASS_TW\nFCLASS_IN\nFCLASS_NP\nFCLASS_PK\nFCLASS_DE\nFCLASS_GB\nFCLASS_BR\nFCLASS_IL\nFCLASS_PS\nFCLASS_SA\nFCLASS_EG\nFCLASS_MA\nFCLASS_PT\nFCLASS_AR\nFCLASS_JP\nFCLASS_KO\nFCLASS_VN\nFCLASS_TR\nFCLASS_ID\nFCLASS_PL\nFCLASS_GR\nFCLASS_IT\nFCLASS_NL\nFCLASS_SE\nFCLASS_BD\nFCLASS_UA\ngeometry\n\n\n\n# plot americas\nax_americas = americas.plot(figsize=(10,10), color='whitesmoke', linestyle=':', edgecolor='black')\n\n\n\n\n\nStarting and end journey of birds\n\n# GeoDataFrame showing path for each bird\npath_df = birds.groupby(\"tag-local-identifier\")['geometry'].apply(list).apply(lambda x: LineString(x)).reset_index()\npath_gdf = gpd.GeoDataFrame(path_df, geometry = path_df.geometry)\npath_gdf.crs = ('epsg:4326')\n\n# GeoDataFrame showing starting point for each bird\nstart_df = birds.groupby(\"tag-local-identifier\")['geometry'].apply(list).apply(lambda x: x[0]).reset_index()\nstart_gdf = gpd.GeoDataFrame(start_df, geometry = start_df.geometry)\nstart_gdf.crs = ('epsg:4326')\n\n# Show first five rows of GeoDataFrame\nstart_gdf.head()\n\n\n\n\n\n\n\n\ntag-local-identifier\ngeometry\n\n\n\n\n0\n30048\nPOINT (-90.12992 20.73242)\n\n\n1\n30054\nPOINT (-93.60861 46.50563)\n\n\n2\n30198\nPOINT (-80.31036 25.92545)\n\n\n3\n30263\nPOINT (-76.78146 42.99209)\n\n\n4\n30275\nPOINT (-76.78213 42.99207)\n\n\n\n\n\n\n\n\n# end point of each bird\nend_df = birds.groupby(\"tag-local-identifier\")['geometry'].apply(list).apply(lambda x: x[-1]).reset_index()\nend_gdf = gpd.GeoDataFrame(end_df, geometry = end_df.geometry)\nend_gdf.crs = ('epsg:4326')\n\n\n# plot americas\nax_americas = americas.plot(figsize=(10,10), color='whitesmoke', linestyle=':', edgecolor='black')\n\nstart_gdf.plot(ax = ax_americas, color = 'red', markersize = 10)\npath_gdf.plot(ax = ax_americas, cmap = 'tab20b', linestyle= '-', linewidth = 1, zorder = 1)\nend_gdf.plot(ax = ax_americas, color = 'blue', markersize = 10)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n# no file found; gives 'Driver Error' - à voir plustard\nprotected_filepath = 'data_for_all_courses/add_0.shp'\nprotected_area = gpd.read_file(protected_filepath)"
  },
  {
    "objectID": "posts/python/Geopandas.html#interactive-maps",
    "href": "posts/python/Geopandas.html#interactive-maps",
    "title": "GeoPandas",
    "section": "Interactive maps",
    "text": "Interactive maps\n\n# Create a map\nmontréal_1 = folium.Map(location=[45.50, -73.56], tiles='openstreetmap', zoom_start=10)\n\n\n# Display the map\nmontréal_1\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n# crimes\n\ncrimes = pd.read_csv(\"data_for_all_courses\\crime.csv\", encoding = 'latin-1')\ncrimes.describe()\n\n\n\n\n\n\n\n\n\nOFFENSE_CODE\nYEAR\nMONTH\nHOUR\nLat\nLong\n\n\n\n\ncount\n319073.000000\n319073.000000\n319073.000000\n319073.000000\n299074.000000\n299074.000000\n\n\nmean\n2317.546956\n2016.560586\n6.609719\n13.118205\n42.214381\n-70.908272\n\n\nstd\n1185.285543\n0.996344\n3.273691\n6.294205\n2.159766\n3.493618\n\n\nmin\n111.000000\n2015.000000\n1.000000\n0.000000\n-1.000000\n-71.178674\n\n\n25%\n1001.000000\n2016.000000\n4.000000\n9.000000\n42.297442\n-71.097135\n\n\n50%\n2907.000000\n2017.000000\n7.000000\n14.000000\n42.325538\n-71.077524\n\n\n75%\n3201.000000\n2017.000000\n9.000000\n18.000000\n42.348624\n-71.062467\n\n\nmax\n3831.000000\n2018.000000\n12.000000\n23.000000\n42.395042\n-1.000000\n\n\n\n\n\n\n\n\ncrimes.head()\n\n\n\n\n\n\n\n\nINCIDENT_NUMBER\nOFFENSE_CODE\nOFFENSE_CODE_GROUP\nOFFENSE_DESCRIPTION\nDISTRICT\nREPORTING_AREA\nSHOOTING\nOCCURRED_ON_DATE\nYEAR\nMONTH\nDAY_OF_WEEK\nHOUR\nUCR_PART\nSTREET\nLat\nLong\nLocation\n\n\n\n\n0\nI182070945\n619\nLarceny\nLARCENY ALL OTHERS\nD14\n808\nNaN\n2018-09-02 13:00:00\n2018\n9\nSunday\n13\nPart One\nLINCOLN ST\n42.357791\n-71.139371\n(42.35779134, -71.13937053)\n\n\n1\nI182070943\n1402\nVandalism\nVANDALISM\nC11\n347\nNaN\n2018-08-21 00:00:00\n2018\n8\nTuesday\n0\nPart Two\nHECLA ST\n42.306821\n-71.060300\n(42.30682138, -71.06030035)\n\n\n2\nI182070941\n3410\nTowed\nTOWED MOTOR VEHICLE\nD4\n151\nNaN\n2018-09-03 19:27:00\n2018\n9\nMonday\n19\nPart Three\nCAZENOVE ST\n42.346589\n-71.072429\n(42.34658879, -71.07242943)\n\n\n3\nI182070940\n3114\nInvestigate Property\nINVESTIGATE PROPERTY\nD4\n272\nNaN\n2018-09-03 21:16:00\n2018\n9\nMonday\n21\nPart Three\nNEWCOMB ST\n42.334182\n-71.078664\n(42.33418175, -71.07866441)\n\n\n4\nI182070938\n3114\nInvestigate Property\nINVESTIGATE PROPERTY\nB3\n421\nNaN\n2018-09-03 21:05:00\n2018\n9\nMonday\n21\nPart Three\nDELHI ST\n42.275365\n-71.090361\n(42.27536542, -71.09036101)\n\n\n\n\n\n\n\n\n# drop missing locations\ncrimes.dropna(subset= ['Lat', 'Long', 'DISTRICT'], inplace = True)\n\n\n# focus on major crimes\ncrimes = crimes[crimes.OFFENSE_CODE_GROUP.isin([\n    'Larceny', 'Auto Theft', 'Robbery', 'Larceny From Motor Vehicle', 'Residential Burglary',\n    'Simple Assault', 'Harassment', 'Ballistics', 'Aggravated Assault', 'Other Burglary', \n    'Arson', 'Commercial Burglary'    \n])]\n\ncrimes = crimes[crimes.YEAR&gt;=2018]\n\ncrimes.head()\n\n\n\n\n\n\n\n\nINCIDENT_NUMBER\nOFFENSE_CODE\nOFFENSE_CODE_GROUP\nOFFENSE_DESCRIPTION\nDISTRICT\nREPORTING_AREA\nSHOOTING\nOCCURRED_ON_DATE\nYEAR\nMONTH\nDAY_OF_WEEK\nHOUR\nUCR_PART\nSTREET\nLat\nLong\nLocation\n\n\n\n\n0\nI182070945\n619\nLarceny\nLARCENY ALL OTHERS\nD14\n808\nNaN\n2018-09-02 13:00:00\n2018\n9\nSunday\n13\nPart One\nLINCOLN ST\n42.357791\n-71.139371\n(42.35779134, -71.13937053)\n\n\n6\nI182070933\n724\nAuto Theft\nAUTO THEFT\nB2\n330\nNaN\n2018-09-03 21:25:00\n2018\n9\nMonday\n21\nPart One\nNORMANDY ST\n42.306072\n-71.082733\n(42.30607218, -71.08273260)\n\n\n8\nI182070931\n301\nRobbery\nROBBERY - STREET\nC6\n177\nNaN\n2018-09-03 20:48:00\n2018\n9\nMonday\n20\nPart One\nMASSACHUSETTS AVE\n42.331521\n-71.070853\n(42.33152148, -71.07085307)\n\n\n19\nI182070915\n614\nLarceny From Motor Vehicle\nLARCENY THEFT FROM MV - NON-ACCESSORY\nB2\n181\nNaN\n2018-09-02 18:00:00\n2018\n9\nSunday\n18\nPart One\nSHIRLEY ST\n42.325695\n-71.068168\n(42.32569490, -71.06816778)\n\n\n24\nI182070908\n522\nResidential Burglary\nBURGLARY - RESIDENTIAL - NO FORCE\nB2\n911\nNaN\n2018-09-03 18:38:00\n2018\n9\nMonday\n18\nPart One\nANNUNCIATION RD\n42.335062\n-71.093168\n(42.33506218, -71.09316781)\n\n\n\n\n\n\n\n\n# crimes between 9 to 18 \n\ndaytime_robberies = crimes[((crimes.OFFENSE_CODE_GROUP == 'Robbery') & \n                            crimes.HOUR.isin(range(9,18)))]\n\n\n# create a map\n\nmap2 = folium.Map(location=[42.32,-71.0589], tiles='openstreetmap', zoom_start=13)\n\n# add points\nfor idx, row in daytime_robberies.iterrows():\n    Marker([row['Lat'], row['Long']]).add_to(map2)\n    \n    \n# display\nmap2\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nfolium.plugin.MarkerCluster\n\n# plotting points\nm_3 = folium.Map(location= [42.32,-71.0589], tiles='cartodbpositron', zoom_start=13)\n\n# add points\nmc = MarkerCluster()\nfor idx, row in daytime_robberies.iterrows():\n    if not math.isnan(row['Long']) and not math.isnan(row['Lat']):\n        mc.add_child(Marker([row['Lat'], row['Long']]))\nm_3.add_child(mc)\n    \n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nBubble maps\n\n# create a base map\n\nm_4 = folium.Map(location=[42.32,-71.0589], tiles='cartodbpositron', zoom_start=11)\n\ndef color_producer(val):\n    if val &gt;=12:\n        return 'forestgreen'\n    else:\n        return 'darkred'\n    \n# add bubble map to the base map\nfor i in range(0, len(daytime_robberies)):\n    Circle(\n    location = [daytime_robberies.iloc[i]['Lat'], \n               daytime_robberies.iloc[i]['Long']],\n    radius = 20,\n    color = color_producer(daytime_robberies.iloc[i]['HOUR'])).add_to(m_4)\n    \n# display\nm_4\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nHeatmaps\n\n# basemaps\nm_5 = folium.Map(location=[42.32,-71.0589], tiles='cartodbpositron', zoom_start=11)\n\n# add heatmaps to the base map\nHeatMap(data= crimes[['Lat', 'Long']], radius = 10).add_to(m_5)\n\n# display\nm_5\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nChoropleth maps\n\nhelp(folium.Choropleth)\n\nHelp on class Choropleth in module folium.features:\n\nclass Choropleth(folium.map.FeatureGroup)\n |  Choropleth(geo_data: Any, data: Optional[Any] = None, columns: Optional[Sequence[Any]] = None, key_on: Optional[str] = None, bins: Union[int, Sequence[float]] = 6, fill_color: Optional[str] = None, nan_fill_color: str = 'black', fill_opacity: float = 0.6, nan_fill_opacity: Optional[float] = None, line_color: str = 'black', line_weight: float = 1, line_opacity: float = 1, name: Optional[str] = None, legend_name: str = '', overlay: bool = True, control: bool = True, show: bool = True, topojson: Optional[str] = None, smooth_factor: Optional[float] = None, highlight: bool = False, use_jenks: bool = False, **kwargs)\n |  \n |  Apply a GeoJSON overlay to the map.\n |  \n |  Plot a GeoJSON overlay on the base map. There is no requirement\n |  to bind data (passing just a GeoJSON plots a single-color overlay),\n |  but there is a data binding option to map your columnar data to\n |  different feature objects with a color scale.\n |  \n |  If data is passed as a Pandas DataFrame, the \"columns\" and \"key-on\"\n |  keywords must be included, the first to indicate which DataFrame\n |  columns to use, the second to indicate the layer in the GeoJSON\n |  on which to key the data. The 'columns' keyword does not need to be\n |  passed for a Pandas series.\n |  \n |  Colors are generated from color brewer (https://colorbrewer2.org/)\n |  sequential palettes. By default, linear binning is used between\n |  the min and the max of the values. Custom binning can be achieved\n |  with the `bins` parameter.\n |  \n |  TopoJSONs can be passed as \"geo_data\", but the \"topojson\" keyword must\n |  also be passed with the reference to the topojson objects to convert.\n |  See the topojson.feature method in the TopoJSON API reference:\n |  https://github.com/topojson/topojson/wiki/API-Reference\n |  \n |  \n |  Parameters\n |  ----------\n |  geo_data: string/object\n |      URL, file path, or data (json, dict, geopandas, etc) to your GeoJSON\n |      geometries\n |  data: Pandas DataFrame or Series, default None\n |      Data to bind to the GeoJSON.\n |  columns: tuple with two values, default None\n |      If the data is a Pandas DataFrame, the columns of data to be bound.\n |      Must pass column 1 as the key, and column 2 the values.\n |  key_on: string, default None\n |      Variable in the `geo_data` GeoJSON file to bind the data to. Must\n |      start with 'feature' and be in JavaScript objection notation.\n |      Ex: 'feature.id' or 'feature.properties.statename'.\n |  bins: int or sequence of scalars or str, default 6\n |      If `bins` is an int, it defines the number of equal-width\n |      bins between the min and the max of the values.\n |      If `bins` is a sequence, it directly defines the bin edges.\n |      For more information on this parameter, have a look at\n |      numpy.histogram function.\n |  fill_color: string, optional\n |      Area fill color, defaults to blue. Can pass a hex code, color name,\n |      or if you are binding data, one of the following color brewer palettes:\n |      'BuGn', 'BuPu', 'GnBu', 'OrRd', 'PuBu', 'PuBuGn', 'PuRd', 'RdPu',\n |      'YlGn', 'YlGnBu', 'YlOrBr', and 'YlOrRd'.\n |  nan_fill_color: string, default 'black'\n |      Area fill color for nan or missing values.\n |      Can pass a hex code, color name.\n |  fill_opacity: float, default 0.6\n |      Area fill opacity, range 0-1.\n |  nan_fill_opacity: float, default fill_opacity\n |      Area fill opacity for nan or missing values, range 0-1.\n |  line_color: string, default 'black'\n |      GeoJSON geopath line color.\n |  line_weight: int, default 1\n |      GeoJSON geopath line weight.\n |  line_opacity: float, default 1\n |      GeoJSON geopath line opacity, range 0-1.\n |  legend_name: string, default empty string\n |      Title for data legend.\n |  topojson: string, default None\n |      If using a TopoJSON, passing \"objects.yourfeature\" to the topojson\n |      keyword argument will enable conversion to GeoJSON.\n |  smooth_factor: float, default None\n |      How much to simplify the polyline on each zoom level. More means\n |      better performance and smoother look, and less means more accurate\n |      representation. Leaflet defaults to 1.0.\n |  highlight: boolean, default False\n |      Enable highlight functionality when hovering over a GeoJSON area.\n |  use_jenks: bool, default False\n |      Use jenkspy to calculate bins using \"natural breaks\"\n |      (Fisher-Jenks algorithm). This is useful when your data is unevenly\n |      distributed.\n |  name : string, optional\n |      The name of the layer, as it will appear in LayerControls\n |  overlay : bool, default True\n |      Adds the layer as an optional overlay (True) or the base layer (False).\n |  control : bool, default True\n |      Whether the Layer will be included in LayerControls.\n |  show: bool, default True\n |      Whether the layer will be shown on opening.\n |  \n |  Returns\n |  -------\n |  GeoJSON data layer in obj.template_vars\n |  \n |  Examples\n |  --------\n |  &gt;&gt;&gt; Choropleth(geo_data=\"us-states.json\", line_color=\"blue\", line_weight=3)\n |  &gt;&gt;&gt; Choropleth(\n |  ...     geo_data=\"geo.json\",\n |  ...     data=df,\n |  ...     columns=[\"Data 1\", \"Data 2\"],\n |  ...     key_on=\"feature.properties.myvalue\",\n |  ...     fill_color=\"PuBu\",\n |  ...     bins=[0, 20, 30, 40, 50, 60],\n |  ... )\n |  &gt;&gt;&gt; Choropleth(geo_data=\"countries.json\", topojson=\"objects.countries\")\n |  &gt;&gt;&gt; Choropleth(\n |  ...     geo_data=\"geo.json\",\n |  ...     data=df,\n |  ...     columns=[\"Data 1\", \"Data 2\"],\n |  ...     key_on=\"feature.properties.myvalue\",\n |  ...     fill_color=\"PuBu\",\n |  ...     bins=[0, 20, 30, 40, 50, 60],\n |  ...     highlight=True,\n |  ... )\n |  \n |  Method resolution order:\n |      Choropleth\n |      folium.map.FeatureGroup\n |      folium.map.Layer\n |      branca.element.MacroElement\n |      branca.element.Element\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, geo_data: Any, data: Optional[Any] = None, columns: Optional[Sequence[Any]] = None, key_on: Optional[str] = None, bins: Union[int, Sequence[float]] = 6, fill_color: Optional[str] = None, nan_fill_color: str = 'black', fill_opacity: float = 0.6, nan_fill_opacity: Optional[float] = None, line_color: str = 'black', line_weight: float = 1, line_opacity: float = 1, name: Optional[str] = None, legend_name: str = '', overlay: bool = True, control: bool = True, show: bool = True, topojson: Optional[str] = None, smooth_factor: Optional[float] = None, highlight: bool = False, use_jenks: bool = False, **kwargs)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  render(self, **kwargs) -&gt; None\n |      Render the GeoJson/TopoJson and color scale objects.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from branca.element.Element:\n |  \n |  __getstate__(self)\n |      Modify object state when pickling the object.\n |      jinja2 Environment cannot be pickled, so set\n |      the ._env attribute to None. This will be added back\n |      when unpickling (see __setstate__)\n |  \n |  __setstate__(self, state: dict)\n |      Re-add ._env attribute when unpickling\n |  \n |  add_child(self, child, name=None, index=None)\n |      Add a child.\n |  \n |  add_children(self, child, name=None, index=None)\n |      Add a child.\n |  \n |  add_to(self, parent, name=None, index=None)\n |      Add element to a parent.\n |  \n |  get_bounds(self)\n |      Computes the bounds of the object and all it's children\n |      in the form [[lat_min, lon_min], [lat_max, lon_max]].\n |  \n |  get_name(self)\n |      Returns a string representation of the object.\n |      This string has to be unique and to be a python and\n |      javascript-compatible\n |      variable name.\n |  \n |  get_root(self)\n |      Returns the root of the elements tree.\n |  \n |  save(self, outfile, close_file=True, **kwargs)\n |      Saves an Element into a file.\n |      \n |      Parameters\n |      ----------\n |      outfile : str or file object\n |          The file (or filename) where you want to output the html.\n |      close_file : bool, default True\n |          Whether the file has to be closed after write.\n |  \n |  to_dict(self, depth=-1, ordered=True, **kwargs)\n |      Returns a dict representation of the object.\n |  \n |  to_json(self, depth=-1, **kwargs)\n |      Returns a JSON representation of the object.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from branca.element.Element:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)"
  },
  {
    "objectID": "posts/python/Files.html",
    "href": "posts/python/Files.html",
    "title": "Files (reading and writing)",
    "section": "",
    "text": "apath = \"cv_KK1.txt\"\n\nf= open (path, encoding = 'utf-8')\n\n\nfor line in f:\n    print(line)\n\nKunal Khurana\n\nEmail: Khuranasoilpau@gmail.com\n\nWebsite: ilovesoils.com\n\nLinkedIn: [https://www.linkedin.com/in/khuranasoils/]\n\nGitHub: [https://github.com/Kkhurana007]\n\nGoogle Scholar: [https://scholar.google.com/citations?hl=en&user=wpx6SKUAAAAJ]\n\n\n\nProfessional Summary\n\nDetail-oriented researcher with a strong background in statistical analysis, machine learning, and data science. Proficient in Python and modern data analysis tools with over 5 years of experience. M.Sc. in Soil Science. Fluent in English, French, Hindi, and Punjabi.\n\n\n\nSkills\n\n\n\nData Science and Machine Learning: Python (NumPy, Pandas, SciPy, Scikit-learn), TensorFlow, PyTorch, machine learning algorithms, deep learning, neural networks, statistical analysis.\n\n\n\nData Visualization: Matplotlib, Seaborn, Plotly, Power BI, Altair.\n\n\n\nDatabase: SQL (PostgreSQL, MySQL), NoSQL.\n\n\n\nNLP and Language Processing: Natural Language Processing algorithms, Neural Machine Translation (NMT).\n\n\n\nProgramming: Proficient in Python, software engineering principles, Flask.\n\n\n\nProblem Solving: Strong analytical and problem-solving skills.\n\n\n\nCommunication: Effective communicator, capable of conveying complex technical concepts.\n\n\n\nContinuous Learning: Committed to staying updated with the latest advancements in machine learning and related fields.\n\n\n\nProfessional Experience\n\n\n\nResearcher\n\nMcGill University, Montreal, CA\n\nSeptember 2022 - December 2023\n\n\n\nConducted data analysis and research in the field of soil science, focusing on climate trends and agricultural crop yield prediction.\n\nDeveloped predictive models using machine learning techniques.\n\nManaged and analyzed large datasets using Python and SQL databases.\n\nCollaborated with a multidisciplinary team to achieve research objectives.\n\n\n\nGraduate Research Assistant\n\nUniversité Laval, Quebec, CA\n\nJanuary 2019 - June 2022\n\n\n\nInvestigated the production of phenolics-rich biochar for controlling greenhouse gas emissions.\n\nManaged and analyzed wetland datasets for Exploratory Data Analysis using Python.\n\nCollaborated with a multidisciplinary team to optimize the production process, ensuring scalability and environmental sustainability.\n\nUtilized advanced statistical techniques to identify key patterns and trends within the wetland datasets, facilitating evidence-based recommendations.\n\nContributed to research publications and presentations.\n\n\n\nResearch Fellow\n\nPunjab Agricultural University, Ludhiana, IN\n\nOctober 2016 - December 2018\n\nImproved bio-efficacy of zinc in rice-wheat cropping systems to enhance grain quality and health.\n\nConducted surveys for observation, sampling, and mapping with ArcGIS.\n\nGained expertise in laboratory analysis using an Atomic Adsorption spectrophotometer for plant and elemental soil composition.\n\nConducted statistical hypothesis testing using SPSS.\n\nPrepared and submitted reports and presentations for the project.\n\n\n\nEducation\n\n\n\nMaster of Science in Soil Science\n\nPunjab Agricultural University, Ludhiana, IN\n\nJuly 2013- March 2016\n\n\n\nBachelor of Agriculture\n\nPunjabi University Patiala, Patiala, IN\n\nAugust 2009- May 2013\n\n\n\nVolunteer Experience\n\n\n\nParticipant\n\nClub d’IA, Quebec, CA\n\nMarch 2021 - Present\n\n\n\nInterests\n\n\n\nTriathlon\n\nReading\n\nTraveling\n\nThis functional CV highlights Kunal Khurana's skills, professional experience, education, and interests in a clear and structured format.\n\n\n\nlines = [x.rstrip() for x in open (path, encoding = \"utf-8\")]\n\nlines\n\n['Kunal Khurana',\n 'Email: Khuranasoilpau@gmail.com',\n 'Website: ilovesoils.com',\n 'LinkedIn: [https://www.linkedin.com/in/khuranasoils/]',\n 'GitHub: [https://github.com/Kkhurana007]',\n 'Google Scholar: [https://scholar.google.com/citations?hl=en&user=wpx6SKUAAAAJ]',\n '',\n 'Professional Summary',\n 'Detail-oriented researcher with a strong background in statistical analysis, machine learning, and data science. Proficient in Python and modern data analysis tools with over 5 years of experience. M.Sc. in Soil Science. Fluent in English, French, Hindi, and Punjabi.',\n '',\n 'Skills',\n '',\n 'Data Science and Machine Learning: Python (NumPy, Pandas, SciPy, Scikit-learn), TensorFlow, PyTorch, machine learning algorithms, deep learning, neural networks, statistical analysis.',\n '',\n 'Data Visualization: Matplotlib, Seaborn, Plotly, Power BI, Altair.',\n '',\n 'Database: SQL (PostgreSQL, MySQL), NoSQL.',\n '',\n 'NLP and Language Processing: Natural Language Processing algorithms, Neural Machine Translation (NMT).',\n '',\n 'Programming: Proficient in Python, software engineering principles, Flask.',\n '',\n 'Problem Solving: Strong analytical and problem-solving skills.',\n '',\n 'Communication: Effective communicator, capable of conveying complex technical concepts.',\n '',\n 'Continuous Learning: Committed to staying updated with the latest advancements in machine learning and related fields.',\n '',\n 'Professional Experience',\n '',\n 'Researcher',\n 'McGill University, Montreal, CA',\n 'September 2022 - December 2023',\n '',\n 'Conducted data analysis and research in the field of soil science, focusing on climate trends and agricultural crop yield prediction.',\n 'Developed predictive models using machine learning techniques.',\n 'Managed and analyzed large datasets using Python and SQL databases.',\n 'Collaborated with a multidisciplinary team to achieve research objectives.',\n '',\n 'Graduate Research Assistant',\n 'Université Laval, Quebec, CA',\n 'January 2019 - June 2022',\n '',\n 'Investigated the production of phenolics-rich biochar for controlling greenhouse gas emissions.',\n 'Managed and analyzed wetland datasets for Exploratory Data Analysis using Python.',\n 'Collaborated with a multidisciplinary team to optimize the production process, ensuring scalability and environmental sustainability.',\n 'Utilized advanced statistical techniques to identify key patterns and trends within the wetland datasets, facilitating evidence-based recommendations.',\n 'Contributed to research publications and presentations.',\n '',\n 'Research Fellow',\n 'Punjab Agricultural University, Ludhiana, IN',\n 'October 2016 - December 2018',\n 'Improved bio-efficacy of zinc in rice-wheat cropping systems to enhance grain quality and health.',\n 'Conducted surveys for observation, sampling, and mapping with ArcGIS.',\n 'Gained expertise in laboratory analysis using an Atomic Adsorption spectrophotometer for plant and elemental soil composition.',\n 'Conducted statistical hypothesis testing using SPSS.',\n 'Prepared and submitted reports and presentations for the project.',\n '',\n 'Education',\n '',\n 'Master of Science in Soil Science',\n 'Punjab Agricultural University, Ludhiana, IN',\n 'July 2013- March 2016',\n '',\n 'Bachelor of Agriculture',\n 'Punjabi University Patiala, Patiala, IN',\n 'August 2009- May 2013',\n '',\n 'Volunteer Experience',\n '',\n 'Participant',\n 'Club d’IA, Quebec, CA',\n 'March 2021 - Present',\n '',\n 'Interests',\n '',\n 'Triathlon',\n 'Reading',\n 'Traveling',\n \"This functional CV highlights Kunal Khurana's skills, professional experience, education, and interests in a clear and structured format.\"]\n\n\n\nf.close()\n\n\nf1 = open(path)\n\nf1.read(14)\n\n'Kunal Khurana\\n'\n\n\n\nf2 = open (path, mode = 'rb')  # binary mode\n\nf2.read(14)\n\nb'Kunal Khurana\\r'\n\n\n\nf1.tell()\n\n15\n\n\n\nf2.tell()\n\n14\n\n\n\n# check default encoding\n\nimport sys\nsys.getdefaultencoding()\n\n'utf-8'\n\n\n\nf1.seek(3)\n\n3\n\n\n\nf1.read(1)\n\n'a'\n\n\n\nf1.tell()\n\n4\n\n\n\nf1.close()\n\n\nf2.close()\n\n\n# bytes and unicode with files\n\nwith open(path) as f:\n    chars = f.read(13)\n\n\nchars\n\n'Kunal Khurana'\n\n\n\nlen(chars)\n\n13\n\n\n\nf = open(path, encoding = 'utf-8')\n\nf.read(5)\n\n'Kunal'\n\n\n\nf.read(1)\n\n' '\n\n\n\nf.seek(1)\n\n1"
  },
  {
    "objectID": "posts/python/Data_agg+group_operations-checkpoint.html",
    "href": "posts/python/Data_agg+group_operations-checkpoint.html",
    "title": "Group operations",
    "section": "",
    "text": "Iterating over groups\nSelecting a column or subset of columns\nGrouping with dictionaries and series\nGrouping with functions\nGrouping by Index levels"
  },
  {
    "objectID": "posts/python/Data_agg+group_operations-checkpoint.html#how-to-think-about-group-operations",
    "href": "posts/python/Data_agg+group_operations-checkpoint.html#how-to-think-about-group-operations",
    "title": "Group operations",
    "section": "",
    "text": "Iterating over groups\nSelecting a column or subset of columns\nGrouping with dictionaries and series\nGrouping with functions\nGrouping by Index levels"
  },
  {
    "objectID": "posts/python/Data_agg+group_operations-checkpoint.html#data-aggregation",
    "href": "posts/python/Data_agg+group_operations-checkpoint.html#data-aggregation",
    "title": "Group operations",
    "section": "Data Aggregation",
    "text": "Data Aggregation\n\nColumn-wise and Multiple Function Application\nReturning aggregated Data without Row Indexes"
  },
  {
    "objectID": "posts/python/Data_agg+group_operations-checkpoint.html#apply-general-split-apply-combine",
    "href": "posts/python/Data_agg+group_operations-checkpoint.html#apply-general-split-apply-combine",
    "title": "Group operations",
    "section": "Apply: General split-apply-combine",
    "text": "Apply: General split-apply-combine\n\nSuppressing the Group keys\nQuantile and Bucked analysis\nFilling missing and group specific values\nRandom sampling and permutation\ngroup Weighted Average and Correlation\ngroup wise linear regression"
  },
  {
    "objectID": "posts/python/Data_agg+group_operations-checkpoint.html#group-transformations-adn-unwrapped-groupbys",
    "href": "posts/python/Data_agg+group_operations-checkpoint.html#group-transformations-adn-unwrapped-groupbys",
    "title": "Group operations",
    "section": "Group Transformations adn ‘Unwrapped’ GroupBys",
    "text": "Group Transformations adn ‘Unwrapped’ GroupBys"
  },
  {
    "objectID": "posts/python/Data_agg+group_operations-checkpoint.html#pivot-tables-adn-cross-tabulation",
    "href": "posts/python/Data_agg+group_operations-checkpoint.html#pivot-tables-adn-cross-tabulation",
    "title": "Group operations",
    "section": "Pivot tables adn Cross-tabulation",
    "text": "Pivot tables adn Cross-tabulation\n\ncross-tabulations: crosstab\n\n\nimport numpy as np\nimport pandas as pd\n\n\ndf = pd.DataFrame({\"key1\": ['a', 'a', None, 'b', 'c'],\n                  'key2' : pd.Series([1, 2, 3, None, 5], \n                                     dtype='Int64'),\n                   'data1' : np.random.standard_normal(5),\n                   'data2' : np.random.standard_normal(5),\n                                     \n                    \n                                     })\n\n\ndf\n\n\n\n\n\n\n\n\nkey1\nkey2\ndata1\ndata2\n\n\n\n\n0\na\n1\n-0.949232\n-0.859027\n\n\n1\na\n2\n0.902370\n0.987569\n\n\n2\nNone\n3\n0.805328\n-0.892250\n\n\n3\nb\n&lt;NA&gt;\n0.269996\n0.291597\n\n\n4\nc\n5\n-1.341779\n-0.229023\n\n\n\n\n\n\n\n\n # mean of data1 using labels from key1\n    \ngrouped = df[\"data1\"].groupby(df['key1'])\n\n\ngrouped\n\n&lt;pandas.core.groupby.generic.SeriesGroupBy object at 0x0000018F93327910&gt;\n\n\n\nmeans = df['data1'].groupby([df['key1'], df[\"key2\"]]).mean()\n\n\nmeans\n\nkey1  key2\na     1      -0.949232\n      2       0.902370\nc     5      -1.341779\nName: data1, dtype: float64\n\n\n\nmeans.unstack()\n\n\n\n\n\n\n\nkey2\n1\n2\n5\n\n\nkey1\n\n\n\n\n\n\n\na\n-0.949232\n0.90237\nNaN\n\n\nc\nNaN\nNaN\n-1.341779\n\n\n\n\n\n\n\n\n# using Series as groupkeys\nstates = np.array(['PB', 'DL', 'UK', 'HP', 'UP',])\nyears = [2001, 2002, 2005, 2001, 2009,]\n\ndf['data1'].groupby([states, years]).mean()\n\nDL  2002    0.902370\nHP  2001    0.269996\nPB  2001   -0.949232\nUK  2005    0.805328\nUP  2009   -1.341779\nName: data1, dtype: float64\n\n\n\ngrouped.mean()\n\nkey1\na   -0.023431\nb    0.269996\nc   -1.341779\nName: data1, dtype: float64\n\n\n\nmeans2 = df['data1'].groupby([df['key1'], df['key2']]).mean()\n\nmeans2\n\nkey1  key2\na     1      -0.949232\n      2       0.902370\nc     5      -1.341779\nName: data1, dtype: float64\n\n\n\nmeans2.unstack()\n\n\n\n\n\n\n\nkey2\n1\n2\n5\n\n\nkey1\n\n\n\n\n\n\n\na\n-0.949232\n0.90237\nNaN\n\n\nc\nNaN\nNaN\n-1.341779\n\n\n\n\n\n\n\n\nmeans2.unstack(0)\n\n\n\n\n\n\n\nkey1\na\nc\n\n\nkey2\n\n\n\n\n\n\n1\n-0.949232\nNaN\n\n\n2\n0.902370\nNaN\n\n\n5\nNaN\n-1.341779\n\n\n\n\n\n\n\n\ndf.groupby('key1').mean()\n\n\n\n\n\n\n\n\nkey2\ndata1\ndata2\n\n\nkey1\n\n\n\n\n\n\n\na\n1.5\n-0.023431\n0.064271\n\n\nb\n&lt;NA&gt;\n0.269996\n0.291597\n\n\nc\n5.0\n-1.341779\n-0.229023\n\n\n\n\n\n\n\n\ndf\n\n\n\n\n\n\n\n\nkey1\nkey2\ndata1\ndata2\n\n\n\n\n0\na\n1\n-0.949232\n-0.859027\n\n\n1\na\n2\n0.902370\n0.987569\n\n\n2\nNone\n3\n0.805328\n-0.892250\n\n\n3\nb\n&lt;NA&gt;\n0.269996\n0.291597\n\n\n4\nc\n5\n-1.341779\n-0.229023\n\n\n\n\n\n\n\n\ndf.groupby(['key1', 'key2']).mean()\n\n\n\n\n\n\n\n\n\ndata1\ndata2\n\n\nkey1\nkey2\n\n\n\n\n\n\na\n1\n-0.949232\n-0.859027\n\n\n2\n0.902370\n0.987569\n\n\nc\n5\n-1.341779\n-0.229023\n\n\n\n\n\n\n\n\ndf.groupby(['key1', 'key2']).size()\n\nkey1  key2\na     1       1\n      2       1\nc     5       1\ndtype: int64\n\n\n\ndf.groupby('key1', dropna= False).size()\n\nkey1\na      2\nb      1\nc      1\nNaN    1\ndtype: int64\n\n\n\ndf.groupby(['key1', 'key2'], dropna=False).size()\n\nkey1  key2\na     1       1\n      2       1\nb     &lt;NA&gt;    1\nc     5       1\nNaN   3       1\ndtype: int64\n\n\n\ndf.groupby('key1').count()\n\n\n\n\n\n\n\n\nkey2\ndata1\ndata2\n\n\nkey1\n\n\n\n\n\n\n\na\n2\n2\n2\n\n\nb\n0\n1\n1\n\n\nc\n1\n1\n1\n\n\n\n\n\n\n\n\nIterating over groups\n\nfor name, group in df.groupby('key1'):\n    print(name)\n    print(group)\n\na\n  key1  key2     data1     data2\n0    a     1 -0.949232 -0.859027\n1    a     2  0.902370  0.987569\nb\n  key1  key2     data1     data2\n3    b  &lt;NA&gt;  0.269996  0.291597\nc\n  key1  key2     data1     data2\n4    c     5 -1.341779 -0.229023\n\n\n\n# in case of multiple key elements\nfor (k1, k2), group in df.groupby(['key1','key2']):\n    print((k1, k2))\n    print(group)\n\n('a', 1)\n  key1  key2     data1     data2\n0    a     1 -0.949232 -0.859027\n('a', 2)\n  key1  key2    data1     data2\n1    a     2  0.90237  0.987569\n('c', 5)\n  key1  key2     data1     data2\n4    c     5 -1.341779 -0.229023\n\n\n\ndictionary data in the form of pieces\n\n\npieces = {name : group for name, group in df.groupby('key1')}\npieces['b']\n\n\n\n\n\n\n\n\nkey1\nkey2\ndata1\ndata2\n\n\n\n\n3\nb\n&lt;NA&gt;\n0.269996\n0.291597\n\n\n\n\n\n\n\n\npieces['c']\n\n\n\n\n\n\n\n\nkey1\nkey2\ndata1\ndata2\n\n\n\n\n4\nc\n5\n-1.341779\n-0.229023\n\n\n\n\n\n\n\n\ngrouped = df.groupby({'key1': 'key','key2': 'key',\n                     'data1': 'data', 'data2': 'data'})\n\n\ngrouped\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x0000018F9795F9A0&gt;\n\n\n\nfor group_key, group_values in grouped:\n    print(group_key)\n    print(group_values)"
  },
  {
    "objectID": "posts/python/Data_agg+group_operations-checkpoint.html#selecting-a-column-or-subset-of-columns",
    "href": "posts/python/Data_agg+group_operations-checkpoint.html#selecting-a-column-or-subset-of-columns",
    "title": "Group operations",
    "section": "Selecting a Column or Subset of Columns",
    "text": "Selecting a Column or Subset of Columns\n\ndf.groupby('key1')['data1']\n\n&lt;pandas.core.groupby.generic.SeriesGroupBy object at 0x0000018F978AF040&gt;\n\n\n\ndf.groupby('key1')[['data2']]\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x0000018F97822580&gt;\n\n\n\n# this can also be called by-\ndf['data1'].groupby(df['key1'])\ndf['data2'].groupby(df['key1'])\n\n&lt;pandas.core.groupby.generic.SeriesGroupBy object at 0x0000018F978EE970&gt;\n\n\n\ndf.groupby(['key1', 'key2'])[['data2']].mean()\n\n\n\n\n\n\n\n\n\ndata2\n\n\nkey1\nkey2\n\n\n\n\n\na\n1\n-0.859027\n\n\n2\n0.987569\n\n\nc\n5\n-0.229023\n\n\n\n\n\n\n\n\ns_grouped = df.groupby(['key1', 'key2'])['data2']\n\n\ns_grouped\n\n&lt;pandas.core.groupby.generic.SeriesGroupBy object at 0x0000018F97770520&gt;\n\n\n\ns_grouped.mean()\n\nkey1  key2\na     1      -0.859027\n      2       0.987569\nc     5      -0.229023\nName: data2, dtype: float64\n\n\n\nGrouping with Dictionaries and Series\n\npeople = pd.DataFrame(np.random.standard_normal((5,5)),\n                     columns=['a', 'b', 'c', 'd','e'],\n                     index = ['Kunal', 'Rahul', 'Sachin', 'Sorav', 'Andrew'])\n\n\n# add few NA values\npeople.iloc[2:3, [1, 2]] = np.nan\n\n\npeople\n\n\n\n\n\n\n\n\na\nb\nc\nd\ne\n\n\n\n\nKunal\n0.168348\n-1.118059\n-0.800172\n-0.866632\n-0.197982\n\n\nRahul\n0.200227\n0.640015\n1.108393\n-0.085850\n-0.888133\n\n\nSachin\n0.847146\nNaN\nNaN\n0.463212\n-0.077719\n\n\nSorav\n1.440603\n0.431537\n-0.670088\n-0.121215\n-1.211302\n\n\nAndrew\n-0.637420\n0.237133\n-0.426589\n-0.631543\n0.098273\n\n\n\n\n\n\n\n\nmapping = {'a': 'red', 'b': 'red', 'c': 'blue',\n          'd': 'blue', 'e': 'red', 'f': 'orange'}\n\n\nby_column = people.groupby(mapping, axis= 'columns')\nby_column.sum()\n\n\n\n\n\n\n\n\nblue\nred\n\n\n\n\nKunal\n-1.666804\n-1.147693\n\n\nRahul\n1.022543\n-0.047891\n\n\nSachin\n0.463212\n0.769427\n\n\nSorav\n-0.791303\n0.660838\n\n\nAndrew\n-1.058132\n-0.302014\n\n\n\n\n\n\n\n\n# same functionality holds for Series\n\nmap_series = pd.Series(mapping)\n\nmap_series\n\na       red\nb       red\nc      blue\nd      blue\ne       red\nf    orange\ndtype: object\n\n\n\npeople.groupby(map_series, axis= 'columns').count()\n\n\n\n\n\n\n\n\nblue\nred\n\n\n\n\nKunal\n2\n3\n\n\nRahul\n2\n3\n\n\nSachin\n1\n2\n\n\nSorav\n2\n3\n\n\nAndrew\n2\n3\n\n\n\n\n\n\n\n\n\nGrouping with Functions\n\npeople.groupby(len).sum()\n\n\n\n\n\n\n\n\na\nb\nc\nd\ne\n\n\n\n\n5\n1.809178\n-0.046507\n-0.361866\n-1.073697\n-2.297416\n\n\n6\n0.209726\n0.237133\n-0.426589\n-0.168331\n0.020554\n\n\n\n\n\n\n\n\n# mixing functions with arrays, dicitonaries, or Series\n\nkey_list = ['one', 'one', 'two', 'one', 'two']\n\npeople.groupby([len, key_list]).min()\n\n\n\n\n\n\n\n\n\na\nb\nc\nd\ne\n\n\n\n\n5\none\n0.168348\n-1.118059\n-0.800172\n-0.866632\n-1.211302\n\n\n6\ntwo\n-0.637420\n0.237133\n-0.426589\n-0.631543\n-0.077719\n\n\n\n\n\n\n\n\n\nGrouping with Index levels\n\nlevels = pd.MultiIndex.from_arrays([['IN', 'US', 'CAN'],\n                                   [1,2,3]],\n                                   names=['fdk', 'goleala'])\n\n\nhier_df = pd.DataFrame(np.random.standard_normal((4,3)),\n                      columns = levels)\n\nhier_df\n\n\n\n\n\n\n\nfdk\nIN\nUS\nCAN\n\n\ngoleala\n1\n2\n3\n\n\n\n\n0\n-0.973483\n0.175499\n-1.375942\n\n\n1\n-0.417278\n-0.593448\n-0.097291\n\n\n2\n-1.864057\n-1.157927\n0.219800\n\n\n3\n-0.062763\n0.796581\n-0.905844\n\n\n\n\n\n\n\n\n# to groupby level, pass level keyword\n\n\nhier_df.groupby(level = 'fdk', axis = 'columns').count()\n\n\n\n\n\n\n\nfdk\nCAN\nIN\nUS\n\n\n\n\n0\n1\n1\n1\n\n\n1\n1\n1\n1\n\n\n2\n1\n1\n1\n\n\n3\n1\n1\n1"
  },
  {
    "objectID": "posts/python/Data_agg+group_operations-checkpoint.html#data-aggregation-1",
    "href": "posts/python/Data_agg+group_operations-checkpoint.html#data-aggregation-1",
    "title": "Group operations",
    "section": "Data Aggregation",
    "text": "Data Aggregation\n\ntransformation that produces scalar values\noptimised methods\n\n\ndf\n\n\n\n\n\n\n\n\nkey1\nkey2\ndata1\ndata2\n\n\n\n\n0\na\n1\n-0.949232\n-0.859027\n\n\n1\na\n2\n0.902370\n0.987569\n\n\n2\nNone\n3\n0.805328\n-0.892250\n\n\n3\nb\n&lt;NA&gt;\n0.269996\n0.291597\n\n\n4\nc\n5\n-1.341779\n-0.229023\n\n\n\n\n\n\n\n\ngrouped = df.groupby('key1')\n\n\ngrouped['data1'].nsmallest(2)\n\nkey1   \na     0   -0.949232\n      1    0.902370\nb     3    0.269996\nc     4   -1.341779\nName: data1, dtype: float64\n\n\n\n# aggregation function\ndef peak_to_peak(arr):\n    return arr.max() - arr.min()\n\n\ngrouped.agg(peak_to_peak)\n\n\n\n\n\n\n\n\nkey2\ndata1\ndata2\n\n\nkey1\n\n\n\n\n\n\n\na\n1\n1.851602\n1.846596\n\n\nb\n&lt;NA&gt;\n0.000000\n0.000000\n\n\nc\n0\n0.000000\n0.000000\n\n\n\n\n\n\n\n\ngrouped.describe()\n\n\n\n\n\n\n\n\nkey2\ndata1\ndata2\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\ncount\nmean\n...\n75%\nmax\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nkey1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\na\n2.0\n1.5\n0.707107\n1.0\n1.25\n1.5\n1.75\n2.0\n2.0\n-0.023431\n...\n0.439470\n0.902370\n2.0\n0.064271\n1.305741\n-0.859027\n-0.397378\n0.064271\n0.525920\n0.987569\n\n\nb\n0.0\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n1.0\n0.269996\n...\n0.269996\n0.269996\n1.0\n0.291597\nNaN\n0.291597\n0.291597\n0.291597\n0.291597\n0.291597\n\n\nc\n1.0\n5.0\n&lt;NA&gt;\n5.0\n5.0\n5.0\n5.0\n5.0\n1.0\n-1.341779\n...\n-1.341779\n-1.341779\n1.0\n-0.229023\nNaN\n-0.229023\n-0.229023\n-0.229023\n-0.229023\n-0.229023\n\n\n\n\n3 rows × 24 columns\n\n\n\n\nColumn-Wise multiple function application\n\niris = pd.read_csv(r\"E:\\pythonfordatanalysis\\semainedu26fevrier\\iris.csv\")\n\n\niris.head()\n\n\n\n\n\n\n\n\nId\nSepal Length (cm)\nSepal Width (cm)\nPetal Length (cm)\nPetal Width (cm)\nSpecies\n\n\n\n\n0\n1\n5.1\n3.5\n1.4\n0.2\nIris-setosa\n\n\n1\n2\n4.9\n3.0\n1.4\n0.2\nIris-setosa\n\n\n2\n3\n4.7\n3.2\n1.3\n0.2\nIris-setosa\n\n\n3\n4\n4.6\n3.1\n1.5\n0.2\nIris-setosa\n\n\n4\n5\n5.0\n3.6\n1.4\n0.2\nIris-setosa\n\n\n\n\n\n\n\n\niris.columns\n\nIndex(['Id', 'Sepal Length (cm)', 'Sepal Width (cm)', 'Petal Length (cm)',\n       'Petal Width (cm)', 'Species'],\n      dtype='object')\n\n\n\ngrouped2 = iris.groupby(['Sepal Length (cm)', 'Sepal Width (cm)', 'Petal Length (cm)',\n       'Petal Width (cm)'])\n\n\nmissing_values = iris.isna()\n\nmissing_values.sum = iris.isna().sum()\n\nmissing_values_percent = (iris.isna().sum() / len(iris)) * 100\n\niris.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 150 entries, 0 to 149\nData columns (total 6 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   Id                 150 non-null    int64  \n 1   Sepal Length (cm)  150 non-null    float64\n 2   Sepal Width (cm)   150 non-null    float64\n 3   Petal Length (cm)  150 non-null    float64\n 4   Petal Width (cm)   150 non-null    float64\n 5   Species            150 non-null    object \ndtypes: float64(4), int64(1), object(1)\nmemory usage: 7.2+ KB\n\n\n\ngrouped.agg(['mean', 'std', peak_to_peak])\n\n\n\n\n\n\n\n\nkey2\ndata1\ndata2\n\n\n\nmean\nstd\npeak_to_peak\nmean\nstd\npeak_to_peak\nmean\nstd\npeak_to_peak\n\n\nkey1\n\n\n\n\n\n\n\n\n\n\n\n\n\na\n1.5\n0.707107\n1\n-0.023431\n1.30928\n1.851602\n0.064271\n1.305741\n1.846596\n\n\nb\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n0.269996\nNaN\n0.000000\n0.291597\nNaN\n0.000000\n\n\nc\n5.0\n&lt;NA&gt;\n0\n-1.341779\nNaN\n0.000000\n-0.229023\nNaN\n0.000000\n\n\n\n\n\n\n\n\n# dropping column as it gave traceback before\niris2 = iris.drop('Species', axis=1)\n\n\niris2.agg(['mean', 'std', peak_to_peak])\n\n\n\n\n\n\n\n\nId\nSepal Length (cm)\nSepal Width (cm)\nPetal Length (cm)\nPetal Width (cm)\n\n\n\n\nmean\n75.500000\n5.843333\n3.054000\n3.758667\n1.198667\n\n\nstd\n43.445368\n0.828066\n0.433594\n1.764420\n0.763161\n\n\npeak_to_peak\n149.000000\n3.600000\n2.400000\n5.900000\n2.400000"
  },
  {
    "objectID": "posts/python/Data_agg+group_operations-checkpoint.html#apply-general-split-apply-combine-1",
    "href": "posts/python/Data_agg+group_operations-checkpoint.html#apply-general-split-apply-combine-1",
    "title": "Group operations",
    "section": "Apply: General split-apply-combine",
    "text": "Apply: General split-apply-combine\n\ndef top(iris2, n=5, column = 'Sepal Length (cm)'):\n    return iris2.sort_values(column, ascending= False)[:n]\n\n\ntop(iris2, n=6)\n\n\n\n\n\n\n\n\nId\nSepal Length (cm)\nSepal Width (cm)\nPetal Length (cm)\nPetal Width (cm)\n\n\n\n\n131\n132\n7.9\n3.8\n6.4\n2.0\n\n\n135\n136\n7.7\n3.0\n6.1\n2.3\n\n\n122\n123\n7.7\n2.8\n6.7\n2.0\n\n\n117\n118\n7.7\n3.8\n6.7\n2.2\n\n\n118\n119\n7.7\n2.6\n6.9\n2.3\n\n\n105\n106\n7.6\n3.0\n6.6\n2.1\n\n\n\n\n\n\n\n\niris2.groupby('Sepal Length (cm)').apply(top)\n\n\n\n\n\n\n\n\n\nId\nSepal Length (cm)\nSepal Width (cm)\nPetal Length (cm)\nPetal Width (cm)\n\n\nSepal Length (cm)\n\n\n\n\n\n\n\n\n\n\n4.3\n13\n14\n4.3\n3.0\n1.1\n0.1\n\n\n4.4\n8\n9\n4.4\n2.9\n1.4\n0.2\n\n\n38\n39\n4.4\n3.0\n1.3\n0.2\n\n\n42\n43\n4.4\n3.2\n1.3\n0.2\n\n\n4.5\n41\n42\n4.5\n2.3\n1.3\n0.3\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n7.7\n117\n118\n7.7\n3.8\n6.7\n2.2\n\n\n118\n119\n7.7\n2.6\n6.9\n2.3\n\n\n122\n123\n7.7\n2.8\n6.7\n2.0\n\n\n135\n136\n7.7\n3.0\n6.1\n2.3\n\n\n7.9\n131\n132\n7.9\n3.8\n6.4\n2.0\n\n\n\n\n120 rows × 5 columns\n\n\n\n\n# recall by groupby object\n\nresult = iris2.groupby(\"Sepal Length (cm)\").describe()\n\nresult\n\n\n\n\n\n\n\n\nId\nSepal Width (cm)\n...\nPetal Length (cm)\nPetal Width (cm)\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\ncount\nmean\n...\n75%\nmax\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nSepal Length (cm)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.3\n1.0\n14.000000\nNaN\n14.0\n14.00\n14.0\n14.00\n14.0\n1.0\n3.000000\n...\n1.100\n1.1\n1.0\n0.100000\nNaN\n0.1\n0.100\n0.10\n0.100\n0.1\n\n\n4.4\n3.0\n30.333333\n18.583146\n9.0\n24.00\n39.0\n41.00\n43.0\n3.0\n3.033333\n...\n1.350\n1.4\n3.0\n0.200000\n3.399350e-17\n0.2\n0.200\n0.20\n0.200\n0.2\n\n\n4.5\n1.0\n42.000000\nNaN\n42.0\n42.00\n42.0\n42.00\n42.0\n1.0\n2.300000\n...\n1.300\n1.3\n1.0\n0.300000\nNaN\n0.3\n0.300\n0.30\n0.300\n0.3\n\n\n4.6\n4.0\n20.500000\n20.141168\n4.0\n6.25\n15.0\n29.25\n48.0\n4.0\n3.325000\n...\n1.425\n1.5\n4.0\n0.225000\n5.000000e-02\n0.2\n0.200\n0.20\n0.225\n0.3\n\n\n4.7\n2.0\n16.500000\n19.091883\n3.0\n9.75\n16.5\n23.25\n30.0\n2.0\n3.200000\n...\n1.525\n1.6\n2.0\n0.200000\n0.000000e+00\n0.2\n0.200\n0.20\n0.200\n0.2\n\n\n4.8\n5.0\n25.400000\n14.046352\n12.0\n13.00\n25.0\n31.00\n46.0\n5.0\n3.180000\n...\n1.600\n1.9\n5.0\n0.200000\n7.071068e-02\n0.1\n0.200\n0.20\n0.200\n0.3\n\n\n4.9\n6.0\n41.666667\n37.866432\n2.0\n16.25\n36.5\n53.00\n107.0\n6.0\n2.866667\n...\n2.850\n4.5\n6.0\n0.533333\n6.713171e-01\n0.1\n0.100\n0.15\n0.800\n1.7\n\n\n5.0\n10.0\n39.200000\n26.029044\n5.0\n26.25\n38.5\n48.50\n94.0\n10.0\n3.120000\n...\n1.600\n3.5\n10.0\n0.430000\n3.267687e-01\n0.2\n0.200\n0.25\n0.550\n1.0\n\n\n5.1\n9.0\n35.111111\n28.117808\n1.0\n20.00\n24.0\n45.00\n99.0\n9.0\n3.477778\n...\n1.700\n3.0\n9.0\n0.400000\n2.828427e-01\n0.2\n0.200\n0.30\n0.400\n1.1\n\n\n5.2\n4.0\n37.500000\n15.154757\n28.0\n28.75\n31.0\n39.75\n60.0\n4.0\n3.425000\n...\n2.100\n3.9\n4.0\n0.475000\n6.184658e-01\n0.1\n0.175\n0.20\n0.500\n1.4\n\n\n5.3\n1.0\n49.000000\nNaN\n49.0\n49.00\n49.0\n49.00\n49.0\n1.0\n3.700000\n...\n1.500\n1.5\n1.0\n0.200000\nNaN\n0.2\n0.200\n0.20\n0.200\n0.2\n\n\n5.4\n6.0\n28.666667\n29.001149\n6.0\n12.50\n19.0\n29.25\n85.0\n6.0\n3.550000\n...\n1.700\n4.5\n6.0\n0.516667\n4.915960e-01\n0.2\n0.250\n0.40\n0.400\n1.5\n\n\n5.5\n7.0\n67.000000\n24.779023\n34.0\n45.50\n81.0\n86.00\n91.0\n7.0\n2.842857\n...\n4.000\n4.4\n7.0\n0.900000\n4.898979e-01\n0.2\n0.600\n1.10\n1.250\n1.3\n\n\n5.6\n6.0\n84.666667\n22.060523\n65.0\n67.75\n79.5\n93.50\n122.0\n6.0\n2.816667\n...\n4.425\n4.9\n6.0\n1.416667\n3.125167e-01\n1.1\n1.300\n1.30\n1.450\n2.0\n\n\n5.7\n8.0\n72.250000\n37.821951\n16.0\n46.75\n88.0\n97.75\n114.0\n8.0\n3.100000\n...\n4.275\n5.0\n8.0\n1.100000\n5.451081e-01\n0.3\n0.850\n1.25\n1.300\n2.0\n\n\n5.8\n7.0\n88.428571\n40.265192\n15.0\n75.50\n93.0\n108.50\n143.0\n7.0\n2.885714\n...\n5.100\n5.1\n7.0\n1.400000\n7.280110e-01\n0.2\n1.100\n1.20\n1.900\n2.4\n\n\n5.9\n3.0\n94.333333\n48.418316\n62.0\n66.50\n71.0\n110.50\n150.0\n3.0\n3.066667\n...\n4.950\n5.1\n3.0\n1.700000\n1.732051e-01\n1.5\n1.650\n1.80\n1.800\n1.8\n\n\n6.0\n6.0\n95.166667\n28.435307\n63.0\n80.25\n85.0\n111.50\n139.0\n6.0\n2.733333\n...\n4.950\n5.1\n6.0\n1.500000\n2.683282e-01\n1.0\n1.500\n1.55\n1.600\n1.8\n\n\n6.1\n6.0\n94.166667\n30.413265\n64.0\n72.50\n83.0\n119.00\n135.0\n6.0\n2.850000\n...\n4.850\n5.6\n6.0\n1.416667\n2.041241e-01\n1.2\n1.325\n1.40\n1.400\n1.8\n\n\n6.2\n4.0\n110.750000\n34.798228\n69.0\n90.75\n112.5\n132.50\n149.0\n4.0\n2.825000\n...\n4.950\n5.4\n4.0\n1.725000\n4.349329e-01\n1.3\n1.450\n1.65\n1.925\n2.3\n\n\n6.3\n9.0\n107.222222\n30.780586\n57.0\n88.00\n104.0\n134.00\n147.0\n9.0\n2.855556\n...\n5.600\n6.0\n9.0\n1.811111\n4.075673e-01\n1.3\n1.500\n1.80\n1.900\n2.5\n\n\n6.4\n7.0\n107.857143\n32.328669\n52.0\n93.50\n116.0\n131.00\n138.0\n7.0\n2.957143\n...\n5.550\n5.6\n7.0\n1.871429\n3.683942e-01\n1.3\n1.650\n1.90\n2.150\n2.3\n\n\n6.5\n5.0\n107.200000\n33.558903\n55.0\n105.00\n111.0\n117.00\n148.0\n5.0\n3.000000\n...\n5.500\n5.8\n5.0\n1.900000\n2.645751e-01\n1.5\n1.800\n2.00\n2.000\n2.2\n\n\n6.6\n2.0\n67.500000\n12.020815\n59.0\n63.25\n67.5\n71.75\n76.0\n2.0\n2.950000\n...\n4.550\n4.6\n2.0\n1.350000\n7.071068e-02\n1.3\n1.325\n1.35\n1.375\n1.4\n\n\n6.7\n8.0\n112.125000\n31.984092\n66.0\n84.75\n117.0\n142.00\n146.0\n8.0\n3.050000\n...\n5.700\n5.8\n8.0\n1.962500\n4.206712e-01\n1.4\n1.650\n1.95\n2.325\n2.5\n\n\n6.8\n3.0\n111.333333\n33.531080\n77.0\n95.00\n113.0\n128.50\n144.0\n3.0\n3.000000\n...\n5.700\n5.9\n3.0\n1.933333\n4.725816e-01\n1.4\n1.750\n2.10\n2.200\n2.3\n\n\n6.9\n4.0\n114.000000\n41.753243\n53.0\n104.00\n130.5\n140.50\n142.0\n4.0\n3.125000\n...\n5.475\n5.7\n4.0\n2.050000\n3.785939e-01\n1.5\n1.950\n2.20\n2.300\n2.3\n\n\n7.0\n1.0\n51.000000\nNaN\n51.0\n51.00\n51.0\n51.00\n51.0\n1.0\n3.200000\n...\n4.700\n4.7\n1.0\n1.400000\nNaN\n1.4\n1.400\n1.40\n1.400\n1.4\n\n\n7.1\n1.0\n103.000000\nNaN\n103.0\n103.00\n103.0\n103.00\n103.0\n1.0\n3.000000\n...\n5.900\n5.9\n1.0\n2.100000\nNaN\n2.1\n2.100\n2.10\n2.100\n2.1\n\n\n7.2\n3.0\n122.000000\n10.583005\n110.0\n118.00\n126.0\n128.00\n130.0\n3.0\n3.266667\n...\n6.050\n6.1\n3.0\n1.966667\n4.725816e-01\n1.6\n1.700\n1.80\n2.150\n2.5\n\n\n7.3\n1.0\n108.000000\nNaN\n108.0\n108.00\n108.0\n108.00\n108.0\n1.0\n2.900000\n...\n6.300\n6.3\n1.0\n1.800000\nNaN\n1.8\n1.800\n1.80\n1.800\n1.8\n\n\n7.4\n1.0\n131.000000\nNaN\n131.0\n131.00\n131.0\n131.00\n131.0\n1.0\n2.800000\n...\n6.100\n6.1\n1.0\n1.900000\nNaN\n1.9\n1.900\n1.90\n1.900\n1.9\n\n\n7.6\n1.0\n106.000000\nNaN\n106.0\n106.00\n106.0\n106.00\n106.0\n1.0\n3.000000\n...\n6.600\n6.6\n1.0\n2.100000\nNaN\n2.1\n2.100\n2.10\n2.100\n2.1\n\n\n7.7\n4.0\n124.000000\n8.286535\n118.0\n118.75\n121.0\n126.25\n136.0\n4.0\n3.050000\n...\n6.750\n6.9\n4.0\n2.200000\n1.414214e-01\n2.0\n2.150\n2.25\n2.300\n2.3\n\n\n7.9\n1.0\n132.000000\nNaN\n132.0\n132.00\n132.0\n132.00\n132.0\n1.0\n3.800000\n...\n6.400\n6.4\n1.0\n2.000000\nNaN\n2.0\n2.000\n2.00\n2.000\n2.0\n\n\n\n\n35 rows × 32 columns\n\n\n\n\n# invoking a method inside GroupBy\ndef f(group):\n    return group.describe()\ngrouped.apply(f)\n\n\n\n\n\n\n\n\n\nkey2\ndata1\ndata2\n\n\nkey1\n\n\n\n\n\n\n\n\na\ncount\n2.0\n2.000000\n2.000000\n\n\nmean\n1.5\n-0.023431\n0.064271\n\n\nstd\n0.707107\n1.309280\n1.305741\n\n\nmin\n1.0\n-0.949232\n-0.859027\n\n\n25%\n1.25\n-0.486331\n-0.397378\n\n\n50%\n1.5\n-0.023431\n0.064271\n\n\n75%\n1.75\n0.439470\n0.525920\n\n\nmax\n2.0\n0.902370\n0.987569\n\n\nb\ncount\n0.0\n1.000000\n1.000000\n\n\nmean\n&lt;NA&gt;\n0.269996\n0.291597\n\n\nstd\n&lt;NA&gt;\nNaN\nNaN\n\n\nmin\n&lt;NA&gt;\n0.269996\n0.291597\n\n\n25%\n&lt;NA&gt;\n0.269996\n0.291597\n\n\n50%\n&lt;NA&gt;\n0.269996\n0.291597\n\n\n75%\n&lt;NA&gt;\n0.269996\n0.291597\n\n\nmax\n&lt;NA&gt;\n0.269996\n0.291597\n\n\nc\ncount\n1.0\n1.000000\n1.000000\n\n\nmean\n5.0\n-1.341779\n-0.229023\n\n\nstd\n&lt;NA&gt;\nNaN\nNaN\n\n\nmin\n5.0\n-1.341779\n-0.229023\n\n\n25%\n5.0\n-1.341779\n-0.229023\n\n\n50%\n5.0\n-1.341779\n-0.229023\n\n\n75%\n5.0\n-1.341779\n-0.229023\n\n\nmax\n5.0\n-1.341779\n-0.229023\n\n\n\n\n\n\n\n\niris2.apply(f)\n\n\n\n\n\n\n\n\nId\nSepal Length (cm)\nSepal Width (cm)\nPetal Length (cm)\nPetal Width (cm)\n\n\n\n\ncount\n150.000000\n150.000000\n150.000000\n150.000000\n150.000000\n\n\nmean\n75.500000\n5.843333\n3.054000\n3.758667\n1.198667\n\n\nstd\n43.445368\n0.828066\n0.433594\n1.764420\n0.763161\n\n\nmin\n1.000000\n4.300000\n2.000000\n1.000000\n0.100000\n\n\n25%\n38.250000\n5.100000\n2.800000\n1.600000\n0.300000\n\n\n50%\n75.500000\n5.800000\n3.000000\n4.350000\n1.300000\n\n\n75%\n112.750000\n6.400000\n3.300000\n5.100000\n1.800000\n\n\nmax\n150.000000\n7.900000\n4.400000\n6.900000\n2.500000\n\n\n\n\n\n\n\n\nSuppressing the Group Keys\n\niris2.groupby('Sepal Length (cm)', group_keys = False).apply(top)\n\n\n\n\n\n\n\n\nId\nSepal Length (cm)\nSepal Width (cm)\nPetal Length (cm)\nPetal Width (cm)\n\n\n\n\n13\n14\n4.3\n3.0\n1.1\n0.1\n\n\n8\n9\n4.4\n2.9\n1.4\n0.2\n\n\n38\n39\n4.4\n3.0\n1.3\n0.2\n\n\n42\n43\n4.4\n3.2\n1.3\n0.2\n\n\n41\n42\n4.5\n2.3\n1.3\n0.3\n\n\n...\n...\n...\n...\n...\n...\n\n\n117\n118\n7.7\n3.8\n6.7\n2.2\n\n\n118\n119\n7.7\n2.6\n6.9\n2.3\n\n\n122\n123\n7.7\n2.8\n6.7\n2.0\n\n\n135\n136\n7.7\n3.0\n6.1\n2.3\n\n\n131\n132\n7.9\n3.8\n6.4\n2.0\n\n\n\n\n120 rows × 5 columns\n\n\n\n\n\nQuantile and Bucket analysis\n\nframe = pd.DataFrame({'data1': np.random.standard_normal(1000),\n                     'data2': np.random.standard_normal(1000)})\n\nframe.head()\n\n\n\n\n\n\n\n\ndata1\ndata2\n\n\n\n\n0\n-0.218315\n-1.593930\n\n\n1\n2.851876\n0.110652\n\n\n2\n-2.136658\n-0.057209\n\n\n3\n1.380738\n-1.014417\n\n\n4\n0.375207\n0.910869\n\n\n\n\n\n\n\n\nquartiles = pd.cut(frame['data1'],4)\n\n\nquartiles.head()\n\n0     (-1.6, 0.127]\n1    (1.854, 3.581]\n2    (-3.333, -1.6]\n3    (0.127, 1.854]\n4    (0.127, 1.854]\nName: data1, dtype: category\nCategories (4, interval[float64, right]): [(-3.333, -1.6] &lt; (-1.6, 0.127] &lt; (0.127, 1.854] &lt; (1.854, 3.581]]\n\n\n\ndef get_stats(group):\n    return pd.DataFrame({\n        'min': group.min(), 'max':group.max(),\n        'count':group.count(), \"mean\":group.mean()\n    })\n\n\ngrouped3 = frame.groupby(quartiles)\n\n\ngrouped3.apply(get_stats)\n\n\n\n\n\n\n\n\n\nmin\nmax\ncount\nmean\n\n\ndata1\n\n\n\n\n\n\n\n\n\n(-3.333, -1.6]\ndata1\n-3.326377\n-1.615895\n47\n-2.008459\n\n\ndata2\n-2.449983\n1.891096\n47\n-0.031678\n\n\n(-1.6, 0.127]\ndata1\n-1.592835\n0.127111\n499\n-0.559453\n\n\ndata2\n-3.688831\n2.891848\n499\n0.060442\n\n\n(0.127, 1.854]\ndata1\n0.129218\n1.845841\n419\n0.767489\n\n\ndata2\n-3.107630\n3.210868\n419\n0.063923\n\n\n(1.854, 3.581]\ndata1\n1.860336\n3.580781\n35\n2.272007\n\n\ndata2\n-1.915169\n1.595983\n35\n-0.040774\n\n\n\n\n\n\n\n\n# the same result can also be computed with;\ngrouped3.agg({'min', 'max', 'count', 'mean'})\n\n\n\n\n\n\n\n\ndata1\ndata2\n\n\n\nmax\ncount\nmin\nmean\nmax\ncount\nmin\nmean\n\n\ndata1\n\n\n\n\n\n\n\n\n\n\n\n\n(-3.333, -1.6]\n-1.615895\n47\n-3.326377\n-2.008459\n1.891096\n47\n-2.449983\n-0.031678\n\n\n(-1.6, 0.127]\n0.127111\n499\n-1.592835\n-0.559453\n2.891848\n499\n-3.688831\n0.060442\n\n\n(0.127, 1.854]\n1.845841\n419\n0.129218\n0.767489\n3.210868\n419\n-3.107630\n0.063923\n\n\n(1.854, 3.581]\n3.580781\n35\n1.860336\n2.272007\n1.595983\n35\n-1.915169\n-0.040774\n\n\n\n\n\n\n\n\n# using pandas.qcut\n\nquartiles_samp = pd.qcut(frame['data1'], 4, labels= False)\n\nquartiles_samp.head()\n\n0    1\n1    3\n2    0\n3    3\n4    2\nName: data1, dtype: int64\n\n\n\ngrouped4 = frame.groupby(quartiles_samp)\n\n\ngrouped4.apply(get_stats)\n\n\n\n\n\n\n\n\n\nmin\nmax\ncount\nmean\n\n\ndata1\n\n\n\n\n\n\n\n\n\n0\ndata1\n-3.326377\n-0.634348\n250\n-1.220039\n\n\ndata2\n-2.449983\n2.891848\n250\n0.170686\n\n\n1\ndata1\n-0.629175\n0.018952\n250\n-0.288017\n\n\ndata2\n-2.673280\n2.854084\n250\n-0.041068\n\n\n2\ndata1\n0.023148\n0.676789\n250\n0.326941\n\n\ndata2\n-3.688831\n3.210868\n250\n0.070652\n\n\n3\ndata1\n0.681385\n3.580781\n250\n1.291250\n\n\ndata2\n-3.107630\n2.653283\n250\n0.015843\n\n\n\n\n\n\n\n\n\nEx: Filling missing values with group-spscific values\n\n\ns = pd.Series(np.random.standard_normal(6))\n\ns[::2] = np.nan\n\ns\n\n0         NaN\n1   -0.860730\n2         NaN\n3    1.412749\n4         NaN\n5    0.419197\ndtype: float64\n\n\n\ns.fillna(s.mean())\n\n0    0.323739\n1   -0.860730\n2    0.323739\n3    1.412749\n4    0.323739\n5    0.419197\ndtype: float64\n\n\n\nstates = ['MP', 'UP', 'MH', 'RJ',\n         'HP', 'UK', 'PB', 'KR']\n\ngroup_key = ['Centre', 'Centre', 'Centre', 'Centre',\n            'North', 'North', \"North\", \"North\"]\n\n\ndata4 = pd.Series(np.random.standard_normal(8),\n                index= states)\n\n\ndata4\n\nMP    0.712152\nUP    0.211319\nMH    0.925109\nRJ   -0.204536\nHP    0.293047\nUK    0.497221\nPB    0.034935\nKR   -0.842300\ndtype: float64\n\n\n\n# missing data for states\n\ndata4[['MP', \"PB\", \"KR\"]] = np.nan\n\n\ndata4\n\nMP         NaN\nUP    0.211319\nMH    0.925109\nRJ   -0.204536\nHP    0.293047\nUK    0.497221\nPB         NaN\nKR         NaN\ndtype: float64\n\n\n\ndata4.groupby(group_key).size()\n\nCentre    4\nNorth     4\ndtype: int64\n\n\n\ndata4.groupby(group_key).mean()\n\nCentre    0.310630\nNorth     0.395134\ndtype: float64\n\n\n\ndata4.groupby(group_key).count()\n\nCentre    3\nNorth     2\ndtype: int64\n\n\n\ndata4.groupby(group_key).mean()\n\nCentre    0.310630\nNorth     0.395134\ndtype: float64\n\n\n\n# filling NA values from group means\ndef fill_mean(group):\n    return group.fillna(group.mean())\n\ndata.groupby(group_key).apply(fill_mean)\n\nCentre  MP   -1.190913\n        UP    0.136560\n        MH   -0.046813\n        RJ    0.962568\nNorth   HP    0.477671\n        UK   -0.695437\n        PB    0.745012\n        KR    2.058104\ndtype: float64\n\n\n\ndata\n\nMP   -1.190913\nUP    0.136560\nMH   -0.046813\nRJ    0.962568\nHP    0.477671\nUK   -0.695437\nPB    0.745012\nKR    2.058104\ndtype: float64\n\n\n\ndata4\n\nMP         NaN\nUP    0.211319\nMH    0.925109\nRJ   -0.204536\nHP    0.293047\nUK    0.497221\nPB         NaN\nKR         NaN\ndtype: float64\n\n\n\n# filling predefined vlaues\nfill_values = {'Centre': 0.5,\n              'North': -1}\n\ndef fill_func(group):\n    return group.fillna(fill_values[group.name])\n\ndata4.groupby(group_key).apply(fill_func)\n\nCentre  MP    0.500000\n        UP    0.211319\n        MH    0.925109\n        RJ   -0.204536\nNorth   HP    0.293047\n        UK    0.497221\n        PB   -1.000000\n        KR   -1.000000\ndtype: float64\n\n\n\n\nEx: Random Sampling and Permutation\n\nsuits = ['H', 'S', 'C', 'D'] # hearts, club, diammonds, spades\ncard_val = (list(range(1, 11)) + [10] * 3) * 4\n\n\nbase_names = ['A'] + list(range(2, 11)) + ['J', 'K', 'Q']\ncards = []\nfor suit in suits:\n    cards.extend(str(num) + suit for num in base_names)\n    \ndeck= pd.Series(card_val, index = cards)\n\n\ndeck.head(13)\n\nAH      1\n2H      2\n3H      3\n4H      4\n5H      5\n6H      6\n7H      7\n8H      8\n9H      9\n10H    10\nJH     10\nKH     10\nQH     10\ndtype: int64\n\n\n\n# drawing first five cards from deck \ndef draw(deck, n=5):\n    return deck.sample(n)\n\ndraw(deck)\n\n6H      6\n6S      6\n7S      7\n10H    10\n9D      9\ndtype: int64\n\n\n\n# two random cards from each suit\ndef get_suit(card):\n    # last letter is suit\n    return card[-1]\n\n\ndeck.groupby(get_suit).apply(draw, n = 2)\n\nC  KC    10\n   AC     1\nD  JD    10\n   6D     6\nH  AH     1\n   2H     2\nS  9S     9\n   AS     1\ndtype: int64\n\n\n\n# alternatively, group_keys = False\ndeck.groupby(get_suit, group_keys= False).apply(draw, n=2)\n\n4C     4\nKC    10\n6D     6\nJD    10\n8H     8\nAH     1\nJS    10\n8S     8\ndtype: int64\n\n\n\n\nEx: Group weighted Average and Correlation\n\ndf5 = pd.DataFrame({\"category\": ['a', 'a', 'a', 'a'],\n                  \"data\": np.random.standard_normal(4),\n                   'weights': np.random.uniform(size = 4)})\n\n\ndf5\n\n\n\n\n\n\n\n\ncategory\ndata\nweights\n\n\n\n\n0\na\n-0.272834\n0.453331\n\n\n1\na\n1.032855\n0.573681\n\n\n2\na\n0.123029\n0.239600\n\n\n3\na\n-0.715634\n0.474301\n\n\n\n\n\n\n\n\n# average weight by category\ngrouped = df5.groupby('category')\n\ndef get_wavg(group):\n    return np.average(group['data'],\n                     weights=group['weights'])\n\ngrouped.apply(get_wavg)\n\ncategory\na    0.091272\ndtype: float64\n\n\n\n\nEx: Group-Wise linear Regression\n\nusing regress function of statsmodel econometrics library\n\n\nimport statsmodels.api as sm\n\n\n! pip install statsmodels\n\n\ndef regress(data, yvar= None, xvars = None):\n    Y = data[yvar]\n    X = data [xvar]\n    X['intercept'] = 1\n    result = sm.OLS(Y, X).fit()\n    return result.params\n\n\niris2.columns\n\nIndex(['Id', 'Sepal Length (cm)', 'Sepal Width (cm)', 'Petal Length (cm)',\n       'Petal Width (cm)'],\n      dtype='object')\n\n\n\niris2.rename(columns={'Sepal Length (cm)' : 'sepal_length'}, inplace = True)\n\n\niris2.columns\n\nIndex(['Id', 'sepal_length', 'Sepal Width (cm)', 'Petal Length (cm)',\n       'Petal Width (cm)'],\n      dtype='object')\n\n\n\nsepal_length.apply(regress, yvar= \"AAPl\", \n             xvars = [\"SPX\"])\n\n\n\nGroup transforms and ‘Unwrapped’ GroupBys\n\ndf6 = pd.DataFrame({'key': ['a', 'b', 'c'] * 4,\n                   'value' : np.arange(12.)\n                   })\n\n\ndf6\n\n\n\n\n\n\n\n\nkey\nvalue\n\n\n\n\n0\na\n0.0\n\n\n1\nb\n1.0\n\n\n2\nc\n2.0\n\n\n3\na\n3.0\n\n\n4\nb\n4.0\n\n\n5\nc\n5.0\n\n\n6\na\n6.0\n\n\n7\nb\n7.0\n\n\n8\nc\n8.0\n\n\n9\na\n9.0\n\n\n10\nb\n10.0\n\n\n11\nc\n11.0\n\n\n\n\n\n\n\n\n# group means by key\n\ng = df6.groupby('key')['value']\n\ng.mean()\n\nkey\na    4.5\nb    5.5\nc    6.5\nName: value, dtype: float64\n\n\n\ndef get_mean(group):\n    return group.mean()\n\ng.transform(get_mean)\n\n0     4.5\n1     5.5\n2     6.5\n3     4.5\n4     5.5\n5     6.5\n6     4.5\n7     5.5\n8     6.5\n9     4.5\n10    5.5\n11    6.5\nName: value, dtype: float64\n\n\n\ng.transform('mean')\n\n0     4.5\n1     5.5\n2     6.5\n3     4.5\n4     5.5\n5     6.5\n6     4.5\n7     5.5\n8     6.5\n9     4.5\n10    5.5\n11    6.5\nName: value, dtype: float64\n\n\n\ndef times_two(group):\n    return group* 2\n\ng.transform(times_two)\n\n0      0.0\n1      2.0\n2      4.0\n3      6.0\n4      8.0\n5     10.0\n6     12.0\n7     14.0\n8     16.0\n9     18.0\n10    20.0\n11    22.0\nName: value, dtype: float64\n\n\n\n# ranks in descending order\ndef get_ranks(group):\n    return group.rank(ascending=False)\n\ng.transform(get_ranks)\n\n0     4.0\n1     4.0\n2     4.0\n3     3.0\n4     3.0\n5     3.0\n6     2.0\n7     2.0\n8     2.0\n9     1.0\n10    1.0\n11    1.0\nName: value, dtype: float64\n\n\n\n# transformation function composed of aggregations\n\ndef normalize(x):\n    return (x - x.mean())/x.std()\n\n\ng.transform(normalize)\n\n0    -1.161895\n1    -1.161895\n2    -1.161895\n3    -0.387298\n4    -0.387298\n5    -0.387298\n6     0.387298\n7     0.387298\n8     0.387298\n9     1.161895\n10    1.161895\n11    1.161895\nName: value, dtype: float64\n\n\n\ng.apply(normalize)\n\nkey    \na    0    -1.161895\n     3    -0.387298\n     6     0.387298\n     9     1.161895\nb    1    -1.161895\n     4    -0.387298\n     7     0.387298\n     10    1.161895\nc    2    -1.161895\n     5    -0.387298\n     8     0.387298\n     11    1.161895\nName: value, dtype: float64\n\n\n\n# using built-in 'mean' and 'sum' functions\n\ng.transform('mean')\n\n0     4.5\n1     5.5\n2     6.5\n3     4.5\n4     5.5\n5     6.5\n6     4.5\n7     5.5\n8     6.5\n9     4.5\n10    5.5\n11    6.5\nName: value, dtype: float64\n\n\n\nnormalized2 = (df6['value'] - g.transform('mean')) / g.transform('std')\n\n\nnormalized2\n\n0    -1.161895\n1    -1.161895\n2    -1.161895\n3    -0.387298\n4    -0.387298\n5    -0.387298\n6     0.387298\n7     0.387298\n8     0.387298\n9     1.161895\n10    1.161895\n11    1.161895\nName: value, dtype: float64\n\n\n\n\nPivot tables\n\nsummarization tool\nuses groupby facility\npandas.pivot_table fucntion can add partial tools known as margins\nsummary - pivot table\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as mlt\n\n\ndf_iris = pd.read_csv('E:\\pythonfordatanalysis\\semainedu26fevrier\\iris.csv')\n\n\ndf_iris.head()\n\n\n\n\n\n\n\n\nId\nSepal Length (cm)\nSepal Width (cm)\nPetal Length (cm)\nPetal Width (cm)\nSpecies\n\n\n\n\n0\n1\n5.1\n3.5\n1.4\n0.2\nIris-setosa\n\n\n1\n2\n4.9\n3.0\n1.4\n0.2\nIris-setosa\n\n\n2\n3\n4.7\n3.2\n1.3\n0.2\nIris-setosa\n\n\n3\n4\n4.6\n3.1\n1.5\n0.2\nIris-setosa\n\n\n4\n5\n5.0\n3.6\n1.4\n0.2\nIris-setosa\n\n\n\n\n\n\n\n\ndf_iris.pivot_table(index = ['Species'])\n\n\n\n\n\n\n\n\nId\nPetal Length (cm)\nPetal Width (cm)\nSepal Length (cm)\nSepal Width (cm)\n\n\nSpecies\n\n\n\n\n\n\n\n\n\nIris-setosa\n25.5\n1.464\n0.244\n5.006\n3.418\n\n\nIris-versicolor\n75.5\n4.260\n1.326\n5.936\n2.770\n\n\nIris-virginica\n125.5\n5.552\n2.026\n6.588\n2.974\n\n\n\n\n\n\n\n\ndf_iris.columns\n\nIndex(['Id', 'Sepal Length (cm)', 'Sepal Width (cm)', 'Petal Length (cm)',\n       'Petal Width (cm)', 'Species'],\n      dtype='object')\n\n\n\ndf_iris.pivot_table(index = ['Species', 'Petal Length (cm)'])\n\n\n\n\n\n\n\n\n\nId\nPetal Width (cm)\nSepal Length (cm)\nSepal Width (cm)\n\n\nSpecies\nPetal Length (cm)\n\n\n\n\n\n\n\n\nIris-setosa\n1.0\n23.000000\n0.200000\n4.600000\n3.600000\n\n\n1.1\n14.000000\n0.100000\n4.300000\n3.000000\n\n\n1.2\n25.500000\n0.200000\n5.400000\n3.600000\n\n\n1.3\n31.714286\n0.257143\n4.842857\n3.228571\n\n\n1.4\n21.833333\n0.216667\n4.916667\n3.333333\n\n\n1.5\n24.714286\n0.221429\n5.128571\n3.535714\n\n\n1.6\n31.000000\n0.285714\n4.914286\n3.342857\n\n\n1.7\n17.500000\n0.350000\n5.400000\n3.600000\n\n\n1.9\n35.000000\n0.300000\n4.950000\n3.600000\n\n\nIris-versicolor\n3.0\n99.000000\n1.100000\n5.100000\n2.500000\n\n\n3.3\n76.000000\n1.000000\n4.950000\n2.350000\n\n\n3.5\n70.500000\n1.000000\n5.350000\n2.300000\n\n\n3.6\n65.000000\n1.300000\n5.600000\n2.900000\n\n\n3.7\n82.000000\n1.000000\n5.500000\n2.400000\n\n\n3.8\n81.000000\n1.100000\n5.500000\n2.400000\n\n\n3.9\n71.000000\n1.233333\n5.533333\n2.633333\n\n\n4.0\n74.400000\n1.220000\n5.780000\n2.480000\n\n\n4.1\n85.666667\n1.200000\n5.700000\n2.833333\n\n\n4.2\n87.500000\n1.325000\n5.725000\n2.900000\n\n\n4.3\n86.500000\n1.300000\n6.300000\n2.900000\n\n\n4.4\n80.250000\n1.325000\n6.275000\n2.750000\n\n\n4.5\n70.571429\n1.485714\n5.900000\n2.928571\n\n\n4.6\n68.666667\n1.400000\n6.400000\n2.900000\n\n\n4.7\n66.600000\n1.420000\n6.440000\n3.060000\n\n\n4.8\n74.000000\n1.600000\n6.350000\n3.000000\n\n\n4.9\n63.000000\n1.500000\n6.600000\n2.800000\n\n\n5.0\n78.000000\n1.700000\n6.700000\n3.000000\n\n\n5.1\n84.000000\n1.600000\n6.000000\n2.700000\n\n\nIris-virginica\n4.5\n107.000000\n1.700000\n4.900000\n2.500000\n\n\n4.8\n133.000000\n1.800000\n6.100000\n2.900000\n\n\n4.9\n124.666667\n1.866667\n6.000000\n2.833333\n\n\n5.0\n127.000000\n1.800000\n6.000000\n2.400000\n\n\n5.1\n128.142857\n1.971429\n6.142857\n2.900000\n\n\n5.2\n147.000000\n2.150000\n6.600000\n3.000000\n\n\n5.3\n114.000000\n2.100000\n6.400000\n2.950000\n\n\n5.4\n144.500000\n2.200000\n6.550000\n3.250000\n\n\n5.5\n122.666667\n1.900000\n6.566667\n3.033333\n\n\n5.6\n129.833333\n2.050000\n6.366667\n2.933333\n\n\n5.7\n130.333333\n2.300000\n6.766667\n3.266667\n\n\n5.8\n114.666667\n1.866667\n6.800000\n2.833333\n\n\n5.9\n123.500000\n2.200000\n6.950000\n3.100000\n\n\n6.0\n113.500000\n2.150000\n6.750000\n3.250000\n\n\n6.1\n125.666667\n2.233333\n7.433333\n3.133333\n\n\n6.3\n108.000000\n1.800000\n7.300000\n2.900000\n\n\n6.4\n132.000000\n2.000000\n7.900000\n3.800000\n\n\n6.6\n106.000000\n2.100000\n7.600000\n3.000000\n\n\n6.7\n120.500000\n2.100000\n7.700000\n3.300000\n\n\n6.9\n119.000000\n2.300000\n7.700000\n2.600000\n\n\n\n\n\n\n\n\ndf_iris.pivot_table(index = ['Species', 'Petal Length (cm)'],\n                   columns = 'Sepal Width (cm)', )\n\n\n\n\n\n\n\n\n\nId\n...\nSepal Length (cm)\n\n\n\nSepal Width (cm)\n2.0\n2.2\n2.3\n2.4\n2.5\n2.6\n2.7\n2.8\n2.9\n3.0\n...\n3.4\n3.5\n3.6\n3.7\n3.8\n3.9\n4.0\n4.1\n4.2\n4.4\n\n\nSpecies\nPetal Length (cm)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIris-setosa\n1.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\n4.6\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1.1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n14.000000\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1.2\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n5.8\nNaN\nNaN\nNaN\n\n\n1.3\nNaN\nNaN\n42.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n39.000000\n...\nNaN\n5.25\nNaN\nNaN\nNaN\n5.4\nNaN\nNaN\nNaN\nNaN\n\n\n1.4\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n9.0\n20.333333\n...\n4.900000\n5.10\n5.0\nNaN\nNaN\nNaN\nNaN\nNaN\n5.5\nNaN\n\n\n1.5\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n5.166667\n5.20\nNaN\n5.266667\n5.1\nNaN\nNaN\n5.2\nNaN\n5.7\n\n\n1.6\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n26.000000\n...\n4.900000\n5.00\nNaN\nNaN\n5.1\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1.7\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n5.400000\nNaN\nNaN\nNaN\n5.7\n5.4\nNaN\nNaN\nNaN\nNaN\n\n\n1.9\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n4.800000\nNaN\nNaN\nNaN\n5.1\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nIris-versicolor\n3.0\nNaN\nNaN\nNaN\nNaN\n99.0\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3.3\nNaN\nNaN\n94.0\n58.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3.5\n61.0\nNaN\nNaN\nNaN\nNaN\n80.0\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3.6\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n65.0\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3.7\nNaN\nNaN\nNaN\n82.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3.8\nNaN\nNaN\nNaN\n81.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3.9\nNaN\nNaN\nNaN\nNaN\n70.0\nNaN\n71.5\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4.0\nNaN\n63.0\n54.0\nNaN\n90.0\n93.0\nNaN\n72.0\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4.1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n68.0\n100.0\nNaN\n89.000000\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4.2\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n95.0\nNaN\n97.0\n79.000000\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4.3\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n86.5\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4.4\nNaN\nNaN\n88.0\nNaN\nNaN\n91.0\nNaN\nNaN\nNaN\n76.000000\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4.5\nNaN\n69.0\nNaN\nNaN\nNaN\nNaN\nNaN\n56.0\n79.0\n76.000000\n...\n6.000000\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4.6\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n55.0\n59.0\n92.000000\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4.7\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n74.0\n64.0\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4.8\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n77.0\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4.9\nNaN\nNaN\nNaN\nNaN\n73.0\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n5.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n78.000000\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n5.1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n84.0\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nIris-virginica\n4.5\nNaN\nNaN\nNaN\nNaN\n107.0\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4.8\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n127.0\nNaN\n139.000000\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4.9\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n124.0\n122.0\nNaN\n128.000000\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n5.0\nNaN\n120.0\nNaN\nNaN\n130.5\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n5.1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n122.5\n124.5\nNaN\n150.000000\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n5.2\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n147.000000\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n5.3\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n112.0\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n5.4\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n6.200000\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n5.5\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n115.000000\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n5.6\nNaN\nNaN\nNaN\nNaN\nNaN\n135.0\nNaN\n131.0\n104.0\nNaN\n...\n6.300000\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n5.7\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n5.8\nNaN\nNaN\nNaN\nNaN\n109.0\nNaN\nNaN\nNaN\nNaN\n117.500000\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n5.9\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n103.000000\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n6.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n6.1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n131.0\nNaN\n136.000000\n...\nNaN\nNaN\n7.2\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n6.3\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n108.0\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n6.4\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\n7.9\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n6.6\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n106.000000\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n6.7\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n123.0\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\n7.7\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n6.9\nNaN\nNaN\nNaN\nNaN\nNaN\n119.0\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n48 rows × 69 columns\n\n\n\n\n# passing fill_value for empty combinations\ndf_iris.pivot_table(index = ['Species', 'Petal Length (cm)'],\n                   columns = 'Sepal Width (cm)', fill_value=5)\n\n\n\n\n\n\n\n\n\nId\n...\nSepal Length (cm)\n\n\n\nSepal Width (cm)\n2.0\n2.2\n2.3\n2.4\n2.5\n2.6\n2.7\n2.8\n2.9\n3.0\n...\n3.4\n3.5\n3.6\n3.7\n3.8\n3.9\n4.0\n4.1\n4.2\n4.4\n\n\nSpecies\nPetal Length (cm)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIris-setosa\n1.0\n5\n5\n5\n5\n5.0\n5\n5.0\n5.0\n5.0\n5.000000\n...\n5.000000\n5.00\n4.6\n5.000000\n5.0\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\n1.1\n5\n5\n5\n5\n5.0\n5\n5.0\n5.0\n5.0\n14.000000\n...\n5.000000\n5.00\n5.0\n5.000000\n5.0\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\n1.2\n5\n5\n5\n5\n5.0\n5\n5.0\n5.0\n5.0\n5.000000\n...\n5.000000\n5.00\n5.0\n5.000000\n5.0\n5.0\n5.8\n5.0\n5.0\n5.0\n\n\n1.3\n5\n5\n42\n5\n5.0\n5\n5.0\n5.0\n5.0\n39.000000\n...\n5.000000\n5.25\n5.0\n5.000000\n5.0\n5.4\n5.0\n5.0\n5.0\n5.0\n\n\n1.4\n5\n5\n5\n5\n5.0\n5\n5.0\n5.0\n9.0\n20.333333\n...\n4.900000\n5.10\n5.0\n5.000000\n5.0\n5.0\n5.0\n5.0\n5.5\n5.0\n\n\n1.5\n5\n5\n5\n5\n5.0\n5\n5.0\n5.0\n5.0\n5.000000\n...\n5.166667\n5.20\n5.0\n5.266667\n5.1\n5.0\n5.0\n5.2\n5.0\n5.7\n\n\n1.6\n5\n5\n5\n5\n5.0\n5\n5.0\n5.0\n5.0\n26.000000\n...\n4.900000\n5.00\n5.0\n5.000000\n5.1\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\n1.7\n5\n5\n5\n5\n5.0\n5\n5.0\n5.0\n5.0\n5.000000\n...\n5.400000\n5.00\n5.0\n5.000000\n5.7\n5.4\n5.0\n5.0\n5.0\n5.0\n\n\n1.9\n5\n5\n5\n5\n5.0\n5\n5.0\n5.0\n5.0\n5.000000\n...\n4.800000\n5.00\n5.0\n5.000000\n5.1\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\nIris-versicolor\n3.0\n5\n5\n5\n5\n99.0\n5\n5.0\n5.0\n5.0\n5.000000\n...\n5.000000\n5.00\n5.0\n5.000000\n5.0\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\n3.3\n5\n5\n94\n58\n5.0\n5\n5.0\n5.0\n5.0\n5.000000\n...\n5.000000\n5.00\n5.0\n5.000000\n5.0\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\n3.5\n61\n5\n5\n5\n5.0\n80\n5.0\n5.0\n5.0\n5.000000\n...\n5.000000\n5.00\n5.0\n5.000000\n5.0\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\n3.6\n5\n5\n5\n5\n5.0\n5\n5.0\n5.0\n65.0\n5.000000\n...\n5.000000\n5.00\n5.0\n5.000000\n5.0\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\n3.7\n5\n5\n5\n82\n5.0\n5\n5.0\n5.0\n5.0\n5.000000\n...\n5.000000\n5.00\n5.0\n5.000000\n5.0\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\n3.8\n5\n5\n5\n81\n5.0\n5\n5.0\n5.0\n5.0\n5.000000\n...\n5.000000\n5.00\n5.0\n5.000000\n5.0\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\n3.9\n5\n5\n5\n5\n70.0\n5\n71.5\n5.0\n5.0\n5.000000\n...\n5.000000\n5.00\n5.0\n5.000000\n5.0\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\n4.0\n5\n63\n54\n5\n90.0\n93\n5.0\n72.0\n5.0\n5.000000\n...\n5.000000\n5.00\n5.0\n5.000000\n5.0\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\n4.1\n5\n5\n5\n5\n5.0\n5\n68.0\n100.0\n5.0\n89.000000\n...\n5.000000\n5.00\n5.0\n5.000000\n5.0\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\n4.2\n5\n5\n5\n5\n5.0\n5\n95.0\n5.0\n97.0\n79.000000\n...\n5.000000\n5.00\n5.0\n5.000000\n5.0\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\n4.3\n5\n5\n5\n5\n5.0\n5\n5.0\n5.0\n86.5\n5.000000\n...\n5.000000\n5.00\n5.0\n5.000000\n5.0\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\n4.4\n5\n5\n88\n5\n5.0\n91\n5.0\n5.0\n5.0\n76.000000\n...\n5.000000\n5.00\n5.0\n5.000000\n5.0\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\n4.5\n5\n69\n5\n5\n5.0\n5\n5.0\n56.0\n79.0\n76.000000\n...\n6.000000\n5.00\n5.0\n5.000000\n5.0\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\n4.6\n5\n5\n5\n5\n5.0\n5\n5.0\n55.0\n59.0\n92.000000\n...\n5.000000\n5.00\n5.0\n5.000000\n5.0\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\n4.7\n5\n5\n5\n5\n5.0\n5\n5.0\n74.0\n64.0\n5.000000\n...\n5.000000\n5.00\n5.0\n5.000000\n5.0\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\n4.8\n5\n5\n5\n5\n5.0\n5\n5.0\n77.0\n5.0\n5.000000\n...\n5.000000\n5.00\n5.0\n5.000000\n5.0\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\n4.9\n5\n5\n5\n5\n73.0\n5\n5.0\n5.0\n5.0\n5.000000\n...\n5.000000\n5.00\n5.0\n5.000000\n5.0\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\n5.0\n5\n5\n5\n5\n5.0\n5\n5.0\n5.0\n5.0\n78.000000\n...\n5.000000\n5.00\n5.0\n5.000000\n5.0\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\n5.1\n5\n5\n5\n5\n5.0\n5\n84.0\n5.0\n5.0\n5.000000\n...\n5.000000\n5.00\n5.0\n5.000000\n5.0\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\nIris-virginica\n4.5\n5\n5\n5\n5\n107.0\n5\n5.0\n5.0\n5.0\n5.000000\n...\n5.000000\n5.00\n5.0\n5.000000\n5.0\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\n4.8\n5\n5\n5\n5\n5.0\n5\n5.0\n127.0\n5.0\n139.000000\n...\n5.000000\n5.00\n5.0\n5.000000\n5.0\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\n4.9\n5\n5\n5\n5\n5.0\n5\n124.0\n122.0\n5.0\n128.000000\n...\n5.000000\n5.00\n5.0\n5.000000\n5.0\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\n5.0\n5\n120\n5\n5\n130.5\n5\n5.0\n5.0\n5.0\n5.000000\n...\n5.000000\n5.00\n5.0\n5.000000\n5.0\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\n5.1\n5\n5\n5\n5\n5.0\n5\n122.5\n124.5\n5.0\n150.000000\n...\n5.000000\n5.00\n5.0\n5.000000\n5.0\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\n5.2\n5\n5\n5\n5\n5.0\n5\n5.0\n5.0\n5.0\n147.000000\n...\n5.000000\n5.00\n5.0\n5.000000\n5.0\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\n5.3\n5\n5\n5\n5\n5.0\n5\n112.0\n5.0\n5.0\n5.000000\n...\n5.000000\n5.00\n5.0\n5.000000\n5.0\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\n5.4\n5\n5\n5\n5\n5.0\n5\n5.0\n5.0\n5.0\n5.000000\n...\n6.200000\n5.00\n5.0\n5.000000\n5.0\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\n5.5\n5\n5\n5\n5\n5.0\n5\n5.0\n5.0\n5.0\n115.000000\n...\n5.000000\n5.00\n5.0\n5.000000\n5.0\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\n5.6\n5\n5\n5\n5\n5.0\n135\n5.0\n131.0\n104.0\n5.000000\n...\n6.300000\n5.00\n5.0\n5.000000\n5.0\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\n5.7\n5\n5\n5\n5\n5.0\n5\n5.0\n5.0\n5.0\n5.000000\n...\n5.000000\n5.00\n5.0\n5.000000\n5.0\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\n5.8\n5\n5\n5\n5\n109.0\n5\n5.0\n5.0\n5.0\n117.500000\n...\n5.000000\n5.00\n5.0\n5.000000\n5.0\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\n5.9\n5\n5\n5\n5\n5.0\n5\n5.0\n5.0\n5.0\n103.000000\n...\n5.000000\n5.00\n5.0\n5.000000\n5.0\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\n6.0\n5\n5\n5\n5\n5.0\n5\n5.0\n5.0\n5.0\n5.000000\n...\n5.000000\n5.00\n5.0\n5.000000\n5.0\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\n6.1\n5\n5\n5\n5\n5.0\n5\n5.0\n131.0\n5.0\n136.000000\n...\n5.000000\n5.00\n7.2\n5.000000\n5.0\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\n6.3\n5\n5\n5\n5\n5.0\n5\n5.0\n5.0\n108.0\n5.000000\n...\n5.000000\n5.00\n5.0\n5.000000\n5.0\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\n6.4\n5\n5\n5\n5\n5.0\n5\n5.0\n5.0\n5.0\n5.000000\n...\n5.000000\n5.00\n5.0\n5.000000\n7.9\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\n6.6\n5\n5\n5\n5\n5.0\n5\n5.0\n5.0\n5.0\n106.000000\n...\n5.000000\n5.00\n5.0\n5.000000\n5.0\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\n6.7\n5\n5\n5\n5\n5.0\n5\n5.0\n123.0\n5.0\n5.000000\n...\n5.000000\n5.00\n5.0\n5.000000\n7.7\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\n6.9\n5\n5\n5\n5\n5.0\n119\n5.0\n5.0\n5.0\n5.000000\n...\n5.000000\n5.00\n5.0\n5.000000\n5.0\n5.0\n5.0\n5.0\n5.0\n5.0\n\n\n\n\n48 rows × 69 columns\n\n\n\n\n\nCross- tabulation\n\nfrom io import StringIO\n\n\ndata = \"\"\" Sample Court_paragraph\n1. Il était une fois, dans un village paisible au bord de la mer, \n2. un jeune garçon nommé Luca. \n3. Luca aimait explorer les forêts environnantes, \n4. où il découvrait des trésors cachés et des mystères oubliés. \n5. Un jour, en suivant un papillon multicolore, \n6. Luca découvrit une grotte secrète.\n7. À l'intérieur, il trouva une carte ancienne indiquant \n8. l'emplacement d'un trésor légendaire. \n9. Déterminé à le trouver, \n10. Luca partit en voyage, bravant des tempêtes et affrontant\n11. des créatures magiques. \n12. Après de nombreuses aventures, il trouva enfin le trésor, \n13. qui se révéla être l'amitié des habitants de ce village enchanté. \n14. Luca apprit alors que les véritables trésors \n15. sont souvent cachés à l'intérieur de nous-mêmes.\n\"\"\"\n\n\ndata = pd.read_table(StringIO(data), sep = '\\s+')\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSample\nCourt_paragraph\n\n\n\n\n1.0\nIl\nétait\nune\nfois,\ndans\nun\nvillage\npaisible\nau\nbord\nde\nla\nmer,\n\n\n2.0\nun\njeune\ngarçon\nnommé\nLuca.\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3.0\nLuca\naimait\nexplorer\nles\nforêts\nenvironnantes,\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4.0\noù\nil\ndécouvrait\ndes\ntrésors\ncachés\net\ndes\nmystères\noubliés.\nNaN\nNaN\nNaN\n\n\n5.0\nUn\njour,\nen\nsuivant\nun\npapillon\nmulticolore,\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n6.0\nLuca\ndécouvrit\nune\ngrotte\nsecrète.\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n7.0\nÀ\nl'intérieur,\nil\ntrouva\nune\ncarte\nancienne\nindiquant\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n8.0\nl'emplacement\nd'un\ntrésor\nlégendaire.\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n9.0\nDéterminé\nà\nle\ntrouver,\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n10.0\nLuca\npartit\nen\nvoyage,\nbravant\ndes\ntempêtes\net\naffrontant\nNaN\nNaN\nNaN\nNaN\n\n\n11.0\ndes\ncréatures\nmagiques.\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n12.0\nAprès\nde\nnombreuses\naventures,\nil\ntrouva\nenfin\nle\ntrésor,\nNaN\nNaN\nNaN\nNaN\n\n\n13.0\nqui\nse\nrévéla\nêtre\nl'amitié\ndes\nhabitants\nde\nce\nvillage\nenchanté.\nNaN\nNaN\n\n\n14.0\nLuca\napprit\nalors\nque\nles\nvéritables\ntrésors\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n15.0\nsont\nsouvent\ncachés\nà\nl'intérieur\nde\nnous-mêmes.\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\npd.crosstab(data['Sample'], data['Court_paragraph'], \n            margins = True)\n\n\n\n\n\n\n\nCourt_paragraph\nmer,\nAll\n\n\nSample\n\n\n\n\n\n\nla\n1\n1\n\n\nAll\n1\n1"
  },
  {
    "objectID": "posts/python/datastructures.html",
    "href": "posts/python/datastructures.html",
    "title": "Data Structures",
    "section": "",
    "text": "Tuple\n\n\nList\n\n\nDictionary (hash maps or associated arrays)\n\n\nSet\n\n\n\n\n\ncannot be changed\n\n\n# example\ntup = tuple([\"foo\", [1,2], True])\n\ntup[2] = False\n\nTypeError: 'tuple' object does not support item assignment\n\n\n\ntup[1].append(3)\ntup\n\n('foo', [1, 2, 3, 3], True)\n\n\n\n\n\ntup_2 = (4, None, 'zeal') + (5, 6, 32) + ('bar',) #no comma gives a type error\ntup_2\n\n(4, None, 'zeal', 5, 6, 32, 'bar')\n\n\n\n\n\n\nseq = [(1,2,3), (4, 5, 6), (7,8,9)]\n\nfor a, b, c in seq:\n    print(f'a = {a}, b = {b}, c= {c}')\n    \n\na = 1, b = 2, c= 3\na = 4, b = 5, c= 6\na = 7, b = 8, c= 9\n\n\n\n# another method\nvalues= 1, 2, 3, 4, 5\n\na, b, *rest = values\n\nrest  # used to discard\n\n[3, 4, 5]\n\n\n\nb\n\n2\n\n\n\n\n\n\n\nsame as tuples, but can be modified and lists use [ ] brackets\n\n\n\n\nx = [4, 5, 6 , None, 'foo']\nx.extend([7,8, (1, 2)])\n\nx\n\n[4, 5, 6, None, 'foo', 7, 8, (1, 2)]\n\n\n\n\n\n\neverything = []\nfor chunk in x:\n    everything.extend(x)\n    print(x)\n\n[4, 5, 6, None, 'foo', 7, 8, (1, 2)]\n[4, 5, 6, None, 'foo', 7, 8, (1, 2)]\n[4, 5, 6, None, 'foo', 7, 8, (1, 2)]\n[4, 5, 6, None, 'foo', 7, 8, (1, 2)]\n[4, 5, 6, None, 'foo', 7, 8, (1, 2)]\n[4, 5, 6, None, 'foo', 7, 8, (1, 2)]\n[4, 5, 6, None, 'foo', 7, 8, (1, 2)]\n[4, 5, 6, None, 'foo', 7, 8, (1, 2)]\n\n\n\n\n\n\neverything = []\nfor chunk in x:\n    everything = everything + x\n    print(x)\n\n[4, 5, 6, None, 'foo', 7, 8, (1, 2)]\n[4, 5, 6, None, 'foo', 7, 8, (1, 2)]\n[4, 5, 6, None, 'foo', 7, 8, (1, 2)]\n[4, 5, 6, None, 'foo', 7, 8, (1, 2)]\n[4, 5, 6, None, 'foo', 7, 8, (1, 2)]\n[4, 5, 6, None, 'foo', 7, 8, (1, 2)]\n[4, 5, 6, None, 'foo', 7, 8, (1, 2)]\n[4, 5, 6, None, 'foo', 7, 8, (1, 2)]\n\n\n\n\n\n\nx[-4:]\n\n['foo', 7, 8, (1, 2)]\n\n\n\n\n\n\nx[::2] # provived elemnents till 2nd index\n\n[4, 6, 'foo', 8]\n\n\n\n### reversing and getting every \nx [::-1]\n\n[(1, 2), 8, 7, 'foo', None, 6, 5, 4]\n\n\n\ny =[1, 2, 3, 4]\n\n\n\n\n\n\nempty_dict = {}\n\nd1 = {\"a\": 'some value', 'b': [1,2,3,4]}\n\nd1\n\n{'a': 'some value', 'b': [1, 2, 3, 4]}\n\n\n\n\n\nd1[3] = 'continue'\n\n\nd1\n\n{'a': 'some value', 'b': [1, 2, 3, 4], 3: 'continue'}\n\n\n\n'b' in d1\n\nTrue\n\n\n\ndel d1[3]\nd1\n\n{'a': 'some value', 'b': [1, 2, 3, 4]}\n\n\n\n\n\n\nd1.update({'d': 'food', 'e': 'à la maison'})\nd1\n\n{'a': 'some value', 'b': [1, 2, 3, 4], 'd': 'food', 'e': 'à la maison'}\n\n\n\n\n\n\nmapping =  {}\nfor key, value in zip(x, y):\n    mapping[key] = value\nprint(mapping)\n\n{4: 1, 5: 2, 6: 3, None: 4}\n\n\n\ntuples = zip(range(5), reversed(range(5)))\n\ntuples\n\n&lt;zip at 0x1e1f49ed440&gt;\n\n\n\nmapping = dict(tuples)\nmapping\n\n{}\n\n\n\nmapping\n\n{}\n\n\n\n# write a function to club the words by same first alphabet\n\nwords = ['apple', 'bat', 'bar', 'atom', 'book'] # list\n\nby_letter = {} #empty dict\n\nfor word in words:\n    letter = word[0] #first goes in\n    if letter not in by_letter:\n        by_letter[letter] = [word]\n    else:\n        by_letter[letter].append(word)\n        \nprint(by_letter)\n\n{'a': ['apple', 'atom'], 'b': ['bat', 'bar', 'book']}\n\n\n\n\n\n\nhash('string')\n\n5928582044493709413\n\n\n\nhash((1, 2 ,(2, 3)))\n\n-9209053662355515447\n\n\n\nhash((1, 2, [2,3])) #fails because lists are mutable\n\nTypeError: unhashable type: 'list'\n\n\n\nd= {}\n\nd[tuple([1,2,3])] = 5\n\nd\n\n{(1, 2, 3): 5}\n\n\n\nd[tuple('strength')] = 'persistance'\n\nd\n\n{(1, 2, 3): 5, ('s', 't', 'r', 'e', 'n', 'g', 't', 'h'): 'persistance'}\n\n\n\n\n\n\n\nunordered collection of unique elements\nrepresented by curly brackets\nset operations (union, intersection, difference, and symmetric difference)\nimmutable = hashable = like tuple\n\n\nset([1, 2, 2,2,3,4,5,5,6])\n\n{1, 2, 3, 4, 5, 6}\n\n\n\na = {1,2,3}\nb = {4,5,6}\n\na.union(b)\n\n{1, 2, 3, 4, 5, 6}\n\n\n\na | b   # means union\n\n{1, 2, 3, 4, 5, 6}\n\n\n\na.intersection(b)\n\nset()\n\n\n\na & b # interection\n\nset()\n\n\n\na.add(4)   #doesn't overwrite\na \n\n{1, 2, 3, 4}\n\n\n\na & b\n\n{4}"
  },
  {
    "objectID": "posts/python/datastructures.html#tuple",
    "href": "posts/python/datastructures.html#tuple",
    "title": "Data Structures",
    "section": "",
    "text": "cannot be changed\n\n\n# example\ntup = tuple([\"foo\", [1,2], True])\n\ntup[2] = False\n\nTypeError: 'tuple' object does not support item assignment\n\n\n\ntup[1].append(3)\ntup\n\n('foo', [1, 2, 3, 3], True)\n\n\n\n\n\ntup_2 = (4, None, 'zeal') + (5, 6, 32) + ('bar',) #no comma gives a type error\ntup_2\n\n(4, None, 'zeal', 5, 6, 32, 'bar')\n\n\n\n\n\n\nseq = [(1,2,3), (4, 5, 6), (7,8,9)]\n\nfor a, b, c in seq:\n    print(f'a = {a}, b = {b}, c= {c}')\n    \n\na = 1, b = 2, c= 3\na = 4, b = 5, c= 6\na = 7, b = 8, c= 9\n\n\n\n# another method\nvalues= 1, 2, 3, 4, 5\n\na, b, *rest = values\n\nrest  # used to discard\n\n[3, 4, 5]\n\n\n\nb\n\n2"
  },
  {
    "objectID": "posts/python/datastructures.html#list",
    "href": "posts/python/datastructures.html#list",
    "title": "Data Structures",
    "section": "",
    "text": "same as tuples, but can be modified and lists use [ ] brackets\n\n\n\n\nx = [4, 5, 6 , None, 'foo']\nx.extend([7,8, (1, 2)])\n\nx\n\n[4, 5, 6, None, 'foo', 7, 8, (1, 2)]\n\n\n\n\n\n\neverything = []\nfor chunk in x:\n    everything.extend(x)\n    print(x)\n\n[4, 5, 6, None, 'foo', 7, 8, (1, 2)]\n[4, 5, 6, None, 'foo', 7, 8, (1, 2)]\n[4, 5, 6, None, 'foo', 7, 8, (1, 2)]\n[4, 5, 6, None, 'foo', 7, 8, (1, 2)]\n[4, 5, 6, None, 'foo', 7, 8, (1, 2)]\n[4, 5, 6, None, 'foo', 7, 8, (1, 2)]\n[4, 5, 6, None, 'foo', 7, 8, (1, 2)]\n[4, 5, 6, None, 'foo', 7, 8, (1, 2)]\n\n\n\n\n\n\neverything = []\nfor chunk in x:\n    everything = everything + x\n    print(x)\n\n[4, 5, 6, None, 'foo', 7, 8, (1, 2)]\n[4, 5, 6, None, 'foo', 7, 8, (1, 2)]\n[4, 5, 6, None, 'foo', 7, 8, (1, 2)]\n[4, 5, 6, None, 'foo', 7, 8, (1, 2)]\n[4, 5, 6, None, 'foo', 7, 8, (1, 2)]\n[4, 5, 6, None, 'foo', 7, 8, (1, 2)]\n[4, 5, 6, None, 'foo', 7, 8, (1, 2)]\n[4, 5, 6, None, 'foo', 7, 8, (1, 2)]\n\n\n\n\n\n\nx[-4:]\n\n['foo', 7, 8, (1, 2)]\n\n\n\n\n\n\nx[::2] # provived elemnents till 2nd index\n\n[4, 6, 'foo', 8]\n\n\n\n### reversing and getting every \nx [::-1]\n\n[(1, 2), 8, 7, 'foo', None, 6, 5, 4]\n\n\n\ny =[1, 2, 3, 4]"
  },
  {
    "objectID": "posts/python/datastructures.html#dictionaries",
    "href": "posts/python/datastructures.html#dictionaries",
    "title": "Data Structures",
    "section": "",
    "text": "empty_dict = {}\n\nd1 = {\"a\": 'some value', 'b': [1,2,3,4]}\n\nd1\n\n{'a': 'some value', 'b': [1, 2, 3, 4]}\n\n\n\n\n\nd1[3] = 'continue'\n\n\nd1\n\n{'a': 'some value', 'b': [1, 2, 3, 4], 3: 'continue'}\n\n\n\n'b' in d1\n\nTrue\n\n\n\ndel d1[3]\nd1\n\n{'a': 'some value', 'b': [1, 2, 3, 4]}\n\n\n\n\n\n\nd1.update({'d': 'food', 'e': 'à la maison'})\nd1\n\n{'a': 'some value', 'b': [1, 2, 3, 4], 'd': 'food', 'e': 'à la maison'}\n\n\n\n\n\n\nmapping =  {}\nfor key, value in zip(x, y):\n    mapping[key] = value\nprint(mapping)\n\n{4: 1, 5: 2, 6: 3, None: 4}\n\n\n\ntuples = zip(range(5), reversed(range(5)))\n\ntuples\n\n&lt;zip at 0x1e1f49ed440&gt;\n\n\n\nmapping = dict(tuples)\nmapping\n\n{}\n\n\n\nmapping\n\n{}\n\n\n\n# write a function to club the words by same first alphabet\n\nwords = ['apple', 'bat', 'bar', 'atom', 'book'] # list\n\nby_letter = {} #empty dict\n\nfor word in words:\n    letter = word[0] #first goes in\n    if letter not in by_letter:\n        by_letter[letter] = [word]\n    else:\n        by_letter[letter].append(word)\n        \nprint(by_letter)\n\n{'a': ['apple', 'atom'], 'b': ['bat', 'bar', 'book']}\n\n\n\n\n\n\nhash('string')\n\n5928582044493709413\n\n\n\nhash((1, 2 ,(2, 3)))\n\n-9209053662355515447\n\n\n\nhash((1, 2, [2,3])) #fails because lists are mutable\n\nTypeError: unhashable type: 'list'\n\n\n\nd= {}\n\nd[tuple([1,2,3])] = 5\n\nd\n\n{(1, 2, 3): 5}\n\n\n\nd[tuple('strength')] = 'persistance'\n\nd\n\n{(1, 2, 3): 5, ('s', 't', 'r', 'e', 'n', 'g', 't', 'h'): 'persistance'}"
  },
  {
    "objectID": "posts/python/datastructures.html#set",
    "href": "posts/python/datastructures.html#set",
    "title": "Data Structures",
    "section": "",
    "text": "unordered collection of unique elements\nrepresented by curly brackets\nset operations (union, intersection, difference, and symmetric difference)\nimmutable = hashable = like tuple\n\n\nset([1, 2, 2,2,3,4,5,5,6])\n\n{1, 2, 3, 4, 5, 6}\n\n\n\na = {1,2,3}\nb = {4,5,6}\n\na.union(b)\n\n{1, 2, 3, 4, 5, 6}\n\n\n\na | b   # means union\n\n{1, 2, 3, 4, 5, 6}\n\n\n\na.intersection(b)\n\nset()\n\n\n\na & b # interection\n\nset()\n\n\n\na.add(4)   #doesn't overwrite\na \n\n{1, 2, 3, 4}\n\n\n\na & b\n\n{4}"
  },
  {
    "objectID": "posts/python/basics_python_kaggle.html",
    "href": "posts/python/basics_python_kaggle.html",
    "title": "Python",
    "section": "",
    "text": "python methods\n\n\nsequence types\n\n\nLists\n\n\nTuples"
  },
  {
    "objectID": "posts/python/basics_python_kaggle.html#documentation",
    "href": "posts/python/basics_python_kaggle.html#documentation",
    "title": "Python",
    "section": "",
    "text": "python methods\n\n\nsequence types\n\n\nLists\n\n\nTuples"
  },
  {
    "objectID": "posts/python/basics_python_kaggle.html#for-loops",
    "href": "posts/python/basics_python_kaggle.html#for-loops",
    "title": "Python",
    "section": "For loops",
    "text": "For loops\n\nmultiplicands = (2, 2, 2, 3, 3, 5)\nproduct = 1\nfor mult in multiplicands:\n    product = product * mult\nprint(product)\n\n360\n\n\n\n# loop through each character in a string\ns = 'steganograpHy is the practicE of conceaLing a file, message, image, or video within another fiLe, message, image, Or video.'\nmsg = ''\n# print all the uppercase letters in s, one at a time\nfor char in s:\n    if char.isupper():\n        print(char, end='') \n\nHELLO\n\n\n\n# range function\nfor i in range(5):\n    print(f\"Doing important work = \", i)\n\nDoing important work =  0\nDoing important work =  1\nDoing important work =  2\nDoing important work =  3\nDoing important work =  4"
  },
  {
    "objectID": "posts/python/basics_python_kaggle.html#while-loops",
    "href": "posts/python/basics_python_kaggle.html#while-loops",
    "title": "Python",
    "section": "While loops",
    "text": "While loops\n\n# iterates until condition is met\n\"\"\"write a while loop of numbers from 0 until the sum reaches 50 by continuous additions of the previous number\"\"\"\ni = 1\nwhile i &lt; 50: \n    print(i , end = \" \")\n\n1 2 4 8 16 32"
  },
  {
    "objectID": "posts/python/basics_python_kaggle.html#submodules",
    "href": "posts/python/basics_python_kaggle.html#submodules",
    "title": "Python",
    "section": "Submodules",
    "text": "Submodules\n\nimport numpy\nprint (\"numpy.random is a\", type(numpy.random))\nprint(\"it contains names such as....\", \n      dir(numpy.random)[-15:])\n\nnumpy.random is a &lt;class 'module'&gt;\nit contains names such as.... ['set_bit_generator', 'set_state', 'shuffle', 'standard_cauchy', 'standard_exponential', 'standard_gamma', 'standard_normal', 'standard_t', 'test', 'triangular', 'uniform', 'vonmises', 'wald', 'weibull', 'zipf']\n\n\n\n# roll 10 dice\nrolls = numpy.random.randint(low=1, high =6, size = 10)   #Two dots required; for numpy and its sub-module\nrolls\n\narray([1, 1, 2, 2, 2, 5, 3, 3, 1, 1])\n\n\n\nprint(rolls)\n\n[3 2 1 1 3 4 2 1 3 2]\n\n\n\nthree tools for understaing strange objects\n\ntype(),\n\n\ndir(),\n\n\nhelp()\n\n\ntype(rolls)\n\nnumpy.ndarray\n\n\n\nprint(dir(rolls))\n\n['T', '__abs__', '__add__', '__and__', '__array__', '__array_finalize__', '__array_function__', '__array_interface__', '__array_prepare__', '__array_priority__', '__array_struct__', '__array_ufunc__', '__array_wrap__', '__bool__', '__class__', '__class_getitem__', '__complex__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__delitem__', '__dir__', '__divmod__', '__dlpack__', '__dlpack_device__', '__doc__', '__eq__', '__float__', '__floordiv__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__iadd__', '__iand__', '__ifloordiv__', '__ilshift__', '__imatmul__', '__imod__', '__imul__', '__index__', '__init__', '__init_subclass__', '__int__', '__invert__', '__ior__', '__ipow__', '__irshift__', '__isub__', '__iter__', '__itruediv__', '__ixor__', '__le__', '__len__', '__lshift__', '__lt__', '__matmul__', '__mod__', '__mul__', '__ne__', '__neg__', '__new__', '__or__', '__pos__', '__pow__', '__radd__', '__rand__', '__rdivmod__', '__reduce__', '__reduce_ex__', '__repr__', '__rfloordiv__', '__rlshift__', '__rmatmul__', '__rmod__', '__rmul__', '__ror__', '__rpow__', '__rrshift__', '__rshift__', '__rsub__', '__rtruediv__', '__rxor__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__xor__', 'all', 'any', 'argmax', 'argmin', 'argpartition', 'argsort', 'astype', 'base', 'byteswap', 'choose', 'clip', 'compress', 'conj', 'conjugate', 'copy', 'ctypes', 'cumprod', 'cumsum', 'data', 'diagonal', 'dot', 'dtype', 'dump', 'dumps', 'fill', 'flags', 'flat', 'flatten', 'getfield', 'imag', 'item', 'itemset', 'itemsize', 'max', 'mean', 'min', 'nbytes', 'ndim', 'newbyteorder', 'nonzero', 'partition', 'prod', 'ptp', 'put', 'ravel', 'real', 'repeat', 'reshape', 'resize', 'round', 'searchsorted', 'setfield', 'setflags', 'shape', 'size', 'sort', 'squeeze', 'std', 'strides', 'sum', 'swapaxes', 'take', 'tobytes', 'tofile', 'tolist', 'tostring', 'trace', 'transpose', 'var', 'view']\n\n\n\nhelp(rolls.__abs__)\n\nHelp on method-wrapper:\n\n__abs__()\n    abs(self)\n\n\n\n\nhelp(rolls.prod)\n\nHelp on built-in function prod:\n\nprod(...) method of numpy.ndarray instance\n    a.prod(axis=None, dtype=None, out=None, keepdims=False, initial=1, where=True)\n    \n    Return the product of the array elements over the given axis\n    \n    Refer to `numpy.prod` for full documentation.\n    \n    See Also\n    --------\n    numpy.prod : equivalent function\n\n\n\n\nrolls + 10\n\narray([11, 11, 12, 12, 12, 15, 13, 13, 11, 11])\n\n\n\nrolls &gt;= 3\n\narray([False, False, False, False, False,  True,  True,  True, False,\n       False])\n\n\n\n# create a 2-d array\n\na = [[1,23,4],[3,2,4]]\nx = (f\"this is how a 2-d array looks like \\n{numpy.array(a)}\")\n\n\nprint(x)\n\nthis is how a 2-d array looks like \n[[ 1 23  4]\n [ 3  2  4]]\n\n\n\ndef blackjack_hand_greater_than(hand_1, hand_2):\n    \"\"\"\n    Return True if hand_1 beats hand_2, and False otherwise.\n    \n    In order for hand_1 to beat hand_2 the following must be true:\n    - The total of hand_1 must not exceed 21\n    - The total of hand_1 must exceed the total of hand_2 OR hand_2's total must exceed 21\n    \n    Hands are represented as a list of cards. Each card is represented by a string.\n    \n    When adding up a hand's total, cards with numbers count for that many points. Face\n    cards ('J', 'Q', and 'K') are worth 10 points. 'A' can count for 1 or 11.\n    \n    When determining a hand's total, you should try to count aces in the way that \n    maximizes the hand's total without going over 21. e.g. the total of ['A', 'A', '9'] is 21,\n    the total of ['A', 'A', '9', '3'] is 14.\n    \n    Examples:\n    &gt;&gt;&gt; blackjack_hand_greater_than(['K'], ['3', '4'])\n    True\n    &gt;&gt;&gt; blackjack_hand_greater_than(['K'], ['10'])\n    False\n    &gt;&gt;&gt; blackjack_hand_greater_than(['K', 'K', '2'], ['3'])\n    False\n    \"\"\"\n\n\nhelp(blackjack_hand_greater_than)\n\nHelp on function blackjack_hand_greater_than in module __main__:\n\nblackjack_hand_greater_than(hand_1, hand_2)\n    Return True if hand_1 beats hand_2, and False otherwise.\n    \n    In order for hand_1 to beat hand_2 the following must be true:\n    - The total of hand_1 must not exceed 21\n    - The total of hand_1 must exceed the total of hand_2 OR hand_2's total must exceed 21\n    \n    Hands are represented as a list of cards. Each card is represented by a string.\n    \n    When adding up a hand's total, cards with numbers count for that many points. Face\n    cards ('J', 'Q', and 'K') are worth 10 points. 'A' can count for 1 or 11.\n    \n    When determining a hand's total, you should try to count aces in the way that \n    maximizes the hand's total without going over 21. e.g. the total of ['A', 'A', '9'] is 21,\n    the total of ['A', 'A', '9', '3'] is 14.\n    \n    Examples:\n    &gt;&gt;&gt; blackjack_hand_greater_than(['K'], ['3', '4'])\n    True\n    &gt;&gt;&gt; blackjack_hand_greater_than(['K'], ['10'])\n    False\n    &gt;&gt;&gt; blackjack_hand_greater_than(['K', 'K', '2'], ['3'])\n    False\n\n\n\n\n#defining card values, here we've kept A=11\ndef card_value(card):\n    if card in ['J', 'Q', 'K']:\n        return 10\n    elif card == 'A':\n        return 11\n    else:\n        return int(card)\n\n# totalling one hand and adjusting aces\n\ndef hand_total(hand):\n    \"\"\"Calculate the total value of a hand.\"\"\"\n    total = sum(card_value(card) for card in hand)\n    \n    #adjust for aces\n    aces_count = hand.count('A')\n    while total &gt; 21 and aces_count &gt; 0:\n        # subtract 10 from total to change an ace from 11 to 1\n        total -= 10\n        # decrease the count of aces being treated as 11\n        aces_count -= 1\n    \n    return total \n\n# defining the comparison operator\ndef blackjack_hand_greater_than(hand_1, hand_2):\n    \"\"\"return True if hand_1 beats the hand_2 and False otherwise\"\"\"\n    total_1= hand_total(hand_1)\n    total_2= hand_total(hand_2)\n    \n    return total_1 &lt;= 21 and (total_1 &gt; total_2 or total_2 &gt; 21)\n\n# Example usage\nprint(blackjack_hand_greater_than(['K'], ['3', '4']))  # True\nprint(blackjack_hand_greater_than(['K'], ['10']))  # False\nprint(blackjack_hand_greater_than(['K', 'K', '2'], ['3']))  # False\n\nTrue\nFalse\nFalse"
  },
  {
    "objectID": "posts/pandas/pandas2.html",
    "href": "posts/pandas/pandas2.html",
    "title": "Pandas_2",
    "section": "",
    "text": "helps in numerical computing (NumPy, SciPy)\nhelps with analytical libraries (scikit-learn, and data visualizatioon,\nprocesses data without for loops"
  },
  {
    "objectID": "posts/pandas/pandas2.html#series",
    "href": "posts/pandas/pandas2.html#series",
    "title": "Pandas_2",
    "section": "Series",
    "text": "Series\n\nimport pandas as pd\n\nimport numpy as np\n\nfrom pandas import Series, DataFrame\n\n\nobj = pd.Series([4,2, 312, -3])\n\nobj\n\n0      4\n1      2\n2    312\n3     -3\ndtype: int64\n\n\n\nobj2 = pd.Series([4,2, 312, -3], index = ['a', 'b', 'c', 'd'])\n\nobj2\n\na      4\nb      2\nc    312\nd     -3\ndtype: int64\n\n\n\nobj2.index\n\nIndex(['a', 'b', 'c', 'd'], dtype='object')\n\n\n\nobj2[obj2 &gt; 0]\n\na      4\nb      2\nc    312\ndtype: int64\n\n\n\nnp.exp(obj2)\n\na     5.459815e+01\nb     7.389056e+00\nc    3.161392e+135\nd     4.978707e-02\ndtype: float64\n\n\n\n'b' in obj2\n\nTrue\n\n\n\n'e' in obj2\n\nFalse\n\n\n\nsdata = {'ohio': 232, 'Texas': 332, 'Oregon': 34343}\n\nobj3 = pd.Series(sdata)\n\nobj3\n\nohio        232\nTexas       332\nOregon    34343\ndtype: int64\n\n\n\nobj3.to_dict()\n\n{'ohio': 232, 'Texas': 332, 'Oregon': 34343}\n\n\n\nstates = ['California', 'ohio', 'orgeon']\n\nobj4 = pd.Series(sdata, index = states)\n\nobj4\n\nCalifornia      NaN\nohio          232.0\norgeon          NaN\ndtype: float64\n\n\n\npd.isna(obj4) # is null\n\nCalifornia     True\nohio          False\norgeon         True\ndtype: bool\n\n\n\npd.notna(obj4)  #not null\n\nCalifornia    False\nohio           True\norgeon        False\ndtype: bool\n\n\n\nobj3 + obj4\n\nCalifornia      NaN\nOregon          NaN\nTexas           NaN\nohio          464.0\norgeon          NaN\ndtype: float64\n\n\n\nobj4.name = 'population'\n\nobj4.index.name = 'state'\n\nobj4\n\nstate\nCalifornia      NaN\nohio          232.0\norgeon          NaN\nName: population, dtype: float64\n\n\n\nobj\n\n0      4\n1      2\n2    312\n3     -3\ndtype: int64\n\n\n\n# altering the index in place\n\nobj.index = ['Kunal', 'Rahul', 'Raghav', 'Ryan']\n\nobj\n\nKunal       4\nRahul       2\nRaghav    312\nRyan       -3\ndtype: int64"
  },
  {
    "objectID": "posts/pandas/pandas2.html#dataframe",
    "href": "posts/pandas/pandas2.html#dataframe",
    "title": "Pandas_2",
    "section": "DataFrame",
    "text": "DataFrame\n\ndata = {'state': ['ohio', 'ohio', 'nevada',], \n       'year': [2000, 2001, 2002,], \n       'pop': [1.2, 1.3, 1.4,]}\n\nframe = pd.DataFrame(data)\n\n\nframe\n\n\n\n\n\n\n\n\nstate\nyear\npop\n\n\n\n\n0\nohio\n2000\n1.2\n\n\n1\nohio\n2001\n1.3\n\n\n2\nnevada\n2002\n1.4\n\n\n\n\n\n\n\n\nframe.head()\n\n\n\n\n\n\n\n\nstate\nyear\npop\n\n\n\n\n0\nohio\n2000\n1.2\n\n\n1\nohio\n2001\n1.3\n\n\n2\nnevada\n2002\n1.4\n\n\n\n\n\n\n\n\nframe.tail()\n\n\n\n\n\n\n\n\nstate\nyear\npop\n\n\n\n\n0\nohio\n2000\n1.2\n\n\n1\nohio\n2001\n1.3\n\n\n2\nnevada\n2002\n1.4\n\n\n\n\n\n\n\n\n# passing another column in the dataframe\n\nframe2 = pd.DataFrame(data, columns = ['state', 'year', 'pop', 'debt'])\n\nframe2\n\n\n\n\n\n\n\n\nstate\nyear\npop\ndebt\n\n\n\n\n0\nohio\n2000\n1.2\nNaN\n\n\n1\nohio\n2001\n1.3\nNaN\n\n\n2\nnevada\n2002\n1.4\nNaN\n\n\n\n\n\n\n\n\n# changing the order of columns\n\n\nframe2 = pd.DataFrame(data, columns = [ 'year', 'pop', 'debt', 'state'])\n\n\nframe2\n\n\n\n\n\n\n\n\nstate\nyear\npop\ndebt\n\n\n\n\n0\nohio\n2000\n1.2\nNaN\n\n\n1\nohio\n2001\n1.3\nNaN\n\n\n2\nnevada\n2002\n1.4\nNaN\n\n\n\n\n\n\n\n\nframe2.year\n\n0    2000\n1    2001\n2    2002\nName: year, dtype: int64\n\n\n\nframe2.loc[1]\n\nyear     2001\npop       1.3\ndebt      NaN\nstate    ohio\nName: 1, dtype: object\n\n\n\nframe2.iloc[2]\n\nyear       2002\npop         1.4\ndebt        NaN\nstate    nevada\nName: 2, dtype: object\n\n\n\nframe2.pop\n\n&lt;bound method DataFrame.pop of    year  pop debt   state\n0  2000  1.2  NaN    ohio\n1  2001  1.3  NaN    ohio\n2  2002  1.4  NaN  nevada&gt;\n\n\n\nframe2.year\n\n0    2000\n1    2001\n2    2002\nName: year, dtype: int64\n\n\n\n# assigning values\n\nframe2['debt'] = 14.5\n\nframe2\n\n\n\n\n\n\n\n\nyear\npop\ndebt\nstate\n\n\n\n\n0\n2000\n1.2\n14.5\nohio\n\n\n1\n2001\n1.3\n14.5\nohio\n\n\n2\n2002\n1.4\n14.5\nnevada\n\n\n\n\n\n\n\n\n# assiging a new column (resuls in new column if it does not exist before)\n\nframe2['eastern'] = frame2['state']  =='ohio'\n\nframe2\n\n\n\n\n\n\n\n\nyear\npop\ndebt\nstate\neastern\n\n\n\n\n0\n2000\n1.2\n14.5\nohio\nTrue\n\n\n1\n2001\n1.3\n14.5\nohio\nTrue\n\n\n2\n2002\n1.4\n14.5\nnevada\nFalse\n\n\n\n\n\n\n\n\n# transposing\nframe2.T\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\nyear\n2000\n2001\n2002\n\n\npop\n1.2\n1.3\n1.4\n\n\ndebt\n14.5\n14.5\n14.5\n\n\nstate\nohio\nohio\nnevada\n\n\neastern\nTrue\nTrue\nFalse\n\n\n\n\n\n\n\n\npd.DataFrame(data)\n\n\n\n\n\n\n\n\nstate\nyear\npop\n\n\n\n\n0\nohio\n2000\n1.2\n\n\n1\nohio\n2001\n1.3\n\n\n2\nnevada\n2002\n1.4\n\n\n\n\n\n\n\n\nframe2.index.name = 'year' \n\n\nframe2.columns.name = 'state'  # starts with state column\n\n\nframe2\n\n\n\n\n\n\n\nstate\nyear\npop\ndebt\nstate\neastern\n\n\nyear\n\n\n\n\n\n\n\n\n\n0\n2000\n1.2\n14.5\nohio\nTrue\n\n\n1\n2001\n1.3\n14.5\nohio\nTrue\n\n\n2\n2002\n1.4\n14.5\nnevada\nFalse\n\n\n\n\n\n\n\n\nframe2.to_numpy()\n\narray([[2000, 1.2, 14.5, 'ohio', True],\n       [2001, 1.3, 14.5, 'ohio', True],\n       [2002, 1.4, 14.5, 'nevada', False]], dtype=object)"
  },
  {
    "objectID": "posts/pandas/pandas2.html#index-objects",
    "href": "posts/pandas/pandas2.html#index-objects",
    "title": "Pandas_2",
    "section": "index objects",
    "text": "index objects\n\nobj4 = pd.Series(np.arange(3), index = ['a', 'b', 'c'])\n\nindex = obj4.index\n\n\nindex\n\nIndex(['a', 'b', 'c'], dtype='object')\n\n\n\nindex [1:]\n\nIndex(['b', 'c'], dtype='object')\n\n\n\n# index objects are immutable\n\nindex[1]= 'd' #type error\n\n\nlabels = pd.Index(np.arange(3))\n\nlabels\n\nIndex([0, 1, 2], dtype='int32')\n\n\n\nobj2 = pd.Series([1.5, -2.5, 0], index = labels)\n\n\nobj2\n\n0    1.5\n1   -2.5\n2    0.0\ndtype: float64\n\n\n\nobj2.index is labels\n\nTrue\n\n\n\nframe2\n\n\n\n\n\n\n\nstate\nyear\npop\ndebt\nstate\neastern\n\n\nyear\n\n\n\n\n\n\n\n\n\n0\n2000\n1.2\n14.5\nohio\nTrue\n\n\n1\n2001\n1.3\n14.5\nohio\nTrue\n\n\n2\n2002\n1.4\n14.5\nnevada\nFalse\n\n\n\n\n\n\n\n\nframe2.columns\n\nIndex(['year', 'pop', 'debt', 'state', 'eastern'], dtype='object', name='state')\n\n\n\n2003 in frame2.index\n\nFalse\n\n\n\n# unlike python, a pandas index can contain duplicate labels\n\npd.Index (['foo', 'boo', 'bar', 'baa', 'etc', 'foo'])\n\n\n\n\nIndex(['foo', 'boo', 'bar', 'baa', 'etc', 'foo'], dtype='object')"
  },
  {
    "objectID": "posts/pandas/pandas2.html#reindexing",
    "href": "posts/pandas/pandas2.html#reindexing",
    "title": "Pandas_2",
    "section": "Reindexing",
    "text": "Reindexing\n\nobj = pd.Series([4.5,48, -3,2,3.9], index= ['a', 'b', 'c', 'd', 'e'])\n\nobj\n\na     4.5\nb    48.0\nc    -3.0\nd     2.0\ne     3.9\ndtype: float64\n\n\n\n# reindexing\nobj2 = obj.reindex(['b', 'a', 'c', 'd', 'e'])\n\nobj2\n\nb    48.0\na     4.5\nc    -3.0\nd     2.0\ne     3.9\ndtype: float64\n\n\n\n# time series data fill\nobj3 = pd.Series(['blue', 'purple', 'yellow'], index = [0, 2, 4])\n\nobj3\n\n0      blue\n2    purple\n4    yellow\ndtype: object\n\n\n\n# forward filling the values using ffill\nobj3.reindex(np.arange(6), method='ffill')\n\n0      blue\n1      blue\n2    purple\n3    purple\n4    yellow\n5    yellow\ndtype: object\n\n\n\n# backward fill\nobj3.reindex(np.arange(6), method = 'bfill')\n\n0      blue\n1    purple\n2    purple\n3    yellow\n4    yellow\n5       NaN\ndtype: object\n\n\n\nframe = pd.DataFrame(np.arange(9).reshape((3, 3)),\n                    index = ['a', 'b', 'c'],\n                    columns= ['ohio', 'texas', 'burmingham'])\n\nframe\n\n\n\n\n\n\n\n\nohio\ntexas\nburmingham\n\n\n\n\na\n0\n1\n2\n\n\nb\n3\n4\n5\n\n\nc\n6\n7\n8\n\n\n\n\n\n\n\n\nframe2 = frame.reindex(index=['a', 'b', 'c', 'd'])\n\nframe2\n\n\n\n\n\n\n\n\nohio\ntexas\nburmingham\n\n\n\n\na\n0.0\n1.0\n2.0\n\n\nb\n3.0\n4.0\n5.0\n\n\nc\n6.0\n7.0\n8.0\n\n\nd\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n# reindexing columns with column keyword\n\nstates = ['london', 'texus', 'surrey']\n\nframe.reindex(columns = states)\n\n\n\n\n\n\n\n\n\nlondon\ntexus\nsurrey\n\n\n\n\na\nNaN\nNaN\nNaN\n\n\nb\nNaN\nNaN\nNaN\n\n\nc\nNaN\nNaN\nNaN"
  },
  {
    "objectID": "posts/pandas/pandas2.html#dropping-entries-from-axis",
    "href": "posts/pandas/pandas2.html#dropping-entries-from-axis",
    "title": "Pandas_2",
    "section": "Dropping entries from Axis",
    "text": "Dropping entries from Axis\n\nobj = pd.Series(np.arange(5.), index = ['a', 'b', 'c', 'd', 'e'])\n\nobj\n\na    0.0\nb    1.0\nc    2.0\nd    3.0\ne    4.0\ndtype: float64\n\n\n\nnew_obj = obj.drop('c')\nnew_obj\n\na    0.0\nb    1.0\nd    3.0\ne    4.0\ndtype: float64\n\n\n\nobj.drop(['d', 'e'])\n\na    0.0\nb    1.0\nc    2.0\ndtype: float64\n\n\n\n# in DataFrame\ndata = pd.DataFrame(np.arange(16).reshape((4,4)),\n                    index=['québec', 'montréal', 'toronto', 'sainte-anne'],\n                    columns = ['one', 'two', 'three', 'four'])\ndata\n\n\n\n\n\n\n\n\none\ntwo\nthree\nfour\n\n\n\n\nquébec\n0\n1\n2\n3\n\n\nmontréal\n4\n5\n6\n7\n\n\ntoronto\n8\n9\n10\n11\n\n\nsainte-anne\n12\n13\n14\n15\n\n\n\n\n\n\n\n\n# using drop method\ndata.drop(index=['toronto', 'sainte-anne'])\n\n\n\n\n\n\n\n\none\ntwo\nthree\nfour\n\n\n\n\nquébec\n0\n1\n2\n3\n\n\nmontréal\n4\n5\n6\n7\n\n\n\n\n\n\n\n\n# dropping using axis method (axis = 1 = columns)\n\ndata.drop('two', axis=1)\n\n\n\n\n\n\n\n\none\nthree\nfour\n\n\n\n\nquébec\n0\n2\n3\n\n\nmontréal\n4\n6\n7\n\n\ntoronto\n8\n10\n11\n\n\nsainte-anne\n12\n14\n15\n\n\n\n\n\n\n\n\ndata.drop(['three', 'four'], axis='columns')\n\n\n\n\n\n\n\n\none\ntwo\n\n\n\n\nquébec\n0\n1\n\n\nmontréal\n4\n5\n\n\ntoronto\n8\n9\n\n\nsainte-anne\n12\n13"
  },
  {
    "objectID": "posts/pandas/pandas2.html#indexing-selecting-and-filtering",
    "href": "posts/pandas/pandas2.html#indexing-selecting-and-filtering",
    "title": "Pandas_2",
    "section": "Indexing, Selecting, and Filtering",
    "text": "Indexing, Selecting, and Filtering\n\nobj = pd.Series(np.arange(4.), index= ['a', 'b', 'c', 'd'])\n\nobj\n\na    0.0\nb    1.0\nc    2.0\nd    3.0\ndtype: float64\n\n\n\nobj['b']\n\n1.0\n\n\n\nobj[1]\n\n1.0\n\n\n\nobj[2:4]\n\nc    2.0\nd    3.0\ndtype: float64\n\n\n\nobj[obj&lt;2]\n\na    0.0\nb    1.0\ndtype: float64\n\n\n\nobj.loc[['b', 'c']]\n\nb    1.0\nc    2.0\ndtype: float64\n\n\n\nobj1 = pd.Series([1,2,3], index = [2,0,1])\n\nobj2 = pd.Series([1,2,3], index = ['a', 'b', 'c'])\n\nobj1\n\n2    1\n0    2\n1    3\ndtype: int64\n\n\n\nobj2\n\na    1\nb    2\nc    3\ndtype: int64\n\n\n\n# loc fails as index doesnot contain integers\nobj2.loc[[0, 1]]\n\n\n# fix this\n\nobj2.loc['b':'c']\n\nb    2\nc    3\ndtype: int64\n\n\n\n# so, prefer using iloc with integers\n\nobj1.iloc[[0,1,2]]\n\n2    1\n0    2\n1    3\ndtype: int64\n\n\n\nobj2.iloc[[0,1,2]]\n\na    1\nb    2\nc    3\ndtype: int64\n\n\n\n# assigning values\n\nobj2.loc['b':'c'] = 5\n\nobj2\n\na    1\nb    5\nc    5\ndtype: int64\n\n\n\ndata\n\n\n\n\n\n\n\n\none\ntwo\nthree\nfour\n\n\n\n\nquébec\n0\n1\n2\n3\n\n\nmontréal\n4\n5\n6\n7\n\n\ntoronto\n8\n9\n10\n11\n\n\nsainte-anne\n12\n13\n14\n15\n\n\n\n\n\n\n\n\ndata[:2]\n\n\n\n\n\n\n\n\none\ntwo\nthree\nfour\n\n\n\n\nquébec\n0\n1\n2\n3\n\n\nmontréal\n4\n5\n6\n7\n\n\n\n\n\n\n\n\n# booleans\ndata &lt; 5\n\n\n\n\n\n\n\n\none\ntwo\nthree\nfour\n\n\n\n\nquébec\nTrue\nTrue\nTrue\nTrue\n\n\nmontréal\nTrue\nFalse\nFalse\nFalse\n\n\ntoronto\nFalse\nFalse\nFalse\nFalse\n\n\nsainte-anne\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n\n\n\n\n# assigning values\ndata[data &lt; 5] = 0\n\ndata\n\n\n\n\n\n\n\n\none\ntwo\nthree\nfour\n\n\n\n\nquébec\n0\n0\n0\n0\n\n\nmontréal\n0\n5\n6\n7\n\n\ntoronto\n8\n9\n10\n11\n\n\nsainte-anne\n12\n13\n14\n15"
  },
  {
    "objectID": "posts/pandas/pandas2.html#selection-of-dataframe-with-loc-and-iloc",
    "href": "posts/pandas/pandas2.html#selection-of-dataframe-with-loc-and-iloc",
    "title": "Pandas_2",
    "section": "selection of DataFrame with loc and iloc",
    "text": "selection of DataFrame with loc and iloc\n\n\n\ndata\n\n\n\n\n\n\n\n\none\ntwo\nthree\nfour\n\n\n\n\nquébec\n0\n0\n0\n0\n\n\nmontréal\n0\n5\n6\n7\n\n\ntoronto\n8\n9\n10\n11\n\n\nsainte-anne\n12\n13\n14\n15\n\n\n\n\n\n\n\n\ndata.loc['montréal']\n\none      0\ntwo      5\nthree    6\nfour     7\nName: montréal, dtype: int32\n\n\n\ndata.loc[['montréal', 'québec']]\n\n\n\n\n\n\n\n\none\ntwo\nthree\nfour\n\n\n\n\nmontréal\n0\n5\n6\n7\n\n\nquébec\n0\n0\n0\n0\n\n\n\n\n\n\n\n\ndata.loc['montréal', ['two', 'three']]\n\ntwo      5\nthree    6\nName: montréal, dtype: int32\n\n\n\n# similar operations with iloc\ndata.iloc[2]\n\none       8\ntwo       9\nthree    10\nfour     11\nName: toronto, dtype: int32\n\n\n\ndata.iloc[[2,1]]  #third row and second row\n\n\n\n\n\n\n\n\none\ntwo\nthree\nfour\n\n\n\n\ntoronto\n8\n9\n10\n11\n\n\nmontréal\n0\n5\n6\n7\n\n\n\n\n\n\n\n\ndata.iloc[2,[3,0,1]] #third row (three elements in order)\n\nfour    11\none      8\ntwo      9\nName: toronto, dtype: int32\n\n\n\ndata.iloc[[1,2],[3,0,1]]\n\n\n\n\n\n\n\n\nfour\none\ntwo\n\n\n\n\nmontréal\n7\n0\n5\n\n\ntoronto\n11\n8\n9"
  },
  {
    "objectID": "posts/pandas/pandas2.html#integer-indexing-pitfalls",
    "href": "posts/pandas/pandas2.html#integer-indexing-pitfalls",
    "title": "Pandas_2",
    "section": "integer indexing pitfalls",
    "text": "integer indexing pitfalls\n\nseries = pd.Series(np.arange(3.))\n\n\nseries\n\n0    0.0\n1    1.0\n2    2.0\ndtype: float64\n\n\n\n# fails here but works fine with iloc and loc \nseries[-1]\n\n\n\n# value error; key error: -1\n\n\nseries.iloc[-1]\n\n2.0\n\n\n\n# non-integer doesnot do this ambiguity\n\nseries2 = pd.Series(np.arange(3.0), index = ['a', 'b', 'c'])\n\n\n\nseries2[-1]\n\n2.0"
  },
  {
    "objectID": "posts/pandas/pandas2.html#pitfalls-with-chained-indexing",
    "href": "posts/pandas/pandas2.html#pitfalls-with-chained-indexing",
    "title": "Pandas_2",
    "section": "Pitfalls with chained indexing",
    "text": "Pitfalls with chained indexing\n\ndata.loc[:, 'one'] = 1\n\ndata\n\n\n\n\n\n\n\n\none\ntwo\nthree\nfour\n\n\n\n\nquébec\n1\n0\n0\n0\n\n\nmontréal\n1\n5\n6\n7\n\n\ntoronto\n1\n9\n10\n11\n\n\nsainte-anne\n1\n13\n14\n15\n\n\n\n\n\n\n\n\ndata.iloc[2] = 5\n\n\ndata\n\n\n\n\n\n\n\n\none\ntwo\nthree\nfour\n\n\n\n\nquébec\n1\n0\n0\n0\n\n\nmontréal\n1\n5\n6\n7\n\n\ntoronto\n5\n5\n5\n5\n\n\nsainte-anne\n1\n13\n14\n15\n\n\n\n\n\n\n\n\ndata.loc[data['four'] &gt; 5]  = 3\n\ndata\n\n\n\n\n\n\n\n\none\ntwo\nthree\nfour\n\n\n\n\nquébec\n1\n0\n0\n0\n\n\nmontréal\n3\n3\n3\n3\n\n\ntoronto\n5\n5\n5\n5\n\n\nsainte-anne\n3\n3\n3\n3\n\n\n\n\n\n\n\n\n# the data gets modified, but it is not the way that was asked for\n\n# fixing it with loc operation\n\ndata.loc[data.three == 10, \"three\"] = 9\n\ndata\n\n\n\n\n\n\n\n\none\ntwo\nthree\nfour\n\n\n\n\nquébec\n1\n0\n0\n0\n\n\nmontréal\n3\n3\n3\n3\n\n\ntoronto\n5\n5\n5\n5\n\n\nsainte-anne\n3\n3\n3\n3"
  },
  {
    "objectID": "posts/pandas/pandas2.html#arithmetic-and-data-alignment",
    "href": "posts/pandas/pandas2.html#arithmetic-and-data-alignment",
    "title": "Pandas_2",
    "section": "Arithmetic and Data Alignment",
    "text": "Arithmetic and Data Alignment\n\ns1 = pd.Series([7.3, -2.5, 3.4, 1.5], index = ['a', 'c', 'd', 'e'])\n\ns2 = pd.Series([1.2, -3, -.3, -.33, -43.2], index = ['e', 'j', 'o', 't', 'y'])\n\ns1\n\na    7.3\nc   -2.5\nd    3.4\ne    1.5\ndtype: float64\n\n\n\ns2\n\ne     1.20\nj    -3.00\no    -0.30\nt    -0.33\ny   -43.20\ndtype: float64\n\n\n\n# adding these- missing values donot overlap\n\ns1+s2\n\na    NaN\nc    NaN\nd    NaN\ne    2.7\nj    NaN\no    NaN\nt    NaN\ny    NaN\ndtype: float64\n\n\n\n# in case of DataFrame, alignment is performed on both rows and columns\n\ndf1 = pd.DataFrame(np.arange(9.).reshape((3,3)), \n                  columns = list('abc'),\n                  index = ['ferozpur', 'faridkot', 'montréal'])\n\ndf2 = pd.DataFrame(np.arange(12.).reshape((4,3)),\n                  columns = list('abc'),\n                   index = ['faridkot', 'toronto', 'québec', 'montréal'])\n\n\ndf1\n\n\n\n\n\n\n\n\na\nb\nc\n\n\n\n\nferozpur\n0.0\n1.0\n2.0\n\n\nfaridkot\n3.0\n4.0\n5.0\n\n\nmontréal\n6.0\n7.0\n8.0\n\n\n\n\n\n\n\n\ndf2\n\n\n\n\n\n\n\n\na\nb\nc\n\n\n\n\nfaridkot\n0.0\n1.0\n2.0\n\n\ntoronto\n3.0\n4.0\n5.0\n\n\nquébec\n6.0\n7.0\n8.0\n\n\nmontréal\n9.0\n10.0\n11.0\n\n\n\n\n\n\n\n\ndf1 + df2  #because the columns were same, it added those numbers\n\n\n\n\n\n\n\n\na\nb\nc\n\n\n\n\nfaridkot\n3.0\n5.0\n7.0\n\n\nferozpur\nNaN\nNaN\nNaN\n\n\nmontréal\n15.0\n17.0\n19.0\n\n\nquébec\nNaN\nNaN\nNaN\n\n\ntoronto\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n# changing columns names will give all NAN (null values)\ndf3 = pd.DataFrame(np.arange(12.).reshape((4,3)),\n                  columns = list('xyz'),\n                   index = ['faridkot', 'toronto', 'québec', 'montréal'])\n\n\ndf1 + df3\n\n\n\n\n\n\n\n\na\nb\nc\nx\ny\nz\n\n\n\n\nfaridkot\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nferozpur\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nmontréal\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nquébec\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\ntoronto\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN"
  },
  {
    "objectID": "posts/pandas/pandas2.html#arithmetic-methods-with-fill-values",
    "href": "posts/pandas/pandas2.html#arithmetic-methods-with-fill-values",
    "title": "Pandas_2",
    "section": "Arithmetic methods with fill values",
    "text": "Arithmetic methods with fill values\n\ndf2\n\n\n\n\n\n\n\n\na\nb\nc\n\n\n\n\nfaridkot\n0.0\n1.0\n2.0\n\n\ntoronto\n3.0\n4.0\n5.0\n\n\nquébec\n6.0\n7.0\n8.0\n\n\nmontréal\n9.0\n10.0\n11.0\n\n\n\n\n\n\n\n\ndf2.loc['faridkot', 'y'] = np.nan\n\n\ndf2\n\n\n\n\n\n\n\n\na\nb\nc\ny\n\n\n\n\nfaridkot\n0.0\n1.0\n2.0\nNaN\n\n\ntoronto\n3.0\n4.0\n5.0\nNaN\n\n\nquébec\n6.0\n7.0\n8.0\nNaN\n\n\nmontréal\n9.0\n10.0\n11.0\nNaN\n\n\n\n\n\n\n\n\nhelp(pd.DataFrame._drop_axis)\n\nHelp on function _drop_axis in module pandas.core.generic:\n\n_drop_axis(self: 'NDFrameT', labels, axis, level=None, errors: 'IgnoreRaise' = 'raise', only_slice: 'bool_t' = False) -&gt; 'NDFrameT'\n    Drop labels from specified axis. Used in the ``drop`` method\n    internally.\n    \n    Parameters\n    ----------\n    labels : single label or list-like\n    axis : int or axis name\n    level : int or level name, default None\n        For MultiIndex\n    errors : {'ignore', 'raise'}, default 'raise'\n        If 'ignore', suppress error and existing labels are dropped.\n    only_slice : bool, default False\n        Whether indexing along columns should be view-only.\n\n\n\n\nhelp(pd.DataFrame.drop)\n\n\nprint(dir(DataFrame))\n\n['T', '_AXIS_LEN', '_AXIS_ORDERS', '_AXIS_TO_AXIS_NUMBER', '_HANDLED_TYPES', '__abs__', '__add__', '__and__', '__annotations__', '__array__', '__array_priority__', '__array_ufunc__', '__bool__', '__class__', '__contains__', '__copy__', '__dataframe__', '__deepcopy__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__divmod__', '__doc__', '__eq__', '__finalize__', '__floordiv__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__iadd__', '__iand__', '__ifloordiv__', '__imod__', '__imul__', '__init__', '__init_subclass__', '__invert__', '__ior__', '__ipow__', '__isub__', '__iter__', '__itruediv__', '__ixor__', '__le__', '__len__', '__lt__', '__matmul__', '__mod__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__nonzero__', '__or__', '__pos__', '__pow__', '__radd__', '__rand__', '__rdivmod__', '__reduce__', '__reduce_ex__', '__repr__', '__rfloordiv__', '__rmatmul__', '__rmod__', '__rmul__', '__ror__', '__round__', '__rpow__', '__rsub__', '__rtruediv__', '__rxor__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '__xor__', '_accessors', '_accum_func', '_add_numeric_operations', '_agg_examples_doc', '_agg_summary_and_see_also_doc', '_align_frame', '_align_series', '_append', '_arith_method', '_as_manager', '_box_col_values', '_can_fast_transpose', '_check_inplace_and_allows_duplicate_labels', '_check_inplace_setting', '_check_is_chained_assignment_possible', '_check_label_or_level_ambiguity', '_check_setitem_copy', '_clear_item_cache', '_clip_with_one_bound', '_clip_with_scalar', '_cmp_method', '_combine_frame', '_consolidate', '_consolidate_inplace', '_construct_axes_dict', '_construct_result', '_constructor', '_constructor_sliced', '_create_data_for_split_and_tight_to_dict', '_data', '_dir_additions', '_dir_deletions', '_dispatch_frame_op', '_drop_axis', '_drop_labels_or_levels', '_ensure_valid_index', '_find_valid_index', '_from_arrays', '_get_agg_axis', '_get_axis', '_get_axis_name', '_get_axis_number', '_get_axis_resolvers', '_get_block_manager_axis', '_get_bool_data', '_get_cleaned_column_resolvers', '_get_column_array', '_get_index_resolvers', '_get_item_cache', '_get_label_or_level_values', '_get_numeric_data', '_get_value', '_getitem_bool_array', '_getitem_multilevel', '_getitem_nocopy', '_gotitem', '_hidden_attrs', '_indexed_same', '_info_axis', '_info_axis_name', '_info_axis_number', '_info_repr', '_init_mgr', '_inplace_method', '_internal_names', '_internal_names_set', '_is_copy', '_is_homogeneous_type', '_is_label_or_level_reference', '_is_label_reference', '_is_level_reference', '_is_mixed_type', '_is_view', '_iset_item', '_iset_item_mgr', '_iset_not_inplace', '_iter_column_arrays', '_ixs', '_join_compat', '_logical_func', '_logical_method', '_maybe_cache_changed', '_maybe_update_cacher', '_metadata', '_min_count_stat_function', '_needs_reindex_multi', '_protect_consolidate', '_reduce', '_reduce_axis1', '_reindex_axes', '_reindex_columns', '_reindex_index', '_reindex_multi', '_reindex_with_indexers', '_rename', '_replace_columnwise', '_repr_data_resource_', '_repr_fits_horizontal_', '_repr_fits_vertical_', '_repr_html_', '_repr_latex_', '_reset_cache', '_reset_cacher', '_sanitize_column', '_series', '_set_axis', '_set_axis_name', '_set_axis_nocheck', '_set_is_copy', '_set_item', '_set_item_frame_value', '_set_item_mgr', '_set_value', '_setitem_array', '_setitem_frame', '_setitem_slice', '_slice', '_stat_axis', '_stat_axis_name', '_stat_axis_number', '_stat_function', '_stat_function_ddof', '_take', '_take_with_is_copy', '_to_dict_of_blocks', '_to_latex_via_styler', '_typ', '_update_inplace', '_validate_dtype', '_values', '_where', 'abs', 'add', 'add_prefix', 'add_suffix', 'agg', 'aggregate', 'align', 'all', 'any', 'apply', 'applymap', 'asfreq', 'asof', 'assign', 'astype', 'at', 'at_time', 'attrs', 'axes', 'backfill', 'between_time', 'bfill', 'bool', 'boxplot', 'clip', 'columns', 'combine', 'combine_first', 'compare', 'convert_dtypes', 'copy', 'corr', 'corrwith', 'count', 'cov', 'cummax', 'cummin', 'cumprod', 'cumsum', 'describe', 'diff', 'div', 'divide', 'dot', 'drop', 'drop_duplicates', 'droplevel', 'dropna', 'dtypes', 'duplicated', 'empty', 'eq', 'equals', 'eval', 'ewm', 'expanding', 'explode', 'ffill', 'fillna', 'filter', 'first', 'first_valid_index', 'flags', 'floordiv', 'from_dict', 'from_records', 'ge', 'get', 'groupby', 'gt', 'head', 'hist', 'iat', 'idxmax', 'idxmin', 'iloc', 'index', 'infer_objects', 'info', 'insert', 'interpolate', 'isetitem', 'isin', 'isna', 'isnull', 'items', 'iterrows', 'itertuples', 'join', 'keys', 'kurt', 'kurtosis', 'last', 'last_valid_index', 'le', 'loc', 'lt', 'mask', 'max', 'mean', 'median', 'melt', 'memory_usage', 'merge', 'min', 'mod', 'mode', 'mul', 'multiply', 'ndim', 'ne', 'nlargest', 'notna', 'notnull', 'nsmallest', 'nunique', 'pad', 'pct_change', 'pipe', 'pivot', 'pivot_table', 'plot', 'pop', 'pow', 'prod', 'product', 'quantile', 'query', 'radd', 'rank', 'rdiv', 'reindex', 'reindex_like', 'rename', 'rename_axis', 'reorder_levels', 'replace', 'resample', 'reset_index', 'rfloordiv', 'rmod', 'rmul', 'rolling', 'round', 'rpow', 'rsub', 'rtruediv', 'sample', 'select_dtypes', 'sem', 'set_axis', 'set_flags', 'set_index', 'shape', 'shift', 'size', 'skew', 'sort_index', 'sort_values', 'sparse', 'squeeze', 'stack', 'std', 'style', 'sub', 'subtract', 'sum', 'swapaxes', 'swaplevel', 'tail', 'take', 'to_clipboard', 'to_csv', 'to_dict', 'to_excel', 'to_feather', 'to_gbq', 'to_hdf', 'to_html', 'to_json', 'to_latex', 'to_markdown', 'to_numpy', 'to_orc', 'to_parquet', 'to_period', 'to_pickle', 'to_records', 'to_sql', 'to_stata', 'to_string', 'to_timestamp', 'to_xarray', 'to_xml', 'transform', 'transpose', 'truediv', 'truncate', 'tz_convert', 'tz_localize', 'unstack', 'update', 'value_counts', 'values', 'var', 'where', 'xs']\n\n\n\nhelp(pd.DataFrame.describe)\n\n\nhelp(pd.DataFrame._drop_axis)\n\n\ndf4 = df2\n\n\ndf4\n\n\n\n\n\n\n\n\na\nb\nc\ny\n\n\n\n\nfaridkot\n0.0\n1.0\n2.0\nNaN\n\n\ntoronto\n3.0\n4.0\n5.0\nNaN\n\n\nquébec\n6.0\n7.0\n8.0\nNaN\n\n\nmontréal\n9.0\n10.0\n11.0\nNaN\n\n\n\n\n\n\n\n\ndf1 + df4\n\n\n\n\n\n\n\n\na\nb\nc\ny\n\n\n\n\nfaridkot\n3.0\n5.0\n7.0\nNaN\n\n\nferozpur\nNaN\nNaN\nNaN\nNaN\n\n\nmontréal\n15.0\n17.0\n19.0\nNaN\n\n\nquébec\nNaN\nNaN\nNaN\nNaN\n\n\ntoronto\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\ndf4.fill_value = 0\n\ndf4\n\n\n\n\n\n\n\n\na\nb\nc\ny\n\n\n\n\nfaridkot\n0.0\n1.0\n2.0\nNaN\n\n\ntoronto\n3.0\n4.0\n5.0\nNaN\n\n\nquébec\n6.0\n7.0\n8.0\nNaN\n\n\nmontréal\n9.0\n10.0\n11.0\nNaN\n\n\n\n\n\n\n\n\n1/df4\n\n\n\n\n\n\n\n\na\nb\nc\ny\n\n\n\n\nfaridkot\ninf\n1.000000\n0.500000\nNaN\n\n\ntoronto\n0.333333\n0.250000\n0.200000\nNaN\n\n\nquébec\n0.166667\n0.142857\n0.125000\nNaN\n\n\nmontréal\n0.111111\n0.100000\n0.090909\nNaN\n\n\n\n\n\n\n\n\ndf4.rdiv(1)\n\n\n\n\n\n\n\n\na\nb\nc\ny\n\n\n\n\nfaridkot\ninf\n1.000000\n0.500000\nNaN\n\n\ntoronto\n0.333333\n0.250000\n0.200000\nNaN\n\n\nquébec\n0.166667\n0.142857\n0.125000\nNaN\n\n\nmontréal\n0.111111\n0.100000\n0.090909\nNaN\n\n\n\n\n\n\n\n\ndf4.reindex(columns = df4.columns, fill_value=0) # not working\n\n\n\n\n\n\n\n\na\nb\nc\ny\n\n\n\n\nfaridkot\n0.0\n1.0\n2.0\nNaN\n\n\ntoronto\n3.0\n4.0\n5.0\nNaN\n\n\nquébec\n6.0\n7.0\n8.0\nNaN\n\n\nmontréal\n9.0\n10.0\n11.0\nNaN"
  },
  {
    "objectID": "posts/pandas/pandas2.html#operations-between-dataframe-and-series",
    "href": "posts/pandas/pandas2.html#operations-between-dataframe-and-series",
    "title": "Pandas_2",
    "section": "Operations between DataFrame and Series",
    "text": "Operations between DataFrame and Series\n\narr = np.arange(12.).reshape((3,4))\n\narr\n\narray([[ 0.,  1.,  2.,  3.],\n       [ 4.,  5.,  6.,  7.],\n       [ 8.,  9., 10., 11.]])\n\n\n\narr[0]\n\narray([0., 1., 2., 3.])\n\n\n\n# broadcasting\n\narr - arr[0]   #subtracts from all rows  \n\narray([[0., 0., 0., 0.],\n       [4., 4., 4., 4.],\n       [8., 8., 8., 8.]])\n\n\n\nframe\n\n\n\n\n\n\n\n\nohio\ntexas\nburmingham\n\n\n\n\na\n0\n1\n2\n\n\nb\n3\n4\n5\n\n\nc\n6\n7\n8\n\n\n\n\n\n\n\n\nhelp(pd.Series)\n\n\nseries\n\nseries1 = pd.Series(data = np.arange(3), index = ['a', 'b', 'c'])\n\nseries1\n\na    0\nb    1\nc    2\ndtype: int32\n\n\n\nframe-series1\n\n\n\n\n\n\n\n\na\nb\nburmingham\nc\nohio\ntexas\n\n\n\n\na\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nb\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nc\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\nframe + series2\n\n\n\n\n\n\n\n\na\nb\nburmingham\nc\nohio\ntexas\n\n\n\n\na\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nb\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nc\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN"
  },
  {
    "objectID": "posts/pandas/pandas2.html#function-application-and-mapping",
    "href": "posts/pandas/pandas2.html#function-application-and-mapping",
    "title": "Pandas_2",
    "section": "Function application and ‘mapping’",
    "text": "Function application and ‘mapping’\n\nframe2 = pd.DataFrame(np.random.standard_normal((4,3)),\n                     columns = list('bde'),\n                     index = ['utah', 'faridkot', 'shahkot', 'malsahian'])\nframe2\n\n\n\n\n\n\n\n\nb\nd\ne\n\n\n\n\nutah\n-1.549165\n0.443756\n1.013167\n\n\nfaridkot\n1.130587\n-1.289388\n-1.210530\n\n\nshahkot\n1.195553\n0.274397\n0.510043\n\n\nmalsahian\n0.713024\n-1.223282\n1.857681\n\n\n\n\n\n\n\n\nnp.abs(frame2)  #converts non-negative values to positive\n\n\n\n\n\n\n\n\nb\nd\ne\n\n\n\n\nutah\n1.549165\n0.443756\n1.013167\n\n\nfaridkot\n1.130587\n1.289388\n1.210530\n\n\nshahkot\n1.195553\n0.274397\n0.510043\n\n\nmalsahian\n0.713024\n1.223282\n1.857681\n\n\n\n\n\n\n\n\nhelp(np.abs)\n\nHelp on ufunc:\n\nabsolute = &lt;ufunc 'absolute'&gt;\n    absolute(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n    \n    Calculate the absolute value element-wise.\n    \n    ``np.abs`` is a shorthand for this function.\n    \n    Parameters\n    ----------\n    x : array_like\n        Input array.\n    out : ndarray, None, or tuple of ndarray and None, optional\n        A location into which the result is stored. If provided, it must have\n        a shape that the inputs broadcast to. If not provided or None,\n        a freshly-allocated array is returned. A tuple (possible only as a\n        keyword argument) must have length equal to the number of outputs.\n    where : array_like, optional\n        This condition is broadcast over the input. At locations where the\n        condition is True, the `out` array will be set to the ufunc result.\n        Elsewhere, the `out` array will retain its original value.\n        Note that if an uninitialized `out` array is created via the default\n        ``out=None``, locations within it where the condition is False will\n        remain uninitialized.\n    **kwargs\n        For other keyword-only arguments, see the\n        :ref:`ufunc docs &lt;ufuncs.kwargs&gt;`.\n    \n    Returns\n    -------\n    absolute : ndarray\n        An ndarray containing the absolute value of\n        each element in `x`.  For complex input, ``a + ib``, the\n        absolute value is :math:`\\sqrt{ a^2 + b^2 }`.\n        This is a scalar if `x` is a scalar.\n    \n    Examples\n    --------\n    &gt;&gt;&gt; x = np.array([-1.2, 1.2])\n    &gt;&gt;&gt; np.absolute(x)\n    array([ 1.2,  1.2])\n    &gt;&gt;&gt; np.absolute(1.2 + 1j)\n    1.5620499351813308\n    \n    Plot the function over ``[-10, 10]``:\n    \n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    \n    &gt;&gt;&gt; x = np.linspace(start=-10, stop=10, num=101)\n    &gt;&gt;&gt; plt.plot(x, np.absolute(x))\n    &gt;&gt;&gt; plt.show()\n    \n    Plot the function over the complex plane:\n    \n    &gt;&gt;&gt; xx = x + 1j * x[:, np.newaxis]\n    &gt;&gt;&gt; plt.imshow(np.abs(xx), extent=[-10, 10, -10, 10], cmap='gray')\n    &gt;&gt;&gt; plt.show()\n    \n    The `abs` function can be used as a shorthand for ``np.absolute`` on\n    ndarrays.\n    \n    &gt;&gt;&gt; x = np.array([-1.2, 1.2])\n    &gt;&gt;&gt; abs(x)\n    array([1.2, 1.2])\n\n\n\n\n#DataFrame's apply method\n\ndef f1(x):\n    return x.max() - x.min()\n\nframe2.apply(f1)\n\nb    2.744718\nd    1.733145\ne    3.068212\ndtype: float64\n\n\n\n# applying across columns\n\nframe2.apply(f1, axis = 'columns')\n\nutah         2.562332\nfaridkot     2.419976\nshahkot      0.921156\nmalsahian    3.080963\ndtype: float64\n\n\n\n# modifying the function to return Series with multiple values\n\ndef f2(x):\n    return pd.Series([x.min(), x.max()], index= ['min', 'max'])\n\nframe.apply(f2)\n\n\n\n\n\n\n\n\nohio\ntexas\nburmingham\n\n\n\n\nmin\n0\n1\n2\n\n\nmax\n6\n7\n8\n\n\n\n\n\n\n\n\nframe\n\n\n\n\n\n\n\n\nohio\ntexas\nburmingham\n\n\n\n\na\n0\n1\n2\n\n\nb\n3\n4\n5\n\n\nc\n6\n7\n8\n\n\n\n\n\n\n\n\nframe2.apply(f2)\n\n\n\n\n\n\n\n\nb\nd\ne\n\n\n\n\nmin\n-1.549165\n-1.289388\n-1.210530\n\n\nmax\n1.195553\n0.443756\n1.857681\n\n\n\n\n\n\n\n\n# apply map function\n\ndef my_format(x):\n    return f\"{x:.2f}\"\n\nframe2.applymap(my_format)\n\n\n\n\n\n\n\n\nb\nd\ne\n\n\n\n\nutah\n-1.55\n0.44\n1.01\n\n\nfaridkot\n1.13\n-1.29\n-1.21\n\n\nshahkot\n1.20\n0.27\n0.51\n\n\nmalsahian\n0.71\n-1.22\n1.86\n\n\n\n\n\n\n\n\n# applying map function in Series\n\nframe2['e'].map(my_format)\n\nutah          1.01\nfaridkot     -1.21\nshahkot       0.51\nmalsahian     1.86\nName: e, dtype: object"
  },
  {
    "objectID": "posts/pandas/pandas2.html#sorting-and-ranking",
    "href": "posts/pandas/pandas2.html#sorting-and-ranking",
    "title": "Pandas_2",
    "section": "Sorting and Ranking",
    "text": "Sorting and Ranking\n\nobj2 = pd.Series(np.arange(4), index = ['d', 'a', 'b', 'c'])\n\nobj2\n\nd    0\na    1\nb    2\nc    3\ndtype: int32\n\n\n\nobj2.sort_index()\n\na    1\nb    2\nc    3\nd    0\ndtype: int32\n\n\n\n# sorting in DataFrame can be done with either axis\n\nframe = pd.DataFrame(np.arange(8).reshape((2, 4)),\n                     index = ['three', 'one'],\n                     columns = ['d', 'a', 'b', 'c'])\n\n\nframe\n\n                     \n\n\n\n\n\n\n\n\nd\na\nb\nc\n\n\n\n\nthree\n0\n1\n2\n3\n\n\none\n4\n5\n6\n7\n\n\n\n\n\n\n\n\nframe.sort_index()\n\n\n\n\n\n\n\n\nd\na\nb\nc\n\n\n\n\none\n4\n5\n6\n7\n\n\nthree\n0\n1\n2\n3\n\n\n\n\n\n\n\n\nframe.sort_index(axis= 'columns')\n\n\n\n\n\n\n\n\na\nb\nc\nd\n\n\n\n\nthree\n1\n2\n3\n0\n\n\none\n5\n6\n7\n4\n\n\n\n\n\n\n\n\n# data can be stored in descending order aswell\n\nframe.sort_index(axis = 'columns', ascending = False)\n\n\n\n\n\n\n\n\nd\nc\nb\na\n\n\n\n\nthree\n0\n3\n2\n1\n\n\none\n4\n7\n6\n5\n\n\n\n\n\n\n\n\n# sorting a series by its values\n\nobj = pd.Series([4, 7, -3, -2])\n\nobj.sort_values()\n\n2   -3\n3   -2\n0    4\n1    7\ndtype: int64\n\n\n\n# missing values get sorted to the end by default\n\nobj = pd.Series([4, 3, 4, np.nan, 33, np.nan, -3, 3])\n\nobj.sort_values()\n\n6    -3.0\n1     3.0\n7     3.0\n0     4.0\n2     4.0\n4    33.0\n3     NaN\n5     NaN\ndtype: float64\n\n\n\n# using na_position to bring missing values to the front\nobj.sort_values(na_position= 'first')\n\n3     NaN\n5     NaN\n6    -3.0\n1     3.0\n7     3.0\n0     4.0\n2     4.0\n4    33.0\ndtype: float64\n\n\n\n# while sorting a DataFrame \n\nframe = pd.DataFrame({'b': [1, 2, 3, 4, 5], 'a':[3, 43, 33, 1, 5]})\n\nframe\n\n\n\n\n\n\n\n\nb\na\n\n\n\n\n0\n1\n3\n\n\n1\n2\n43\n\n\n2\n3\n33\n\n\n3\n4\n1\n\n\n4\n5\n5\n\n\n\n\n\n\n\n\nframe.sort_values('a')\n\n\n\n\n\n\n\n\nb\na\n\n\n\n\n3\n4\n1\n\n\n0\n1\n3\n\n\n4\n5\n5\n\n\n2\n3\n33\n\n\n1\n2\n43\n\n\n\n\n\n\n\n\n# ranking\nobj = pd.Series([4, 5, -5, 7, 8, 0, 4])\n\nobj.rank()\n\n0    3.5\n1    5.0\n2    1.0\n3    6.0\n4    7.0\n5    2.0\n6    3.5\ndtype: float64\n\n\n\n# ranking in order the data is observed\n\nobj.rank(method='first')\n\n0    3.0\n1    5.0\n2    1.0\n3    6.0\n4    7.0\n5    2.0\n6    4.0\ndtype: float64\n\n\n\nobj.rank(ascending = False)\n\n0    4.5\n1    3.0\n2    7.0\n3    2.0\n4    1.0\n5    6.0\n6    4.5\ndtype: float64\n\n\n\n# DataFrame for rank computation\n\nframe\n\n\n\n\n\n\n\n\nb\na\n\n\n\n\n0\n1\n3\n\n\n1\n2\n43\n\n\n2\n3\n33\n\n\n3\n4\n1\n\n\n4\n5\n5\n\n\n\n\n\n\n\n\nframe.rank(axis = 'columns')\n\n\n\n\n\n\n\n\nb\na\n\n\n\n\n0\n1.0\n2.0\n\n\n1\n1.0\n2.0\n\n\n2\n1.0\n2.0\n\n\n3\n2.0\n1.0\n\n\n4\n1.5\n1.5"
  },
  {
    "objectID": "posts/pandas/pandas2.html#axis-indices-with-duplicate-labels",
    "href": "posts/pandas/pandas2.html#axis-indices-with-duplicate-labels",
    "title": "Pandas_2",
    "section": "Axis indices with duplicate labels",
    "text": "Axis indices with duplicate labels\n\nobj = pd.Series(np.arange(5), index=['a', 'a', 'b','b', 'c' ])\n\nobj\n\na    0\na    1\nb    2\nb    3\nc    4\ndtype: int32\n\n\n\nobj.index.is_unique\n\nFalse\n\n\n\nobj['a']\n\na    0\na    1\ndtype: int32\n\n\n\nobj['c']\n\n4\n\n\n\n# DataFrame\ndf = pd.DataFrame(np.random.standard_normal((5, 3)),\n                 index = ['a', 'a', 'b', 'c', 'b'])\n\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\na\n-1.817751\n0.915854\n-0.389590\n\n\na\n0.603020\n0.573012\n1.070691\n\n\nb\n-0.903033\n1.109707\n0.874381\n\n\nc\n2.529357\n-1.169854\n0.676702\n\n\nb\n-0.368763\n0.723758\n0.375079\n\n\n\n\n\n\n\n\ndf.loc['b']\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\nb\n-0.903033\n1.109707\n0.874381\n\n\nb\n-0.368763\n0.723758\n0.375079\n\n\n\n\n\n\n\n\ndf.loc['c']\n\n0    2.529357\n1   -1.169854\n2    0.676702\nName: c, dtype: float64"
  },
  {
    "objectID": "posts/pandas/pandas2.html#descriptive-statistics",
    "href": "posts/pandas/pandas2.html#descriptive-statistics",
    "title": "Pandas_2",
    "section": "Descriptive statistics",
    "text": "Descriptive statistics\n\ndf.sum()\n\n0    0.042830\n1    2.152477\n2    2.607263\ndtype: float64\n\n\n\ndf.sum(axis = 'columns')\n\na   -1.291487\na    2.246723\nb    1.081055\nc    2.036205\nb    0.730074\ndtype: float64\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\ncount\n5.000000\n5.000000\n5.000000\n\n\nmean\n0.008566\n0.430495\n0.521453\n\n\nstd\n1.659562\n0.917106\n0.570471\n\n\nmin\n-1.817751\n-1.169854\n-0.389590\n\n\n25%\n-0.903033\n0.573012\n0.375079\n\n\n50%\n-0.368763\n0.723758\n0.676702\n\n\n75%\n0.603020\n0.915854\n0.874381\n\n\nmax\n2.529357\n1.109707\n1.070691\n\n\n\n\n\n\n\n\ndf.cumsum()\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\na\n-1.817751\n0.915854\n-0.389590\n\n\na\n-1.214731\n1.488866\n0.681101\n\n\nb\n-2.117763\n2.598573\n1.555481\n\n\nc\n0.411594\n1.428719\n2.232183\n\n\nb\n0.042830\n2.152477\n2.607263"
  },
  {
    "objectID": "posts/pandas/pandas2.html#unique-values-value-counts-and-membership",
    "href": "posts/pandas/pandas2.html#unique-values-value-counts-and-membership",
    "title": "Pandas_2",
    "section": "Unique Values, Value counts, and Membership",
    "text": "Unique Values, Value counts, and Membership\n\nobj = pd.Series(['c', 'd', 'a', 'b', 'n', 'm', 'g', \"k\", 'b', 'c', 'k'])\n\nuniques = obj.unique()\n\nuniques\n\narray(['c', 'd', 'a', 'b', 'n', 'm', 'g', 'k'], dtype=object)\n\n\n\nobj.value_counts()\n\nc    2\nb    2\nk    2\nd    1\na    1\nn    1\nm    1\ng    1\nName: count, dtype: int64\n\n\n\npd.value_counts(obj.to_numpy(), sort = False)\n\nc    2\nd    1\na    1\nb    2\nn    1\nm    1\ng    1\nk    2\nName: count, dtype: int64\n\n\n\n# 'isin' is used for vectorized set memebership\n\nobj\n\n0     c\n1     d\n2     a\n3     b\n4     n\n5     m\n6     g\n7     k\n8     b\n9     c\n10    k\ndtype: object\n\n\n\nmask = obj.isin(['b', 'c'])\n\nmask\n\n0      True\n1     False\n2     False\n3      True\n4     False\n5     False\n6     False\n7     False\n8      True\n9      True\n10    False\ndtype: bool\n\n\n\nobj[mask]\n\n0    c\n3    b\n8    b\n9    c\ndtype: object\n\n\n\nto_match = pd.Series(['c', 'a', 'b', 'b', 'c'])\n\nunique_vals = pd.Series(['c', 'b', 'a'])\n\nindices = pd.Index(unique_vals).get_indexer(to_match)\n\nindices\n\narray([0, 2, 1, 1, 0], dtype=int64)\n\n\n\ndata = pd.DataFrame({'ça va': [1, 2, 3, 4, 5],\n                    'oui, ça va' : [43,3, 2, 4, 2], \n                    'et toi': [3, 2, 44, 1, 5]})\n\ndata\n\n\n\n\n\n\n\n\nça va\noui, ça va\net toi\n\n\n\n\n0\n1\n43\n3\n\n\n1\n2\n3\n2\n\n\n2\n3\n2\n44\n\n\n3\n4\n4\n1\n\n\n4\n5\n2\n5\n\n\n\n\n\n\n\n\n# computing value counts\ndata['ça va'].value_counts().sort_index()\n\nça va\n1    1\n2    1\n3    1\n4    1\n5    1\nName: count, dtype: int64\n\n\n\ndata['et toi'].value_counts().sort_index()\n\net toi\n1     1\n2     1\n3     1\n5     1\n44    1\nName: count, dtype: int64\n\n\n\nresult = data.apply(pd.value_counts).fillna(0)\n\nresult\n\n\n\n\n\n\n\n\nça va\noui, ça va\net toi\n\n\n\n\n1\n1.0\n0.0\n1.0\n\n\n2\n1.0\n2.0\n1.0\n\n\n3\n1.0\n1.0\n1.0\n\n\n4\n1.0\n1.0\n0.0\n\n\n5\n1.0\n0.0\n1.0\n\n\n43\n0.0\n1.0\n0.0\n\n\n44\n0.0\n0.0\n1.0"
  },
  {
    "objectID": "posts/numpy/Numpy_tutorial.html",
    "href": "posts/numpy/Numpy_tutorial.html",
    "title": "Tools - Numpy",
    "section": "",
    "text": "Python is a great general-purpose programming language on its own, but with the help of a few popular libraries (numpy, scipy, matplotlib) it becomes a powerful environment for scientific computing.\nWe expect that many of you will have some experience with Python and numpy; for the rest of you, this section will serve as a quick crash course both on the Python programming language and on the use of Python for scientific computing.\nSome of you may have previous knowledge in Matlab, in which case we also recommend the numpy for Matlab users page (https://docs.scipy.org/doc/numpy-dev/user/numpy-for-matlab-users.html).\nIn this tutorial, we will cover:\n\nBasic Python: Basic data types (Containers, Lists, Dictionaries, Sets, Tuples), Functions, Classes\nNumpy: Arrays, Array indexing, Datatypes, Array math, Broadcasting\nMatplotlib: Plotting, Subplots, Images\nIPython: Creating notebooks, Typical workflows"
  },
  {
    "objectID": "posts/numpy/Numpy_tutorial.html#introduction",
    "href": "posts/numpy/Numpy_tutorial.html#introduction",
    "title": "Tools - Numpy",
    "section": "",
    "text": "Python is a great general-purpose programming language on its own, but with the help of a few popular libraries (numpy, scipy, matplotlib) it becomes a powerful environment for scientific computing.\nWe expect that many of you will have some experience with Python and numpy; for the rest of you, this section will serve as a quick crash course both on the Python programming language and on the use of Python for scientific computing.\nSome of you may have previous knowledge in Matlab, in which case we also recommend the numpy for Matlab users page (https://docs.scipy.org/doc/numpy-dev/user/numpy-for-matlab-users.html).\nIn this tutorial, we will cover:\n\nBasic Python: Basic data types (Containers, Lists, Dictionaries, Sets, Tuples), Functions, Classes\nNumpy: Arrays, Array indexing, Datatypes, Array math, Broadcasting\nMatplotlib: Plotting, Subplots, Images\nIPython: Creating notebooks, Typical workflows"
  },
  {
    "objectID": "posts/numpy/Numpy_tutorial.html#a-brief-note-on-python-versions",
    "href": "posts/numpy/Numpy_tutorial.html#a-brief-note-on-python-versions",
    "title": "Tools - Numpy",
    "section": "A Brief Note on Python Versions",
    "text": "A Brief Note on Python Versions\nAs of Janurary 1, 2020, Python has officially dropped support for python2. We’ll be using Python 3.7 for this iteration of the course. You can check your Python version at the command line by running python --version. In Colab, we can enforce the Python version by clicking Runtime -&gt; Change Runtime Type and selecting python3. Note that as of April 2020, Colab uses Python 3.6.9 which should run everything without any errors.\n\n!python --version\n\nPython 3.6.9"
  },
  {
    "objectID": "posts/numpy/Numpy_tutorial.html#basics-of-python",
    "href": "posts/numpy/Numpy_tutorial.html#basics-of-python",
    "title": "Tools - Numpy",
    "section": "Basics of Python",
    "text": "Basics of Python\nPython is a high-level, dynamically typed multiparadigm programming language. Python code is often said to be almost like pseudocode, since it allows you to express very powerful ideas in very few lines of code while being very readable. As an example, here is an implementation of the classic quicksort algorithm in Python:\n\ndef quicksort(arr):\n    if len(arr) &lt;= 1:\n        return arr\n    pivot = arr[len(arr) // 2]\n    left = [x for x in arr if x &lt; pivot]\n    middle = [x for x in arr if x == pivot]\n    right = [x for x in arr if x &gt; pivot]\n    return quicksort(left) + middle + quicksort(right)\n\nprint(quicksort([3,6,8,10,1,2,1]))\n\n[1, 1, 2, 3, 6, 8, 10]\n\n\n\nBasic data types\n\nNumbers\nIntegers and floats work as you would expect from other languages:\n\nx = 3\nprint(x, type(x))\n\n3 &lt;class 'int'&gt;\nERROR! Session/line number was not unique in database. History logging moved to new session 60\n\n\n\nprint(x + 1)   # Addition\nprint(x - 1)   # Subtraction\nprint(x * 2)   # Multiplication\nprint(x ** 2)  # Exponentiation\n\n4\n2\n6\n9\n\n\n\nx += 1\nprint(x)\nx *= 2\nprint(x)\n\n9\n18\n\n\n\ny = 2.5\nprint(type(y))\nprint(y, y + 1, y * 2, y ** 2)\n\n&lt;class 'float'&gt;\n2.5 3.5 5.0 6.25\n\n\nNote that unlike many languages, Python does not have unary increment (x++) or decrement (x–) operators.\nPython also has built-in types for long integers and complex numbers; you can find all of the details in the documentation.\n\n\nBooleans\nPython implements all of the usual operators for Boolean logic, but uses English words rather than symbols (&&, ||, etc.):\n\nt, f = True, False\nprint(type(t))\n\n&lt;class 'bool'&gt;\n\n\nNow we let’s look at the operations:\n\nprint(t and f) # Logical AND;\nprint(t or f)  # Logical OR;\nprint(not t)   # Logical NOT;\nprint(t != f)  # Logical XOR;\n\nFalse\nTrue\nFalse\nTrue\n\n\n\n\nStrings\n\nhello = 'hello'   # String literals can use single quotes\nworld = \"world\"   # or double quotes; it does not matter\nprint(hello, len(hello))\n\nhello 5\n\n\n\nhw = hello + ' ' + world  # String concatenation\nprint(hw)\n\nhello world\n\n\n\nhw12 = '{} {} {}'.format(hello, world, 12)  # string formatting\nprint(hw12)\n\nhello world 12\n\n\nString objects have a bunch of useful methods; for example:\n\ns = \"hello\"\nprint(s.capitalize())  # Capitalize a string\nprint(s.upper())       # Convert a string to uppercase; prints \"HELLO\"\nprint(s.rjust(7))      # Right-justify a string, padding with spaces\nprint(s.center(7))     # Center a string, padding with spaces\nprint(s.replace('l', '(ell)'))  # Replace all instances of one substring with another\nprint('  world '.strip())  # Strip leading and trailing whitespace\n\nHello\nHELLO\n  hello\n hello \nhe(ell)(ell)o\nworld\n\n\nYou can find a list of all string methods in the documentation.\n\n\n\nContainers\nPython includes several built-in container types: lists, dictionaries, sets, and tuples.\n\nLists\nA list is the Python equivalent of an array, but is resizeable and can contain elements of different types:\n\nxs = [3, 1, 2]   # Create a list\nprint(xs, xs[2])\nprint(xs[-1])     # Negative indices count from the end of the list; prints \"2\"\n\n[3, 1, 2] 2\n2\n\n\n\nxs[2] = 'foo'    # Lists can contain elements of different types\nprint(xs)\n\n[3, 1, 'foo']\n\n\n\nxs.append('bar') # Add a new element to the end of the list\nprint(xs)  \n\n[3, 1, 'foo', 'bar']\n\n\n\nx = xs.pop()     # Remove and return the last element of the list\nprint(x, xs)\n\nbar [3, 1, 'foo']\n\n\nAs usual, you can find all the gory details about lists in the documentation.\n\n\nSlicing\nIn addition to accessing list elements one at a time, Python provides concise syntax to access sublists; this is known as slicing:\n\nnums = list(range(5))    # range is a built-in function that creates a list of integers\nprint(nums)         # Prints \"[0, 1, 2, 3, 4]\"\nprint(nums[2:4])    # Get a slice from index 2 to 4 (exclusive); prints \"[2, 3]\"\nprint(nums[2:])     # Get a slice from index 2 to the end; prints \"[2, 3, 4]\"\nprint(nums[:2])     # Get a slice from the start to index 2 (exclusive); prints \"[0, 1]\"\nprint(nums[:])      # Get a slice of the whole list; prints [\"0, 1, 2, 3, 4]\"\nprint(nums[:-1])    # Slice indices can be negative; prints [\"0, 1, 2, 3]\"\nnums[2:4] = [8, 9] # Assign a new sublist to a slice\nprint(nums)         # Prints \"[0, 1, 8, 9, 4]\"\n\n[0, 1, 2, 3, 4]\n[2, 3]\n[2, 3, 4]\n[0, 1]\n[0, 1, 2, 3, 4]\n[0, 1, 2, 3]\n[0, 1, 8, 9, 4]\n\n\n\n\nLoops\nYou can loop over the elements of a list like this:\n\nanimals = ['cat', 'dog', 'monkey']\nfor animal in animals:\n    print(animal)\n\ncat\ndog\nmonkey\n\n\nIf you want access to the index of each element within the body of a loop, use the built-in enumerate function:\n\nanimals = ['cat', 'dog', 'monkey']\nfor idx, animal in enumerate(animals):\n    print('#{}: {}'.format(idx + 1, animal))\n\n#1: cat\n#2: dog\n#3: monkey\n\n\n\n\nList comprehensions\nWhen programming, frequently we want to transform one type of data into another. As a simple example, consider the following code that computes square numbers:\n\nnums = [0, 1, 2, 3, 4]\nsquares = []\nfor x in nums:\n    squares.append(x ** 2)\nprint(squares)\n\n[0, 1, 4, 9, 16]\n\n\nYou can make this code simpler using a list comprehension:\n\nnums = [0, 1, 2, 3, 4]\nsquares = [x ** 2 for x in nums]\nprint(squares)\n\n[0, 1, 4, 9, 16]\n\n\nList comprehensions can also contain conditions:\n\nnums = [0, 1, 2, 3, 4]\neven_squares = [x ** 2 for x in nums if x % 2 == 0]\nprint(even_squares)\n\n[0, 4, 16]\n\n\n\n\nDictionaries\nA dictionary stores (key, value) pairs, similar to a Map in Java or an object in Javascript. You can use it like this:\n\nd = {'cat': 'cute', 'dog': 'furry'}  # Create a new dictionary with some data\nprint(d['cat'])       # Get an entry from a dictionary; prints \"cute\"\nprint('cat' in d)     # Check if a dictionary has a given key; prints \"True\"\n\ncute\nTrue\n\n\n\nd['fish'] = 'wet'    # Set an entry in a dictionary\nprint(d['fish'])      # Prints \"wet\"\n\nwet\n\n\n\nprint(d['monkey'])  # KeyError: 'monkey' not a key of d\n\nKeyError: ignored\n\n\n\nprint(d.get('monkey', 'N/A'))  # Get an element with a default; prints \"N/A\"\nprint(d.get('fish', 'N/A'))    # Get an element with a default; prints \"wet\"\n\nN/A\nwet\n\n\n\ndel d['fish']        # Remove an element from a dictionary\nprint(d.get('fish', 'N/A')) # \"fish\" is no longer a key; prints \"N/A\"\n\nN/A\n\n\nYou can find all you need to know about dictionaries in the documentation.\nIt is easy to iterate over the keys in a dictionary:\n\nd = {'person': 2, 'cat': 4, 'spider': 8}\nfor animal, legs in d.items():\n    print('A {} has {} legs'.format(animal, legs))\n\nA person has 2 legs\nA cat has 4 legs\nA spider has 8 legs\n\n\nDictionary comprehensions: These are similar to list comprehensions, but allow you to easily construct dictionaries. For example:\n\nnums = [0, 1, 2, 3, 4]\neven_num_to_square = {x: x ** 2 for x in nums if x % 2 == 0}\nprint(even_num_to_square)\n\n{0: 0, 2: 4, 4: 16}\n\n\n\n\nSets\nA set is an unordered collection of distinct elements. As a simple example, consider the following:\n\nanimals = {'cat', 'dog'}\nprint('cat' in animals)   # Check if an element is in a set; prints \"True\"\nprint('fish' in animals)  # prints \"False\"\n\nTrue\nFalse\n\n\n\nanimals.add('fish')      # Add an element to a set\nprint('fish' in animals)\nprint(len(animals))       # Number of elements in a set;\n\nTrue\n3\n\n\n\nanimals.add('cat')       # Adding an element that is already in the set does nothing\nprint(len(animals))       \nanimals.remove('cat')    # Remove an element from a set\nprint(len(animals))       \n\n3\n2\n\n\nLoops: Iterating over a set has the same syntax as iterating over a list; however since sets are unordered, you cannot make assumptions about the order in which you visit the elements of the set:\n\nanimals = {'cat', 'dog', 'fish'}\nfor idx, animal in enumerate(animals):\n    print('#{}: {}'.format(idx + 1, animal))\n\n#1: dog\n#2: cat\n#3: fish\n\n\nSet comprehensions: Like lists and dictionaries, we can easily construct sets using set comprehensions:\n\nfrom math import sqrt\nprint({int(sqrt(x)) for x in range(30)})\n\n{0, 1, 2, 3, 4, 5}\n\n\n\n\nTuples\nA tuple is an (immutable) ordered list of values. A tuple is in many ways similar to a list; one of the most important differences is that tuples can be used as keys in dictionaries and as elements of sets, while lists cannot. Here is a trivial example:\n\nd = {(x, x + 1): x for x in range(10)}  # Create a dictionary with tuple keys\nt = (5, 6)       # Create a tuple\nprint(type(t))\nprint(d[t])       \nprint(d[(1, 2)])\n\n&lt;class 'tuple'&gt;\n5\n1\n\n\n\nt[0] = 1\n\nTypeError: ignored\n\n\n\n\n\nFunctions\nPython functions are defined using the def keyword. For example:\n\ndef sign(x):\n    if x &gt; 0:\n        return 'positive'\n    elif x &lt; 0:\n        return 'negative'\n    else:\n        return 'zero'\n\nfor x in [-1, 0, 1]:\n    print(sign(x))\n\nnegative\nzero\npositive\n\n\nWe will often define functions to take optional keyword arguments, like this:\n\ndef hello(name, loud=False):\n    if loud:\n        print('HELLO, {}'.format(name.upper()))\n    else:\n        print('Hello, {}!'.format(name))\n\nhello('Bob')\nhello('Fred', loud=True)\n\nHello, Bob!\nHELLO, FRED\n\n\n###Classes\nThe syntax for defining classes in Python is straightforward:\n\nclass Greeter:\n\n    # Constructor\n    def __init__(self, name):\n        self.name = name  # Create an instance variable\n\n    # Instance method\n    def greet(self, loud=False):\n        if loud:\n          print('HELLO, {}'.format(self.name.upper()))\n        else:\n          print('Hello, {}!'.format(self.name))\n\ng = Greeter('Fred')  # Construct an instance of the Greeter class\ng.greet()            # Call an instance method; prints \"Hello, Fred\"\ng.greet(loud=True)   # Call an instance method; prints \"HELLO, FRED!\"\n\nHello, Fred!\nHELLO, FRED\n\n\n##Numpy\nNumpy is the core library for scientific computing in Python. It provides a high-performance multidimensional array object, and tools for working with these arrays. If you are already familiar with MATLAB, you might find this tutorial useful to get started with Numpy.\nTo use Numpy, we first need to import the numpy package:\n\nimport numpy as np\n\n###Arrays\nA numpy array is a grid of values, all of the same type, and is indexed by a tuple of nonnegative integers. The number of dimensions is the rank of the array; the shape of an array is a tuple of integers giving the size of the array along each dimension.\nWe can initialize numpy arrays from nested Python lists, and access elements using square brackets:\n\na = np.array([1, 2, 3])  # Create a rank 1 array\nprint(type(a), a.shape, a[0], a[1], a[2])\na[0] = 5                 # Change an element of the array\nprint(a)                  \n\n&lt;class 'numpy.ndarray'&gt; (3,) 1 2 3\n[5 2 3]\n\n\n\nb = np.array([[1,2,3],[4,5,6]])   # Create a rank 2 array\nprint(b)\n\n[[1 2 3]\n [4 5 6]]\n\n\n\nprint(b.shape)\nprint(b[0, 0], b[0, 1], b[1, 0])\n\n(2, 3)\n1 2 4\n\n\nNumpy also provides many functions to create arrays:\n\na = np.zeros((2,2))  # Create an array of all zeros\nprint(a)\n\n[[0. 0.]\n [0. 0.]]\n\n\n\nb = np.ones((1,2))   # Create an array of all ones\nprint(b)\n\n[[1. 1.]]\n\n\n\nc = np.full((2,2), 7) # Create a constant array\nprint(c)\n\n[[7 7]\n [7 7]]\n\n\n\nd = np.eye(2)        # Create a 2x2 identity matrix\nprint(d)\n\n[[1. 0.]\n [0. 1.]]\n\n\n\ne = np.random.random((2,2)) # Create an array filled with random values\nprint(e)\n\n[[0.8690054  0.57244319]\n [0.29647245 0.81464494]]\n\n\n\n\nArray indexing\nNumpy offers several ways to index into arrays.\nSlicing: Similar to Python lists, numpy arrays can be sliced. Since arrays may be multidimensional, you must specify a slice for each dimension of the array:\n\nimport numpy as np\n\n# Create the following rank 2 array with shape (3, 4)\n# [[ 1  2  3  4]\n#  [ 5  6  7  8]\n#  [ 9 10 11 12]]\na = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n\n# Use slicing to pull out the subarray consisting of the first 2 rows\n# and columns 1 and 2; b is the following array of shape (2, 2):\n# [[2 3]\n#  [6 7]]\nb = a[:2, 1:3]\nprint(b)\n\n[[2 3]\n [6 7]]\n\n\nA slice of an array is a view into the same data, so modifying it will modify the original array.\n\nprint(a[0, 1])\nb[0, 0] = 77    # b[0, 0] is the same piece of data as a[0, 1]\nprint(a[0, 1]) \n\n2\n77\n\n\nYou can also mix integer indexing with slice indexing. However, doing so will yield an array of lower rank than the original array. Note that this is quite different from the way that MATLAB handles array slicing:\n\n# Create the following rank 2 array with shape (3, 4)\na = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\nprint(a)\n\n[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]]\n\n\nTwo ways of accessing the data in the middle row of the array. Mixing integer indexing with slices yields an array of lower rank, while using only slices yields an array of the same rank as the original array:\n\nrow_r1 = a[1, :]    # Rank 1 view of the second row of a  \nrow_r2 = a[1:2, :]  # Rank 2 view of the second row of a\nrow_r3 = a[[1], :]  # Rank 2 view of the second row of a\nprint(row_r1, row_r1.shape)\nprint(row_r2, row_r2.shape)\nprint(row_r3, row_r3.shape)\n\n[5 6 7 8] (4,)\n[[5 6 7 8]] (1, 4)\n[[5 6 7 8]] (1, 4)\n\n\n\n# We can make the same distinction when accessing columns of an array:\ncol_r1 = a[:, 1]\ncol_r2 = a[:, 1:2]\nprint(col_r1, col_r1.shape)\nprint()\nprint(col_r2, col_r2.shape)\n\n[ 2  6 10] (3,)\n\n[[ 2]\n [ 6]\n [10]] (3, 1)\n\n\nInteger array indexing: When you index into numpy arrays using slicing, the resulting array view will always be a subarray of the original array. In contrast, integer array indexing allows you to construct arbitrary arrays using the data from another array. Here is an example:\n\na = np.array([[1,2], [3, 4], [5, 6]])\n\n# An example of integer array indexing.\n# The returned array will have shape (3,) and \nprint(a[[0, 1, 2], [0, 1, 0]])\n\n# The above example of integer array indexing is equivalent to this:\nprint(np.array([a[0, 0], a[1, 1], a[2, 0]]))\n\n[1 4 5]\n[1 4 5]\n\n\n\n# When using integer array indexing, you can reuse the same\n# element from the source array:\nprint(a[[0, 0], [1, 1]])\n\n# Equivalent to the previous integer array indexing example\nprint(np.array([a[0, 1], a[0, 1]]))\n\n[2 2]\n[2 2]\n\n\nOne useful trick with integer array indexing is selecting or mutating one element from each row of a matrix:\n\n# Create a new array from which we will select elements\na = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\nprint(a)\n\n[[ 1  2  3]\n [ 4  5  6]\n [ 7  8  9]\n [10 11 12]]\n\n\n\n# Create an array of indices\nb = np.array([0, 2, 0, 1])\n\n# Select one element from each row of a using the indices in b\nprint(a[np.arange(4), b])  # Prints \"[ 1  6  7 11]\"\n\n[ 1  6  7 11]\n\n\n\n# Mutate one element from each row of a using the indices in b\na[np.arange(4), b] += 10\nprint(a)\n\n[[11  2  3]\n [ 4  5 16]\n [17  8  9]\n [10 21 12]]\n\n\nBoolean array indexing: Boolean array indexing lets you pick out arbitrary elements of an array. Frequently this type of indexing is used to select the elements of an array that satisfy some condition. Here is an example:\n\nimport numpy as np\n\na = np.array([[1,2], [3, 4], [5, 6]])\n\nbool_idx = (a &gt; 2)  # Find the elements of a that are bigger than 2;\n                    # this returns a numpy array of Booleans of the same\n                    # shape as a, where each slot of bool_idx tells\n                    # whether that element of a is &gt; 2.\n\nprint(bool_idx)\n\n[[False False]\n [ True  True]\n [ True  True]]\n\n\n\n# We use boolean array indexing to construct a rank 1 array\n# consisting of the elements of a corresponding to the True values\n# of bool_idx\nprint(a[bool_idx])\n\n# We can do all of the above in a single concise statement:\nprint(a[a &gt; 2])\n\n[3 4 5 6]\n[3 4 5 6]\n\n\nFor brevity we have left out a lot of details about numpy array indexing; if you want to know more you should read the documentation.\n###Datatypes\nEvery numpy array is a grid of elements of the same type. Numpy provides a large set of numeric datatypes that you can use to construct arrays. Numpy tries to guess a datatype when you create an array, but functions that construct arrays usually also include an optional argument to explicitly specify the datatype. Here is an example:\n\nx = np.array([1, 2])  # Let numpy choose the datatype\ny = np.array([1.0, 2.0])  # Let numpy choose the datatype\nz = np.array([1, 2], dtype=np.int64)  # Force a particular datatype\n\nprint(x.dtype, y.dtype, z.dtype)\n\nint64 float64 int64\n\n\nYou can read all about numpy datatypes in the documentation.\n\n\nArray math\nBasic mathematical functions operate elementwise on arrays, and are available both as operator overloads and as functions in the numpy module:\n\nx = np.array([[1,2],[3,4]], dtype=np.float64)\ny = np.array([[5,6],[7,8]], dtype=np.float64)\n\n# Elementwise sum; both produce the array\nprint(x + y)\nprint(np.add(x, y))\n\n[[ 6.  8.]\n [10. 12.]]\n[[ 6.  8.]\n [10. 12.]]\n\n\n\n# Elementwise difference; both produce the array\nprint(x - y)\nprint(np.subtract(x, y))\n\n[[-4. -4.]\n [-4. -4.]]\n[[-4. -4.]\n [-4. -4.]]\n\n\n\n# Elementwise product; both produce the array\nprint(x * y)\nprint(np.multiply(x, y))\n\n[[ 5. 12.]\n [21. 32.]]\n[[ 5. 12.]\n [21. 32.]]\n\n\n\n# Elementwise division; both produce the array\n# [[ 0.2         0.33333333]\n#  [ 0.42857143  0.5       ]]\nprint(x / y)\nprint(np.divide(x, y))\n\n[[0.2        0.33333333]\n [0.42857143 0.5       ]]\n[[0.2        0.33333333]\n [0.42857143 0.5       ]]\n\n\n\n# Elementwise square root; produces the array\n# [[ 1.          1.41421356]\n#  [ 1.73205081  2.        ]]\nprint(np.sqrt(x))\n\n[[1.         1.41421356]\n [1.73205081 2.        ]]\n\n\nNote that unlike MATLAB, * is elementwise multiplication, not matrix multiplication. We instead use the dot function to compute inner products of vectors, to multiply a vector by a matrix, and to multiply matrices. dot is available both as a function in the numpy module and as an instance method of array objects:\n\nx = np.array([[1,2],[3,4]])\ny = np.array([[5,6],[7,8]])\n\nv = np.array([9,10])\nw = np.array([11, 12])\n\n# Inner product of vectors; both produce 219\nprint(v.dot(w))\nprint(np.dot(v, w))\n\n219\n219\n\n\nYou can also use the @ operator which is equivalent to numpy’s dot operator.\n\nprint(v @ w)\n\n219\n\n\n\n# Matrix / vector product; both produce the rank 1 array [29 67]\nprint(x.dot(v))\nprint(np.dot(x, v))\nprint(x @ v)\n\n[29 67]\n[29 67]\n[29 67]\n\n\n\n# Matrix / matrix product; both produce the rank 2 array\n# [[19 22]\n#  [43 50]]\nprint(x.dot(y))\nprint(np.dot(x, y))\nprint(x @ y)\n\n[[19 22]\n [43 50]]\n[[19 22]\n [43 50]]\n[[19 22]\n [43 50]]\n\n\nNumpy provides many useful functions for performing computations on arrays; one of the most useful is sum:\n\nx = np.array([[1,2],[3,4]])\n\nprint(np.sum(x))  # Compute sum of all elements; prints \"10\"\nprint(np.sum(x, axis=0))  # Compute sum of each column; prints \"[4 6]\"\nprint(np.sum(x, axis=1))  # Compute sum of each row; prints \"[3 7]\"\n\n10\n[4 6]\n[3 7]\n\n\nYou can find the full list of mathematical functions provided by numpy in the documentation.\nApart from computing mathematical functions using arrays, we frequently need to reshape or otherwise manipulate data in arrays. The simplest example of this type of operation is transposing a matrix; to transpose a matrix, simply use the T attribute of an array object:\n\nprint(x)\nprint(\"transpose\\n\", x.T)\n\n[[1 2]\n [3 4]]\ntranspose\n [[1 3]\n [2 4]]\n\n\n\nv = np.array([[1,2,3]])\nprint(v )\nprint(\"transpose\\n\", v.T)\n\n[[1 2 3]]\ntranspose\n [[1]\n [2]\n [3]]\n\n\n\n\nBroadcasting\nBroadcasting is a powerful mechanism that allows numpy to work with arrays of different shapes when performing arithmetic operations. Frequently we have a smaller array and a larger array, and we want to use the smaller array multiple times to perform some operation on the larger array.\nFor example, suppose that we want to add a constant vector to each row of a matrix. We could do it like this:\n\n# We will add the vector v to each row of the matrix x,\n# storing the result in the matrix y\nx = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\nv = np.array([1, 0, 1])\ny = np.empty_like(x)   # Create an empty matrix with the same shape as x\n\n# Add the vector v to each row of the matrix x with an explicit loop\nfor i in range(4):\n    y[i, :] = x[i, :] + v\n\nprint(y)\n\n[[ 2  2  4]\n [ 5  5  7]\n [ 8  8 10]\n [11 11 13]]\n\n\nThis works; however when the matrix x is very large, computing an explicit loop in Python could be slow. Note that adding the vector v to each row of the matrix x is equivalent to forming a matrix vv by stacking multiple copies of v vertically, then performing elementwise summation of x and vv. We could implement this approach like this:\n\nvv = np.tile(v, (4, 1))  # Stack 4 copies of v on top of each other\nprint(vv)                # Prints \"[[1 0 1]\n                         #          [1 0 1]\n                         #          [1 0 1]\n                         #          [1 0 1]]\"\n\n[[1 0 1]\n [1 0 1]\n [1 0 1]\n [1 0 1]]\n\n\n\ny = x + vv  # Add x and vv elementwise\nprint(y)\n\n[[ 2  2  4]\n [ 5  5  7]\n [ 8  8 10]\n [11 11 13]]\n\n\nNumpy broadcasting allows us to perform this computation without actually creating multiple copies of v. Consider this version, using broadcasting:\n\nimport numpy as np\n\n# We will add the vector v to each row of the matrix x,\n# storing the result in the matrix y\nx = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\nv = np.array([1, 0, 1])\ny = x + v  # Add v to each row of x using broadcasting\nprint(y)\n\n[[ 2  2  4]\n [ 5  5  7]\n [ 8  8 10]\n [11 11 13]]\n\n\nThe line y = x + v works even though x has shape (4, 3) and v has shape (3,) due to broadcasting; this line works as if v actually had shape (4, 3), where each row was a copy of v, and the sum was performed elementwise.\nBroadcasting two arrays together follows these rules:\n\nIf the arrays do not have the same rank, prepend the shape of the lower rank array with 1s until both shapes have the same length.\nThe two arrays are said to be compatible in a dimension if they have the same size in the dimension, or if one of the arrays has size 1 in that dimension.\nThe arrays can be broadcast together if they are compatible in all dimensions.\nAfter broadcasting, each array behaves as if it had shape equal to the elementwise maximum of shapes of the two input arrays.\nIn any dimension where one array had size 1 and the other array had size greater than 1, the first array behaves as if it were copied along that dimension\n\nIf this explanation does not make sense, try reading the explanation from the documentation or this explanation.\nFunctions that support broadcasting are known as universal functions. You can find the list of all universal functions in the documentation.\nHere are some applications of broadcasting:\n\n# Compute outer product of vectors\nv = np.array([1,2,3])  # v has shape (3,)\nw = np.array([4,5])    # w has shape (2,)\n# To compute an outer product, we first reshape v to be a column\n# vector of shape (3, 1); we can then broadcast it against w to yield\n# an output of shape (3, 2), which is the outer product of v and w:\n\nprint(np.reshape(v, (3, 1)) * w)\n\n[[ 4  5]\n [ 8 10]\n [12 15]]\n\n\n\n# Add a vector to each row of a matrix\nx = np.array([[1,2,3], [4,5,6]])\n# x has shape (2, 3) and v has shape (3,) so they broadcast to (2, 3),\n# giving the following matrix:\n\nprint(x + v)\n\n[[2 4 6]\n [5 7 9]]\n\n\n\n# Add a vector to each column of a matrix\n# x has shape (2, 3) and w has shape (2,).\n# If we transpose x then it has shape (3, 2) and can be broadcast\n# against w to yield a result of shape (3, 2); transposing this result\n# yields the final result of shape (2, 3) which is the matrix x with\n# the vector w added to each column. Gives the following matrix:\n\nprint((x.T + w).T)\n\n[[ 5  6  7]\n [ 9 10 11]]\n\n\n\n# Another solution is to reshape w to be a row vector of shape (2, 1);\n# we can then broadcast it directly against x to produce the same\n# output.\nprint(x + np.reshape(w, (2, 1)))\n\n[[ 5  6  7]\n [ 9 10 11]]\n\n\n\n# Multiply a matrix by a constant:\n# x has shape (2, 3). Numpy treats scalars as arrays of shape ();\n# these can be broadcast together to shape (2, 3), producing the\n# following array:\nprint(x * 2)\n\n[[ 2  4  6]\n [ 8 10 12]]\n\n\nBroadcasting typically makes your code more concise and faster, so you should strive to use it where possible.\nThis brief overview has touched on many of the important things that you need to know about numpy, but is far from complete. Check out the numpy reference to find out much more about numpy."
  },
  {
    "objectID": "posts/numpy/Numpy_tutorial.html#matplotlib",
    "href": "posts/numpy/Numpy_tutorial.html#matplotlib",
    "title": "Tools - Numpy",
    "section": "Matplotlib",
    "text": "Matplotlib\nMatplotlib is a plotting library. In this section give a brief introduction to the matplotlib.pyplot module, which provides a plotting system similar to that of MATLAB.\n\nimport matplotlib.pyplot as plt\n\nBy running this special iPython command, we will be displaying plots inline:\n\n%matplotlib inline\n\n\nPlotting\nThe most important function in matplotlib is plot, which allows you to plot 2D data. Here is a simple example:\n\n# Compute the x and y coordinates for points on a sine curve\nx = np.arange(0, 3 * np.pi, 0.1)\ny = np.sin(x)\n\n# Plot the points using matplotlib\nplt.plot(x, y)\n\n\n\n\nWith just a little bit of extra work we can easily plot multiple lines at once, and add a title, legend, and axis labels:\n\ny_sin = np.sin(x)\ny_cos = np.cos(x)\n\n# Plot the points using matplotlib\nplt.plot(x, y_sin)\nplt.plot(x, y_cos)\nplt.xlabel('x axis label')\nplt.ylabel('y axis label')\nplt.title('Sine and Cosine')\nplt.legend(['Sine', 'Cosine'])\n\n&lt;matplotlib.legend.Legend at 0x7f0f39c04780&gt;\n\n\n\n\n\n\n\nSubplots\nYou can plot different things in the same figure using the subplot function. Here is an example:\n\n# Compute the x and y coordinates for points on sine and cosine curves\nx = np.arange(0, 3 * np.pi, 0.1)\ny_sin = np.sin(x)\ny_cos = np.cos(x)\n\n# Set up a subplot grid that has height 2 and width 1,\n# and set the first such subplot as active.\nplt.subplot(2, 1, 1)\n\n# Make the first plot\nplt.plot(x, y_sin)\nplt.title('Sine')\n\n# Set the second subplot as active, and make the second plot.\nplt.subplot(2, 1, 2)\nplt.plot(x, y_cos)\nplt.title('Cosine')\n\n# Show the figure.\nplt.show()\n\n\n\n\nYou can read much more about the subplot function in the documentation."
  },
  {
    "objectID": "posts/ML/Partial_plots.html",
    "href": "posts/ML/Partial_plots.html",
    "title": "Machine Learning",
    "section": "",
    "text": "1. Partial Dependence Plots \n\n\nUses\n\n\nshows how features affect prediction\n\n\ncalculated after the model has been fit\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\ndata = pd.read_csv('FIFA 2018 Statistics.csv')\ny = (data['Man of the Match'] == \"Yes\")  # Convert from string \"Yes\"/\"No\" to binary\nfeature_names = [i for i in data.columns if data[i].dtype in [np.int64]]\nX = data[feature_names]\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\ntree_model = DecisionTreeClassifier(random_state=0, max_depth=5, min_samples_split=5).fit(train_X, train_y)\n\n\nfrom sklearn import tree\nimport graphviz\n\n\ntree_graph = tree.export_graphviz(tree_model, out_file = None, feature_names = feature_names)\ngraphviz.Source(tree_graph)\n\n\nfrom matplotlib import pyplot as plt\nfrom sklearn.inspection import PartialDependenceDisplay\n\n#create plot\ndisp1 = PartialDependenceDisplay.from_estimator(tree_model, val_X, ['Goal Scored'])\nplt.show()\n\n\n\n\n\nInference from graph-\n\n\nscoring a gaol makes a person ‘Man of the match’\n\n\nBut extra goal seems to have no impact.\n\n\nfeature_to_plot = 'Distance Covered (Kms)'\n\ndisp2 = PartialDependenceDisplay.from_estimator (tree_model, val_X, [feature_to_plot])\nplt.show()\n\n\n\n\n\n# same plot with Random_forest\n\nrf_model = RandomForestClassifier(random_state = 0).fit(train_X, train_y)\n\ndisp3 = PartialDependenceDisplay.from_estimator(rf_model, val_X, [feature_to_plot])\nplt.show()\n\n\n\n\n\nInference\n\n\nThe above graphs feature that if a player covers 100 kms, he becomes ‘Man of the match’\n\n\n1st model- DecisionTreeClassifier\n\n\n2nd model - RandomForestClassifier\n\n\n\n 2. 2D Partial Dependence Plots \n\n\nfig, ax = plt.subplots(figsize = (8,6))\nf_names = [{\"Goal Scored\", \"Distance Covered (Kms)\"}]\n\n# simiar to previous, except use use tuple features\ndisp4 = PartialDependenceDisplay.from_estimator(tree_model, val_X, f_names, ax = ax)\nplt.show()\n\n\n\n\n\n\n 3. Practice exercise \n\n\n# import libraries\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom matplotlib import pyplot as plt\nfrom sklearn.inspection import PartialDependenceDisplay\n\n# load data\ndata2 = pd.read_csv('train.csv')\n\n# Remove data with extreme outlier coordinates or negative fares\ndata2 = data2.query('pickup_latitude &gt; 40.7 and pickup_latitude &lt; 40.8 and ' +\n                  'dropoff_latitude &gt; 40.7 and dropoff_latitude &lt; 40.8 and ' +\n                  'pickup_longitude &gt; -74 and pickup_longitude &lt; -73.9 and ' +\n                  'dropoff_longitude &gt; -74 and dropoff_longitude &lt; -73.9 and ' +\n                  'fare_amount &gt; 0'\n                  )\n\ny = data2.fare_amount\n\nbase_features = ['pickup_longitude',\n                 'pickup_latitude',\n                 'dropoff_longitude',\n                 'dropoff_latitude']\n\nX = data2[base_features]\n\n# train the model\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\nfirst_model = RandomForestRegressor(n_estimators=30, random_state=1).fit(train_X, train_y)\nprint(\"Data sample:\")\ndata2.head()\n\nData sample:\n\n\n\n\n\n\n\n\n\nkey\nfare_amount\npickup_datetime\npickup_longitude\npickup_latitude\ndropoff_longitude\ndropoff_latitude\npassenger_count\n\n\n\n\n2\n2011-08-18 00:35:00.00000049\n5.7\n2011-08-18 00:35:00 UTC\n-73.982738\n40.761270\n-73.991242\n40.750562\n2\n\n\n3\n2012-04-21 04:30:42.0000001\n7.7\n2012-04-21 04:30:42 UTC\n-73.987130\n40.733143\n-73.991567\n40.758092\n1\n\n\n4\n2010-03-09 07:51:00.000000135\n5.3\n2010-03-09 07:51:00 UTC\n-73.968095\n40.768008\n-73.956655\n40.783762\n1\n\n\n6\n2012-11-20 20:35:00.0000001\n7.5\n2012-11-20 20:35:00 UTC\n-73.980002\n40.751662\n-73.973802\n40.764842\n1\n\n\n7\n2012-01-04 17:22:00.00000081\n16.5\n2012-01-04 17:22:00 UTC\n-73.951300\n40.774138\n-73.990095\n40.751048\n1\n\n\n\n\n\n\n\n\nfeature_name = 'pickup_longitude'\n\nPartialDependenceDisplay.from_estimator (first_model, val_X, [feature_name])\nplt.show()\n\n\n\n\n\n# apply 'for' loop for all base_features\n\nfor feature_name in base_features:\n    PartialDependenceDisplay.from_estimator(first_model, val_X,  [feature_name])\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# 2D partial Dependence plots\nfig, ax = plt.subplots(figsize = (8, 6))\n\nfeature_names = [('pickup_longitude', 'dropoff_longitude')]\nPartialDependenceDisplay.from_estimator (first_model, val_X, feature_names, ax= ax)\n\n&lt;sklearn.inspection._plot.partial_dependence.PartialDependenceDisplay at 0x1e6ada40650&gt;\n\n\n\n\n\nConsider a scenario where you have only 2 predictive features, which we will call feat_A and feat_B.\nBoth features have minimum values of -1 and maximum values of 1. The partial dependence plot for feat_A increases steeply over its whole range, whereas the partial dependence plot for feature B increases at a slower rate (less steeply) over its whole range.\nDoes this guarantee that feat_A will have a higher permutation importance than feat_B? Why or why not_\nNo. This doesn’t guarantee feat_a is more important. For example, feat_a could have a big effect in the cases where it varies, but could have a single value 99% of the time. In that case, permuting feat_a wouldn’t matter much, since most values would be unchanged.\n\nCreates two features, X1 and X2, having random values in the range [-2, 2].\nCreates a target variable y, which is always 1.\nTrains a RandomForestRegressor model to predict y given X1 and X2.\nCreates a PDP plot for X1 and a scatter plot of X1 vs. y.\nDo you have a prediction about what the PDP plot will look like?\n\n\n# import libraries\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.inspection import PartialDependenceDisplay\nimport matplotlib.pyplot as plt\n\n# generate random data\nnp.random.seed(0) # for reproducibiity\nX1 = np.random.uniform(-2, 2, 100)\nX2 = np.random.uniform(-2, 2, 100)\nX = np.column_stack([X1, X2])\n\n# create target variable y\n#y = np.ones([X.shape[0]])\ny = -2 * X1 * (X1&lt;-1) + X1 - 2 * X1 * (X1&gt;1) - X2\n\n# train RandomForestRegressor\nmodel = RandomForestRegressor()\nmodel.fit(X,y)\n\n# plot\nfix, ax = plt.subplots(figsize =(8, 6))\nPartialDependenceDisplay.from_estimator (model, X, features = [0], ax= ax)\nax.set_title('display feature X1')\nplt.show()\n\n# Scatter Plot of X1 vs. y\nplt.figure(figsize=(8, 6))\nplt.scatter(X1, y)\nplt.title(\"Scatter Plot of X1 vs. y\")\nplt.xlabel(\"X1\")\nplt.ylabel(\"y\")\nplt.show()"
  },
  {
    "objectID": "posts/ML/machine_learning.html",
    "href": "posts/ML/machine_learning.html",
    "title": "Machine learning",
    "section": "",
    "text": "Step- 1 Figure out which column would you use to make a prediciton.\n\nmake prediction target as y\n\nStep 2 Assign different features you’d use to make predictions\n\nassign ‘features’ to variable X\n\nStep 3 - Specify and Fit model\nStep 4 - Make predictions\nexample of decision tree model (simple basic machine learning model; steps-)\n\ncapturing data (training or fitting the model)\n\n\npredicting (based on what a model is fed)\n\n\nevaluation (how accurate the predictions are)\n\nmore factors can be fed into the decision tree that has more ‘splits’\nthese trees are called ‘deeper’ trees\nthe point where we make a predicition is called ‘leaf’\n\n\nimport pandas as pd\nfile = \"melb_data.csv\"\ndata = pd.read_csv(file)\nprint(data.describe())\n\n\n\n\n\n\n\n\nRooms\nPrice\nDistance\nPostcode\nBedroom2\nBathroom\nCar\nLandsize\nBuildingArea\nYearBuilt\nLattitude\nLongtitude\nPropertycount\n\n\n\n\ncount\n13580.000000\n1.358000e+04\n13580.000000\n13580.000000\n13580.000000\n13580.000000\n13518.000000\n13580.000000\n7130.000000\n8205.000000\n13580.000000\n13580.000000\n13580.000000\n\n\nmean\n2.937997\n1.075684e+06\n10.137776\n3105.301915\n2.914728\n1.534242\n1.610075\n558.416127\n151.967650\n1964.684217\n-37.809203\n144.995216\n7454.417378\n\n\nstd\n0.955748\n6.393107e+05\n5.868725\n90.676964\n0.965921\n0.691712\n0.962634\n3990.669241\n541.014538\n37.273762\n0.079260\n0.103916\n4378.581772\n\n\nmin\n1.000000\n8.500000e+04\n0.000000\n3000.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n1196.000000\n-38.182550\n144.431810\n249.000000\n\n\n25%\n2.000000\n6.500000e+05\n6.100000\n3044.000000\n2.000000\n1.000000\n1.000000\n177.000000\n93.000000\n1940.000000\n-37.856822\n144.929600\n4380.000000\n\n\n50%\n3.000000\n9.030000e+05\n9.200000\n3084.000000\n3.000000\n1.000000\n2.000000\n440.000000\n126.000000\n1970.000000\n-37.802355\n145.000100\n6555.000000\n\n\n75%\n3.000000\n1.330000e+06\n13.000000\n3148.000000\n3.000000\n2.000000\n2.000000\n651.000000\n174.000000\n1999.000000\n-37.756400\n145.058305\n10331.000000\n\n\nmax\n10.000000\n9.000000e+06\n48.100000\n3977.000000\n20.000000\n8.000000\n10.000000\n433014.000000\n44515.000000\n2018.000000\n-37.408530\n145.526350\n21650.000000\n\n\n\n\n\n\n\n\n\n\ncount- how many rows have non-missing values\nmean- average\nstd- measures the numerical spread of values\n\n\nprint(data.columns)\n\nIndex(['Suburb', 'Address', 'Rooms', 'Type', 'Price', 'Method', 'SellerG',\n       'Date', 'Distance', 'Postcode', 'Bedroom2', 'Bathroom', 'Car',\n       'Landsize', 'BuildingArea', 'YearBuilt', 'CouncilArea', 'Lattitude',\n       'Longtitude', 'Regionname', 'Propertycount'],\n      dtype='object')\n\n\n\n# dropping missing values\ndata = data.dropna(axis=0)\nprint(data.columns)\nprint(data.describe())\n\nIndex(['Suburb', 'Address', 'Rooms', 'Type', 'Price', 'Method', 'SellerG',\n       'Date', 'Distance', 'Postcode', 'Bedroom2', 'Bathroom', 'Car',\n       'Landsize', 'BuildingArea', 'YearBuilt', 'CouncilArea', 'Lattitude',\n       'Longtitude', 'Regionname', 'Propertycount'],\n      dtype='object')\n             Rooms         Price     Distance     Postcode     Bedroom2  \\\ncount  6196.000000  6.196000e+03  6196.000000  6196.000000  6196.000000   \nmean      2.931407  1.068828e+06     9.751097  3101.947708     2.902034   \nstd       0.971079  6.751564e+05     5.612065    86.421604     0.970055   \nmin       1.000000  1.310000e+05     0.000000  3000.000000     0.000000   \n25%       2.000000  6.200000e+05     5.900000  3044.000000     2.000000   \n50%       3.000000  8.800000e+05     9.000000  3081.000000     3.000000   \n75%       4.000000  1.325000e+06    12.400000  3147.000000     3.000000   \nmax       8.000000  9.000000e+06    47.400000  3977.000000     9.000000   \n\n          Bathroom          Car      Landsize  BuildingArea    YearBuilt  \\\ncount  6196.000000  6196.000000   6196.000000   6196.000000  6196.000000   \nmean      1.576340     1.573596    471.006940    141.568645  1964.081988   \nstd       0.711362     0.929947    897.449881     90.834824    38.105673   \nmin       1.000000     0.000000      0.000000      0.000000  1196.000000   \n25%       1.000000     1.000000    152.000000     91.000000  1940.000000   \n50%       1.000000     1.000000    373.000000    124.000000  1970.000000   \n75%       2.000000     2.000000    628.000000    170.000000  2000.000000   \nmax       8.000000    10.000000  37000.000000   3112.000000  2018.000000   \n\n         Lattitude   Longtitude  Propertycount  \ncount  6196.000000  6196.000000    6196.000000  \nmean    -37.807904   144.990201    7435.489509  \nstd       0.075850     0.099165    4337.698917  \nmin     -38.164920   144.542370     389.000000  \n25%     -37.855438   144.926198    4383.750000  \n50%     -37.802250   144.995800    6567.000000  \n75%     -37.758200   145.052700   10175.000000  \nmax     -37.457090   145.526350   21650.000000  \n\n\n\n# using *dot notation* to predict the prediction target, y\n\ny = data.Price\n\n\n# building a model with few features\n# features are the columns that are used to make predictions\n\ndata_features = ['Rooms', 'Bedroom2', 'Landsize']\n\n\n# saving these in variable x\n\nx = data[data_features]\n\n\nprint(x.describe())\n\n             Rooms     Bedroom2      Landsize\ncount  6196.000000  6196.000000   6196.000000\nmean      2.931407     2.902034    471.006940\nstd       0.971079     0.970055    897.449881\nmin       1.000000     0.000000      0.000000\n25%       2.000000     2.000000    152.000000\n50%       3.000000     3.000000    373.000000\n75%       4.000000     3.000000    628.000000\nmax       8.000000     9.000000  37000.000000\n\n\n\nprint(x.head())\n\n   Rooms  Bedroom2  Landsize\n1      2       2.0     156.0\n2      3       3.0     134.0\n4      4       3.0     120.0\n6      3       4.0     245.0\n7      2       2.0     256.0\n\n\n\n\n\n\n\nfrom sklearn.tree import DecisionTreeRegressor\n\ndata_model = DecisionTreeRegressor(random_state = 1) # ensures same results\n\n# fit\ndata_model.fit(x,y)\n\nDecisionTreeRegressor(random_state=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(random_state=1)\n\n\n\n\n\n\nprint(\"Making predicitons for the following houses:\")\nprint(x.head())\nprint(\"The predictions are: \")\nprint(data_model.predict(x.head()))\n\nMaking predicitons for the following houses:\n   Rooms  Bedroom2  Landsize\n1      2       2.0     156.0\n2      3       3.0     134.0\n4      4       3.0     120.0\n6      3       4.0     245.0\n7      2       2.0     256.0\nThe predictions are: \n[ 993200.         1035333.33333333 1600000.         1876000.\n 1636000.        ]"
  },
  {
    "objectID": "posts/ML/machine_learning.html#ml-models-basic-steps",
    "href": "posts/ML/machine_learning.html#ml-models-basic-steps",
    "title": "Machine learning",
    "section": "",
    "text": "Step- 1 Figure out which column would you use to make a prediciton.\n\nmake prediction target as y\n\nStep 2 Assign different features you’d use to make predictions\n\nassign ‘features’ to variable X\n\nStep 3 - Specify and Fit model\nStep 4 - Make predictions\nexample of decision tree model (simple basic machine learning model; steps-)\n\ncapturing data (training or fitting the model)\n\n\npredicting (based on what a model is fed)\n\n\nevaluation (how accurate the predictions are)\n\nmore factors can be fed into the decision tree that has more ‘splits’\nthese trees are called ‘deeper’ trees\nthe point where we make a predicition is called ‘leaf’\n\n\nimport pandas as pd\nfile = \"melb_data.csv\"\ndata = pd.read_csv(file)\nprint(data.describe())\n\n\n\n\n\n\n\n\nRooms\nPrice\nDistance\nPostcode\nBedroom2\nBathroom\nCar\nLandsize\nBuildingArea\nYearBuilt\nLattitude\nLongtitude\nPropertycount\n\n\n\n\ncount\n13580.000000\n1.358000e+04\n13580.000000\n13580.000000\n13580.000000\n13580.000000\n13518.000000\n13580.000000\n7130.000000\n8205.000000\n13580.000000\n13580.000000\n13580.000000\n\n\nmean\n2.937997\n1.075684e+06\n10.137776\n3105.301915\n2.914728\n1.534242\n1.610075\n558.416127\n151.967650\n1964.684217\n-37.809203\n144.995216\n7454.417378\n\n\nstd\n0.955748\n6.393107e+05\n5.868725\n90.676964\n0.965921\n0.691712\n0.962634\n3990.669241\n541.014538\n37.273762\n0.079260\n0.103916\n4378.581772\n\n\nmin\n1.000000\n8.500000e+04\n0.000000\n3000.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n1196.000000\n-38.182550\n144.431810\n249.000000\n\n\n25%\n2.000000\n6.500000e+05\n6.100000\n3044.000000\n2.000000\n1.000000\n1.000000\n177.000000\n93.000000\n1940.000000\n-37.856822\n144.929600\n4380.000000\n\n\n50%\n3.000000\n9.030000e+05\n9.200000\n3084.000000\n3.000000\n1.000000\n2.000000\n440.000000\n126.000000\n1970.000000\n-37.802355\n145.000100\n6555.000000\n\n\n75%\n3.000000\n1.330000e+06\n13.000000\n3148.000000\n3.000000\n2.000000\n2.000000\n651.000000\n174.000000\n1999.000000\n-37.756400\n145.058305\n10331.000000\n\n\nmax\n10.000000\n9.000000e+06\n48.100000\n3977.000000\n20.000000\n8.000000\n10.000000\n433014.000000\n44515.000000\n2018.000000\n-37.408530\n145.526350\n21650.000000\n\n\n\n\n\n\n\n\n\n\ncount- how many rows have non-missing values\nmean- average\nstd- measures the numerical spread of values\n\n\nprint(data.columns)\n\nIndex(['Suburb', 'Address', 'Rooms', 'Type', 'Price', 'Method', 'SellerG',\n       'Date', 'Distance', 'Postcode', 'Bedroom2', 'Bathroom', 'Car',\n       'Landsize', 'BuildingArea', 'YearBuilt', 'CouncilArea', 'Lattitude',\n       'Longtitude', 'Regionname', 'Propertycount'],\n      dtype='object')\n\n\n\n# dropping missing values\ndata = data.dropna(axis=0)\nprint(data.columns)\nprint(data.describe())\n\nIndex(['Suburb', 'Address', 'Rooms', 'Type', 'Price', 'Method', 'SellerG',\n       'Date', 'Distance', 'Postcode', 'Bedroom2', 'Bathroom', 'Car',\n       'Landsize', 'BuildingArea', 'YearBuilt', 'CouncilArea', 'Lattitude',\n       'Longtitude', 'Regionname', 'Propertycount'],\n      dtype='object')\n             Rooms         Price     Distance     Postcode     Bedroom2  \\\ncount  6196.000000  6.196000e+03  6196.000000  6196.000000  6196.000000   \nmean      2.931407  1.068828e+06     9.751097  3101.947708     2.902034   \nstd       0.971079  6.751564e+05     5.612065    86.421604     0.970055   \nmin       1.000000  1.310000e+05     0.000000  3000.000000     0.000000   \n25%       2.000000  6.200000e+05     5.900000  3044.000000     2.000000   \n50%       3.000000  8.800000e+05     9.000000  3081.000000     3.000000   \n75%       4.000000  1.325000e+06    12.400000  3147.000000     3.000000   \nmax       8.000000  9.000000e+06    47.400000  3977.000000     9.000000   \n\n          Bathroom          Car      Landsize  BuildingArea    YearBuilt  \\\ncount  6196.000000  6196.000000   6196.000000   6196.000000  6196.000000   \nmean      1.576340     1.573596    471.006940    141.568645  1964.081988   \nstd       0.711362     0.929947    897.449881     90.834824    38.105673   \nmin       1.000000     0.000000      0.000000      0.000000  1196.000000   \n25%       1.000000     1.000000    152.000000     91.000000  1940.000000   \n50%       1.000000     1.000000    373.000000    124.000000  1970.000000   \n75%       2.000000     2.000000    628.000000    170.000000  2000.000000   \nmax       8.000000    10.000000  37000.000000   3112.000000  2018.000000   \n\n         Lattitude   Longtitude  Propertycount  \ncount  6196.000000  6196.000000    6196.000000  \nmean    -37.807904   144.990201    7435.489509  \nstd       0.075850     0.099165    4337.698917  \nmin     -38.164920   144.542370     389.000000  \n25%     -37.855438   144.926198    4383.750000  \n50%     -37.802250   144.995800    6567.000000  \n75%     -37.758200   145.052700   10175.000000  \nmax     -37.457090   145.526350   21650.000000  \n\n\n\n# using *dot notation* to predict the prediction target, y\n\ny = data.Price\n\n\n# building a model with few features\n# features are the columns that are used to make predictions\n\ndata_features = ['Rooms', 'Bedroom2', 'Landsize']\n\n\n# saving these in variable x\n\nx = data[data_features]\n\n\nprint(x.describe())\n\n             Rooms     Bedroom2      Landsize\ncount  6196.000000  6196.000000   6196.000000\nmean      2.931407     2.902034    471.006940\nstd       0.971079     0.970055    897.449881\nmin       1.000000     0.000000      0.000000\n25%       2.000000     2.000000    152.000000\n50%       3.000000     3.000000    373.000000\n75%       4.000000     3.000000    628.000000\nmax       8.000000     9.000000  37000.000000\n\n\n\nprint(x.head())\n\n   Rooms  Bedroom2  Landsize\n1      2       2.0     156.0\n2      3       3.0     134.0\n4      4       3.0     120.0\n6      3       4.0     245.0\n7      2       2.0     256.0"
  },
  {
    "objectID": "posts/ML/machine_learning.html#step-3-specify-and-fit-the-model",
    "href": "posts/ML/machine_learning.html#step-3-specify-and-fit-the-model",
    "title": "Machine learning",
    "section": "",
    "text": "from sklearn.tree import DecisionTreeRegressor\n\ndata_model = DecisionTreeRegressor(random_state = 1) # ensures same results\n\n# fit\ndata_model.fit(x,y)\n\nDecisionTreeRegressor(random_state=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(random_state=1)"
  },
  {
    "objectID": "posts/ML/machine_learning.html#step-4-make-predictions",
    "href": "posts/ML/machine_learning.html#step-4-make-predictions",
    "title": "Machine learning",
    "section": "",
    "text": "print(\"Making predicitons for the following houses:\")\nprint(x.head())\nprint(\"The predictions are: \")\nprint(data_model.predict(x.head()))\n\nMaking predicitons for the following houses:\n   Rooms  Bedroom2  Landsize\n1      2       2.0     156.0\n2      3       3.0     134.0\n4      4       3.0     120.0\n6      3       4.0     245.0\n7      2       2.0     256.0\nThe predictions are: \n[ 993200.         1035333.33333333 1600000.         1876000.\n 1636000.        ]"
  },
  {
    "objectID": "posts/matplotlib/Matplotlib_Tutorial.html",
    "href": "posts/matplotlib/Matplotlib_Tutorial.html",
    "title": "Tools - matplotlib",
    "section": "",
    "text": "Load Necessary Libraries\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\n\n\nBasic Graph\n\n\nCode\nx = [0,1,2,3,4]\ny = [0,2,4,6,8]\n\n# Resize your Graph (dpi specifies pixels per inch. When saving probably should use 300 if possible)\nplt.figure(figsize=(8,5), dpi=100)\n\n# Line 1\n\n# Keyword Argument Notation\n#plt.plot(x,y, label='2x', color='red', linewidth=2, marker='.', linestyle='--', markersize=10, markeredgecolor='blue')\n\n# Shorthand notation\n# fmt = '[color][marker][line]'\nplt.plot(x,y, 'b^--', label='2x')\n\n## Line 2\n\n# select interval we want to plot points at\nx2 = np.arange(0,4.5,0.5)\n\n# Plot part of the graph as line\nplt.plot(x2[:6], x2[:6]**2, 'r', label='X^2')\n\n# Plot remainder of graph as a dot\nplt.plot(x2[5:], x2[5:]**2, 'r--')\n\n# Add a title (specify font parameters with fontdict)\nplt.title('Our First Graph!', fontdict={'fontname': 'Comic Sans MS', 'fontsize': 20})\n\n# X and Y labels\nplt.xlabel('X Axis')\nplt.ylabel('Y Axis')\n\n# X, Y axis Tickmarks (scale of your graph)\nplt.xticks([0,1,2,3,4,])\n#plt.yticks([0,2,4,6,8,10])\n\n# Add a legend\nplt.legend()\n\n# Save figure (dpi 300 is good when saving so graph has high resolution)\nplt.savefig('mygraph.png', dpi=300)\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\nBar Chart\n\n\nCode\nlabels = ['A', 'B', 'C']\nvalues = [1,4,2]\n\nplt.figure(figsize=(5,3), dpi=100)\n\nbars = plt.bar(labels, values)\n\npatterns = ['/', 'O', '*']\nfor bar in bars:\n    bar.set_hatch(patterns.pop(0))\n\nplt.savefig('barchart.png', dpi=300)\n\nplt.show()\n\n\n\n\n\n\n\nReal World Examples\nDownload data from his Github (gas_prices.csv & fifa_data.csv)\n\nLine Graph\n\n\nCode\ngas = pd.read_csv('gas_prices.csv')\n\nplt.figure(figsize=(8,5))\n\nplt.title('Gas Prices over Time (in USD)', fontdict={'fontweight':'bold', 'fontsize': 18})\n\nplt.plot(gas.Year, gas.USA, 'b.-', label='United States')\nplt.plot(gas.Year, gas.Canada, 'r.-')\nplt.plot(gas.Year, gas['South Korea'], 'g.-')\nplt.plot(gas.Year, gas.Australia, 'y.-')\n\n# Another Way to plot many values!\n# countries_to_look_at = ['Australia', 'USA', 'Canada', 'South Korea']\n# for country in gas:\n#     if country in countries_to_look_at:\n#         plt.plot(gas.Year, gas[country], marker='.')\n\nplt.xticks(gas.Year[::3].tolist()+[2011])\n\nplt.xlabel('Year')\nplt.ylabel('US Dollars')\n\nplt.legend()\n\nplt.savefig('Gas_price_figure.png', dpi=300)\n\nplt.show()\n\n\n\n\n\n\n\nLoad Fifa Data\n\n\nCode\nfifa = pd.read_csv('fifa_data.csv')\n\nfifa.head(5)\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\n...\nComposure\nMarking\nStandingTackle\nSlidingTackle\nGKDiving\nGKHandling\nGKKicking\nGKPositioning\nGKReflexes\nRelease Clause\n\n\n\n\n0\n0\n158023\nL. Messi\n31\nhttps://cdn.sofifa.org/players/4/19/158023.png\nArgentina\nhttps://cdn.sofifa.org/flags/52.png\n94\n94\nFC Barcelona\n...\n96.0\n33.0\n28.0\n26.0\n6.0\n11.0\n15.0\n14.0\n8.0\n€226.5M\n\n\n1\n1\n20801\nCristiano Ronaldo\n33\nhttps://cdn.sofifa.org/players/4/19/20801.png\nPortugal\nhttps://cdn.sofifa.org/flags/38.png\n94\n94\nJuventus\n...\n95.0\n28.0\n31.0\n23.0\n7.0\n11.0\n15.0\n14.0\n11.0\n€127.1M\n\n\n2\n2\n190871\nNeymar Jr\n26\nhttps://cdn.sofifa.org/players/4/19/190871.png\nBrazil\nhttps://cdn.sofifa.org/flags/54.png\n92\n93\nParis Saint-Germain\n...\n94.0\n27.0\n24.0\n33.0\n9.0\n9.0\n15.0\n15.0\n11.0\n€228.1M\n\n\n3\n3\n193080\nDe Gea\n27\nhttps://cdn.sofifa.org/players/4/19/193080.png\nSpain\nhttps://cdn.sofifa.org/flags/45.png\n91\n93\nManchester United\n...\n68.0\n15.0\n21.0\n13.0\n90.0\n85.0\n87.0\n88.0\n94.0\n€138.6M\n\n\n4\n4\n192985\nK. De Bruyne\n27\nhttps://cdn.sofifa.org/players/4/19/192985.png\nBelgium\nhttps://cdn.sofifa.org/flags/7.png\n91\n92\nManchester City\n...\n88.0\n68.0\n58.0\n51.0\n15.0\n13.0\n5.0\n10.0\n13.0\n€196.4M\n\n\n\n\n5 rows × 89 columns\n\n\n\n\n\nHistogram\n\n\nCode\nbins = [40,50,60,70,80,90,100]\n\nplt.figure(figsize=(8,5))\n\nplt.hist(fifa.Overall, bins=bins, color='#abcdef')\n\nplt.xticks(bins)\n\nplt.ylabel('Number of Players')\nplt.xlabel('Skill Level')\nplt.title('Distribution of Player Skills in FIFA 2018')\n\nplt.savefig('histogram.png', dpi=300)\n\nplt.show()\n\n\n\n\n\n\n\nPie Chart\n\n\nCode\nleft = fifa.loc[fifa['Preferred Foot'] == 'Left'].count()[0]\nright = fifa.loc[fifa['Preferred Foot'] == 'Right'].count()[0]\n\nplt.figure(figsize=(8,5))\n\nlabels = ['Left', 'Right']\ncolors = ['#abcdef', '#aabbcc']\n\nplt.pie([left, right], labels = labels, colors=colors, autopct='%.2f %%')\n\nplt.title('Foot Preference of FIFA Players')\n\nplt.show()\n\n\n\n\n\n\n\nPie Chart #2\n\n\nCode\nplt.figure(figsize=(8,5), dpi=100)\n\nplt.style.use('ggplot')\n\nfifa.Weight = [int(x.strip('lbs')) if type(x)==str else x for x in fifa.Weight]\n\nlight = fifa.loc[fifa.Weight &lt; 125].count()[0]\nlight_medium = fifa[(fifa.Weight &gt;= 125) & (fifa.Weight &lt; 150)].count()[0]\nmedium = fifa[(fifa.Weight &gt;= 150) & (fifa.Weight &lt; 175)].count()[0]\nmedium_heavy = fifa[(fifa.Weight &gt;= 175) & (fifa.Weight &lt; 200)].count()[0]\nheavy = fifa[fifa.Weight &gt;= 200].count()[0]\n\nweights = [light,light_medium, medium, medium_heavy, heavy]\nlabel = ['under 125', '125-150', '150-175', '175-200', 'over 200']\nexplode = (.4,.2,0,0,.4)\n\nplt.title('Weight of Professional Soccer Players (lbs)')\n\nplt.pie(weights, labels=label, explode=explode, pctdistance=0.8,autopct='%.2f %%')\nplt.show()\n\n\n\n\n\n\n\nBox and Whiskers Chart\n\n\nCode\nplt.figure(figsize=(5,8), dpi=100)\n\nplt.style.use('default')\n\nbarcelona = fifa.loc[fifa.Club == \"FC Barcelona\"]['Overall']\nmadrid = fifa.loc[fifa.Club == \"Real Madrid\"]['Overall']\nrevs = fifa.loc[fifa.Club == \"New England Revolution\"]['Overall']\n\n#bp = plt.boxplot([barcelona, madrid, revs], labels=['a','b','c'], boxprops=dict(facecolor='red'))\nbp = plt.boxplot([barcelona, madrid, revs], labels=['FC Barcelona','Real Madrid','NE Revolution'], patch_artist=True, medianprops={'linewidth': 2})\n\nplt.title('Professional Soccer Team Comparison')\nplt.ylabel('FIFA Overall Rating')\n\nfor box in bp['boxes']:\n    # change outline color\n    box.set(color='#4286f4', linewidth=2)\n    # change fill color\n    box.set(facecolor = '#e0e0e0' )\n    # change hatch\n    #box.set(hatch = '/')\n    \nplt.show()"
  },
  {
    "objectID": "posts/irrigraiton_scheduling/content.html",
    "href": "posts/irrigraiton_scheduling/content.html",
    "title": "Irrigation scheduling with Machine learning",
    "section": "",
    "text": "The paper “Neural Network soil moisture model for irrigration scheduling” proposes an intelligent system for predicting soil moisture and optimizing irrigation scheduling using machine learning algorithms.\nThe proposed system uses a wireless sensor network (WSN) to monitor soil moisture levels and collects data from various sensors installed in the soil. The data collected from the WSN is then processed and analyzed using machine learning algorithms to predict soil moisture levels and determine optimal irrigation schedules.\nThe authors used two machine learning algorithms, support vector regression (SVR) and random forest (RF), to develop the predictive models for soil moisture prediction. The models were trained using data collected from the WSN and validated using real-world data.\nThe results of the study showed that the proposed system is effective in predicting soil moisture levels and optimizing irrigation scheduling. The SVR model outperformed the RF model, achieving a mean absolute error of 0.026 for soil moisture prediction.\nHenceforth, the proposed system provides an automated and intelligent solution for soil moisture prediction and irrigation scheduling, which can help farmers optimize their irrigation practices and reduce water usage."
  },
  {
    "objectID": "posts/fr.html",
    "href": "posts/fr.html",
    "title": "Posts en français",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/fr.html#posts-en-français",
    "href": "posts/fr.html#posts-en-français",
    "title": "Posts en français",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/en.html",
    "href": "posts/en.html",
    "title": "All posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\nEfficiency of knn with sample data from scikit-learn\n\n\n1 min\n\n\n\nknn\n\n\nmachine learning\n\n\nalgorithm\n\n\n\n\nKunal Khurana\n\n\nMay 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEfficiency of logistic regression with sample data from scikit-learn\n\n\n1 min\n\n\n\nlogistic regression\n\n\nmachine learning\n\n\nalgorithm\n\n\n\n\nKunal Khurana\n\n\nMay 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData analysis with Pyspark\n\n\n1 min\n\n\n\nBig data\n\n\nPyspark\n\n\nPython\n\n\nmatplotlib\n\n\n\n\nKunal Khurana\n\n\nMar 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\ndashboarding\n\n\nData Analysis\n\n\nstreamlit\n\n\n\n\nKunal Khurana\n\n\nMar 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nData analysis example workbook\n\n\n2 min\n\n\n\npandas\n\n\nseaborn\n\n\nmatplotlib\n\n\nnumpy\n\n\n\n\nKunal Khurana\n\n\nMar 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData aggregation, grouping, and pivoting\n\n\n1 min\n\n\n\nPython\n\n\nPandas\n\n\npivot_tabes\n\n\ngroupby\n\n\n\n\nKunal Khurana\n\n\nMar 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing data visualization libraries\n\n\n1 min\n\n\n\nPython\n\n\nMatplotlib\n\n\nNumpy\n\n\nSeaborn\n\n\nplotting\n\n\n\n\nKunal Khurana\n\n\nMar 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nA brief overview of Time series\n\n\n1 min\n\n\n\nPython\n\n\nPandas\n\n\ntime_series\n\n\n\n\nKunal Khurana\n\n\nMar 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA brief introduction to modeling libraries in Python\n\n\n1 min\n\n\n\nPython\n\n\npatsy\n\n\nstatsmodels\n\n\nscikit-learn\n\n\n\n\nKunal Khurana\n\n\nMar 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData analysis with Pandas\n\n\n1 min\n\n\n\nPandas\n\n\nData Analysis\n\n\nPython\n\n\n\n\nKunal Khurana\n\n\nFeb 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n1 min\n\n\n\nPython\n\n\nSQL\n\n\nDatabase\n\n\nAPIs\n\n\n\n\nKunal Khurana\n\n\nFeb 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n1 min\n\n\n\nPython\n\n\nTransformation\n\n\nCatagorical\n\n\nMapping\n\n\n\n\nKunal Khurana\n\n\nFeb 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n2 min\n\n\n\nPython\n\n\nIndexing\n\n\nCombining\n\n\nReshaping\n\n\nPivoting\n\n\n\n\nKunal Khurana\n\n\nFeb 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData analysis with Numpy\n\n\n1 min\n\n\n\nNumpy\n\n\nData Analysis\n\n\nPython\n\n\n\n\nKunal Khurana\n\n\nFeb 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with files\n\n\n1 min\n\n\n\n.txt\n\n\nFiles\n\n\n\n\nKunal Khurana\n\n\nFeb 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n1 min\n\n\n\nPython\n\n\n\n\nKunal Khurana\n\n\nFeb 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n1 min\n\n\n\nPython\n\n\n\n\nKunal Khurana\n\n\nFeb 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nSample notebook to handle any data for EDA\n\n\n1 min\n\n\n\nPython\n\n\nMatplotlib\n\n\nData Analysis\n\n\nPandas\n\n\nSeaborn\n\n\n\n\nKunal Khurana\n\n\nJan 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1D and 2D partial dependent plots with RandomForestClassifier and DecisionTreeClassifier\n\n\n2 min\n\n\n\nPython\n\n\nMachine Learning\n\n\nData Analysis\n\n\n\n\nKunal Khurana\n\n\nJan 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData analysis with GeoPandas\n\n\n1 min\n\n\n\nPython\n\n\nGeoPandas\n\n\nData Analysis\n\n\n\n\nKunal Khurana\n\n\nJan 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData analysis with Pandas\n\n\n2 min\n\n\n\nPandas\n\n\nData Analysis\n\n\nPython\n\n\n\n\nKunal Khurana\n\n\nDec 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbasics\n\n\n2 min\n\n\n\nMachine learning\n\n\nBasics\n\n\nData Analysis\n\n\n\n\nKunal Khurana\n\n\nDec 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n1 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nDec 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n3 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nDec 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n1 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nOct 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n2 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nOct 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n2 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nOct 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n2 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nOct 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n1 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nOct 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n1 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nOct 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n1 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nOct 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n2 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nOct 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n2 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nOct 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n2 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nOct 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n7 min\n\n\n\nMachine learning\n\n\nData Analysis\n\n\n\n\n\n\n\nJun 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizatons\n\n\n1 min\n\n\n\nVisualizations\n\n\nData Analysis\n\n\n\n\n\n\n\nJun 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploratory analysis\n\n\n3 min\n\n\n\nSoils\n\n\nData Analysis\n\n\n\n\n\n\n\nJun 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n10 min\n\n\n\nNumpy\n\n\nData Analysis\n\n\nPython\n\n\n\n\nJustin Johnson, Volodymr Kuleshov, and Issac Caswell\n\n\nJun 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n1 min\n\n\n\nMatplotlib\n\n\nData Analysis\n\n\n\n\nKeith Galli\n\n\nJun 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n21 min\n\n\n\nPandas\n\n\nData Analysis\n\n\n\n\n\n\n\nJun 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\npackages\n\n\ndata\n\n\n\n\n\n\n\nMay 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nplot\n\n\ndata\n\n\n\n\n\n\n\nMay 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Visualization\n\n\n7 min\n\n\n\nNumpy\n\n\nPandas\n\n\nData Analysis\n\n\n\n\nNonu Singh\n\n\nMay 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nAI\n\n\nIrrigation\n\n\nMachine learning\n\n\n\n\nKunal Khurana\n\n\nMay 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nC\n\n\nBamboo\n\n\n\n\nKunal Khurana\n\n\nMay 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nMachine learning\n\n\nAI\n\n\nIrrigation\n\n\n\n\nKunal Khurana\n\n\nMay 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nMachine learning\n\n\nSoil Analysis\n\n\nYield prediction\n\n\n\n\nKunal Khurana\n\n\nApr 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nMachine learning\n\n\nIrrigation\n\n\nModelling\n\n\n\n\nKunal Khurana\n\n\nApr 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nDigitalization\n\n\nMachine learning\n\n\nAI\n\n\n\n\nKunal Khurana\n\n\nApr 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData analysis with GeoPandas\n\n\n1 min\n\n\n\nPython\n\n\nGeoPandas\n\n\nData Analysis\n\n\n\n\nKunal Khurana\n\n\nJan 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData analysis with GeoPandas\n\n\n1 min\n\n\n\nPython\n\n\nGeoPandas\n\n\nProximity Analysis\n\n\n\n\nKunal Khurana\n\n\nJan 7, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/en.html#all-posts",
    "href": "posts/en.html#all-posts",
    "title": "All posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\nEfficiency of knn with sample data from scikit-learn\n\n\n1 min\n\n\n\nknn\n\n\nmachine learning\n\n\nalgorithm\n\n\n\n\nKunal Khurana\n\n\nMay 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEfficiency of logistic regression with sample data from scikit-learn\n\n\n1 min\n\n\n\nlogistic regression\n\n\nmachine learning\n\n\nalgorithm\n\n\n\n\nKunal Khurana\n\n\nMay 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData analysis with Pyspark\n\n\n1 min\n\n\n\nBig data\n\n\nPyspark\n\n\nPython\n\n\nmatplotlib\n\n\n\n\nKunal Khurana\n\n\nMar 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\ndashboarding\n\n\nData Analysis\n\n\nstreamlit\n\n\n\n\nKunal Khurana\n\n\nMar 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nData analysis example workbook\n\n\n2 min\n\n\n\npandas\n\n\nseaborn\n\n\nmatplotlib\n\n\nnumpy\n\n\n\n\nKunal Khurana\n\n\nMar 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData aggregation, grouping, and pivoting\n\n\n1 min\n\n\n\nPython\n\n\nPandas\n\n\npivot_tabes\n\n\ngroupby\n\n\n\n\nKunal Khurana\n\n\nMar 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing data visualization libraries\n\n\n1 min\n\n\n\nPython\n\n\nMatplotlib\n\n\nNumpy\n\n\nSeaborn\n\n\nplotting\n\n\n\n\nKunal Khurana\n\n\nMar 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nA brief overview of Time series\n\n\n1 min\n\n\n\nPython\n\n\nPandas\n\n\ntime_series\n\n\n\n\nKunal Khurana\n\n\nMar 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA brief introduction to modeling libraries in Python\n\n\n1 min\n\n\n\nPython\n\n\npatsy\n\n\nstatsmodels\n\n\nscikit-learn\n\n\n\n\nKunal Khurana\n\n\nMar 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData analysis with Pandas\n\n\n1 min\n\n\n\nPandas\n\n\nData Analysis\n\n\nPython\n\n\n\n\nKunal Khurana\n\n\nFeb 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n1 min\n\n\n\nPython\n\n\nSQL\n\n\nDatabase\n\n\nAPIs\n\n\n\n\nKunal Khurana\n\n\nFeb 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n1 min\n\n\n\nPython\n\n\nTransformation\n\n\nCatagorical\n\n\nMapping\n\n\n\n\nKunal Khurana\n\n\nFeb 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n2 min\n\n\n\nPython\n\n\nIndexing\n\n\nCombining\n\n\nReshaping\n\n\nPivoting\n\n\n\n\nKunal Khurana\n\n\nFeb 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData analysis with Numpy\n\n\n1 min\n\n\n\nNumpy\n\n\nData Analysis\n\n\nPython\n\n\n\n\nKunal Khurana\n\n\nFeb 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with files\n\n\n1 min\n\n\n\n.txt\n\n\nFiles\n\n\n\n\nKunal Khurana\n\n\nFeb 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n1 min\n\n\n\nPython\n\n\n\n\nKunal Khurana\n\n\nFeb 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n1 min\n\n\n\nPython\n\n\n\n\nKunal Khurana\n\n\nFeb 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nSample notebook to handle any data for EDA\n\n\n1 min\n\n\n\nPython\n\n\nMatplotlib\n\n\nData Analysis\n\n\nPandas\n\n\nSeaborn\n\n\n\n\nKunal Khurana\n\n\nJan 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1D and 2D partial dependent plots with RandomForestClassifier and DecisionTreeClassifier\n\n\n2 min\n\n\n\nPython\n\n\nMachine Learning\n\n\nData Analysis\n\n\n\n\nKunal Khurana\n\n\nJan 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData analysis with GeoPandas\n\n\n1 min\n\n\n\nPython\n\n\nGeoPandas\n\n\nData Analysis\n\n\n\n\nKunal Khurana\n\n\nJan 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData analysis with Pandas\n\n\n2 min\n\n\n\nPandas\n\n\nData Analysis\n\n\nPython\n\n\n\n\nKunal Khurana\n\n\nDec 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbasics\n\n\n2 min\n\n\n\nMachine learning\n\n\nBasics\n\n\nData Analysis\n\n\n\n\nKunal Khurana\n\n\nDec 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n1 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nDec 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n3 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nDec 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n1 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nOct 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n2 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nOct 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n2 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nOct 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n2 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nOct 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n1 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nOct 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n1 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nOct 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n1 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nOct 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n2 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nOct 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n2 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nOct 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n2 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nOct 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n7 min\n\n\n\nMachine learning\n\n\nData Analysis\n\n\n\n\n\n\n\nJun 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizatons\n\n\n1 min\n\n\n\nVisualizations\n\n\nData Analysis\n\n\n\n\n\n\n\nJun 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploratory analysis\n\n\n3 min\n\n\n\nSoils\n\n\nData Analysis\n\n\n\n\n\n\n\nJun 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n10 min\n\n\n\nNumpy\n\n\nData Analysis\n\n\nPython\n\n\n\n\nJustin Johnson, Volodymr Kuleshov, and Issac Caswell\n\n\nJun 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n1 min\n\n\n\nMatplotlib\n\n\nData Analysis\n\n\n\n\nKeith Galli\n\n\nJun 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n21 min\n\n\n\nPandas\n\n\nData Analysis\n\n\n\n\n\n\n\nJun 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\npackages\n\n\ndata\n\n\n\n\n\n\n\nMay 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nplot\n\n\ndata\n\n\n\n\n\n\n\nMay 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Visualization\n\n\n7 min\n\n\n\nNumpy\n\n\nPandas\n\n\nData Analysis\n\n\n\n\nNonu Singh\n\n\nMay 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nAI\n\n\nIrrigation\n\n\nMachine learning\n\n\n\n\nKunal Khurana\n\n\nMay 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nC\n\n\nBamboo\n\n\n\n\nKunal Khurana\n\n\nMay 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nMachine learning\n\n\nAI\n\n\nIrrigation\n\n\n\n\nKunal Khurana\n\n\nMay 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nMachine learning\n\n\nSoil Analysis\n\n\nYield prediction\n\n\n\n\nKunal Khurana\n\n\nApr 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nMachine learning\n\n\nIrrigation\n\n\nModelling\n\n\n\n\nKunal Khurana\n\n\nApr 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nDigitalization\n\n\nMachine learning\n\n\nAI\n\n\n\n\nKunal Khurana\n\n\nApr 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData analysis with GeoPandas\n\n\n1 min\n\n\n\nPython\n\n\nGeoPandas\n\n\nData Analysis\n\n\n\n\nKunal Khurana\n\n\nJan 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData analysis with GeoPandas\n\n\n1 min\n\n\n\nPython\n\n\nGeoPandas\n\n\nProximity Analysis\n\n\n\n\nKunal Khurana\n\n\nJan 7, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/data_analysis/eda-univariate-bivariate-corelation.html",
    "href": "posts/data_analysis/eda-univariate-bivariate-corelation.html",
    "title": "Data analytics workflow",
    "section": "",
    "text": "#!pip install calmap\n!pip install ydata-profiling\n\n\n1. Libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings as wrn\n\nwrn.filterwarnings('ignore', category = DeprecationWarning) \nwrn.filterwarnings('ignore', category = FutureWarning) \nwrn.filterwarnings('ignore', category = UserWarning) \n#from pandas_profiling import ProfileReport\n\nContext\n\nInvoice ID: A unique identifier for each invoice or transaction.\nBranch: The branch or location where the transaction occurred.\nCity: The city where the branch is located.\nCustomer Type: Indicates whether the customer is a regular or new customer.\nGender: The gender of the customer.\nProduct Line: The category or type of product purchased.\nUnit Price: The price of a single unit of the product.\nQuantity: The number of units of the product purchased.\nTax 5%: The amount of tax (5% of the total cost) applied to the transaction.\nTotal: The total cost of the transaction, including tax.\nDate: The date when the transaction took place.\nTime: The time of day when the transaction occurred.\nPayment: The payment method used (e.g., credit card, cash).\nCOGS (Cost of Goods Sold): The direct costs associated with producing or purchasing the products sold.\nGross Margin Percentage: The profit margin percentage for the transaction.\nGross Income: The total profit earned from the transaction.\nRating: Customer satisfaction rating or feedback on the transaction.\n\nFor instance, if you were interested in predicting customer satisfaction, Rating might be a suitable label. If you were trying to predict sales or revenue, Total or Gross Income could be a potential label.\n\n\n2. Initial Data Exploration\n\ndf = pd.read_csv(\"/kaggle/input/super-market-sales/supermarket_sales.csv\")\n\n\ndf.head(10)\n\n\n\n\n\n\n\n\nInvoice ID\nBranch\nCity\nCustomer type\nGender\nProduct line\nUnit price\nQuantity\nTax 5%\nTotal\nDate\nTime\nPayment\ncogs\ngross margin percentage\ngross income\nRating\n\n\n\n\n0\n750-67-8428\nA\nYangon\nMember\nFemale\nHealth and beauty\n74.69\n7\n26.1415\n548.9715\n1/5/2019\n13:08\nEwallet\n522.83\n4.761905\n26.1415\n9.1\n\n\n1\n226-31-3081\nC\nNaypyitaw\nNormal\nFemale\nElectronic accessories\n15.28\n5\n3.8200\n80.2200\n3/8/2019\n10:29\nCash\n76.40\n4.761905\n3.8200\n9.6\n\n\n2\n631-41-3108\nA\nYangon\nNormal\nMale\nHome and lifestyle\n46.33\n7\n16.2155\n340.5255\n3/3/2019\n13:23\nCredit card\n324.31\n4.761905\n16.2155\n7.4\n\n\n3\n123-19-1176\nA\nYangon\nMember\nMale\nHealth and beauty\n58.22\n8\n23.2880\n489.0480\n1/27/2019\n20:33\nEwallet\n465.76\n4.761905\n23.2880\n8.4\n\n\n4\n373-73-7910\nA\nYangon\nNormal\nMale\nSports and travel\n86.31\n7\n30.2085\n634.3785\n2/8/2019\n10:37\nEwallet\n604.17\n4.761905\n30.2085\n5.3\n\n\n5\n699-14-3026\nC\nNaypyitaw\nNormal\nMale\nElectronic accessories\n85.39\n7\n29.8865\n627.6165\n3/25/2019\n18:30\nEwallet\n597.73\n4.761905\n29.8865\n4.1\n\n\n6\n355-53-5943\nA\nYangon\nMember\nFemale\nElectronic accessories\n68.84\n6\n20.6520\n433.6920\n2/25/2019\n14:36\nEwallet\n413.04\n4.761905\n20.6520\n5.8\n\n\n7\n315-22-5665\nC\nNaypyitaw\nNormal\nFemale\nHome and lifestyle\n73.56\n10\n36.7800\n772.3800\n2/24/2019\n11:38\nEwallet\n735.60\n4.761905\n36.7800\n8.0\n\n\n8\n665-32-9167\nA\nYangon\nMember\nFemale\nHealth and beauty\n36.26\n2\n3.6260\n76.1460\n1/10/2019\n17:15\nCredit card\n72.52\n4.761905\n3.6260\n7.2\n\n\n9\n692-92-5582\nB\nMandalay\nMember\nFemale\nFood and beverages\n54.84\n3\n8.2260\n172.7460\n2/20/2019\n13:27\nCredit card\n164.52\n4.761905\n8.2260\n5.9\n\n\n\n\n\n\n\n\ndf.columns\n\nIndex(['Invoice ID', 'Branch', 'City', 'Customer type', 'Gender',\n       'Product line', 'Unit price', 'Quantity', 'Tax 5%', 'Total', 'Date',\n       'Time', 'Payment', 'cogs', 'gross margin percentage', 'gross income',\n       'Rating'],\n      dtype='object')\n\n\n\ndf.dtypes\n\nInvoice ID                  object\nBranch                      object\nCity                        object\nCustomer type               object\nGender                      object\nProduct line                object\nUnit price                 float64\nQuantity                     int64\nTax 5%                     float64\nTotal                      float64\nDate                        object\nTime                        object\nPayment                     object\ncogs                       float64\ngross margin percentage    float64\ngross income               float64\nRating                     float64\ndtype: object\n\n\n\ndf['Date'] = pd.to_datetime(df['Date'])\n\n\ndf.dtypes\n\nInvoice ID                         object\nBranch                             object\nCity                               object\nCustomer type                      object\nGender                             object\nProduct line                       object\nUnit price                        float64\nQuantity                            int64\nTax 5%                            float64\nTotal                             float64\nDate                       datetime64[ns]\nTime                               object\nPayment                            object\ncogs                              float64\ngross margin percentage           float64\ngross income                      float64\nRating                            float64\ndtype: object\n\n\n\ndf.set_index(\"Date\", inplace=True)\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nUnit price\nQuantity\nTax 5%\nTotal\ncogs\ngross margin percentage\ngross income\nRating\n\n\n\n\ncount\n1000.000000\n1000.000000\n1000.000000\n1000.000000\n1000.00000\n1000.000000\n1000.000000\n1000.00000\n\n\nmean\n55.672130\n5.510000\n15.379369\n322.966749\n307.58738\n4.761905\n15.379369\n6.97270\n\n\nstd\n26.494628\n2.923431\n11.708825\n245.885335\n234.17651\n0.000000\n11.708825\n1.71858\n\n\nmin\n10.080000\n1.000000\n0.508500\n10.678500\n10.17000\n4.761905\n0.508500\n4.00000\n\n\n25%\n32.875000\n3.000000\n5.924875\n124.422375\n118.49750\n4.761905\n5.924875\n5.50000\n\n\n50%\n55.230000\n5.000000\n12.088000\n253.848000\n241.76000\n4.761905\n12.088000\n7.00000\n\n\n75%\n77.935000\n8.000000\n22.445250\n471.350250\n448.90500\n4.761905\n22.445250\n8.50000\n\n\nmax\n99.960000\n10.000000\n49.650000\n1042.650000\n993.00000\n4.761905\n49.650000\n10.00000\n\n\n\n\n\n\n\n\n\n3. Univariate Analysis\nQ1 What does the disribution of customer rating looks like? Is it skewed?\n\nsns.displot(df[\"Rating\"])\nplt.axvline(x=np.mean(df[\"Rating\"]), c='red', ls= \"--\")\nplt.axvline(x=np.percentile(df[\"Rating\"],25), c='green', ls= \"--\")\nplt.axvline(x=np.percentile(df[\"Rating\"],75), c='green', ls= \"--\")\n\n&lt;matplotlib.lines.Line2D at 0x7fa762ae94b0&gt;\n\n\n\n\n\n\ndf.hist(figsize=(10,10))\n\narray([[&lt;Axes: title={'center': 'Unit price'}&gt;,\n        &lt;Axes: title={'center': 'Quantity'}&gt;,\n        &lt;Axes: title={'center': 'Tax 5%'}&gt;],\n       [&lt;Axes: title={'center': 'Total'}&gt;,\n        &lt;Axes: title={'center': 'cogs'}&gt;,\n        &lt;Axes: title={'center': 'gross margin percentage'}&gt;],\n       [&lt;Axes: title={'center': 'gross income'}&gt;,\n        &lt;Axes: title={'center': 'Rating'}&gt;, &lt;Axes: &gt;]], dtype=object)\n\n\n\n\n\n\ndf['Branch'].value_counts()\n\nBranch\nA    340\nB    332\nC    328\nName: count, dtype: int64\n\n\n\n\n4. Bivariate analysis\n\n#sns.countplot(df['Payment'])\n\n\n# comparison between two columns\nsns.scatterplot(df['Rating'])\n\n&lt;Axes: xlabel='Date', ylabel='Rating'&gt;\n\n\n\n\n\nQ2: is there a noticiable time trend in gross income?\n\nsns.boxplot(df, x='Branch', y='gross income')\n\n&lt;Axes: xlabel='Branch', ylabel='gross income'&gt;\n\n\n\n\n\n\nsns.boxplot(df, x=\"Gender\", y=\"gross income\")\n\n&lt;Axes: xlabel='Gender', ylabel='gross income'&gt;\n\n\n\n\n\n\ndf.groupby(by='gross income')\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x7fa75e5eb910&gt;\n\n\n\nsns.pairplot(df)\n\n\n\n\n\n\n5. Dealing with duplicate rows and missing values\n\ndf.duplicated()\n\nDate\n2019-01-05    False\n2019-03-08    False\n2019-03-03    False\n2019-01-27    False\n2019-02-08    False\n              ...  \n2019-01-29    False\n2019-03-02    False\n2019-02-09    False\n2019-02-22    False\n2019-02-18    False\nLength: 1000, dtype: bool\n\n\n\ndf.duplicated().sum()\n\n0\n\n\n\ndf.isna().sum()\n\nInvoice ID                 0\nBranch                     0\nCity                       0\nCustomer type              0\nGender                     0\nProduct line               0\nUnit price                 0\nQuantity                   0\nTax 5%                     0\nTotal                      0\nTime                       0\nPayment                    0\ncogs                       0\ngross margin percentage    0\ngross income               0\nRating                     0\ndtype: int64\n\n\n\nsns.heatmap(df.isnull())\n\n&lt;Axes: ylabel='Date'&gt;\n\n\n\n\n\n\ndf.mode()\n\n\n\n\n\n\n\n\nInvoice ID\nBranch\nCity\nCustomer type\nGender\nProduct line\nUnit price\nQuantity\nTax 5%\nTotal\nTime\nPayment\ncogs\ngross margin percentage\ngross income\nRating\n\n\n\n\n0\n101-17-6199\nA\nYangon\nMember\nFemale\nFashion accessories\n83.77\n10.0\n4.1540\n87.2340\n14:42\nEwallet\n83.08\n4.761905\n4.1540\n6.0\n\n\n1\n101-81-4070\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n4.4640\n93.7440\n19:48\nNaN\n89.28\nNaN\n4.4640\nNaN\n\n\n2\n102-06-2002\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n8.3770\n175.9170\nNaN\nNaN\n167.54\nNaN\n8.3770\nNaN\n\n\n3\n102-77-2261\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n9.0045\n189.0945\nNaN\nNaN\n180.09\nNaN\n9.0045\nNaN\n\n\n4\n105-10-6182\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n10.3260\n216.8460\nNaN\nNaN\n206.52\nNaN\n10.3260\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n995\n894-41-5205\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n996\n895-03-6665\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n997\n895-66-0685\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n998\n896-34-0956\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n999\n898-04-2717\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n1000 rows × 16 columns\n\n\n\n\ndf.mode().iloc[0]\n\nInvoice ID                         101-17-6199\nBranch                                       A\nCity                                    Yangon\nCustomer type                           Member\nGender                                  Female\nProduct line               Fashion accessories\nUnit price                               83.77\nQuantity                                  10.0\nTax 5%                                   4.154\nTotal                                   87.234\nTime                                     14:42\nPayment                                Ewallet\ncogs                                     83.08\ngross margin percentage               4.761905\ngross income                             4.154\nRating                                     6.0\nName: 0, dtype: object\n\n\n\n\n6. Correlation analysis\n\nnp.corrcoef(df[\"gross income\"], df['Rating'])\n\narray([[ 1.       , -0.0364417],\n       [-0.0364417,  1.       ]])\n\n\n\nnp.corrcoef(df[\"gross income\"], df['Rating'])[1][0]\n\n-0.03644170499701839\n\n\n\n# rounding off\nround(np.corrcoef(df['gross income'], df['Rating'])[1][0],2)\n\n-0.04\n\n\n\n\n7. Profiling\n\ndataset = pd.read_csv(\"/kaggle/input/super-market-sales/supermarket_sales.csv\")\n\nfrom ydata_profiling import ProfileReport\nprofile = ProfileReport(dataset, title='Profiling Report')\nprofile\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8. Resources\n\nhttps://www.data-to-viz.com/\nhttps://seaborn.pydata.org/examples/index.html\nhttps://pypi.org/project/pandas-profiling/"
  },
  {
    "objectID": "posts/charting/charting.html",
    "href": "posts/charting/charting.html",
    "title": "Visualizations with matplotlib, seaborn, ploty, altair",
    "section": "",
    "text": "A common use for notebooks is data visualization using charts. Colaboratory makes this easy with several charting tools available as Python imports.\n\n\nMatplotlib is the most common charting package, see its documentation for details, and its examples for inspiration.\n\n\n\nimport matplotlib.pyplot as plt\n\nx  = [1, 2, 3, 4, 5, 6, 7, 8, 9]\ny1 = [1, 3, 5, 3, 1, 3, 5, 3, 1]\ny2 = [2, 4, 6, 4, 2, 4, 6, 4, 2]\nplt.plot(x, y1, label=\"line L\")\nplt.plot(x, y2, label=\"line H\")\nplt.plot()\n\nplt.xlabel(\"x axis\")\nplt.ylabel(\"y axis\")\nplt.title(\"Line Graph Example\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\n# Look at index 4 and 6, which demonstrate overlapping cases.\nx1 = [1, 3, 4, 5, 6, 7, 9]\ny1 = [4, 7, 2, 4, 7, 8, 3]\n\nx2 = [2, 4, 6, 8, 10]\ny2 = [5, 6, 2, 6, 2]\n\n# Colors: https://matplotlib.org/api/colors_api.html\n\nplt.bar(x1, y1, label=\"Blue Bar\", color='b')\nplt.bar(x2, y2, label=\"Green Bar\", color='g')\nplt.plot()\n\nplt.xlabel(\"bar number\")\nplt.ylabel(\"bar height\")\nplt.title(\"Bar Chart Example\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Use numpy to generate a bunch of random data in a bell curve around 5.\nn = 5 + np.random.randn(1000)\n\nm = [m for m in range(len(n))]\nplt.bar(m, n)\nplt.title(\"Raw Data\")\nplt.show()\n\nplt.hist(n, bins=20)\nplt.title(\"Histogram\")\nplt.show()\n\nplt.hist(n, cumulative=True, bins=20)\nplt.title(\"Cumulative Histogram\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nx1 = [2, 3, 4]\ny1 = [5, 5, 5]\n\nx2 = [1, 2, 3, 4, 5]\ny2 = [2, 3, 2, 3, 4]\ny3 = [6, 8, 7, 8, 7]\n\n# Markers: https://matplotlib.org/api/markers_api.html\n\nplt.scatter(x1, y1)\nplt.scatter(x2, y2, marker='v', color='r')\nplt.scatter(x2, y3, marker='^', color='m')\nplt.title('Scatter Plot Example')\nplt.show()\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nidxes = [ 1,  2,  3,  4,  5,  6,  7,  8,  9]\narr1  = [23, 40, 28, 43,  8, 44, 43, 18, 17]\narr2  = [17, 30, 22, 14, 17, 17, 29, 22, 30]\narr3  = [15, 31, 18, 22, 18, 19, 13, 32, 39]\n\n# Adding legend for stack plots is tricky.\nplt.plot([], [], color='r', label = 'D 1')\nplt.plot([], [], color='g', label = 'D 2')\nplt.plot([], [], color='b', label = 'D 3')\n\nplt.stackplot(idxes, arr1, arr2, arr3, colors= ['r', 'g', 'b'])\nplt.title('Stack Plot Example')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nlabels = 'S1', 'S2', 'S3'\nsections = [56, 66, 24]\ncolors = ['c', 'g', 'y']\n\nplt.pie(sections, labels=labels, colors=colors,\n        startangle=90,\n        explode = (0, 0.1, 0),\n        autopct = '%1.2f%%')\n\nplt.axis('equal') # Try commenting this out.\nplt.title('Pie Chart Example')\nplt.show()\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nys = 200 + np.random.randn(100)\nx = [x for x in range(len(ys))]\n\nplt.plot(x, ys, '-')\nplt.fill_between(x, ys, 195, where=(ys &gt; 195), facecolor='g', alpha=0.6)\n\nplt.title(\"Fills and Alpha Example\")\nplt.show()\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef random_plots():\n  xs = []\n  ys = []\n\n  for i in range(20):\n    x = i\n    y = np.random.randint(10)\n\n    xs.append(x)\n    ys.append(y)\n\n  return xs, ys\n\nfig = plt.figure()\nax1 = plt.subplot2grid((5, 2), (0, 0), rowspan=1, colspan=2)\nax2 = plt.subplot2grid((5, 2), (1, 0), rowspan=3, colspan=2)\nax3 = plt.subplot2grid((5, 2), (4, 0), rowspan=1, colspan=1)\nax4 = plt.subplot2grid((5, 2), (4, 1), rowspan=1, colspan=1)\n\nx, y = random_plots()\nax1.plot(x, y)\n\nx, y = random_plots()\nax2.plot(x, y)\n\nx, y = random_plots()\nax3.plot(x, y)\n\nx, y = random_plots()\nax4.plot(x, y)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nColaboratory charts use Seaborn’s custom styling by default. To customize styling further please see the matplotlib docs.\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom mpl_toolkits.mplot3d import axes3d\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection = '3d')\n\nx1 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\ny1 = np.random.randint(10, size=10)\nz1 = np.random.randint(10, size=10)\n\nx2 = [-1, -2, -3, -4, -5, -6, -7, -8, -9, -10]\ny2 = np.random.randint(-10, 0, size=10)\nz2 = np.random.randint(10, size=10)\n\nax.scatter(x1, y1, z1, c='b', marker='o', label='blue')\nax.scatter(x2, y2, z2, c='g', marker='D', label='green')\n\nax.set_xlabel('x axis')\nax.set_ylabel('y axis')\nax.set_zlabel('z axis')\nplt.title(\"3D Scatter Plot Example\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection = '3d')\n\nx = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\ny = np.random.randint(10, size=10)\nz = np.zeros(10)\n\ndx = np.ones(10)\ndy = np.ones(10)\ndz = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\nax.bar3d(x, y, z, dx, dy, dz, color='g')\n\nax.set_xlabel('x axis')\nax.set_ylabel('y axis')\nax.set_zlabel('z axis')\nplt.title(\"3D Bar Chart Example\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection = '3d')\n\nx, y, z = axes3d.get_test_data()\n\nax.plot_wireframe(x, y, z, rstride = 2, cstride = 2)\n\nplt.title(\"Wireframe Plot Example\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThere are several libraries layered on top of Matplotlib that you can use in Colab. One that is worth highlighting is Seaborn:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n# Generate some random data\nnum_points = 20\n# x will be 5, 6, 7... but also twiddled randomly\nx = 5 + np.arange(num_points) + np.random.randn(num_points)\n# y will be 10, 11, 12... but twiddled even more randomly\ny = 10 + np.arange(num_points) + 5 * np.random.randn(num_points)\nsns.regplot(x, y)\nplt.show()\n\n\n\n\nThat’s a simple scatterplot with a nice regression line fit to it, all with just one call to Seaborn’s regplot.\nHere’s a Seaborn heatmap:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Make a 10 x 10 heatmap of some random data\nside_length = 10\n# Start with a 10 x 10 matrix with values randomized around 5\ndata = 5 + np.random.randn(side_length, side_length)\n# The next two lines make the values larger as we get closer to (9, 9)\ndata += np.arange(side_length)\ndata += np.reshape(np.arange(side_length), (side_length, 1))\n# Generate the heatmap\nsns.heatmap(data)\nplt.show()\n\n\n\n\n\n\n\nAltair is a declarative visualization library for creating interactive visualizations in Python, and is installed and enabled in Colab by default.\nFor example, here is an interactive scatter plot:\n\nimport altair as alt\nfrom vega_datasets import data\ncars = data.cars()\n\nalt.Chart(cars).mark_point().encode(\n    x='Horsepower',\n    y='Miles_per_Gallon',\n    color='Origin',\n).interactive()\n\n\n\n\n\n\nFor more examples of Altair plots, see the Altair snippets notebook or the external Altair Example Gallery.\n\n\n\n\n\n\nfrom plotly.offline import iplot\nimport plotly.graph_objs as go\n\ndata = [\n    go.Contour(\n        z=[[10, 10.625, 12.5, 15.625, 20],\n           [5.625, 6.25, 8.125, 11.25, 15.625],\n           [2.5, 3.125, 5., 8.125, 12.5],\n           [0.625, 1.25, 3.125, 6.25, 10.625],\n           [0, 0.625, 2.5, 5.625, 10]]\n    )\n]\niplot(data)\n\n\n        \n  \n\n\n\n\n        \n        \n            \n            \n        \n\n\n\n\n\n\n\n\n\nimport numpy as np\nfrom bokeh.plotting import figure, show\nfrom bokeh.io import output_notebook\n\n# Call once to configure Bokeh to display plots inline in the notebook.\noutput_notebook()\n\n\nN = 4000\nx = np.random.random(size=N) * 100\ny = np.random.random(size=N) * 100\nradii = np.random.random(size=N) * 1.5\ncolors = [\"#%02x%02x%02x\" % (r, g, 150) for r, g in zip(np.floor(50+2*x).astype(int), np.floor(30+2*y).astype(int))]\n\np = figure()\np.circle(x, y, radius=radii, fill_color=colors, fill_alpha=0.6, line_color=None)\nshow(p)"
  },
  {
    "objectID": "posts/charting/charting.html#matplotlib",
    "href": "posts/charting/charting.html#matplotlib",
    "title": "Visualizations with matplotlib, seaborn, ploty, altair",
    "section": "",
    "text": "Matplotlib is the most common charting package, see its documentation for details, and its examples for inspiration.\n\n\n\nimport matplotlib.pyplot as plt\n\nx  = [1, 2, 3, 4, 5, 6, 7, 8, 9]\ny1 = [1, 3, 5, 3, 1, 3, 5, 3, 1]\ny2 = [2, 4, 6, 4, 2, 4, 6, 4, 2]\nplt.plot(x, y1, label=\"line L\")\nplt.plot(x, y2, label=\"line H\")\nplt.plot()\n\nplt.xlabel(\"x axis\")\nplt.ylabel(\"y axis\")\nplt.title(\"Line Graph Example\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\n# Look at index 4 and 6, which demonstrate overlapping cases.\nx1 = [1, 3, 4, 5, 6, 7, 9]\ny1 = [4, 7, 2, 4, 7, 8, 3]\n\nx2 = [2, 4, 6, 8, 10]\ny2 = [5, 6, 2, 6, 2]\n\n# Colors: https://matplotlib.org/api/colors_api.html\n\nplt.bar(x1, y1, label=\"Blue Bar\", color='b')\nplt.bar(x2, y2, label=\"Green Bar\", color='g')\nplt.plot()\n\nplt.xlabel(\"bar number\")\nplt.ylabel(\"bar height\")\nplt.title(\"Bar Chart Example\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Use numpy to generate a bunch of random data in a bell curve around 5.\nn = 5 + np.random.randn(1000)\n\nm = [m for m in range(len(n))]\nplt.bar(m, n)\nplt.title(\"Raw Data\")\nplt.show()\n\nplt.hist(n, bins=20)\nplt.title(\"Histogram\")\nplt.show()\n\nplt.hist(n, cumulative=True, bins=20)\nplt.title(\"Cumulative Histogram\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nx1 = [2, 3, 4]\ny1 = [5, 5, 5]\n\nx2 = [1, 2, 3, 4, 5]\ny2 = [2, 3, 2, 3, 4]\ny3 = [6, 8, 7, 8, 7]\n\n# Markers: https://matplotlib.org/api/markers_api.html\n\nplt.scatter(x1, y1)\nplt.scatter(x2, y2, marker='v', color='r')\nplt.scatter(x2, y3, marker='^', color='m')\nplt.title('Scatter Plot Example')\nplt.show()\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nidxes = [ 1,  2,  3,  4,  5,  6,  7,  8,  9]\narr1  = [23, 40, 28, 43,  8, 44, 43, 18, 17]\narr2  = [17, 30, 22, 14, 17, 17, 29, 22, 30]\narr3  = [15, 31, 18, 22, 18, 19, 13, 32, 39]\n\n# Adding legend for stack plots is tricky.\nplt.plot([], [], color='r', label = 'D 1')\nplt.plot([], [], color='g', label = 'D 2')\nplt.plot([], [], color='b', label = 'D 3')\n\nplt.stackplot(idxes, arr1, arr2, arr3, colors= ['r', 'g', 'b'])\nplt.title('Stack Plot Example')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nlabels = 'S1', 'S2', 'S3'\nsections = [56, 66, 24]\ncolors = ['c', 'g', 'y']\n\nplt.pie(sections, labels=labels, colors=colors,\n        startangle=90,\n        explode = (0, 0.1, 0),\n        autopct = '%1.2f%%')\n\nplt.axis('equal') # Try commenting this out.\nplt.title('Pie Chart Example')\nplt.show()\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nys = 200 + np.random.randn(100)\nx = [x for x in range(len(ys))]\n\nplt.plot(x, ys, '-')\nplt.fill_between(x, ys, 195, where=(ys &gt; 195), facecolor='g', alpha=0.6)\n\nplt.title(\"Fills and Alpha Example\")\nplt.show()\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef random_plots():\n  xs = []\n  ys = []\n\n  for i in range(20):\n    x = i\n    y = np.random.randint(10)\n\n    xs.append(x)\n    ys.append(y)\n\n  return xs, ys\n\nfig = plt.figure()\nax1 = plt.subplot2grid((5, 2), (0, 0), rowspan=1, colspan=2)\nax2 = plt.subplot2grid((5, 2), (1, 0), rowspan=3, colspan=2)\nax3 = plt.subplot2grid((5, 2), (4, 0), rowspan=1, colspan=1)\nax4 = plt.subplot2grid((5, 2), (4, 1), rowspan=1, colspan=1)\n\nx, y = random_plots()\nax1.plot(x, y)\n\nx, y = random_plots()\nax2.plot(x, y)\n\nx, y = random_plots()\nax3.plot(x, y)\n\nx, y = random_plots()\nax4.plot(x, y)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/charting/charting.html#plot-styles",
    "href": "posts/charting/charting.html#plot-styles",
    "title": "Visualizations with matplotlib, seaborn, ploty, altair",
    "section": "",
    "text": "Colaboratory charts use Seaborn’s custom styling by default. To customize styling further please see the matplotlib docs."
  },
  {
    "objectID": "posts/charting/charting.html#d-graphs",
    "href": "posts/charting/charting.html#d-graphs",
    "title": "Visualizations with matplotlib, seaborn, ploty, altair",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom mpl_toolkits.mplot3d import axes3d\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection = '3d')\n\nx1 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\ny1 = np.random.randint(10, size=10)\nz1 = np.random.randint(10, size=10)\n\nx2 = [-1, -2, -3, -4, -5, -6, -7, -8, -9, -10]\ny2 = np.random.randint(-10, 0, size=10)\nz2 = np.random.randint(10, size=10)\n\nax.scatter(x1, y1, z1, c='b', marker='o', label='blue')\nax.scatter(x2, y2, z2, c='g', marker='D', label='green')\n\nax.set_xlabel('x axis')\nax.set_ylabel('y axis')\nax.set_zlabel('z axis')\nplt.title(\"3D Scatter Plot Example\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection = '3d')\n\nx = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\ny = np.random.randint(10, size=10)\nz = np.zeros(10)\n\ndx = np.ones(10)\ndy = np.ones(10)\ndz = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\nax.bar3d(x, y, z, dx, dy, dz, color='g')\n\nax.set_xlabel('x axis')\nax.set_ylabel('y axis')\nax.set_zlabel('z axis')\nplt.title(\"3D Bar Chart Example\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection = '3d')\n\nx, y, z = axes3d.get_test_data()\n\nax.plot_wireframe(x, y, z, rstride = 2, cstride = 2)\n\nplt.title(\"Wireframe Plot Example\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/charting/charting.html#seaborn",
    "href": "posts/charting/charting.html#seaborn",
    "title": "Visualizations with matplotlib, seaborn, ploty, altair",
    "section": "",
    "text": "There are several libraries layered on top of Matplotlib that you can use in Colab. One that is worth highlighting is Seaborn:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n# Generate some random data\nnum_points = 20\n# x will be 5, 6, 7... but also twiddled randomly\nx = 5 + np.arange(num_points) + np.random.randn(num_points)\n# y will be 10, 11, 12... but twiddled even more randomly\ny = 10 + np.arange(num_points) + 5 * np.random.randn(num_points)\nsns.regplot(x, y)\nplt.show()\n\n\n\n\nThat’s a simple scatterplot with a nice regression line fit to it, all with just one call to Seaborn’s regplot.\nHere’s a Seaborn heatmap:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Make a 10 x 10 heatmap of some random data\nside_length = 10\n# Start with a 10 x 10 matrix with values randomized around 5\ndata = 5 + np.random.randn(side_length, side_length)\n# The next two lines make the values larger as we get closer to (9, 9)\ndata += np.arange(side_length)\ndata += np.reshape(np.arange(side_length), (side_length, 1))\n# Generate the heatmap\nsns.heatmap(data)\nplt.show()"
  },
  {
    "objectID": "posts/charting/charting.html#altair",
    "href": "posts/charting/charting.html#altair",
    "title": "Visualizations with matplotlib, seaborn, ploty, altair",
    "section": "",
    "text": "Altair is a declarative visualization library for creating interactive visualizations in Python, and is installed and enabled in Colab by default.\nFor example, here is an interactive scatter plot:\n\nimport altair as alt\nfrom vega_datasets import data\ncars = data.cars()\n\nalt.Chart(cars).mark_point().encode(\n    x='Horsepower',\n    y='Miles_per_Gallon',\n    color='Origin',\n).interactive()\n\n\n\n\n\n\nFor more examples of Altair plots, see the Altair snippets notebook or the external Altair Example Gallery."
  },
  {
    "objectID": "posts/charting/charting.html#plotly",
    "href": "posts/charting/charting.html#plotly",
    "title": "Visualizations with matplotlib, seaborn, ploty, altair",
    "section": "",
    "text": "from plotly.offline import iplot\nimport plotly.graph_objs as go\n\ndata = [\n    go.Contour(\n        z=[[10, 10.625, 12.5, 15.625, 20],\n           [5.625, 6.25, 8.125, 11.25, 15.625],\n           [2.5, 3.125, 5., 8.125, 12.5],\n           [0.625, 1.25, 3.125, 6.25, 10.625],\n           [0, 0.625, 2.5, 5.625, 10]]\n    )\n]\niplot(data)"
  },
  {
    "objectID": "posts/charting/charting.html#bokeh",
    "href": "posts/charting/charting.html#bokeh",
    "title": "Visualizations with matplotlib, seaborn, ploty, altair",
    "section": "",
    "text": "import numpy as np\nfrom bokeh.plotting import figure, show\nfrom bokeh.io import output_notebook\n\n# Call once to configure Bokeh to display plots inline in the notebook.\noutput_notebook()\n\n\nN = 4000\nx = np.random.random(size=N) * 100\ny = np.random.random(size=N) * 100\nradii = np.random.random(size=N) * 1.5\ncolors = [\"#%02x%02x%02x\" % (r, g, 150) for r, g in zip(np.floor(50+2*x).astype(int), np.floor(30+2*y).astype(int))]\n\np = figure()\np.circle(x, y, radius=radii, fill_color=colors, fill_alpha=0.6, line_color=None)\nshow(p)"
  },
  {
    "objectID": "posts/bamboo_plantation/index.html",
    "href": "posts/bamboo_plantation/index.html",
    "title": "Can bamboo help to solve climate crisis?",
    "section": "",
    "text": "The article discusses how bamboo construction can be a sustainable and affordable solution to housing while also helping mitigate climate change. Bamboo is a fast-growing, renewable resource that can be used to build structurally sound buildings that are earthquake-resistant and can withstand extreme weather conditions. Bamboo also has a lower carbon footprint compared to traditional building materials like concrete and steel. The article highlights several examples of successful bamboo construction projects, including a 22-story bamboo skyscraper in Colombia and a village in China where all the houses are made of bamboo. By using bamboo in construction, we can create more eco-friendly and affordable housing options while also reducing carbon emissions."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kunal Khurana",
    "section": "",
    "text": "Efficiency of knn with sample data from scikit-learn\n\n\n1 min\n\n\n\nknn\n\n\nmachine learning\n\n\nalgorithm\n\n\n\n\nKunal Khurana\n\n\nMay 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEfficiency of logistic regression with sample data from scikit-learn\n\n\n1 min\n\n\n\nlogistic regression\n\n\nmachine learning\n\n\nalgorithm\n\n\n\n\nKunal Khurana\n\n\nMay 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData analysis with Pyspark\n\n\n1 min\n\n\n\nBig data\n\n\nPyspark\n\n\nPython\n\n\nmatplotlib\n\n\n\n\nKunal Khurana\n\n\nMar 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\ndashboarding\n\n\nData Analysis\n\n\nstreamlit\n\n\n\n\nKunal Khurana\n\n\nMar 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nData analysis example workbook\n\n\n2 min\n\n\n\npandas\n\n\nseaborn\n\n\nmatplotlib\n\n\nnumpy\n\n\n\n\nKunal Khurana\n\n\nMar 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData aggregation, grouping, and pivoting\n\n\n1 min\n\n\n\nPython\n\n\nPandas\n\n\npivot_tabes\n\n\ngroupby\n\n\n\n\nKunal Khurana\n\n\nMar 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing data visualization libraries\n\n\n1 min\n\n\n\nPython\n\n\nMatplotlib\n\n\nNumpy\n\n\nSeaborn\n\n\nplotting\n\n\n\n\nKunal Khurana\n\n\nMar 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nA brief overview of Time series\n\n\n1 min\n\n\n\nPython\n\n\nPandas\n\n\ntime_series\n\n\n\n\nKunal Khurana\n\n\nMar 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA brief introduction to modeling libraries in Python\n\n\n1 min\n\n\n\nPython\n\n\npatsy\n\n\nstatsmodels\n\n\nscikit-learn\n\n\n\n\nKunal Khurana\n\n\nMar 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData analysis with Pandas\n\n\n1 min\n\n\n\nPandas\n\n\nData Analysis\n\n\nPython\n\n\n\n\nKunal Khurana\n\n\nFeb 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n1 min\n\n\n\nPython\n\n\nSQL\n\n\nDatabase\n\n\nAPIs\n\n\n\n\nKunal Khurana\n\n\nFeb 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n1 min\n\n\n\nPython\n\n\nTransformation\n\n\nCatagorical\n\n\nMapping\n\n\n\n\nKunal Khurana\n\n\nFeb 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n2 min\n\n\n\nPython\n\n\nIndexing\n\n\nCombining\n\n\nReshaping\n\n\nPivoting\n\n\n\n\nKunal Khurana\n\n\nFeb 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData analysis with Numpy\n\n\n1 min\n\n\n\nNumpy\n\n\nData Analysis\n\n\nPython\n\n\n\n\nKunal Khurana\n\n\nFeb 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with files\n\n\n1 min\n\n\n\n.txt\n\n\nFiles\n\n\n\n\nKunal Khurana\n\n\nFeb 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n1 min\n\n\n\nPython\n\n\n\n\nKunal Khurana\n\n\nFeb 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n1 min\n\n\n\nPython\n\n\n\n\nKunal Khurana\n\n\nFeb 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nSample notebook to handle any data for EDA\n\n\n1 min\n\n\n\nPython\n\n\nMatplotlib\n\n\nData Analysis\n\n\nPandas\n\n\nSeaborn\n\n\n\n\nKunal Khurana\n\n\nJan 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1D and 2D partial dependent plots with RandomForestClassifier and DecisionTreeClassifier\n\n\n2 min\n\n\n\nPython\n\n\nMachine Learning\n\n\nData Analysis\n\n\n\n\nKunal Khurana\n\n\nJan 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData analysis with GeoPandas\n\n\n1 min\n\n\n\nPython\n\n\nGeoPandas\n\n\nData Analysis\n\n\n\n\nKunal Khurana\n\n\nJan 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData analysis with Pandas\n\n\n2 min\n\n\n\nPandas\n\n\nData Analysis\n\n\nPython\n\n\n\n\nKunal Khurana\n\n\nDec 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbasics\n\n\n2 min\n\n\n\nMachine learning\n\n\nBasics\n\n\nData Analysis\n\n\n\n\nKunal Khurana\n\n\nDec 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n1 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nDec 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n3 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nDec 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n1 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nOct 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n2 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nOct 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n2 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nOct 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n2 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nOct 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n1 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nOct 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n1 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nOct 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n1 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nOct 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n2 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nOct 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n2 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nOct 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n2 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nOct 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n7 min\n\n\n\nMachine learning\n\n\nData Analysis\n\n\n\n\n\n\n\nJun 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizatons\n\n\n1 min\n\n\n\nVisualizations\n\n\nData Analysis\n\n\n\n\n\n\n\nJun 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploratory analysis\n\n\n3 min\n\n\n\nSoils\n\n\nData Analysis\n\n\n\n\n\n\n\nJun 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n10 min\n\n\n\nNumpy\n\n\nData Analysis\n\n\nPython\n\n\n\n\nJustin Johnson, Volodymr Kuleshov, and Issac Caswell\n\n\nJun 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n1 min\n\n\n\nMatplotlib\n\n\nData Analysis\n\n\n\n\nKeith Galli\n\n\nJun 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n21 min\n\n\n\nPandas\n\n\nData Analysis\n\n\n\n\n\n\n\nJun 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\npackages\n\n\ndata\n\n\n\n\n\n\n\nMay 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nplot\n\n\ndata\n\n\n\n\n\n\n\nMay 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Visualization\n\n\n7 min\n\n\n\nNumpy\n\n\nPandas\n\n\nData Analysis\n\n\n\n\nNonu Singh\n\n\nMay 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nAI\n\n\nIrrigation\n\n\nMachine learning\n\n\n\n\nKunal Khurana\n\n\nMay 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nC\n\n\nBamboo\n\n\n\n\nKunal Khurana\n\n\nMay 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nMachine learning\n\n\nAI\n\n\nIrrigation\n\n\n\n\nKunal Khurana\n\n\nMay 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nMachine learning\n\n\nSoil Analysis\n\n\nYield prediction\n\n\n\n\nKunal Khurana\n\n\nApr 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nMachine learning\n\n\nIrrigation\n\n\nModelling\n\n\n\n\nKunal Khurana\n\n\nApr 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nDigitalization\n\n\nMachine learning\n\n\nAI\n\n\n\n\nKunal Khurana\n\n\nApr 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData analysis with GeoPandas\n\n\n1 min\n\n\n\nPython\n\n\nGeoPandas\n\n\nData Analysis\n\n\n\n\nKunal Khurana\n\n\nJan 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData analysis with GeoPandas\n\n\n1 min\n\n\n\nPython\n\n\nGeoPandas\n\n\nProximity Analysis\n\n\n\n\nKunal Khurana\n\n\nJan 7, 2023\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n See all posts in English"
  },
  {
    "objectID": "index.html#posts-in-english",
    "href": "index.html#posts-in-english",
    "title": "Kunal Khurana",
    "section": "",
    "text": "Efficiency of knn with sample data from scikit-learn\n\n\n1 min\n\n\n\nknn\n\n\nmachine learning\n\n\nalgorithm\n\n\n\n\nKunal Khurana\n\n\nMay 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEfficiency of logistic regression with sample data from scikit-learn\n\n\n1 min\n\n\n\nlogistic regression\n\n\nmachine learning\n\n\nalgorithm\n\n\n\n\nKunal Khurana\n\n\nMay 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData analysis with Pyspark\n\n\n1 min\n\n\n\nBig data\n\n\nPyspark\n\n\nPython\n\n\nmatplotlib\n\n\n\n\nKunal Khurana\n\n\nMar 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\ndashboarding\n\n\nData Analysis\n\n\nstreamlit\n\n\n\n\nKunal Khurana\n\n\nMar 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nData analysis example workbook\n\n\n2 min\n\n\n\npandas\n\n\nseaborn\n\n\nmatplotlib\n\n\nnumpy\n\n\n\n\nKunal Khurana\n\n\nMar 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData aggregation, grouping, and pivoting\n\n\n1 min\n\n\n\nPython\n\n\nPandas\n\n\npivot_tabes\n\n\ngroupby\n\n\n\n\nKunal Khurana\n\n\nMar 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing data visualization libraries\n\n\n1 min\n\n\n\nPython\n\n\nMatplotlib\n\n\nNumpy\n\n\nSeaborn\n\n\nplotting\n\n\n\n\nKunal Khurana\n\n\nMar 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nA brief overview of Time series\n\n\n1 min\n\n\n\nPython\n\n\nPandas\n\n\ntime_series\n\n\n\n\nKunal Khurana\n\n\nMar 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA brief introduction to modeling libraries in Python\n\n\n1 min\n\n\n\nPython\n\n\npatsy\n\n\nstatsmodels\n\n\nscikit-learn\n\n\n\n\nKunal Khurana\n\n\nMar 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData analysis with Pandas\n\n\n1 min\n\n\n\nPandas\n\n\nData Analysis\n\n\nPython\n\n\n\n\nKunal Khurana\n\n\nFeb 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n1 min\n\n\n\nPython\n\n\nSQL\n\n\nDatabase\n\n\nAPIs\n\n\n\n\nKunal Khurana\n\n\nFeb 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n1 min\n\n\n\nPython\n\n\nTransformation\n\n\nCatagorical\n\n\nMapping\n\n\n\n\nKunal Khurana\n\n\nFeb 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n2 min\n\n\n\nPython\n\n\nIndexing\n\n\nCombining\n\n\nReshaping\n\n\nPivoting\n\n\n\n\nKunal Khurana\n\n\nFeb 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData analysis with Numpy\n\n\n1 min\n\n\n\nNumpy\n\n\nData Analysis\n\n\nPython\n\n\n\n\nKunal Khurana\n\n\nFeb 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with files\n\n\n1 min\n\n\n\n.txt\n\n\nFiles\n\n\n\n\nKunal Khurana\n\n\nFeb 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n1 min\n\n\n\nPython\n\n\n\n\nKunal Khurana\n\n\nFeb 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n1 min\n\n\n\nPython\n\n\n\n\nKunal Khurana\n\n\nFeb 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nSample notebook to handle any data for EDA\n\n\n1 min\n\n\n\nPython\n\n\nMatplotlib\n\n\nData Analysis\n\n\nPandas\n\n\nSeaborn\n\n\n\n\nKunal Khurana\n\n\nJan 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1D and 2D partial dependent plots with RandomForestClassifier and DecisionTreeClassifier\n\n\n2 min\n\n\n\nPython\n\n\nMachine Learning\n\n\nData Analysis\n\n\n\n\nKunal Khurana\n\n\nJan 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData analysis with GeoPandas\n\n\n1 min\n\n\n\nPython\n\n\nGeoPandas\n\n\nData Analysis\n\n\n\n\nKunal Khurana\n\n\nJan 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData analysis with Pandas\n\n\n2 min\n\n\n\nPandas\n\n\nData Analysis\n\n\nPython\n\n\n\n\nKunal Khurana\n\n\nDec 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbasics\n\n\n2 min\n\n\n\nMachine learning\n\n\nBasics\n\n\nData Analysis\n\n\n\n\nKunal Khurana\n\n\nDec 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n1 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nDec 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n3 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nDec 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n1 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nOct 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n2 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nOct 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n2 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nOct 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n2 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nOct 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n1 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nOct 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n1 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nOct 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n1 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nOct 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n2 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nOct 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n2 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nOct 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n2 min\n\n\n\nPython\n\n\nBasics\n\n\n\n\nKunal Khurana\n\n\nOct 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n7 min\n\n\n\nMachine learning\n\n\nData Analysis\n\n\n\n\n\n\n\nJun 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizatons\n\n\n1 min\n\n\n\nVisualizations\n\n\nData Analysis\n\n\n\n\n\n\n\nJun 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploratory analysis\n\n\n3 min\n\n\n\nSoils\n\n\nData Analysis\n\n\n\n\n\n\n\nJun 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n10 min\n\n\n\nNumpy\n\n\nData Analysis\n\n\nPython\n\n\n\n\nJustin Johnson, Volodymr Kuleshov, and Issac Caswell\n\n\nJun 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n1 min\n\n\n\nMatplotlib\n\n\nData Analysis\n\n\n\n\nKeith Galli\n\n\nJun 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n21 min\n\n\n\nPandas\n\n\nData Analysis\n\n\n\n\n\n\n\nJun 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\npackages\n\n\ndata\n\n\n\n\n\n\n\nMay 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nplot\n\n\ndata\n\n\n\n\n\n\n\nMay 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Visualization\n\n\n7 min\n\n\n\nNumpy\n\n\nPandas\n\n\nData Analysis\n\n\n\n\nNonu Singh\n\n\nMay 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nAI\n\n\nIrrigation\n\n\nMachine learning\n\n\n\n\nKunal Khurana\n\n\nMay 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nC\n\n\nBamboo\n\n\n\n\nKunal Khurana\n\n\nMay 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nMachine learning\n\n\nAI\n\n\nIrrigation\n\n\n\n\nKunal Khurana\n\n\nMay 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nMachine learning\n\n\nSoil Analysis\n\n\nYield prediction\n\n\n\n\nKunal Khurana\n\n\nApr 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nMachine learning\n\n\nIrrigation\n\n\nModelling\n\n\n\n\nKunal Khurana\n\n\nApr 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nDigitalization\n\n\nMachine learning\n\n\nAI\n\n\n\n\nKunal Khurana\n\n\nApr 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData analysis with GeoPandas\n\n\n1 min\n\n\n\nPython\n\n\nGeoPandas\n\n\nData Analysis\n\n\n\n\nKunal Khurana\n\n\nJan 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData analysis with GeoPandas\n\n\n1 min\n\n\n\nPython\n\n\nGeoPandas\n\n\nProximity Analysis\n\n\n\n\nKunal Khurana\n\n\nJan 7, 2023\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n See all posts in English"
  },
  {
    "objectID": "about-fr.html",
    "href": "about-fr.html",
    "title": "Kunal Khurana",
    "section": "",
    "text": "English  Français"
  },
  {
    "objectID": "about-fr.html#education",
    "href": "about-fr.html#education",
    "title": "Kunal Khurana",
    "section": "📝Education",
    "text": "📝Education\nFrancisation et Intégration au Milieu Québécois| Université Laval, CA\nMaitrise en Science du Sol| Punjab Agricultural University, IN\nBaccalauréat en Agriculture | Punjabi University Patiala, IN"
  },
  {
    "objectID": "about-fr.html#contacer",
    "href": "about-fr.html#contacer",
    "title": "Kunal Khurana",
    "section": "Contacer!",
    "text": "Contacer!\nL’une de mes plus grandes motivations pour créer du contenu en ligne est de communiquer avec des personnes fascinantes du monde entier. Si vous souhaitez en discuter, n’hésitez pas à m’écrire. Je serais content d’avoir de vos nouvelles !"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Kunal Khurana",
    "section": "",
    "text": "English  Français"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Kunal Khurana",
    "section": "📝Education",
    "text": "📝Education\nFrancisation et Intégration au Milieu Québécois| Université Laval, CA\nMaster’s in Soil Science| Punjab Agricultural University, IN\nBachelor’s in Agriculture| Punjabi University Patiala, IN"
  },
  {
    "objectID": "about.html#lets-connect",
    "href": "about.html#lets-connect",
    "title": "Kunal Khurana",
    "section": "Let’s Connect!",
    "text": "Let’s Connect!\nOne of my greatest motivations for creating online content is to engage with fascinating individuals from all over the world. If you’re interested in having a chat, feel free to reach out to me. I would love to hear from you!"
  },
  {
    "objectID": "posts/Ai_watersaving/index.html",
    "href": "posts/Ai_watersaving/index.html",
    "title": "How helpful can AI be in solving the water crisis?",
    "section": "",
    "text": "Wasting water, especially in areas in which it is a scarce resource, is a huge headache (and expense) for farmers and food growers worldwide. This article discusses how AI can help improve water irrigation in agriculture. The article notes that agriculture accounts for approximately 70% of global freshwater usage and that water scarcity is becoming an increasingly pressing issue in many parts of the world. By using AI, farmers can optimize their irrigation practices resulting in reduced water consumption but increased crop yields. The article highlights the use of Machine learning, and in particular deep-learning, algorithms to collect and interpret data from images and identify patterns that spotlight irrigation issues. AI can also be used to create predictive models that help farmers anticipate (real-time) crop water needs to identify areas of a field that need more or less water. By using AI in agriculture, farmers can become more efficient and sustainable in their use of water, ultimately helping to address global water scarcity concerns."
  },
  {
    "objectID": "posts/Big_data/Pyspark.html",
    "href": "posts/Big_data/Pyspark.html",
    "title": "Big Data Analysis",
    "section": "",
    "text": "!pip install pyspark\n\nCollecting pyspark\n  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 317.0/317.0 MB 2.8 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\nRequirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... done\n  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=5a72176bdbb100fe9611d828be35fe034a7c320431f0d36ecee1247bc4eb9a7e\n  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\nSuccessfully built pyspark\nInstalling collected packages: pyspark\nSuccessfully installed pyspark-3.5.1\n\n\n\n\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import count, desc, col, max, struct\nimport matplotlib.pyplot as plt\n\n\nspark = SparkSession.builder.appName('spark_app').getOrCreate()\n\n\ntraffic_collision_data = '/content/traffic-collision-data-from-2010-to-present.csv'\n\n\n# prompt: load traffic_collision_data with spark\n\ndf = spark.read.csv(traffic_collision_data, header=True, inferSchema=True)\n\n\ndf.show()\n\n+---------+-------------------+--------------------+-------------+-------+-----------+------------------+----------+----------------------+--------------------+----------+----------+--------------+------------+-------------------+--------------------+--------------------+--------------------+---------------+----------------+-------------------+--------------------+-----------------+---------------------------------+\n|DR Number|      Date Reported|       Date Occurred|Time Occurred|Area ID|  Area Name|Reporting District|Crime Code|Crime Code Description|            MO Codes|Victim Age|Victim Sex|Victim Descent|Premise Code|Premise Description|             Address|        Cross Street|            Location|      Zip Codes|   Census Tracts|Precinct Boundaries|   LA Specific Plans|Council Districts|Neighborhood Councils (Certified)|\n+---------+-------------------+--------------------+-------------+-------+-----------+------------------+----------+----------------------+--------------------+----------+----------+--------------+------------+-------------------+--------------------+--------------------+--------------------+---------------+----------------+-------------------+--------------------+-----------------+---------------------------------+\n|191323054|2019-11-30 00:00:00|2019-11-30T00:00:...|         0130|     13|     Newton|              1385|       997|     TRAFFIC COLLISION|                NULL|      NULL|         F|             H|         101|             STREET|SAN PEDRO        ...|SLAUSON          ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '33....|            22352|                              786|\n|192020666|2019-11-30 00:00:00|2019-11-30T00:00:...|         0015|     20|    Olympic|              2054|       997|     TRAFFIC COLLISION|                NULL|        40|         M|             W|         101|             STREET|OLYMPIC          ...|KINGSLEY         ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            22723|                              617|\n|191616992|2019-11-30 00:00:00|2019-11-30T00:00:...|         0230|     16|   Foothill|              1669|       997|     TRAFFIC COLLISION|                NULL|        18|         M|             W|         101|             STREET|TUJUNGA CANYON   ...|LA TUNA CANYON   ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|             3222|                                5|\n|191824082|2019-11-30 00:00:00|2019-11-30T00:00:...|         0730|     18|  Southeast|              1802|       997|     TRAFFIC COLLISION|                0605|        23|         M|             H|         101|             STREET|88TH             ...|MAIN             ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '33....|            22352|                              800|\n|191616980|2019-11-30 00:00:00|2019-11-30T00:00:...|         0720|     16|   Foothill|              1689|       997|     TRAFFIC COLLISION|                NULL|      NULL|         M|             H|         101|             STREET|            CROCKETT|             SUNLAND|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            19335|                              166|\n|191824078|2019-11-30 00:00:00|2019-11-30T00:00:...|         1050|     18|  Southeast|              1836|       997|     TRAFFIC COLLISION|4025 3037 3004 30...|        54|         F|             B|         101|             STREET|COMPTON          ...|109TH            ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '33....|            24354|                              817|\n|190417458|2019-11-30 00:00:00|2019-11-30T00:00:...|         0130|     04| Hollenbeck|              0411|       997|     TRAFFIC COLLISION|                0605|        33|         F|             B|         101|             STREET|            BROADWAY|PASADENA         ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            23448|                              492|\n|191616985|2019-11-30 00:00:00|2019-11-30T00:00:...|         0700|     16|   Foothill|              1687|       997|     TRAFFIC COLLISION|                NULL|        35|         M|             H|         101|             STREET|TUJUNGA          ...|PENROSE          ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            19335|                              162|\n|191718751|2019-11-30 00:00:00|2019-11-30T00:00:...|         1230|     17| Devonshire|              1775|       997|     TRAFFIC COLLISION|                NULL|        51|         M|             O|         101|             STREET|                WISH|           VINCENNES|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            18514|                              104|\n|191718743|2019-11-30 00:00:00|2019-11-30T00:00:...|         0010|     17| Devonshire|              1737|       997|     TRAFFIC COLLISION|                NULL|        23|         F|             H|         101|             STREET|         HAYVENHURST|SAN FERNANDO MISSION|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            19329|                               78|\n|191824080|2019-11-30 00:00:00|2019-11-30T00:00:...|         0945|     18|  Southeast|              1842|       997|     TRAFFIC COLLISION|4025 0101 3028 30...|        26|         M|             H|         101|             STREET|111TH            ...|MAIN             ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '33....|            24355|                              812|\n|190720157|2019-11-29 00:00:00|2019-11-29T00:00:...|         1115|     07|   Wilshire|              0766|       997|     TRAFFIC COLLISION|                NULL|        17|         M|             H|         101|             STREET|VENICE           ...|LA FAYETTE       ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            23080|                              651|\n|190518783|2019-11-29 00:00:00|2019-11-29T00:00:...|         0650|     05|     Harbor|              0515|       997|     TRAFFIC COLLISION|4025 3036 3004 30...|        44|         F|             H|         101|             STREET|AVALON           ...|R                ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '33....|             3350|                              952|\n|192119165|2019-11-29 00:00:00|2019-11-29T00:00:...|         1005|     21|    Topanga|              2143|       997|     TRAFFIC COLLISION|                NULL|        58|         M|             A|         101|             STREET|22800    VICTORY ...|                NULL|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            19347|                              300|\n|191018309|2019-11-29 00:00:00|2019-11-29T00:00:...|         0710|     10|West Valley|              1035|       997|     TRAFFIC COLLISION|                NULL|        99|         X|             X|         101|             STREET|VICTORY          ...|WHITE OAK        ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|             4286|                             NULL|\n|192119188|2019-11-29 00:00:00|2019-11-29T00:00:...|         1800|     21|    Topanga|              2126|       997|     TRAFFIC COLLISION|                NULL|        22|         M|             H|         101|             STREET|DE SOTO          ...|STRATHERN        ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|             4282|                              276|\n|191018335|2019-11-29 00:00:00|2019-11-29T00:00:...|         1600|     10|West Valley|              1023|       997|     TRAFFIC COLLISION|                NULL|        29|         M|             H|         101|             STREET|6600    TAMPA    ...|                NULL|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            18909|                              269|\n|191616964|2019-11-29 00:00:00|2019-11-29T00:00:...|         0645|     16|   Foothill|              1602|       997|     TRAFFIC COLLISION|                NULL|        28|         M|             H|         101|             STREET|FILMORE          ...|DRONFIELD        ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            18907|                               22|\n|190222525|2019-11-29 00:00:00|2019-11-29T00:00:...|         0545|     02|    Rampart|              0237|       997|     TRAFFIC COLLISION|                NULL|        38|         F|             H|         101|             STREET|3RD              ...|WITMER           ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            23444|                              550|\n|190518807|2019-11-30 00:00:00|2019-11-29T00:00:...|         2220|     05|     Harbor|              0515|       997|     TRAFFIC COLLISION|                NULL|        44|         F|             H|         101|             STREET|               FRIES|PACIFIC COAST    ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '33....|             3350|                              954|\n+---------+-------------------+--------------------+-------------+-------+-----------+------------------+----------+----------------------+--------------------+----------+----------+--------------+------------+-------------------+--------------------+--------------------+--------------------+---------------+----------------+-------------------+--------------------+-----------------+---------------------------------+\nonly showing top 20 rows\n\n\n\n\n\n\n\n# prompt: drop the DR number column\n\ndf = df.drop('DR Number')\n\n\ndf.show()\n\n+-------------------+--------------------+-------------+-------+-----------+------------------+----------+----------------------+--------------------+----------+----------+--------------+------------+-------------------+--------------------+--------------------+--------------------+---------------+----------------+-------------------+--------------------+-----------------+---------------------------------+\n|      Date Reported|       Date Occurred|Time Occurred|Area ID|  Area Name|Reporting District|Crime Code|Crime Code Description|            MO Codes|Victim Age|Victim Sex|Victim Descent|Premise Code|Premise Description|             Address|        Cross Street|            Location|      Zip Codes|   Census Tracts|Precinct Boundaries|   LA Specific Plans|Council Districts|Neighborhood Councils (Certified)|\n+-------------------+--------------------+-------------+-------+-----------+------------------+----------+----------------------+--------------------+----------+----------+--------------+------------+-------------------+--------------------+--------------------+--------------------+---------------+----------------+-------------------+--------------------+-----------------+---------------------------------+\n|2019-11-30 00:00:00|2019-11-30T00:00:...|         0130|     13|     Newton|              1385|       997|     TRAFFIC COLLISION|                NULL|      NULL|         F|             H|         101|             STREET|SAN PEDRO        ...|SLAUSON          ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '33....|            22352|                              786|\n|2019-11-30 00:00:00|2019-11-30T00:00:...|         0015|     20|    Olympic|              2054|       997|     TRAFFIC COLLISION|                NULL|        40|         M|             W|         101|             STREET|OLYMPIC          ...|KINGSLEY         ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            22723|                              617|\n|2019-11-30 00:00:00|2019-11-30T00:00:...|         0230|     16|   Foothill|              1669|       997|     TRAFFIC COLLISION|                NULL|        18|         M|             W|         101|             STREET|TUJUNGA CANYON   ...|LA TUNA CANYON   ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|             3222|                                5|\n|2019-11-30 00:00:00|2019-11-30T00:00:...|         0730|     18|  Southeast|              1802|       997|     TRAFFIC COLLISION|                0605|        23|         M|             H|         101|             STREET|88TH             ...|MAIN             ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '33....|            22352|                              800|\n|2019-11-30 00:00:00|2019-11-30T00:00:...|         0720|     16|   Foothill|              1689|       997|     TRAFFIC COLLISION|                NULL|      NULL|         M|             H|         101|             STREET|            CROCKETT|             SUNLAND|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            19335|                              166|\n|2019-11-30 00:00:00|2019-11-30T00:00:...|         1050|     18|  Southeast|              1836|       997|     TRAFFIC COLLISION|4025 3037 3004 30...|        54|         F|             B|         101|             STREET|COMPTON          ...|109TH            ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '33....|            24354|                              817|\n|2019-11-30 00:00:00|2019-11-30T00:00:...|         0130|     04| Hollenbeck|              0411|       997|     TRAFFIC COLLISION|                0605|        33|         F|             B|         101|             STREET|            BROADWAY|PASADENA         ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            23448|                              492|\n|2019-11-30 00:00:00|2019-11-30T00:00:...|         0700|     16|   Foothill|              1687|       997|     TRAFFIC COLLISION|                NULL|        35|         M|             H|         101|             STREET|TUJUNGA          ...|PENROSE          ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            19335|                              162|\n|2019-11-30 00:00:00|2019-11-30T00:00:...|         1230|     17| Devonshire|              1775|       997|     TRAFFIC COLLISION|                NULL|        51|         M|             O|         101|             STREET|                WISH|           VINCENNES|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            18514|                              104|\n|2019-11-30 00:00:00|2019-11-30T00:00:...|         0010|     17| Devonshire|              1737|       997|     TRAFFIC COLLISION|                NULL|        23|         F|             H|         101|             STREET|         HAYVENHURST|SAN FERNANDO MISSION|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            19329|                               78|\n|2019-11-30 00:00:00|2019-11-30T00:00:...|         0945|     18|  Southeast|              1842|       997|     TRAFFIC COLLISION|4025 0101 3028 30...|        26|         M|             H|         101|             STREET|111TH            ...|MAIN             ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '33....|            24355|                              812|\n|2019-11-29 00:00:00|2019-11-29T00:00:...|         1115|     07|   Wilshire|              0766|       997|     TRAFFIC COLLISION|                NULL|        17|         M|             H|         101|             STREET|VENICE           ...|LA FAYETTE       ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            23080|                              651|\n|2019-11-29 00:00:00|2019-11-29T00:00:...|         0650|     05|     Harbor|              0515|       997|     TRAFFIC COLLISION|4025 3036 3004 30...|        44|         F|             H|         101|             STREET|AVALON           ...|R                ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '33....|             3350|                              952|\n|2019-11-29 00:00:00|2019-11-29T00:00:...|         1005|     21|    Topanga|              2143|       997|     TRAFFIC COLLISION|                NULL|        58|         M|             A|         101|             STREET|22800    VICTORY ...|                NULL|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            19347|                              300|\n|2019-11-29 00:00:00|2019-11-29T00:00:...|         0710|     10|West Valley|              1035|       997|     TRAFFIC COLLISION|                NULL|        99|         X|             X|         101|             STREET|VICTORY          ...|WHITE OAK        ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|             4286|                             NULL|\n|2019-11-29 00:00:00|2019-11-29T00:00:...|         1800|     21|    Topanga|              2126|       997|     TRAFFIC COLLISION|                NULL|        22|         M|             H|         101|             STREET|DE SOTO          ...|STRATHERN        ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|             4282|                              276|\n|2019-11-29 00:00:00|2019-11-29T00:00:...|         1600|     10|West Valley|              1023|       997|     TRAFFIC COLLISION|                NULL|        29|         M|             H|         101|             STREET|6600    TAMPA    ...|                NULL|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            18909|                              269|\n|2019-11-29 00:00:00|2019-11-29T00:00:...|         0645|     16|   Foothill|              1602|       997|     TRAFFIC COLLISION|                NULL|        28|         M|             H|         101|             STREET|FILMORE          ...|DRONFIELD        ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            18907|                               22|\n|2019-11-29 00:00:00|2019-11-29T00:00:...|         0545|     02|    Rampart|              0237|       997|     TRAFFIC COLLISION|                NULL|        38|         F|             H|         101|             STREET|3RD              ...|WITMER           ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            23444|                              550|\n|2019-11-30 00:00:00|2019-11-29T00:00:...|         2220|     05|     Harbor|              0515|       997|     TRAFFIC COLLISION|                NULL|        44|         F|             H|         101|             STREET|               FRIES|PACIFIC COAST    ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '33....|             3350|                              954|\n+-------------------+--------------------+-------------+-------+-----------+------------------+----------+----------------------+--------------------+----------+----------+--------------+------------+-------------------+--------------------+--------------------+--------------------+---------------+----------------+-------------------+--------------------+-----------------+---------------------------------+\nonly showing top 20 rows\n\n\n\n\ndf.summary\n\n\n    pyspark.sql.dataframe.DataFrame.summarydef summary(*statistics: str) -&gt; 'DataFrame'/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.pyComputes specified statistics for numeric and string columns. Available statistics are:\n- count\n- mean\n- stddev\n- min\n- max\n- arbitrary approximate percentiles specified as a percentage (e.g., 75%)\n\nIf no statistics are given, this function computes count, mean, stddev, min,\napproximate quartiles (percentiles at 25%, 50%, and 75%), and max.\n\n.. versionadded:: 2.3.0\n\n.. versionchanged:: 3.4.0\n    Supports Spark Connect.\n\nParameters\n----------\nstatistics : str, optional\n     Column names to calculate statistics by (default All columns).\n\nReturns\n-------\n:class:`DataFrame`\n    A new DataFrame that provides statistics for the given DataFrame.\n\nNotes\n-----\nThis function is meant for exploratory data analysis, as we make no\nguarantee about the backward compatibility of the schema of the resulting\n:class:`DataFrame`.\n\nExamples\n--------\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     [(\"Bob\", 13, 40.3, 150.5), (\"Alice\", 12, 37.8, 142.3), (\"Tom\", 11, 44.1, 142.2)],\n...     [\"name\", \"age\", \"weight\", \"height\"],\n... )\n&gt;&gt;&gt; df.select(\"age\", \"weight\", \"height\").summary().show()\n+-------+----+------------------+-----------------+\n|summary| age|            weight|           height|\n+-------+----+------------------+-----------------+\n|  count|   3|                 3|                3|\n|   mean|12.0| 40.73333333333333|            145.0|\n| stddev| 1.0|3.1722757341273704|4.763402145525822|\n|    min|  11|              37.8|            142.2|\n|    25%|  11|              37.8|            142.2|\n|    50%|  12|              40.3|            142.3|\n|    75%|  13|              44.1|            150.5|\n|    max|  13|              44.1|            150.5|\n+-------+----+------------------+-----------------+\n\n&gt;&gt;&gt; df.select(\"age\", \"weight\", \"height\").summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show()\n+-------+---+------+------+\n|summary|age|weight|height|\n+-------+---+------+------+\n|  count|  3|     3|     3|\n|    min| 11|  37.8| 142.2|\n|    25%| 11|  37.8| 142.2|\n|    75%| 13|  44.1| 150.5|\n|    max| 13|  44.1| 150.5|\n+-------+---+------+------+\n\nSee Also\n--------\nDataFrame.display\n      \n      \n\n\n\n# prompt: print Schema\n\ndf.printSchema()\n\nroot\n |-- Date Reported: timestamp (nullable = true)\n |-- Date Occurred: string (nullable = true)\n |-- Time Occurred: string (nullable = true)\n |-- Area ID: string (nullable = true)\n |-- Area Name: string (nullable = true)\n |-- Reporting District: string (nullable = true)\n |-- Crime Code: integer (nullable = true)\n |-- Crime Code Description: string (nullable = true)\n |-- MO Codes: string (nullable = true)\n |-- Victim Age: string (nullable = true)\n |-- Victim Sex: string (nullable = true)\n |-- Victim Descent: string (nullable = true)\n |-- Premise Code: string (nullable = true)\n |-- Premise Description: string (nullable = true)\n |-- Address: string (nullable = true)\n |-- Cross Street: string (nullable = true)\n |-- Location: string (nullable = true)\n |-- Zip Codes: string (nullable = true)\n |-- Census Tracts: string (nullable = true)\n |-- Precinct Boundaries: string (nullable = true)\n |-- LA Specific Plans: string (nullable = true)\n |-- Council Districts: string (nullable = true)\n |-- Neighborhood Councils (Certified): string (nullable = true)\n\n\n\n\n# prompt: see the shape of a dataframe\n\nprint(f\"Number of rows: {df.count()}\")\nprint(f\"Number of columns: {len(df.columns)}\")\n\nNumber of rows: 294684\nNumber of columns: 23\n\n\n\n# prompt: select two columns area id and victim age\n\ndf.select('Area ID', 'Victim Age').show()\n\n+-------+----------+\n|Area ID|Victim Age|\n+-------+----------+\n|     13|      NULL|\n|     20|        40|\n|     16|        18|\n|     18|        23|\n|     16|      NULL|\n|     18|        54|\n|     04|        33|\n|     16|        35|\n|     17|        51|\n|     17|        23|\n|     18|        26|\n|     07|        17|\n|     05|        44|\n|     21|        58|\n|     10|        99|\n|     21|        22|\n|     10|        29|\n|     16|        28|\n|     02|        38|\n|     05|        44|\n+-------+----------+\nonly showing top 20 rows\n\n\n\n\n# prompt: select those records where the victin age  is 17\n\ndf.select('Area ID', 'Victim Age').where(df['Victim Age'] == 17).show()\n\n+-------+----------+\n|Area ID|Victim Age|\n+-------+----------+\n|     07|        17|\n|     09|        17|\n|     09|        17|\n|     03|        17|\n|     19|        17|\n|     20|        17|\n|     15|        17|\n|     03|        17|\n|     17|        17|\n|     21|        17|\n|     03|        17|\n|     17|        17|\n|     08|        17|\n|     13|        17|\n|     21|        17|\n|     13|        17|\n|     09|        17|\n|     07|        17|\n|     15|        17|\n|     06|        17|\n+-------+----------+\nonly showing top 20 rows\n\n\n\n\n\ndf.select('*').where(df['Victim Age'] == 17).show()\n\n+-------------------+--------------------+-------------+-------+-----------+------------------+----------+----------------------+--------------------+----------+----------+--------------+------------+-------------------+--------------------+--------------------+--------------------+---------------+----------------+-------------------+--------------------+-----------------+---------------------------------+\n|      Date Reported|       Date Occurred|Time Occurred|Area ID|  Area Name|Reporting District|Crime Code|Crime Code Description|            MO Codes|Victim Age|Victim Sex|Victim Descent|Premise Code|Premise Description|             Address|        Cross Street|            Location|      Zip Codes|   Census Tracts|Precinct Boundaries|   LA Specific Plans|Council Districts|Neighborhood Councils (Certified)|\n+-------------------+--------------------+-------------+-------+-----------+------------------+----------+----------------------+--------------------+----------+----------+--------------+------------+-------------------+--------------------+--------------------+--------------------+---------------+----------------+-------------------+--------------------+-----------------+---------------------------------+\n|2019-11-29 00:00:00|2019-11-29T00:00:...|         1115|     07|   Wilshire|              0766|       997|     TRAFFIC COLLISION|                NULL|        17|         M|             H|         101|             STREET|VENICE           ...|LA FAYETTE       ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            23080|                              651|\n|2019-11-29 00:00:00|2019-11-28T00:00:...|         2341|     09|   Van Nuys|              0935|       997|     TRAFFIC COLLISION|                0605|        17|         M|             O|         101|             STREET|            VAN NUYS|VICTORY          ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            19729|                              236|\n|2019-11-20 00:00:00|2019-11-20T00:00:...|         1640|     09|   Van Nuys|              0929|       997|     TRAFFIC COLLISION|                NULL|        17|         F|             W|         101|             STREET|VICTORY          ...|FULTON           ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            19729|                              184|\n|2019-11-18 00:00:00|2019-11-18T00:00:...|         1625|     03|  Southwest|              0333|       997|     TRAFFIC COLLISION|4025 3036 3004 30...|        17|         F|             B|         101|             STREET|CRENSHAW         ...|30TH             ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            23079|                              668|\n|2019-11-15 00:00:00|2019-11-15T00:00:...|         1820|     19|    Mission|              1956|       997|     TRAFFIC COLLISION|                NULL|        17|         M|             W|         101|             STREET|             WOODMAN|          DEVONSHIRE|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            19330|                               72|\n|2019-11-14 00:00:00|2019-11-14T00:00:...|         2222|     20|    Olympic|              2064|       997|     TRAFFIC COLLISION|3003 3026 3029 30...|        17|         F|             H|         101|             STREET|HOOVER           ...|8TH              ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            22723|                              618|\n|2019-11-14 00:00:00|2019-11-14T00:00:...|         0935|     15|N Hollywood|              1566|       997|     TRAFFIC COLLISION|                NULL|        17|         M|             B|         101|             STREET|VINELAND         ...|RIVERSIDE        ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|             8489|                              343|\n|2019-11-11 00:00:00|2019-11-11T00:00:...|         1500|     03|  Southwest|              0311|       997|     TRAFFIC COLLISION|4025 3037 3004 30...|        17|         M|             B|         101|             STREET|              HAUSER|                GEER|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            23077|                              666|\n|2019-05-09 00:00:00|2019-05-09T00:00:...|         2030|     17| Devonshire|              1701|       997|     TRAFFIC COLLISION|3004 3030 3028 31...|        17|         M|             W|         101|             STREET|PORTER RANCH     ...|RINALDI          ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            18905|                               66|\n|2019-05-09 00:00:00|2019-05-09T00:00:...|         2000|     21|    Topanga|              2113|       997|     TRAFFIC COLLISION|3004 3037 3029 30...|        17|         F|             W|         101|             STREET|SATICOY          ...|TOPANGA CANYON   ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|             4279|                              277|\n|2019-05-05 00:00:00|2019-05-05T00:00:...|         1240|     03|  Southwest|              0338|       997|     TRAFFIC COLLISION|4003 3036 3008 30...|        17|         M|             H|         101|             STREET|WALTON           ...|30TH             ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            22724|                              685|\n|2019-04-28 00:00:00|2019-04-28T00:00:...|         2040|     17| Devonshire|              1752|       997|     TRAFFIC COLLISION|3036 3101 3003 30...|        17|         M|             H|         101|             STREET|CANOGA           ...|LASSEN           ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|             4284|                             NULL|\n|2019-04-26 00:00:00|2019-04-26T00:00:...|         2030|     08|    West LA|              0801|       997|     TRAFFIC COLLISION|3004 3028 3030 30...|        17|         M|             A|         101|             STREET|CASTELLAMMARE    ...|TRAMONTO         ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            25066|                             2332|\n|2019-11-06 00:00:00|2019-11-06T00:00:...|         0700|     13|     Newton|              1344|       997|     TRAFFIC COLLISION|3101 3401 3701 30...|        17|         F|             H|         101|             STREET|41ST             ...|MCKINLEY         ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            22727|                              713|\n|2019-11-07 00:00:00|2019-11-06T00:00:...|         1740|     21|    Topanga|              2136|       997|     TRAFFIC COLLISION|3003 3036 3029 30...|        17|         M|             H|         101|             STREET|SATICOY          ...|DEERING          ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|             4278|                              276|\n|2019-11-04 00:00:00|2019-11-04T00:00:...|         1220|     13|     Newton|              1343|       997|     TRAFFIC COLLISION|3401 3101 3701 30...|        17|         M|             H|         101|             STREET|JEFFERSON        ...|WADSWORTH        ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            22727|                              712|\n|2019-11-01 00:00:00|2019-11-01T00:00:...|         1645|     09|   Van Nuys|              0943|       997|     TRAFFIC COLLISION|                NULL|        17|         F|             O|         101|             STREET|              OXNARD|           HAZELTINE|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            19729|                              243|\n|2019-10-29 00:00:00|2019-10-29T00:00:...|         0810|     07|   Wilshire|              0749|       997|     TRAFFIC COLLISION|3003 3025 3029 30...|        17|         F|             H|         101|             STREET|CRENSHAW         ...|COUNTRY CLUB     ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            23080|                              612|\n|2019-10-27 00:00:00|2019-10-27T00:00:...|         0945|     15|N Hollywood|              1581|       997|     TRAFFIC COLLISION|                0605|        17|         F|             W|         101|             STREET|COLDWATER CANYON ...|WOODBRIDGE       ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|             8492|                              346|\n|2019-10-27 00:00:00|2019-10-27T00:00:...|         1305|     06|  Hollywood|              0639|       997|     TRAFFIC COLLISION|3004 3025 3035 30...|        17|         M|             W|         101|             STREET|HOLLYWOOD        ...|HOBART           ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            23445|                              419|\n+-------------------+--------------------+-------------+-------+-----------+------------------+----------+----------------------+--------------------+----------+----------+--------------+------------+-------------------+--------------------+--------------------+--------------------+---------------+----------------+-------------------+--------------------+-----------------+---------------------------------+\nonly showing top 20 rows\n\n\n\n\n# prompt: find out which area is most prone to crimes\n\nmost_crime_prone_area = df.groupBy('Area ID').agg(count('*').alias('total_crimes')).orderBy(desc('total_crimes')).first().asDict()['Area ID']\nprint(f\"Most crime prone area: {most_crime_prone_area}\")\n\nMost crime prone area: 12\n\n\n\n# prompt: find out top 10 crime codes\n\ntop_10_crime_codes = df.groupBy('Crime Code Description').agg(count('*').alias('total_crimes')).orderBy(desc('total_crimes')).limit(10).toPandas()['Crime Code Description'].tolist()\n\nprint(\"Top 10 crime codes:\")\nfor code in top_10_crime_codes:\n    print(f\"\\t- {code}\")\n\nTop 10 crime codes:\n    - TRAFFIC COLLISION\n    - 180\n\n\n\n# prompt: find out top 10 victim age\n\ntop_10_victim_ages = df.groupBy('Victim Age').agg(count('*').alias('total_victims')).orderBy(desc('total_victims')).limit(10).toPandas()['Victim Age'].tolist()\n\nprint(\"Top 10 victim ages:\")\nfor age in top_10_victim_ages:\n    print(f\"\\t- {age}\")\n\nTop 10 victim ages:\n    - None\n    - 30\n    - 25\n    - 27\n    - 24\n    - 28\n    - 26\n    - 23\n    - 35\n    - 29\n\n\n\n\n\n\n# prompt: visualize the result of top 10 victim age in the form of a pie chart\n\n# Get the top 10 victim ages and their counts\ntop_10_victim_ages = df.groupBy('Victim Age').agg(count('*').alias('total_victims')).orderBy(desc('total_victims')).limit(10).toPandas()\n\n# Create a pie chart of the top 10 victim ages\nplt.figure(figsize=(12, 6))\nplt.pie(top_10_victim_ages['total_victims'], labels=top_10_victim_ages['Victim Age'], autopct='%1.1f%%')\nplt.title('Top 10 Victim Ages')\nplt.show()"
  },
  {
    "objectID": "posts/Big_data/Pyspark.html#installing-libraries",
    "href": "posts/Big_data/Pyspark.html#installing-libraries",
    "title": "Big Data Analysis",
    "section": "",
    "text": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import count, desc, col, max, struct\nimport matplotlib.pyplot as plt\n\n\nspark = SparkSession.builder.appName('spark_app').getOrCreate()\n\n\ntraffic_collision_data = '/content/traffic-collision-data-from-2010-to-present.csv'\n\n\n# prompt: load traffic_collision_data with spark\n\ndf = spark.read.csv(traffic_collision_data, header=True, inferSchema=True)\n\n\ndf.show()\n\n+---------+-------------------+--------------------+-------------+-------+-----------+------------------+----------+----------------------+--------------------+----------+----------+--------------+------------+-------------------+--------------------+--------------------+--------------------+---------------+----------------+-------------------+--------------------+-----------------+---------------------------------+\n|DR Number|      Date Reported|       Date Occurred|Time Occurred|Area ID|  Area Name|Reporting District|Crime Code|Crime Code Description|            MO Codes|Victim Age|Victim Sex|Victim Descent|Premise Code|Premise Description|             Address|        Cross Street|            Location|      Zip Codes|   Census Tracts|Precinct Boundaries|   LA Specific Plans|Council Districts|Neighborhood Councils (Certified)|\n+---------+-------------------+--------------------+-------------+-------+-----------+------------------+----------+----------------------+--------------------+----------+----------+--------------+------------+-------------------+--------------------+--------------------+--------------------+---------------+----------------+-------------------+--------------------+-----------------+---------------------------------+\n|191323054|2019-11-30 00:00:00|2019-11-30T00:00:...|         0130|     13|     Newton|              1385|       997|     TRAFFIC COLLISION|                NULL|      NULL|         F|             H|         101|             STREET|SAN PEDRO        ...|SLAUSON          ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '33....|            22352|                              786|\n|192020666|2019-11-30 00:00:00|2019-11-30T00:00:...|         0015|     20|    Olympic|              2054|       997|     TRAFFIC COLLISION|                NULL|        40|         M|             W|         101|             STREET|OLYMPIC          ...|KINGSLEY         ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            22723|                              617|\n|191616992|2019-11-30 00:00:00|2019-11-30T00:00:...|         0230|     16|   Foothill|              1669|       997|     TRAFFIC COLLISION|                NULL|        18|         M|             W|         101|             STREET|TUJUNGA CANYON   ...|LA TUNA CANYON   ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|             3222|                                5|\n|191824082|2019-11-30 00:00:00|2019-11-30T00:00:...|         0730|     18|  Southeast|              1802|       997|     TRAFFIC COLLISION|                0605|        23|         M|             H|         101|             STREET|88TH             ...|MAIN             ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '33....|            22352|                              800|\n|191616980|2019-11-30 00:00:00|2019-11-30T00:00:...|         0720|     16|   Foothill|              1689|       997|     TRAFFIC COLLISION|                NULL|      NULL|         M|             H|         101|             STREET|            CROCKETT|             SUNLAND|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            19335|                              166|\n|191824078|2019-11-30 00:00:00|2019-11-30T00:00:...|         1050|     18|  Southeast|              1836|       997|     TRAFFIC COLLISION|4025 3037 3004 30...|        54|         F|             B|         101|             STREET|COMPTON          ...|109TH            ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '33....|            24354|                              817|\n|190417458|2019-11-30 00:00:00|2019-11-30T00:00:...|         0130|     04| Hollenbeck|              0411|       997|     TRAFFIC COLLISION|                0605|        33|         F|             B|         101|             STREET|            BROADWAY|PASADENA         ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            23448|                              492|\n|191616985|2019-11-30 00:00:00|2019-11-30T00:00:...|         0700|     16|   Foothill|              1687|       997|     TRAFFIC COLLISION|                NULL|        35|         M|             H|         101|             STREET|TUJUNGA          ...|PENROSE          ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            19335|                              162|\n|191718751|2019-11-30 00:00:00|2019-11-30T00:00:...|         1230|     17| Devonshire|              1775|       997|     TRAFFIC COLLISION|                NULL|        51|         M|             O|         101|             STREET|                WISH|           VINCENNES|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            18514|                              104|\n|191718743|2019-11-30 00:00:00|2019-11-30T00:00:...|         0010|     17| Devonshire|              1737|       997|     TRAFFIC COLLISION|                NULL|        23|         F|             H|         101|             STREET|         HAYVENHURST|SAN FERNANDO MISSION|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            19329|                               78|\n|191824080|2019-11-30 00:00:00|2019-11-30T00:00:...|         0945|     18|  Southeast|              1842|       997|     TRAFFIC COLLISION|4025 0101 3028 30...|        26|         M|             H|         101|             STREET|111TH            ...|MAIN             ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '33....|            24355|                              812|\n|190720157|2019-11-29 00:00:00|2019-11-29T00:00:...|         1115|     07|   Wilshire|              0766|       997|     TRAFFIC COLLISION|                NULL|        17|         M|             H|         101|             STREET|VENICE           ...|LA FAYETTE       ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            23080|                              651|\n|190518783|2019-11-29 00:00:00|2019-11-29T00:00:...|         0650|     05|     Harbor|              0515|       997|     TRAFFIC COLLISION|4025 3036 3004 30...|        44|         F|             H|         101|             STREET|AVALON           ...|R                ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '33....|             3350|                              952|\n|192119165|2019-11-29 00:00:00|2019-11-29T00:00:...|         1005|     21|    Topanga|              2143|       997|     TRAFFIC COLLISION|                NULL|        58|         M|             A|         101|             STREET|22800    VICTORY ...|                NULL|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            19347|                              300|\n|191018309|2019-11-29 00:00:00|2019-11-29T00:00:...|         0710|     10|West Valley|              1035|       997|     TRAFFIC COLLISION|                NULL|        99|         X|             X|         101|             STREET|VICTORY          ...|WHITE OAK        ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|             4286|                             NULL|\n|192119188|2019-11-29 00:00:00|2019-11-29T00:00:...|         1800|     21|    Topanga|              2126|       997|     TRAFFIC COLLISION|                NULL|        22|         M|             H|         101|             STREET|DE SOTO          ...|STRATHERN        ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|             4282|                              276|\n|191018335|2019-11-29 00:00:00|2019-11-29T00:00:...|         1600|     10|West Valley|              1023|       997|     TRAFFIC COLLISION|                NULL|        29|         M|             H|         101|             STREET|6600    TAMPA    ...|                NULL|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            18909|                              269|\n|191616964|2019-11-29 00:00:00|2019-11-29T00:00:...|         0645|     16|   Foothill|              1602|       997|     TRAFFIC COLLISION|                NULL|        28|         M|             H|         101|             STREET|FILMORE          ...|DRONFIELD        ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            18907|                               22|\n|190222525|2019-11-29 00:00:00|2019-11-29T00:00:...|         0545|     02|    Rampart|              0237|       997|     TRAFFIC COLLISION|                NULL|        38|         F|             H|         101|             STREET|3RD              ...|WITMER           ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            23444|                              550|\n|190518807|2019-11-30 00:00:00|2019-11-29T00:00:...|         2220|     05|     Harbor|              0515|       997|     TRAFFIC COLLISION|                NULL|        44|         F|             H|         101|             STREET|               FRIES|PACIFIC COAST    ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '33....|             3350|                              954|\n+---------+-------------------+--------------------+-------------+-------+-----------+------------------+----------+----------------------+--------------------+----------+----------+--------------+------------+-------------------+--------------------+--------------------+--------------------+---------------+----------------+-------------------+--------------------+-----------------+---------------------------------+\nonly showing top 20 rows"
  },
  {
    "objectID": "posts/Big_data/Pyspark.html#preprocessing",
    "href": "posts/Big_data/Pyspark.html#preprocessing",
    "title": "Big Data Analysis",
    "section": "",
    "text": "# prompt: drop the DR number column\n\ndf = df.drop('DR Number')\n\n\ndf.show()\n\n+-------------------+--------------------+-------------+-------+-----------+------------------+----------+----------------------+--------------------+----------+----------+--------------+------------+-------------------+--------------------+--------------------+--------------------+---------------+----------------+-------------------+--------------------+-----------------+---------------------------------+\n|      Date Reported|       Date Occurred|Time Occurred|Area ID|  Area Name|Reporting District|Crime Code|Crime Code Description|            MO Codes|Victim Age|Victim Sex|Victim Descent|Premise Code|Premise Description|             Address|        Cross Street|            Location|      Zip Codes|   Census Tracts|Precinct Boundaries|   LA Specific Plans|Council Districts|Neighborhood Councils (Certified)|\n+-------------------+--------------------+-------------+-------+-----------+------------------+----------+----------------------+--------------------+----------+----------+--------------+------------+-------------------+--------------------+--------------------+--------------------+---------------+----------------+-------------------+--------------------+-----------------+---------------------------------+\n|2019-11-30 00:00:00|2019-11-30T00:00:...|         0130|     13|     Newton|              1385|       997|     TRAFFIC COLLISION|                NULL|      NULL|         F|             H|         101|             STREET|SAN PEDRO        ...|SLAUSON          ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '33....|            22352|                              786|\n|2019-11-30 00:00:00|2019-11-30T00:00:...|         0015|     20|    Olympic|              2054|       997|     TRAFFIC COLLISION|                NULL|        40|         M|             W|         101|             STREET|OLYMPIC          ...|KINGSLEY         ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            22723|                              617|\n|2019-11-30 00:00:00|2019-11-30T00:00:...|         0230|     16|   Foothill|              1669|       997|     TRAFFIC COLLISION|                NULL|        18|         M|             W|         101|             STREET|TUJUNGA CANYON   ...|LA TUNA CANYON   ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|             3222|                                5|\n|2019-11-30 00:00:00|2019-11-30T00:00:...|         0730|     18|  Southeast|              1802|       997|     TRAFFIC COLLISION|                0605|        23|         M|             H|         101|             STREET|88TH             ...|MAIN             ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '33....|            22352|                              800|\n|2019-11-30 00:00:00|2019-11-30T00:00:...|         0720|     16|   Foothill|              1689|       997|     TRAFFIC COLLISION|                NULL|      NULL|         M|             H|         101|             STREET|            CROCKETT|             SUNLAND|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            19335|                              166|\n|2019-11-30 00:00:00|2019-11-30T00:00:...|         1050|     18|  Southeast|              1836|       997|     TRAFFIC COLLISION|4025 3037 3004 30...|        54|         F|             B|         101|             STREET|COMPTON          ...|109TH            ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '33....|            24354|                              817|\n|2019-11-30 00:00:00|2019-11-30T00:00:...|         0130|     04| Hollenbeck|              0411|       997|     TRAFFIC COLLISION|                0605|        33|         F|             B|         101|             STREET|            BROADWAY|PASADENA         ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            23448|                              492|\n|2019-11-30 00:00:00|2019-11-30T00:00:...|         0700|     16|   Foothill|              1687|       997|     TRAFFIC COLLISION|                NULL|        35|         M|             H|         101|             STREET|TUJUNGA          ...|PENROSE          ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            19335|                              162|\n|2019-11-30 00:00:00|2019-11-30T00:00:...|         1230|     17| Devonshire|              1775|       997|     TRAFFIC COLLISION|                NULL|        51|         M|             O|         101|             STREET|                WISH|           VINCENNES|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            18514|                              104|\n|2019-11-30 00:00:00|2019-11-30T00:00:...|         0010|     17| Devonshire|              1737|       997|     TRAFFIC COLLISION|                NULL|        23|         F|             H|         101|             STREET|         HAYVENHURST|SAN FERNANDO MISSION|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            19329|                               78|\n|2019-11-30 00:00:00|2019-11-30T00:00:...|         0945|     18|  Southeast|              1842|       997|     TRAFFIC COLLISION|4025 0101 3028 30...|        26|         M|             H|         101|             STREET|111TH            ...|MAIN             ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '33....|            24355|                              812|\n|2019-11-29 00:00:00|2019-11-29T00:00:...|         1115|     07|   Wilshire|              0766|       997|     TRAFFIC COLLISION|                NULL|        17|         M|             H|         101|             STREET|VENICE           ...|LA FAYETTE       ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            23080|                              651|\n|2019-11-29 00:00:00|2019-11-29T00:00:...|         0650|     05|     Harbor|              0515|       997|     TRAFFIC COLLISION|4025 3036 3004 30...|        44|         F|             H|         101|             STREET|AVALON           ...|R                ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '33....|             3350|                              952|\n|2019-11-29 00:00:00|2019-11-29T00:00:...|         1005|     21|    Topanga|              2143|       997|     TRAFFIC COLLISION|                NULL|        58|         M|             A|         101|             STREET|22800    VICTORY ...|                NULL|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            19347|                              300|\n|2019-11-29 00:00:00|2019-11-29T00:00:...|         0710|     10|West Valley|              1035|       997|     TRAFFIC COLLISION|                NULL|        99|         X|             X|         101|             STREET|VICTORY          ...|WHITE OAK        ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|             4286|                             NULL|\n|2019-11-29 00:00:00|2019-11-29T00:00:...|         1800|     21|    Topanga|              2126|       997|     TRAFFIC COLLISION|                NULL|        22|         M|             H|         101|             STREET|DE SOTO          ...|STRATHERN        ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|             4282|                              276|\n|2019-11-29 00:00:00|2019-11-29T00:00:...|         1600|     10|West Valley|              1023|       997|     TRAFFIC COLLISION|                NULL|        29|         M|             H|         101|             STREET|6600    TAMPA    ...|                NULL|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            18909|                              269|\n|2019-11-29 00:00:00|2019-11-29T00:00:...|         0645|     16|   Foothill|              1602|       997|     TRAFFIC COLLISION|                NULL|        28|         M|             H|         101|             STREET|FILMORE          ...|DRONFIELD        ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            18907|                               22|\n|2019-11-29 00:00:00|2019-11-29T00:00:...|         0545|     02|    Rampart|              0237|       997|     TRAFFIC COLLISION|                NULL|        38|         F|             H|         101|             STREET|3RD              ...|WITMER           ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            23444|                              550|\n|2019-11-30 00:00:00|2019-11-29T00:00:...|         2220|     05|     Harbor|              0515|       997|     TRAFFIC COLLISION|                NULL|        44|         F|             H|         101|             STREET|               FRIES|PACIFIC COAST    ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '33....|             3350|                              954|\n+-------------------+--------------------+-------------+-------+-----------+------------------+----------+----------------------+--------------------+----------+----------+--------------+------------+-------------------+--------------------+--------------------+--------------------+---------------+----------------+-------------------+--------------------+-----------------+---------------------------------+\nonly showing top 20 rows\n\n\n\n\ndf.summary\n\n\n    pyspark.sql.dataframe.DataFrame.summarydef summary(*statistics: str) -&gt; 'DataFrame'/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.pyComputes specified statistics for numeric and string columns. Available statistics are:\n- count\n- mean\n- stddev\n- min\n- max\n- arbitrary approximate percentiles specified as a percentage (e.g., 75%)\n\nIf no statistics are given, this function computes count, mean, stddev, min,\napproximate quartiles (percentiles at 25%, 50%, and 75%), and max.\n\n.. versionadded:: 2.3.0\n\n.. versionchanged:: 3.4.0\n    Supports Spark Connect.\n\nParameters\n----------\nstatistics : str, optional\n     Column names to calculate statistics by (default All columns).\n\nReturns\n-------\n:class:`DataFrame`\n    A new DataFrame that provides statistics for the given DataFrame.\n\nNotes\n-----\nThis function is meant for exploratory data analysis, as we make no\nguarantee about the backward compatibility of the schema of the resulting\n:class:`DataFrame`.\n\nExamples\n--------\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     [(\"Bob\", 13, 40.3, 150.5), (\"Alice\", 12, 37.8, 142.3), (\"Tom\", 11, 44.1, 142.2)],\n...     [\"name\", \"age\", \"weight\", \"height\"],\n... )\n&gt;&gt;&gt; df.select(\"age\", \"weight\", \"height\").summary().show()\n+-------+----+------------------+-----------------+\n|summary| age|            weight|           height|\n+-------+----+------------------+-----------------+\n|  count|   3|                 3|                3|\n|   mean|12.0| 40.73333333333333|            145.0|\n| stddev| 1.0|3.1722757341273704|4.763402145525822|\n|    min|  11|              37.8|            142.2|\n|    25%|  11|              37.8|            142.2|\n|    50%|  12|              40.3|            142.3|\n|    75%|  13|              44.1|            150.5|\n|    max|  13|              44.1|            150.5|\n+-------+----+------------------+-----------------+\n\n&gt;&gt;&gt; df.select(\"age\", \"weight\", \"height\").summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show()\n+-------+---+------+------+\n|summary|age|weight|height|\n+-------+---+------+------+\n|  count|  3|     3|     3|\n|    min| 11|  37.8| 142.2|\n|    25%| 11|  37.8| 142.2|\n|    75%| 13|  44.1| 150.5|\n|    max| 13|  44.1| 150.5|\n+-------+---+------+------+\n\nSee Also\n--------\nDataFrame.display\n      \n      \n\n\n\n# prompt: print Schema\n\ndf.printSchema()\n\nroot\n |-- Date Reported: timestamp (nullable = true)\n |-- Date Occurred: string (nullable = true)\n |-- Time Occurred: string (nullable = true)\n |-- Area ID: string (nullable = true)\n |-- Area Name: string (nullable = true)\n |-- Reporting District: string (nullable = true)\n |-- Crime Code: integer (nullable = true)\n |-- Crime Code Description: string (nullable = true)\n |-- MO Codes: string (nullable = true)\n |-- Victim Age: string (nullable = true)\n |-- Victim Sex: string (nullable = true)\n |-- Victim Descent: string (nullable = true)\n |-- Premise Code: string (nullable = true)\n |-- Premise Description: string (nullable = true)\n |-- Address: string (nullable = true)\n |-- Cross Street: string (nullable = true)\n |-- Location: string (nullable = true)\n |-- Zip Codes: string (nullable = true)\n |-- Census Tracts: string (nullable = true)\n |-- Precinct Boundaries: string (nullable = true)\n |-- LA Specific Plans: string (nullable = true)\n |-- Council Districts: string (nullable = true)\n |-- Neighborhood Councils (Certified): string (nullable = true)\n\n\n\n\n# prompt: see the shape of a dataframe\n\nprint(f\"Number of rows: {df.count()}\")\nprint(f\"Number of columns: {len(df.columns)}\")\n\nNumber of rows: 294684\nNumber of columns: 23\n\n\n\n# prompt: select two columns area id and victim age\n\ndf.select('Area ID', 'Victim Age').show()\n\n+-------+----------+\n|Area ID|Victim Age|\n+-------+----------+\n|     13|      NULL|\n|     20|        40|\n|     16|        18|\n|     18|        23|\n|     16|      NULL|\n|     18|        54|\n|     04|        33|\n|     16|        35|\n|     17|        51|\n|     17|        23|\n|     18|        26|\n|     07|        17|\n|     05|        44|\n|     21|        58|\n|     10|        99|\n|     21|        22|\n|     10|        29|\n|     16|        28|\n|     02|        38|\n|     05|        44|\n+-------+----------+\nonly showing top 20 rows\n\n\n\n\n# prompt: select those records where the victin age  is 17\n\ndf.select('Area ID', 'Victim Age').where(df['Victim Age'] == 17).show()\n\n+-------+----------+\n|Area ID|Victim Age|\n+-------+----------+\n|     07|        17|\n|     09|        17|\n|     09|        17|\n|     03|        17|\n|     19|        17|\n|     20|        17|\n|     15|        17|\n|     03|        17|\n|     17|        17|\n|     21|        17|\n|     03|        17|\n|     17|        17|\n|     08|        17|\n|     13|        17|\n|     21|        17|\n|     13|        17|\n|     09|        17|\n|     07|        17|\n|     15|        17|\n|     06|        17|\n+-------+----------+\nonly showing top 20 rows\n\n\n\n\n\ndf.select('*').where(df['Victim Age'] == 17).show()\n\n+-------------------+--------------------+-------------+-------+-----------+------------------+----------+----------------------+--------------------+----------+----------+--------------+------------+-------------------+--------------------+--------------------+--------------------+---------------+----------------+-------------------+--------------------+-----------------+---------------------------------+\n|      Date Reported|       Date Occurred|Time Occurred|Area ID|  Area Name|Reporting District|Crime Code|Crime Code Description|            MO Codes|Victim Age|Victim Sex|Victim Descent|Premise Code|Premise Description|             Address|        Cross Street|            Location|      Zip Codes|   Census Tracts|Precinct Boundaries|   LA Specific Plans|Council Districts|Neighborhood Councils (Certified)|\n+-------------------+--------------------+-------------+-------+-----------+------------------+----------+----------------------+--------------------+----------+----------+--------------+------------+-------------------+--------------------+--------------------+--------------------+---------------+----------------+-------------------+--------------------+-----------------+---------------------------------+\n|2019-11-29 00:00:00|2019-11-29T00:00:...|         1115|     07|   Wilshire|              0766|       997|     TRAFFIC COLLISION|                NULL|        17|         M|             H|         101|             STREET|VENICE           ...|LA FAYETTE       ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            23080|                              651|\n|2019-11-29 00:00:00|2019-11-28T00:00:...|         2341|     09|   Van Nuys|              0935|       997|     TRAFFIC COLLISION|                0605|        17|         M|             O|         101|             STREET|            VAN NUYS|VICTORY          ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            19729|                              236|\n|2019-11-20 00:00:00|2019-11-20T00:00:...|         1640|     09|   Van Nuys|              0929|       997|     TRAFFIC COLLISION|                NULL|        17|         F|             W|         101|             STREET|VICTORY          ...|FULTON           ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            19729|                              184|\n|2019-11-18 00:00:00|2019-11-18T00:00:...|         1625|     03|  Southwest|              0333|       997|     TRAFFIC COLLISION|4025 3036 3004 30...|        17|         F|             B|         101|             STREET|CRENSHAW         ...|30TH             ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            23079|                              668|\n|2019-11-15 00:00:00|2019-11-15T00:00:...|         1820|     19|    Mission|              1956|       997|     TRAFFIC COLLISION|                NULL|        17|         M|             W|         101|             STREET|             WOODMAN|          DEVONSHIRE|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            19330|                               72|\n|2019-11-14 00:00:00|2019-11-14T00:00:...|         2222|     20|    Olympic|              2064|       997|     TRAFFIC COLLISION|3003 3026 3029 30...|        17|         F|             H|         101|             STREET|HOOVER           ...|8TH              ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            22723|                              618|\n|2019-11-14 00:00:00|2019-11-14T00:00:...|         0935|     15|N Hollywood|              1566|       997|     TRAFFIC COLLISION|                NULL|        17|         M|             B|         101|             STREET|VINELAND         ...|RIVERSIDE        ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|             8489|                              343|\n|2019-11-11 00:00:00|2019-11-11T00:00:...|         1500|     03|  Southwest|              0311|       997|     TRAFFIC COLLISION|4025 3037 3004 30...|        17|         M|             B|         101|             STREET|              HAUSER|                GEER|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            23077|                              666|\n|2019-05-09 00:00:00|2019-05-09T00:00:...|         2030|     17| Devonshire|              1701|       997|     TRAFFIC COLLISION|3004 3030 3028 31...|        17|         M|             W|         101|             STREET|PORTER RANCH     ...|RINALDI          ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            18905|                               66|\n|2019-05-09 00:00:00|2019-05-09T00:00:...|         2000|     21|    Topanga|              2113|       997|     TRAFFIC COLLISION|3004 3037 3029 30...|        17|         F|             W|         101|             STREET|SATICOY          ...|TOPANGA CANYON   ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|             4279|                              277|\n|2019-05-05 00:00:00|2019-05-05T00:00:...|         1240|     03|  Southwest|              0338|       997|     TRAFFIC COLLISION|4003 3036 3008 30...|        17|         M|             H|         101|             STREET|WALTON           ...|30TH             ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            22724|                              685|\n|2019-04-28 00:00:00|2019-04-28T00:00:...|         2040|     17| Devonshire|              1752|       997|     TRAFFIC COLLISION|3036 3101 3003 30...|        17|         M|             H|         101|             STREET|CANOGA           ...|LASSEN           ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|             4284|                             NULL|\n|2019-04-26 00:00:00|2019-04-26T00:00:...|         2030|     08|    West LA|              0801|       997|     TRAFFIC COLLISION|3004 3028 3030 30...|        17|         M|             A|         101|             STREET|CASTELLAMMARE    ...|TRAMONTO         ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            25066|                             2332|\n|2019-11-06 00:00:00|2019-11-06T00:00:...|         0700|     13|     Newton|              1344|       997|     TRAFFIC COLLISION|3101 3401 3701 30...|        17|         F|             H|         101|             STREET|41ST             ...|MCKINLEY         ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            22727|                              713|\n|2019-11-07 00:00:00|2019-11-06T00:00:...|         1740|     21|    Topanga|              2136|       997|     TRAFFIC COLLISION|3003 3036 3029 30...|        17|         M|             H|         101|             STREET|SATICOY          ...|DEERING          ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|             4278|                              276|\n|2019-11-04 00:00:00|2019-11-04T00:00:...|         1220|     13|     Newton|              1343|       997|     TRAFFIC COLLISION|3401 3101 3701 30...|        17|         M|             H|         101|             STREET|JEFFERSON        ...|WADSWORTH        ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            22727|                              712|\n|2019-11-01 00:00:00|2019-11-01T00:00:...|         1645|     09|   Van Nuys|              0943|       997|     TRAFFIC COLLISION|                NULL|        17|         F|             O|         101|             STREET|              OXNARD|           HAZELTINE|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            19729|                              243|\n|2019-10-29 00:00:00|2019-10-29T00:00:...|         0810|     07|   Wilshire|              0749|       997|     TRAFFIC COLLISION|3003 3025 3029 30...|        17|         F|             H|         101|             STREET|CRENSHAW         ...|COUNTRY CLUB     ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            23080|                              612|\n|2019-10-27 00:00:00|2019-10-27T00:00:...|         0945|     15|N Hollywood|              1581|       997|     TRAFFIC COLLISION|                0605|        17|         F|             W|         101|             STREET|COLDWATER CANYON ...|WOODBRIDGE       ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|             8492|                              346|\n|2019-10-27 00:00:00|2019-10-27T00:00:...|         1305|     06|  Hollywood|              0639|       997|     TRAFFIC COLLISION|3004 3025 3035 30...|        17|         M|             W|         101|             STREET|HOLLYWOOD        ...|HOBART           ...|\"{'longitude': '-...| \"\"city\"\": \"\"\"\"| \"\"state\"\": \"\"\"\"|    \"\"zip\"\": \"\"\"\"}'| 'latitude': '34....|            23445|                              419|\n+-------------------+--------------------+-------------+-------+-----------+------------------+----------+----------------------+--------------------+----------+----------+--------------+------------+-------------------+--------------------+--------------------+--------------------+---------------+----------------+-------------------+--------------------+-----------------+---------------------------------+\nonly showing top 20 rows\n\n\n\n\n# prompt: find out which area is most prone to crimes\n\nmost_crime_prone_area = df.groupBy('Area ID').agg(count('*').alias('total_crimes')).orderBy(desc('total_crimes')).first().asDict()['Area ID']\nprint(f\"Most crime prone area: {most_crime_prone_area}\")\n\nMost crime prone area: 12\n\n\n\n# prompt: find out top 10 crime codes\n\ntop_10_crime_codes = df.groupBy('Crime Code Description').agg(count('*').alias('total_crimes')).orderBy(desc('total_crimes')).limit(10).toPandas()['Crime Code Description'].tolist()\n\nprint(\"Top 10 crime codes:\")\nfor code in top_10_crime_codes:\n    print(f\"\\t- {code}\")\n\nTop 10 crime codes:\n    - TRAFFIC COLLISION\n    - 180\n\n\n\n# prompt: find out top 10 victim age\n\ntop_10_victim_ages = df.groupBy('Victim Age').agg(count('*').alias('total_victims')).orderBy(desc('total_victims')).limit(10).toPandas()['Victim Age'].tolist()\n\nprint(\"Top 10 victim ages:\")\nfor age in top_10_victim_ages:\n    print(f\"\\t- {age}\")\n\nTop 10 victim ages:\n    - None\n    - 30\n    - 25\n    - 27\n    - 24\n    - 28\n    - 26\n    - 23\n    - 35\n    - 29"
  },
  {
    "objectID": "posts/Big_data/Pyspark.html#visualization",
    "href": "posts/Big_data/Pyspark.html#visualization",
    "title": "Big Data Analysis",
    "section": "",
    "text": "# prompt: visualize the result of top 10 victim age in the form of a pie chart\n\n# Get the top 10 victim ages and their counts\ntop_10_victim_ages = df.groupBy('Victim Age').agg(count('*').alias('total_victims')).orderBy(desc('total_victims')).limit(10).toPandas()\n\n# Create a pie chart of the top 10 victim ages\nplt.figure(figsize=(12, 6))\nplt.pie(top_10_victim_ages['total_victims'], labels=top_10_victim_ages['Victim Age'], autopct='%1.1f%%')\nplt.title('Top 10 Victim Ages')\nplt.show()"
  },
  {
    "objectID": "posts/data_analysis/advance-data-analysis-for-reporting.html",
    "href": "posts/data_analysis/advance-data-analysis-for-reporting.html",
    "title": "Data analysis for reporting",
    "section": "",
    "text": "! pip install streamlit\n\n\n!pip install altair\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings as wrn\nimport streamlit as st\nimport altair as alt\n\nwrn.filterwarnings('ignore', category = DeprecationWarning) \nwrn.filterwarnings('ignore', category = FutureWarning) \nwrn.filterwarnings('ignore', category = UserWarning) \n\n\n\ndf= pd.read_excel(\"//kaggle//input//fenix-shipping-data//bi5EoWE9QkiqEMz37MceAw_2edba123616f40909cb8896b374a31a1_Fenix-Shipping-Data.xlsx\")\n\n\n# Extract year from order_date and calculate yearly freight costs\nyearly_freight_costs = df.groupby(df['order_date'].dt.year)['freight'].sum()\n\n# Creating the Yearly Freight Costs line chart\nplt.figure(figsize=(10, 6))\nyearly_freight_costs.plot(kind='line', marker='o', linestyle='-', color='blue')\nplt.title('Yearly Freight Costs')\nplt.xlabel('Year')\nplt.ylabel('Total Freight Cost')\nplt.grid(True)\n\nplt.show()\n\n\n\n\n\n# Calculate total freight costs by country and select the top 5 countries\ntop_countries_freight = df.groupby('country')['freight'].sum().sort_values(ascending=False).head(5)\n\n# Creating the Top 5 Countries by Freight Cost pie chart\nplt.figure(figsize=(8, 8))\ntop_countries_freight.plot(kind='pie', autopct='%1.1f%%', startangle=140, colors=plt.cm.Set3.colors)\nplt.title('Top 5 Countries by Freight Cost')\nplt.ylabel('')  # Hide the y-label\n\nplt.show()\n\n\n\n\n\n# Creating the Freight Cost Distribution by Order Size histogram\nplt.figure(figsize=(10, 6))\ndf['freight'].plot(kind='hist', bins=30, color='green', edgecolor='black')\nplt.title('Freight Cost Distribution by Order Size')\nplt.xlabel('Freight Cost')\nplt.ylabel('Number of Orders')\nplt.grid(True)\n\nplt.show()\n\n\n\n\n\n# Hypothetical sales data by product category\nproduct_categories = ['Electronics', 'Clothing', 'Home Goods', 'Books', 'Groceries']\nsales_volumes = [20000, 15000, 12000, 8000, 10000]\n\n# Create a pie chart\nplt.figure(figsize=(10, 7))\nplt.pie(sales_volumes, labels=product_categories, autopct='%1.1f%%', startangle=140, colors=plt.cm.tab20.colors)\nplt.title('Total Sales by Product Category')\nplt.show()\n\n\n\n\n\n# Convert order_date to datetime if not already in that format\ndf['order_date'] = pd.to_datetime(df['order_date'])\n\n# Calculate total freight costs\ntotal_freight = df['freight'].sum()\n\n# Analyze sales (freight) over time - monthly\nmonthly_sales = df.set_index('order_date')['freight'].resample('M').sum()\n\ntotal_freight, monthly_sales\n\n(207306.09999999998,\n order_date\n 1996-07-31     4000.88\n 1996-08-31     4348.43\n 1996-09-30     3307.37\n 1996-10-31     5423.29\n 1996-11-30     5985.35\n 1996-12-31     9006.21\n 1997-01-31     7022.50\n 1997-02-28     5099.44\n 1997-03-31     6617.18\n 1997-04-30     9977.39\n 1997-05-31    12271.50\n 1997-06-30     5514.03\n 1997-07-31     8621.37\n 1997-08-31     9686.56\n 1997-09-30    10934.76\n 1997-10-31    14047.60\n 1997-11-30     6040.46\n 1997-12-31    10959.28\n 1998-01-31    19027.55\n 1998-02-28    10541.08\n 1998-03-31    16112.59\n 1998-04-30    20186.53\n 1998-05-31     2574.75\n Freq: ME, Name: freight, dtype: float64)\n\n\n\n\n# Aggregate data to count orders by country\norders_by_country = df.groupby('country').size().reset_index(name='order_count')\n\n# Create pie chart\nfig, ax = plt.subplots()\nax.pie(orders_by_country['order_count'], labels=orders_by_country['country'], autopct='%1.1f%%', startangle=90)\nax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\n# Display the chart\nst.title('Distribution of Orders by Country')\nst.pyplot(fig)\n\nDeltaGenerator()\n\n\n\n\n\n\n# Sidebar filters\nst.sidebar.header('Filters')\ndate_range = st.sidebar.date_input(\"Date range\", [])\nship_via = st.sidebar.multiselect('Ship Via', options=df['ship_via'].unique())\n\n# Filter the data based on selections\nfiltered_data = df.copy()\nif date_range:\n    filtered_data = filtered_data[(filtered_data['order_date'] &gt;= date_range[0]) & (filtered_data['order_date'] &lt;= date_range[1])]\nif ship_via:\n    filtered_data = filtered_data[filtered_data['ship_via'].isin(ship_via)]\n\n# Group data by region\norders_per_region = filtered_data.groupby('region')['order_id'].nunique().reset_index()\n\n# Chart: Orders per Region\nchart = alt.Chart(orders_per_region).mark_bar().encode(\n    x='region:N',\n    y='order_id:Q',\n    tooltip=['region', 'order_id']\n).properties(width=600, height=400, title='Orders per Region')\n\nst.altair_chart(chart, use_container_width=True)\n\nDeltaGenerator()"
  },
  {
    "objectID": "posts/Digitalization_of_agricluture/index.html",
    "href": "posts/Digitalization_of_agricluture/index.html",
    "title": "Digitaliztion of Agriculture",
    "section": "",
    "text": "The article- “The next wave of agriculture in Saskatchewan is digital and Indigenous-led” discusses how Indigenous communities in Saskatchewan are utilizing digital technologies to revitalize traditional farming practices.\nWhile also introducing new techniques to increase crop yields and reduce the impact of climate change and with the use of precision agriculture, remote sensing technologies, and data analysis, these communities are working to improve soil health, increase biodiversity, and foster sustainable practices. The article highlights several Indigenous-led initiatives, such as the One House, Many Nations project and the Whitecap Dakota First Nation’s Regenerative Farming Program, which are helping to promote food sovereignty and economic development in the region. Additionally, these initiatives are providing opportunities for Indigenous youth to learn about their cultural heritage and connect with the land."
  },
  {
    "objectID": "posts/fertility_map_africa/afsis-soil-chem-EDA.html",
    "href": "posts/fertility_map_africa/afsis-soil-chem-EDA.html",
    "title": "Soil Chemistry Dataset",
    "section": "",
    "text": "To download data:\n\naws-cli\n\nTo parse and manage datasets:\n\nbrukeropusreader\npandas\ntqdm\n\n\nInstallation below:\n\n#! pip install awscli brukeropusreader tqdm pandas matplotlib folium seaborn pathlib \n\n#commented out after the installation is completed in case you run again the notebook         \n\n\n#import pyspark\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import Image\nimport folium\nfrom pathlib import Path\nfrom brukeropusreader import read_file\nfrom collections import Counter\n\nfrom tqdm import tqdm_notebook as tqdm"
  },
  {
    "objectID": "posts/fertility_map_africa/afsis-soil-chem-EDA.html#geographical-references",
    "href": "posts/fertility_map_africa/afsis-soil-chem-EDA.html#geographical-references",
    "title": "Soil Chemistry Dataset",
    "section": "2. Geographical references",
    "text": "2. Geographical references\nThe AfSIS Soil Chemistry Dataset contains georeferences for each spectra. It is worth noting that it consists of two datasets, one for the subsaharian Africa and one with additional data recorded only in Tanzania.\n\n2.1 Dataframes exploration\n\nGEOREFS_FILE1 = 'afsis/2009-2013/Georeferences/georeferences.csv' \ndf_geo1 = pd.read_csv(GEOREFS_FILE1)# dataframe for several african countries\ndf_geo1 = df_geo1.sort_values(by=['Site', 'Cultivated'])\nprint(df_geo1.shape)\nprint(df_geo1.isna().sum())\ndf_geo1.head()\n\n(1843, 14)\nSSN                0\nPublic             0\nLatitude           0\nLongitude          0\nCluster            0\nPlot               0\nDepth              0\nSoil material     94\nScientist          0\nSite               0\nCountry            0\nRegion           232\nCultivated       798\nGid                0\ndtype: int64\n\n\n\n\n\n\n\n\n\nSSN\nPublic\nLatitude\nLongitude\nCluster\nPlot\nDepth\nSoil material\nScientist\nSite\nCountry\nRegion\nCultivated\nGid\n\n\n\n\n460\nicr016032\nTrue\n5.423182\n-0.709083\n15\n1\ntop\nAju.15.1.Topsoil.Std fine soil\nJerome Tondoh\nAjumako\nGhana\nWest Africa\nFalse\n345\n\n\n500\nicr016033\nTrue\n5.423182\n-0.709083\n15\n1\nsub\nAju.15.1.Subsoil.Std fine soil\nJerome Tondoh\nAjumako\nGhana\nWest Africa\nFalse\n346\n\n\n666\nicr015913\nTrue\n5.377035\n-0.726425\n9\n1\nsub\nAju.9.1.Subsoil.Std fine soil\nJerome Tondoh\nAjumako\nGhana\nWest Africa\nFalse\n334\n\n\n732\nicr016053\nTrue\n5.447667\n-0.717422\n16\n1\nsub\nAju.16.1.Subsoil.Std fine soil\nJerome Tondoh\nAjumako\nGhana\nWest Africa\nFalse\n348\n\n\n957\nicr015973\nTrue\n5.434675\n-0.728883\n12\n1\nsub\nAju.12.1.Subsoil.Std fine soil\nJerome Tondoh\nAjumako\nGhana\nWest Africa\nFalse\n340\n\n\n\n\n\n\n\n\nGEOREFS_FILE2 = 'afsis/tansis/Georeferences/georeferences.csv' \ndf_geo2 = pd.read_csv(GEOREFS_FILE2)# dataframe with additional data for Tanzania\ndf_geo2 = df_geo2.sort_values(by=['Site', 'Cultivated'])\nprint(df_geo2.shape)\n\ndf_geo2.head()\n\n(18819, 14)\n\n\n\n\n\n\n\n\n\nCluster\nCountry\nCultivated\nDepth\nGid\nLatitude\nLongitude\nPlot\nRegion\nSSN\nSampling date\nScientist\nSite\nSoil material\n\n\n\n\n29\n2.0\nTanzania\nFalse\ntop\n46.0\n-3.029698\n33.015202\n2.0\nEast Africa\nicr011843\nNaN\nLeigh Winoweicki\nBukwaya\nBuk.2.2.Topsoil.Std fine soil\n\n\n31\n4.0\nTanzania\nFalse\ntop\n48.0\n-2.992815\n33.016113\n1.0\nEast Africa\nicr011881\nNaN\nLeigh Winoweicki\nBukwaya\nBuk.4.1.Topsoil.Std fine soil\n\n\n32\n5.0\nTanzania\nFalse\ntop\n49.0\n-3.060982\n33.030663\n1.0\nEast Africa\nicr011901\nNaN\nLeigh Winoweicki\nBukwaya\nBuk.5.1.Topsoil.Std fine soil\n\n\n35\n8.0\nTanzania\nFalse\ntop\n52.0\n-2.987467\n33.033264\n1.0\nEast Africa\nicr011960\nNaN\nLeigh Winoweicki\nBukwaya\nBuk.8.1.Topsoil.Std fine soil\n\n\n36\n9.0\nTanzania\nFalse\ntop\n53.0\n-3.058480\n33.065014\n3.0\nEast Africa\nicr011978\nNaN\nLeigh Winoweicki\nBukwaya\nBuk.9.3.Topsoil.Std fine soil\n\n\n\n\n\n\n\nThe tansis folder contains measurements for Tanzania only. FTIR spectra in this folder are not readable and make it unusable\n\ntodrop = ['Soil material','Scientist', 'Site', 'Region', 'Gid','Plot','Public'] #irrelevant\ndf_geo = df_geo1.drop(todrop, axis = 1)"
  },
  {
    "objectID": "posts/fertility_map_africa/afsis-soil-chem-EDA.html#dry-chemistry",
    "href": "posts/fertility_map_africa/afsis-soil-chem-EDA.html#dry-chemistry",
    "title": "Soil Chemistry Dataset",
    "section": "3. Dry Chemistry",
    "text": "3. Dry Chemistry\nThe AfSIS Soil Chemistry dataset contains dry and wet chemistry data taken at each sampling location.\n\n3.1 X-ray fluorescence (XRF) elemental analysis\nwith XRF we get the concentration of various chemical elements in the sample\nUnits are parts per million (ppm)\nhttps://www.elementalanalysis.com/xrf.html\n\nfile1path = \"afsis/2009-2013/Dry_Chemistry/ICRAF/Bruker_TXRF/TXRF.csv\"\nfile2path = \"afsis/tansis/Dry_Chemistry/ICRAF/Bruker_TXRF/TXRF.csv\"\nxrf_africa = pd.read_csv(file1path)\nxrf_tanzania = pd.read_csv(file2path)\n\nprint('measurements 2009-13',xrf_africa.shape,'measurements  2014', xrf_tanzania.shape)\n\nprint(\"the table below shows the concentration in ppm for each element detected\")\nxrf_africa.head() \n\nmeasurements 2009-13 (1904, 42) measurements  2014 (224, 42)\nthe table below shows the concentration in ppm for each element detected\n\n\n\n\n\n\n\n\n\nSSN\nPublic\nNa\nMg\nAl\nP\nS\nCl\nK\nCa\n...\nPr\nNd\nSm\nHf\nTa\nW\nHg\nPb\nBi\nTh\n\n\n\n\n0\nicr005965\nTrue\n16023.3\n4433.5\n37618.6\n84.4\n45.7\n268.1\n12412.2\n30705.6\n...\n0.9\n14.7\n14.0\n0.9\n2.5\n0.2\n4.9\n3.9\n0.1\n13.4\n\n\n1\nicr005966\nTrue\n20524.6\n5832.2\n40248.2\n72.1\n45.7\n229.6\n12892.2\n23234.5\n...\n1.1\n15.8\n18.2\n0.5\n3.2\n0.2\n4.2\n3.3\n0.1\n19.9\n\n\n2\nicr005985\nTrue\n19350.4\n5085.8\n36766.3\n50.6\n45.7\n157.3\n16839.7\n16746.2\n...\n1.2\n19.2\n14.1\n0.8\n2.0\n1.2\n2.6\n12.0\n0.1\n17.9\n\n\n3\nicr005986\nTrue\n17410.2\n5271.2\n37912.2\n50.6\n45.7\n285.2\n16818.0\n31939.6\n...\n1.1\n16.7\n12.6\n0.3\n1.2\n0.5\n6.3\n10.2\n0.1\n16.5\n\n\n4\nicr005998\nTrue\n19092.5\n9169.8\n37359.8\n50.6\n45.7\n251.4\n17577.9\n25298.2\n...\n1.1\n16.7\n17.2\n0.5\n3.2\n0.4\n4.2\n5.6\n0.1\n18.4\n\n\n\n\n5 rows × 42 columns\n\n\n\n\nmask_diff_xrf = xrf_africa[xrf_africa['SSN'].isin(diff_list )]\ndf_xrf = pd.concat([xrf_tanzania, mask_diff_xrf]) \n\n\nprint(df_xrf.shape, len(df_xrf.SSN.unique()))\n\n(1838, 42) 1838\n\n\n\nprint('important elements for agriculture') \n# https://www.qld.gov.au/environment/land/management/soil/soil-properties/fertility\n\nimportant_elements = ['P','K', 'S','Ca','Mg','Cu','Cl','Zn','Fe', 'Mn','Mo' ]\ndf_xrf_reduced = df_xrf[important_elements]#/10000# express in %\ndf_xrf_reduced['SSN'] = df_xrf['SSN']\ndf_xrf_reduced.head()\n\nimportant elements for agriculture\n\n\nSettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_xrf_reduced['SSN'] = df_xrf['SSN']\n\n\n\n\n\n\n\n\n\nP\nK\nS\nCa\nMg\nCu\nCl\nZn\nFe\nMn\nMo\nSSN\n\n\n\n\n0\n84.4\n12412.2\n45.7\n30705.6\n4433.5\n10.1\n268.1\n25.3\n22263.2\n356.1\n184.5\nicr005965\n\n\n1\n72.1\n12892.2\n45.7\n23234.5\n5832.2\n11.8\n229.6\n29.4\n26269.6\n379.7\n184.5\nicr005966\n\n\n2\n50.6\n16839.7\n45.7\n16746.2\n5085.8\n22.1\n157.3\n32.4\n22790.2\n323.4\n184.5\nicr005985\n\n\n3\n50.6\n16818.0\n45.7\n31939.6\n5271.2\n24.9\n285.2\n34.4\n22939.1\n332.5\n184.5\nicr005986\n\n\n4\n50.6\n17577.9\n45.7\n25298.2\n9169.8\n13.1\n251.4\n35.4\n24985.2\n408.3\n184.5\nicr005998\n\n\n\n\n\n\n\n\n# how disperse are the XRF data?\nprint(df_xrf_reduced.shape)\ndf_xrf_reduced.describe()\n\n(1838, 12)\n\n\n\n\n\n\n\n\n\nP\nK\nS\nCa\nMg\nCu\nCl\nZn\nFe\nMn\nMo\n\n\n\n\ncount\n1838.000000\n1838.000000\n1838.000000\n1838.000000\n1838.000000\n1838.000000\n1838.000000\n1838.000000\n1838.000000\n1838.000000\n1838.000000\n\n\nmean\n0.011513\n1.126107\n0.007641\n0.758606\n0.730171\n0.001519\n0.026368\n0.002566\n2.633472\n0.040358\n0.019379\n\n\nstd\n0.046650\n1.268460\n0.040276\n2.284191\n0.733123\n0.001496\n0.222005\n0.002984\n2.673819\n0.052072\n0.012800\n\n\nmin\n0.002950\n0.009040\n0.003320\n0.004400\n0.402890\n0.000050\n0.000600\n0.000090\n0.007430\n0.000540\n0.017950\n\n\n25%\n0.005060\n0.240008\n0.004570\n0.046742\n0.557500\n0.000450\n0.005687\n0.000780\n0.779240\n0.010405\n0.018450\n\n\n50%\n0.005060\n0.696950\n0.004570\n0.141455\n0.557500\n0.001125\n0.010050\n0.001890\n1.947845\n0.022950\n0.018450\n\n\n75%\n0.005060\n1.624108\n0.004570\n0.518190\n0.557500\n0.002050\n0.016350\n0.003437\n3.452745\n0.052255\n0.018450\n\n\nmax\n1.246380\n10.627340\n0.970400\n42.643090\n13.689340\n0.012840\n7.438340\n0.073000\n22.908970\n0.660790\n0.444250\n\n\n\n\n\n\n\n\n\n3.2 Fourier transform infrared spectroscopy (FTIR)\nA word about units. Most spectra using electromagnetic radiation are presented with wavelength as the X-axis in nm or μm. Originally, IR spectra were presented in units of micrometers. Later (1953) a different measure, the wavenumber given the unit cm-1, was adopted.\nν (cm-1)= 10,000/λ (μm)\nThe spectra may appear to be “backward” (large wavenumber values on the left, running to low values on the right); this is a consequence of the μm to cm-1 conversion\n\n3.2.1 NIR (near infrared range) FTIR\nspectral range: 12500 - 4000 cm-1 or 700 - 2500 nm for near infrared (NIR)\n\nImage(filename='img/NIR.jpeg') \n\n\n\n\n\nNIR_SPECTRA_DIR = 'Bruker_MPA/*'\nAFSIS_PATH = Path('afsis/2009-2013/Dry_Chemistry/ICRAF')\nnames = []\nspectra = []\n\nfor path in tqdm(AFSIS_PATH.glob(NIR_SPECTRA_DIR )):\n    if path.is_file():\n        spect_data = read_file(path)\n        spectra.append(spect_data[\"AB\"])\n        names.append(path.stem)\nwave_nums = spect_data.get_range()\n\ncolumn_names = ['{:.0f}'.format(x) for x in wave_nums]\nnear_infrared_df = pd.DataFrame(spectra, index=names, columns=column_names)\nnear_infrared_df.head()\n\nTqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  for path in tqdm(AFSIS_PATH.glob(NIR_SPECTRA_DIR )):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12493\n12489\n12485\n12482\n12478\n12474\n12470\n12466\n12462\n12458\n...\n3633\n3630\n3626\n3622\n3618\n3614\n3610\n3606\n3603\n3599\n\n\n\n\nicr033603\n0.744053\n0.744943\n0.737978\n0.734149\n0.738894\n0.741579\n0.738796\n0.740573\n0.745435\n0.745959\n...\n2.679350\n2.639192\n2.631729\n2.611440\n2.646616\n2.508814\n2.414326\n2.423105\n2.533958\n2.656340\n\n\nicr042897\n0.553651\n0.549628\n0.549979\n0.555073\n0.561452\n0.566700\n0.571925\n0.578335\n0.582339\n0.579222\n...\n2.455164\n2.484992\n2.565423\n2.588626\n2.550071\n2.503246\n2.361378\n2.318084\n2.421962\n2.429910\n\n\nicr049675\n0.533260\n0.531480\n0.535772\n0.545087\n0.551631\n0.550375\n0.546267\n0.545179\n0.545328\n0.542055\n...\n2.641663\n2.420862\n2.309041\n2.268263\n2.305246\n2.589037\n2.847070\n2.433076\n2.500584\n2.292974\n\n\nicr034693\n0.568058\n0.566100\n0.563312\n0.560136\n0.556757\n0.558751\n0.566576\n0.575022\n0.580073\n0.579828\n...\n2.410610\n2.373128\n2.324080\n2.145765\n2.025990\n2.089124\n2.294537\n2.543749\n2.517684\n2.474452\n\n\nicr033950\n0.555743\n0.550596\n0.548068\n0.554348\n0.559019\n0.554218\n0.551323\n0.556638\n0.561071\n0.562413\n...\n2.714783\n2.360171\n2.265779\n2.247130\n2.206304\n2.150379\n2.145735\n2.169409\n2.158581\n2.150478\n\n\n\n\n5 rows × 2307 columns\n\n\n\n\n# table with FTIR spectra for each sample\n\ndf_NIR_FTIRspectra = near_infrared_df.T.reset_index()\n\ndf_NIR_FTIRspectra = df_NIR_FTIRspectra.rename(columns={'index': 'labda'})\ndf_NIR_FTIRspectra.labda = pd.to_numeric(df_NIR_FTIRspectra.labda)\n\ndf_NIR_FTIRspectra.head()\n\n\n\n\n\n\n\n\nlabda\nicr033603\nicr042897\nicr049675\nicr034693\nicr033950\nicr034794\nicr015953\nicr050394\nicr048771\n...\nicr073540\nicr049437\nicr037699\nicr075004\nicr056181\nicr055563\nicr010159\nicr074792\nicr011321\nicr062275\n\n\n\n\n0\n12493\n0.744053\n0.553651\n0.533260\n0.568058\n0.555743\n0.643860\n0.431300\n0.684270\n0.569734\n...\n0.268871\n0.623325\n0.427489\n0.664087\n0.612543\n0.400455\n0.604818\n0.511006\n0.633803\n0.539674\n\n\n1\n12489\n0.744943\n0.549628\n0.531480\n0.566100\n0.550596\n0.648193\n0.434562\n0.684778\n0.567334\n...\n0.266468\n0.631448\n0.425584\n0.665025\n0.612217\n0.398957\n0.605491\n0.507771\n0.635440\n0.541978\n\n\n2\n12485\n0.737978\n0.549979\n0.535772\n0.563312\n0.548068\n0.648293\n0.435362\n0.677241\n0.563958\n...\n0.265169\n0.638345\n0.430387\n0.654435\n0.615592\n0.396526\n0.604928\n0.507411\n0.637939\n0.545053\n\n\n3\n12482\n0.734149\n0.555073\n0.545087\n0.560136\n0.554348\n0.642154\n0.436202\n0.676640\n0.561916\n...\n0.265009\n0.636849\n0.433966\n0.644498\n0.619221\n0.399731\n0.601859\n0.511584\n0.640378\n0.543280\n\n\n4\n12478\n0.738894\n0.561452\n0.551631\n0.556757\n0.559019\n0.641057\n0.436053\n0.683005\n0.566201\n...\n0.265186\n0.632287\n0.433450\n0.646780\n0.623118\n0.405075\n0.597800\n0.517670\n0.642512\n0.540592\n\n\n\n\n5 rows × 1908 columns\n\n\n\n\nplt.figure(figsize= (6,6))\n\nplt.plot(df_NIR_FTIRspectra['labda'],df_NIR_FTIRspectra.iloc[:, [4,5,6,7,8,9,10,11,12,13,14,15,16,17,18]]) \n\nplt.title('Example spectra')\n\n\nplt.xticks(rotation=90)\nplt.xlim(12000,3500)\nplt.ylabel('Absorbance (A.U.)', fontsize=14)\nplt.xlabel('Wavelengths (cm-1)', fontsize = 14)\n\nplt.axvline(x=7500 , ymin=0, ymax=1, color='r', linewidth = 1)\nplt.axvline(x=4000, ymin=0, ymax=1, color='r', linewidth = 1)\nplt.text(7400, 2.5, 'useful range between \\n red lines')\n\nText(7400, 2.5, 'useful range between \\n red lines')\n\n\n\n\n\nIn the region 12000 - 7500 cm-1 there are no peaks, this part of the spectrum can be ignored below 4000 cm-1 the signal is unrealistic. It is an experimental artifact and this part of the spectrum should be cut off.\nAbsorbance in this range is analized with mid-range FTIR spectrometers (see section 3.2.2 below)\n\ndf_NIR_FTIRspectra =  df_NIR_FTIRspectra[df_NIR_FTIRspectra['labda'] &lt; 7500]  \ndf_NIR_FTIRspectra = df_NIR_FTIRspectra[df_NIR_FTIRspectra['labda'] &gt; 4000]\n\n\nplt.figure(figsize= (6,6))\n\nplt.plot(df_NIR_FTIRspectra['labda'],df_NIR_FTIRspectra.iloc[:, [4,5,6,12,16,17,48]]) \n\nplt.title('Example reduced spectra')\n\nplt.xticks(rotation=90)\nplt.xlim(7500,4000)\nplt.ylabel('Absorbance (A.U.)', fontsize=14)\nplt.xlabel('Wavelengths (cm-1)', fontsize = 14)\n\nText(0.5, 0, 'Wavelengths (cm-1)')\n\n\n\n\n\n\n# change original dataset accordingly\n\ndf_NIR_reindexed = df_NIR_FTIRspectra.set_index('labda')\nnear_infrared_df = df_NIR_reindexed.T\nnear_infrared_df.head()\n\n\n\n\n\n\n\nlabda\n7498\n7494\n7491\n7487\n7483\n7479\n7475\n7471\n7467\n7464\n...\n4038\n4035\n4031\n4027\n4023\n4019\n4015\n4011\n4008\n4004\n\n\n\n\nicr033603\n0.469107\n0.469056\n0.469180\n0.469344\n0.469312\n0.469145\n0.469012\n0.469019\n0.469138\n0.469201\n...\n0.693520\n0.696103\n0.698850\n0.701674\n0.704414\n0.707121\n0.709787\n0.712360\n0.714864\n0.717183\n\n\nicr042897\n0.453903\n0.453874\n0.453821\n0.453781\n0.453818\n0.453874\n0.453957\n0.454082\n0.454211\n0.454340\n...\n0.679447\n0.681886\n0.684869\n0.687917\n0.690609\n0.692926\n0.694870\n0.696561\n0.698304\n0.700157\n\n\nicr049675\n0.311495\n0.311474\n0.311484\n0.311448\n0.311298\n0.311103\n0.310900\n0.310745\n0.310745\n0.310800\n...\n0.471828\n0.473717\n0.475707\n0.477803\n0.479886\n0.481793\n0.483479\n0.485172\n0.487041\n0.488976\n\n\nicr034693\n0.420559\n0.420480\n0.420528\n0.420549\n0.420534\n0.420425\n0.420276\n0.420284\n0.420467\n0.420655\n...\n0.695133\n0.698124\n0.701617\n0.705556\n0.709398\n0.712633\n0.715035\n0.716666\n0.718061\n0.719766\n\n\nicr033950\n0.327227\n0.327036\n0.326880\n0.326711\n0.326502\n0.326389\n0.326441\n0.326477\n0.326314\n0.325999\n...\n0.437366\n0.439201\n0.441031\n0.442873\n0.444723\n0.446564\n0.448291\n0.449815\n0.451223\n0.452619\n\n\n\n\n5 rows × 907 columns\n\n\n\n\n\n3.2.2 MIR (middle infrared range) FTIR\nspectral range 400 - 4000 cm-1\ndata recorded with three spectrometers that differ for the method for collecting data.\n\nALPHA Kbr spectrometer: transmission, using samples pressed into a Kbr pellet\nALPHA ZnSe spectrometer: reflection over a diamond window and ZnSe filter / beam splitter\nTensor27 Kbr spectrometer further reading about experimental details can be found at afsis/2009-2013/Dry_Chemistry/ICRAF/SOP/METH07V01 ALPHA.pdf\n\nand a general introduction can be found here: https://www.shimadzu.com/an/service-support/technical-support/analysis-basics/ftirtalk/talk8.html\n\n2014-2018 Most FTIR spectra could not be opened using the Bruker open files library, in this kernel only those obtained with the Tensor27 spectrometer are analyzed\n\n\n- ALPHA spectrometer - KBr window\n\nKBR_SPECTRA_DIR = 'Bruker_Alpha_KBr/*'\nAFSIS_PATH = Path('afsis/2009-2013/Dry_Chemistry/ICRAF')\nTANSIS_PATH = Path('afsis/tansis/Dry_Chemistry/ICRAF')\nnames = []\nspectra = []\n\nfor path in tqdm(AFSIS_PATH.glob(KBR_SPECTRA_DIR )):\n    if path.is_file():\n        spect_data = read_file(path)\n        spectra.append(spect_data[\"AB\"])\n        names.append(path.stem)\nwave_nums = spect_data.get_range()\n\ncolumn_names = ['{:.0f}'.format(x) for x in wave_nums]\nkbr_df = pd.DataFrame(spectra, index=names, columns=column_names)\nkbr_df.head()\n\nTqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  for path in tqdm(AFSIS_PATH.glob(KBR_SPECTRA_DIR )):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3998\n3997\n3996\n3994\n3993\n3991\n3990\n3988\n3987\n3986\n...\n412\n411\n409\n408\n406\n405\n404\n402\n401\n399\n\n\n\n\nicr033603\n0.908592\n0.908981\n0.909435\n0.909956\n0.910562\n0.911266\n0.912069\n0.912953\n0.913888\n0.914843\n...\n1.849416\n1.860089\n1.872713\n1.885405\n1.896269\n1.903926\n1.908109\n1.910104\n1.912738\n1.919517\n\n\nicr042897\n0.810133\n0.809940\n0.809745\n0.809616\n0.809566\n0.809563\n0.809558\n0.809516\n0.809437\n0.809359\n...\n2.069042\n2.098991\n2.124525\n2.143315\n2.153621\n2.154386\n2.145273\n2.126816\n2.101002\n2.071937\n\n\nicr049675\n0.711836\n0.712355\n0.713021\n0.713747\n0.714438\n0.715037\n0.715535\n0.715951\n0.716298\n0.716546\n...\n2.042677\n2.025819\n2.009498\n1.997672\n1.991367\n1.989609\n1.990601\n1.992822\n1.995704\n1.999495\n\n\nicr034693\n0.788686\n0.789201\n0.789494\n0.789611\n0.789634\n0.789656\n0.789744\n0.789912\n0.790109\n0.790249\n...\n2.083768\n2.084394\n2.084220\n2.081256\n2.072820\n2.055961\n2.028284\n1.989270\n1.941584\n1.891300\n\n\nicr033950\n0.752478\n0.753251\n0.753915\n0.754423\n0.754733\n0.754816\n0.754683\n0.754402\n0.754084\n0.753866\n...\n1.925999\n1.940593\n1.958213\n1.975761\n1.988359\n1.990657\n1.979214\n1.954502\n1.920762\n1.884145\n\n\n\n\n5 rows × 2542 columns\n\n\n\n\n# table with FTIR spectra for each sample\ndf_KBR_FTIRspectra = kbr_df.T.reset_index()\n\ndf_KBR_FTIRspectra = df_KBR_FTIRspectra.rename(columns={'index': 'labda'})\ndf_KBR_FTIRspectra.labda = pd.to_numeric(df_KBR_FTIRspectra.labda)\n\ndf_KBR_FTIRspectra.head()\n\n\n\n\n\n\n\n\nlabda\nicr033603\nicr042897\nicr049675\nicr034693\nicr033950\nicr034794\nicr015953\nicr050394\nicr048771\n...\nicr011182\nicr073540\nicr049437\nicr037699\nicr056181\nicr055563\nicr010159\nicr074792\nicr011321\nicr062275\n\n\n\n\n0\n3998\n0.908592\n0.810133\n0.711836\n0.788686\n0.752478\n0.799947\n0.819920\n0.701762\n0.854325\n...\n0.741123\n0.707601\n0.735885\n0.792272\n0.784010\n0.675416\n0.859956\n0.675066\n0.807336\n0.753118\n\n\n1\n3997\n0.908981\n0.809940\n0.712355\n0.789201\n0.753251\n0.801083\n0.819910\n0.702142\n0.854623\n...\n0.741535\n0.707462\n0.735881\n0.793356\n0.784840\n0.675505\n0.859586\n0.675193\n0.807802\n0.753273\n\n\n2\n3996\n0.909435\n0.809745\n0.713021\n0.789494\n0.753915\n0.802259\n0.820077\n0.702510\n0.854676\n...\n0.741678\n0.707289\n0.735951\n0.794496\n0.785365\n0.675546\n0.859557\n0.675159\n0.808202\n0.753584\n\n\n3\n3994\n0.909956\n0.809616\n0.713747\n0.789611\n0.754423\n0.803318\n0.820496\n0.702824\n0.854543\n...\n0.741524\n0.707115\n0.736090\n0.795609\n0.785554\n0.675588\n0.859890\n0.674942\n0.808552\n0.754089\n\n\n4\n3993\n0.910562\n0.809566\n0.714438\n0.789634\n0.754733\n0.804114\n0.821235\n0.703081\n0.854348\n...\n0.741103\n0.706966\n0.736295\n0.796670\n0.785448\n0.675682\n0.860528\n0.674554\n0.808865\n0.754796\n\n\n\n\n5 rows × 1889 columns\n\n\n\n\nplt.figure(figsize= (6,6))\n\nplt.plot(df_KBR_FTIRspectra['labda'],df_KBR_FTIRspectra.iloc[:, [4,257,100]]) \n\nplt.title('Example spectra MIR-FTIR KBr window')\n\nplt.xticks(rotation=90)\nplt.xlim(4000,400)\nplt.ylabel('Absorbance (A.U.)', fontsize=14)\nplt.xlabel('Wavelengths (cm-1)', fontsize = 14)\n\nText(0.5, 0, 'Wavelengths (cm-1)')\n\n\n\n\n\n\n\n- Alpha spectrometer - ZnSe window\n\nZnSe_SPECTRA_DIR = 'Bruker_Alpha_ZnSe/*'\nAFSIS_PATH = Path('afsis/2009-2013/Dry_Chemistry/ICRAF')\n\nnames = []\nspectra = []\n\nfor path in tqdm(AFSIS_PATH.glob(ZnSe_SPECTRA_DIR )):\n    if path.is_file():\n        spect_data = read_file(path)\n        spectra.append(spect_data[\"AB\"])\n        names.append(path.stem)\nwave_nums = spect_data.get_range()\n\ncolumn_names1 = ['{:.0f}'.format(x) for x in wave_nums]\nZnSe_df = pd.DataFrame(spectra, index=names, columns=column_names1)\n\nTqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  for path in tqdm(AFSIS_PATH.glob(ZnSe_SPECTRA_DIR )):\n\n\n\n\n\n\n\n\n\nZnSe_df.head()\n\n\n\n\n\n\n\n\n3996\n3994\n3992\n3990\n3988\n3986\n3984\n3982\n3980\n3978\n...\n518\n516\n514\n512\n510\n508\n506\n504\n502\n500\n\n\n\n\nicr033603\n1.336817\n1.337777\n1.338591\n1.339180\n1.339602\n1.339999\n1.340485\n1.341091\n1.341770\n1.342445\n...\n2.274969\n2.276752\n2.252275\n2.223028\n2.210677\n2.225878\n2.251971\n2.209258\n2.221467\n0.0\n\n\nicr042897\n1.217690\n1.218229\n1.218768\n1.219244\n1.219670\n1.220118\n1.220660\n1.221307\n1.221991\n1.222615\n...\n2.221671\n2.189322\n2.175592\n2.185125\n2.209400\n2.227240\n2.216305\n2.164982\n2.129341\n0.0\n\n\nicr049675\n1.155588\n1.155937\n1.156262\n1.156588\n1.156925\n1.157270\n1.157606\n1.157913\n1.158161\n1.158312\n...\n2.400493\n2.362777\n2.327363\n2.304740\n2.297083\n2.296757\n2.288914\n2.260693\n2.206604\n0.0\n\n\nicr034693\n1.215348\n1.215774\n1.216170\n1.216544\n1.216940\n1.217407\n1.217960\n1.218564\n1.219142\n1.219622\n...\n2.146795\n2.123838\n2.091934\n2.052702\n2.009385\n1.963674\n1.915830\n1.866890\n1.819085\n0.0\n\n\nicr033950\n1.163011\n1.163466\n1.163930\n1.164330\n1.164612\n1.164770\n1.164867\n1.165003\n1.165248\n1.165577\n...\n2.140887\n2.131997\n2.106665\n2.065614\n2.013958\n1.957800\n1.904114\n1.861121\n1.835173\n0.0\n\n\n\n\n5 rows × 1716 columns\n\n\n\n\n# - table with FTIR spectra for each sample\n\ndf_ZnSe_FTIRspectra = ZnSe_df.T.reset_index()\n\ndf_ZnSe_FTIRspectra = df_ZnSe_FTIRspectra.rename(columns={'index': 'labda'})\ndf_ZnSe_FTIRspectra.labda = pd.to_numeric(df_ZnSe_FTIRspectra.labda)\n\n\nprint(\"spectral range:\", df_ZnSe_FTIRspectra.labda.min(), \"cm-1 - \",df_ZnSe_FTIRspectra.labda.max(), \"cm-1\" )\n\nspectral range: 500 cm-1 -  3996 cm-1\n\n\n\n\n- Tensor 27 HTS-XT spectrometer\nKBr window and wider range: both MID and Near IR\n\nHTSXT_SPECTRA_DIR = 'Bruker_HTSXT/*'\nAFSIS_PATH = Path('afsis/2009-2013/Dry_Chemistry/ICRAF')\n\nnames = []\nspectra = []\n\nfor path in tqdm(AFSIS_PATH.glob(HTSXT_SPECTRA_DIR )):\n    if path.is_file():\n        spect_data = read_file(path)\n        spectra.append(spect_data[\"AB\"])\n        names.append(path.stem)\nwave_nums = spect_data.get_range()\n\ncolumn_names = ['{:.0f}'.format(x) for x in wave_nums]\nhtsxt_df = pd.DataFrame(spectra, index=names, columns=column_names)\n\nTqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  for path in tqdm(AFSIS_PATH.glob(HTSXT_SPECTRA_DIR )):\n\n\n\n\n\n\n\n\n\nprint('Total measurements',len(htsxt_df.index),', unique samples', len(htsxt_df.index.unique()) )\n\nTotal measurements 7346 , unique samples 1839\n\n\n\n# table with FTIR spectra for each sample\n\ndf_htsxt_FTIRspectra = htsxt_df.T.reset_index()\n\ndf_htsxt_FTIRspectra = df_htsxt_FTIRspectra.rename(columns={'index': 'labda'})\ndf_htsxt_FTIRspectra.labda = pd.to_numeric(df_htsxt_FTIRspectra.labda)\nprint(\"spectral range:\", df_htsxt_FTIRspectra.labda.min(), \"cm-1 - \",df_htsxt_FTIRspectra.labda.max(), \"cm-1\" )\n\nspectral range: 600 cm-1 -  7498 cm-1\n\n\nAre Tensor 27 HTS-XT spectrometer measurements reproducible?\n\nplt.plot(df_htsxt_FTIRspectra['labda'],df_htsxt_FTIRspectra['icr034794'], label = 'HTSXT') \n\nplt.title('Example same sample repetitions - HTSXT')\nplt.legend()\n\nplt.xticks(rotation=90)\nplt.xlim(7500,400)\nplt.ylabel('Absorbance (A.U.)', fontsize=14)\nplt.xlabel('Wavelengths (cm-1)', fontsize = 14)\n\nprint('4 repetitions, identical result')\n\n4 repetitions, identical result\n\n\n\n\n\nlet’s average the repetitions\n\nhtsxt_df = htsxt_df.reset_index()\nhtsxt_df.head()\n\n\n\n\n\n\n\n\nindex\n7498\n7496\n7494\n7492\n7490\n7488\n7486\n7485\n7483\n...\n617\n615\n613\n611\n609\n607\n606\n604\n602\n600\n\n\n\n\n0\nicr033603\n0.363767\n0.358597\n0.352962\n0.355229\n0.364906\n0.370382\n0.363909\n0.354324\n0.350436\n...\n1.903912\n1.892159\n1.883104\n1.875069\n1.863860\n1.846478\n1.827501\n1.811521\n1.798762\n1.785432\n\n\n1\nicr010356\n0.138930\n0.132892\n0.136494\n0.148280\n0.153970\n0.145607\n0.134926\n0.131624\n0.130996\n...\n1.799327\n1.789603\n1.779350\n1.767130\n1.751588\n1.736565\n1.723430\n1.712572\n1.705888\nNaN\n\n\n2\nicr055782\n0.198082\n0.193039\n0.187723\n0.190430\n0.200350\n0.205885\n0.199621\n0.190752\n0.188001\n...\n1.612597\n1.624695\n1.637514\n1.644494\n1.651400\n1.663048\n1.675583\n1.689286\n1.701757\n1.708080\n\n\n3\nicr011264\n0.327606\n0.321836\n0.325163\n0.336058\n0.341134\n0.332636\n0.321906\n0.318676\n0.317745\n...\n1.875749\n1.869287\n1.868335\n1.877906\n1.887795\n1.885020\n1.873055\n1.863219\n1.855847\nNaN\n\n\n4\nicr034402\n0.339334\n0.334438\n0.329383\n0.331227\n0.339271\n0.343485\n0.337471\n0.329251\n0.326680\n...\n1.491697\n1.492001\n1.491628\n1.489214\n1.486117\n1.482170\n1.478855\n1.479840\n1.483233\n1.483134\n\n\n\n\n5 rows × 3579 columns\n\n\n\n\nhtsxt_df = htsxt_df.rename(columns={'index': 'SSN'})\n\n\ngb_htsxt = htsxt_df.groupby(['SSN']).mean().reset_index()\nprint(gb_htsxt.shape)\n\n(1839, 3579)\n\n\n\ngb_htsxt = gb_htsxt.set_index('SSN')\n\n\ngb_htsxt_FTIRspectra = gb_htsxt.T.reset_index()\n\ngb_htsxt_FTIRspectra = gb_htsxt_FTIRspectra.rename(columns={'index': 'labda'})\ngb_htsxt_FTIRspectra.labda = pd.to_numeric(gb_htsxt_FTIRspectra.labda)\n\n\nplt.plot(df_htsxt_FTIRspectra['labda'],df_htsxt_FTIRspectra['icr034794'], label = 'repetition') \nplt.plot(gb_htsxt_FTIRspectra['labda'],gb_htsxt_FTIRspectra['icr034794'],color = 'k', label = 'average') \nplt.title('Same sample repetitions and average')\nplt.legend()\n\nplt.xticks(rotation=90)\nplt.xlim(7500,400)\nplt.ylabel('Absorbance (A.U.)', fontsize=14)\nplt.xlabel('Wavelengths (cm-1)', fontsize = 14)\n\nprint('4 repetitions, identical result')\n\n4 repetitions, identical result\n\n\n\n\n\n\nprint('what is the difference between different spectrometers measurements?')\nplt.figure(figsize= (6,6))\n\nplt.plot(df_KBR_FTIRspectra['labda'],df_KBR_FTIRspectra['icr042897'], label = 'Kbr') \nplt.plot(df_ZnSe_FTIRspectra['labda'],df_ZnSe_FTIRspectra['icr042897'], label = 'ZnSe') \nplt.plot(gb_htsxt_FTIRspectra['labda'],gb_htsxt_FTIRspectra['icr042897'], label = 'HTSXT') \n\nplt.title('Example different spectrometers')\nplt.legend()\n\nplt.xticks(rotation=90)\nplt.xlim(4000,400)\nplt.ylabel('Absorbance (A.U.)', fontsize=14)\nplt.xlabel('Wavelengths (cm-1)', fontsize = 14)\n\nwhat is the difference between different spectrometers measurements?\n\n\nText(0.5, 0, 'Wavelengths (cm-1)')\n\n\n\n\n\n\nprint(df_KBR_FTIRspectra.shape)\ndf_KBR_FTIRspectra.head()\n\n(2542, 1889)\n\n\n\n\n\n\n\n\n\nlabda\nicr033603\nicr042897\nicr049675\nicr034693\nicr033950\nicr034794\nicr015953\nicr050394\nicr048771\n...\nicr011182\nicr073540\nicr049437\nicr037699\nicr056181\nicr055563\nicr010159\nicr074792\nicr011321\nicr062275\n\n\n\n\n0\n3998\n0.908592\n0.810133\n0.711836\n0.788686\n0.752478\n0.799947\n0.819920\n0.701762\n0.854325\n...\n0.741123\n0.707601\n0.735885\n0.792272\n0.784010\n0.675416\n0.859956\n0.675066\n0.807336\n0.753118\n\n\n1\n3997\n0.908981\n0.809940\n0.712355\n0.789201\n0.753251\n0.801083\n0.819910\n0.702142\n0.854623\n...\n0.741535\n0.707462\n0.735881\n0.793356\n0.784840\n0.675505\n0.859586\n0.675193\n0.807802\n0.753273\n\n\n2\n3996\n0.909435\n0.809745\n0.713021\n0.789494\n0.753915\n0.802259\n0.820077\n0.702510\n0.854676\n...\n0.741678\n0.707289\n0.735951\n0.794496\n0.785365\n0.675546\n0.859557\n0.675159\n0.808202\n0.753584\n\n\n3\n3994\n0.909956\n0.809616\n0.713747\n0.789611\n0.754423\n0.803318\n0.820496\n0.702824\n0.854543\n...\n0.741524\n0.707115\n0.736090\n0.795609\n0.785554\n0.675588\n0.859890\n0.674942\n0.808552\n0.754089\n\n\n4\n3993\n0.910562\n0.809566\n0.714438\n0.789634\n0.754733\n0.804114\n0.821235\n0.703081\n0.854348\n...\n0.741103\n0.706966\n0.736295\n0.796670\n0.785448\n0.675682\n0.860528\n0.674554\n0.808865\n0.754796\n\n\n\n\n5 rows × 1889 columns\n\n\n\n\n\n- build unique dataset for FTIR\n\nKBr_list = kbr_df.index.tolist()\nZnSe_list = ZnSe_df.index.tolist()\nHTSXT_list = gb_htsxt.index.tolist()\nprint('samples tested with alpha-KBr', len(KBr_list))\nprint('samples tested with alpha-ZnSe', len(ZnSe_list))\nprint('samples tested with Tensor27', len(HTSXT_list))\n\nprint (\"difference samples alpha spectrometers:\",len(KBr_list) - len(ZnSe_list))\nprint (\"difference samples alpha_kbr to tensor27 spectrometers:\", len(KBr_list) - len(HTSXT_list))\n\nsamples tested with alpha-KBr 1888\nsamples tested with alpha-ZnSe 1835\nsamples tested with Tensor27 1839\ndifference samples alpha spectrometers: 53\ndifference samples alpha_kbr to tensor27 spectrometers: 49\n\n\n\ndiff_list1 = np.setdiff1d(KBr_list,ZnSe_list)\nprint(\"samples tested using the Alpha-KBr and not the Alpha-ZnSe spectrometer\")\nprint(len(diff_list1))\n\nsamples tested using the Alpha-KBr and not the Alpha-ZnSe spectrometer\n53\n\n\n\ndiff_list_KBr = np.setdiff1d(HTSXT_list, KBr_list)\nprint(\"samples tested using the Tensor27 and not the Alpha-KBr spectrometer\")\nprint(len(diff_list_KBr))\n\nsamples tested using the Tensor27 and not the Alpha-KBr spectrometer\n8\n\n\n\nmask_diff_kbr = kbr_df[kbr_df.index.isin(diff_list )]\nmask_diff_znse = ZnSe_df[ZnSe_df.index.isin(diff_list )]\nmask_diff_htsxt = gb_htsxt[gb_htsxt.index.isin(diff_list )]\nprint('KBr',len(mask_diff_kbr.index) ,'ZnSe', len(mask_diff_znse.index), 'HTSXT', len(mask_diff_htsxt.index))\n\nKBr 1616 ZnSe 1616 HTSXT 1615\n\n\n\nMost information from Alpha spectrometers and MPA is redundant. Tensor27 NIR peaks provide the same information for both MID and Near IR range"
  },
  {
    "objectID": "posts/fertility_map_africa/afsis-soil-chem-EDA.html#wet-chemistry",
    "href": "posts/fertility_map_africa/afsis-soil-chem-EDA.html#wet-chemistry",
    "title": "Soil Chemistry Dataset",
    "section": "4. Wet chemistry",
    "text": "4. Wet chemistry\n\nWET_CHEM_PATH1 = 'afsis/2009-2013/Wet_Chemistry/CROPNUTS/Wet_Chemistry_CROPNUTS.csv'\n\nwet_chem_df = pd.read_csv(WET_CHEM_PATH1)#, usecols=columns_to_load)\nelements = ['SSN','M3 Ca', 'M3 K', 'M3 Al', 'M3 P', 'M3 S', 'PH']\nwet_chem_df = wet_chem_df[elements]\n\nprint(wet_chem_df.shape)\nwet_chem_df.head()\n\n(1907, 7)\n\n\n\n\n\n\n\n\n\nSSN\nM3 Ca\nM3 K\nM3 Al\nM3 P\nM3 S\nPH\n\n\n\n\n0\nicr006475\n207.1\n306.30\n1095.0\n4.495\n18.960\n4.682\n\n\n1\nicr006586\n1665.0\n1186.00\n1165.0\n12.510\n13.600\n7.062\n\n\n2\nicr007929\n2518.0\n72.57\n727.6\n21.090\n14.810\n7.114\n\n\n3\nicr008008\n734.3\n274.60\n1458.0\n109.200\n11.400\n5.650\n\n\n4\nicr010198\n261.8\n91.76\n2166.0\n3.958\n5.281\n5.501\n\n\n\n\n\n\n\n\nWET_CHEM_PATH2 = 'afsis/2009-2013/Wet_Chemistry/RRES/Wet_Chemistry_RRES.csv'\n\n#columns_to_load = elements + ['SSN']\n\n\nwet_chem_df1 = pd.read_csv(WET_CHEM_PATH2)#, usecols=columns_to_load)\nelements = ['SSN','pH', '%N', 'C % Org', 'ICP OES K mg/kg ', 'ICP OES P mg/kg ']\nwet_chem_df1 = wet_chem_df1[elements]\nwet_chem_df1 = wet_chem_df1.rename(columns={\"%N\": \"Leco_N_ppm\"})\nwet_chem_df1['Leco_N_ppm'] = wet_chem_df1['Leco_N_ppm']*10000\nprint(wet_chem_df1.columns)\nwet_chem_df1.head()\n\nIndex(['SSN', 'pH', 'Leco_N_ppm', 'C % Org', 'ICP OES K mg/kg ',\n       'ICP OES P mg/kg '],\n      dtype='object')\n\n\n\n\n\n\n\n\n\nSSN\npH\nLeco_N_ppm\nC % Org\nICP OES K mg/kg\nICP OES P mg/kg\n\n\n\n\n0\nicr006454\n7.85\n800.0\n0.94\n8517.919223\n96.575131\n\n\n1\nicr006455\n8.03\n600.0\n0.70\n10859.303780\n117.423139\n\n\n2\nicr006474\n5.01\n500.0\n0.57\n1343.124117\n87.040073\n\n\n3\nicr006475\n4.57\n500.0\n0.47\n1487.768795\n83.555482\n\n\n4\nicr006492\n6.78\n900.0\n0.98\n2999.240760\n150.936463\n\n\n\n\n\n\n\n\nWET_CHEM_PATH3 = 'afsis/2009-2013/Wet_Chemistry/ICRAF/Wet_Chemistry_ICRAF.csv'\n#elements = ['M3 Ca', 'M3 K', 'M3 Al']\n#columns_to_load = elements + ['SSN']\n\n\nwet_chem_df2 = pd.read_csv(WET_CHEM_PATH3)#, usecols=columns_to_load)\n#print(wet_chem_df2.columns)\nelements = ['SSN','Psa asand', 'Psa asilt','Psa aclay', 'Volfr', 'Awc1','Lshrinkpct', 'Acidified nitrogen',\n       'Acidified carbon']\nwet_chem_df2 = wet_chem_df2[elements]\nwet_chem_df2['Acidified nitrogen'] = wet_chem_df2['Acidified nitrogen']*10000\nwet_chem_df2 = wet_chem_df2.rename(columns={\"Acidified nitrogen\": \"Flash2000_N_ppm\"})\n\nprint(wet_chem_df2.shape)\nwet_chem_df2.head()\n\n(1907, 9)\n\n\n\n\n\n\n\n\n\nSSN\nPsa asand\nPsa asilt\nPsa aclay\nVolfr\nAwc1\nLshrinkpct\nFlash2000_N_ppm\nAcidified carbon\n\n\n\n\n0\nicr005928\n90.993000\n8.111667\n0.896333\n1.190000\n0.070768\n0.000000\n296.21345\n0.425854\n\n\n1\nicr005929\n87.847000\n11.416000\n0.737000\n1.192000\n0.061710\n5.000000\n230.68986\n0.263235\n\n\n2\nicr005946\n94.408333\n5.335333\n0.256000\n1.171280\n0.115414\n5.714286\n313.39549\n0.392983\n\n\n3\nicr005947\n94.601333\n5.239333\n0.159000\n1.198744\n0.122856\n0.000000\n170.62043\n0.233496\n\n\n4\nicr005965\n90.015333\n9.195667\n0.789000\n1.081575\n0.100874\n5.000000\n831.45402\n0.860801"
  },
  {
    "objectID": "posts/fertility_map_africa/afsis-soil-chem-EDA.html#join-and-clean-datasets",
    "href": "posts/fertility_map_africa/afsis-soil-chem-EDA.html#join-and-clean-datasets",
    "title": "Soil Chemistry Dataset",
    "section": "5. Join and clean datasets",
    "text": "5. Join and clean datasets\n\n5.1 Elemental analysis\n\ndf_elements1 = pd.merge(left=wet_chem_df1, right=df_xrf_reduced, left_on='SSN', right_on='SSN')\nprint(df_elements1.shape)\n\n\ndf_elements2 = pd.merge(left=wet_chem_df2, right=df_elements1, left_on='SSN', right_on='SSN')\nprint(df_elements2.shape)\n\n\ndf_elements = pd.merge(left=wet_chem_df, right=df_elements2, left_on='SSN', right_on='SSN')\nprint(df_elements.shape)\nprint(df_elements.columns)\n\ndf_elements.head()\n\n(467, 17)\n(467, 25)\n(467, 31)\nIndex(['SSN', 'M3 Ca', 'M3 K', 'M3 Al', 'M3 P', 'M3 S', 'PH', 'Psa asand',\n       'Psa asilt', 'Psa aclay', 'Volfr', 'Awc1', 'Lshrinkpct',\n       'Flash2000_N_ppm', 'Acidified carbon', 'pH', 'Leco_N_ppm', 'C % Org',\n       'ICP OES K mg/kg ', 'ICP OES P mg/kg ', 'P', 'K', 'S', 'Ca', 'Mg', 'Cu',\n       'Cl', 'Zn', 'Fe', 'Mn', 'Mo'],\n      dtype='object')\n\n\n\n\n\n\n\n\n\nSSN\nM3 Ca\nM3 K\nM3 Al\nM3 P\nM3 S\nPH\nPsa asand\nPsa asilt\nPsa aclay\n...\nK\nS\nCa\nMg\nCu\nCl\nZn\nFe\nMn\nMo\n\n\n\n\n0\nicr006475\n207.1\n306.30\n1095.000\n4.495\n18.960\n4.682\n97.848667\n1.845333\n0.306000\n...\n12991.3\n45.7\n944.1\n5575.0\n13.0\n210.2\n22.0\n12501.3\n81.9\n184.5\n\n\n1\nicr006586\n1665.0\n1186.00\n1165.000\n12.510\n13.600\n7.062\n89.520000\n9.553667\n0.926333\n...\n15173.5\n45.7\n9301.0\n5519.1\n18.4\n152.6\n38.5\n24094.6\n422.4\n184.5\n\n\n2\nicr021104\n258.7\n35.25\n441.400\n4.424\n3.608\n5.522\n89.950000\n5.205000\n4.845000\n...\n6838.9\n45.7\n884.3\n5575.0\n2.8\n229.5\n2.3\n2213.4\n13.2\n184.5\n\n\n3\nicr033622\n11858.3\n1156.00\n108.286\n31.233\n25.460\n8.583\n91.445000\n6.310000\n2.245000\n...\n15845.8\n45.7\n46529.1\n33771.2\n13.0\n58.2\n29.8\n24135.0\n460.4\n184.5\n\n\n4\nicr006570\n896.2\n607.30\n1151.000\n5.986\n20.080\n6.661\n97.789667\n1.885000\n0.325667\n...\n12201.6\n45.7\n1790.1\n5575.0\n7.7\n122.5\n13.4\n9135.1\n132.6\n184.5\n\n\n\n\n5 rows × 31 columns\n\n\n\n\n#check for possible negative values\nfor col in df_elements.columns.tolist()[1:]:\n    if df_elements[col].dtype == np.float64:\n         df_elements[col][df_elements[col] &lt; 0] =np.nan\n\nprint(df_elements.isna().sum().sum())\n\n54\n\n\nSettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_elements[col][df_elements[col] &lt; 0] =np.nan\n\n\n\n\nmerge geographical and chemical data\n\ndf_geoelements = pd.merge(left=df_elements, right=df_geo, left_on='SSN', right_on='SSN')\n\nprint(df_geoelements.shape)\nprint(df_geoelements.columns)\ndf_geoelements.head()\n\n(467, 37)\nIndex(['SSN', 'M3 Ca', 'M3 K', 'M3 Al', 'M3 P', 'M3 S', 'PH', 'Psa asand',\n       'Psa asilt', 'Psa aclay', 'Volfr', 'Awc1', 'Lshrinkpct',\n       'Flash2000_N_ppm', 'Acidified carbon', 'pH', 'Leco_N_ppm', 'C % Org',\n       'ICP OES K mg/kg ', 'ICP OES P mg/kg ', 'P', 'K', 'S', 'Ca', 'Mg', 'Cu',\n       'Cl', 'Zn', 'Fe', 'Mn', 'Mo', 'Latitude', 'Longitude', 'Cluster',\n       'Depth', 'Country', 'Cultivated'],\n      dtype='object')\n\n\n\n\n\n\n\n\n\nSSN\nM3 Ca\nM3 K\nM3 Al\nM3 P\nM3 S\nPH\nPsa asand\nPsa asilt\nPsa aclay\n...\nZn\nFe\nMn\nMo\nLatitude\nLongitude\nCluster\nDepth\nCountry\nCultivated\n\n\n\n\n0\nicr006475\n207.1\n306.30\n1095.000\n4.495\n18.960\n4.682\n97.848667\n1.845333\n0.306000\n...\n22.0\n12501.3\n81.9\n184.5\n-6.088750\n36.435982\n2\nsub\nTanzania\nFalse\n\n\n1\nicr006586\n1665.0\n1186.00\n1165.000\n12.510\n13.600\n7.062\n89.520000\n9.553667\n0.926333\n...\n38.5\n24094.6\n422.4\n184.5\n-6.055750\n36.457722\n8\ntop\nTanzania\nFalse\n\n\n2\nicr021104\n258.7\n35.25\n441.400\n4.424\n3.608\n5.522\n89.950000\n5.205000\n4.845000\n...\n2.3\n2213.4\n13.2\n184.5\n-8.049305\n37.333698\n14\nsub\nTanzania\nFalse\n\n\n3\nicr033622\n11858.3\n1156.00\n108.286\n31.233\n25.460\n8.583\n91.445000\n6.310000\n2.245000\n...\n29.8\n24135.0\n460.4\n184.5\n4.178087\n38.261890\n2\nsub\nEthiopia\nNaN\n\n\n4\nicr006570\n896.2\n607.30\n1151.000\n5.986\n20.080\n6.661\n97.789667\n1.885000\n0.325667\n...\n13.4\n9135.1\n132.6\n184.5\n-6.069970\n36.464588\n7\ntop\nTanzania\nFalse\n\n\n\n\n5 rows × 37 columns\n\n\n\n\n\ngeographical distribution of the selected samples\n\npd.value_counts(df_geoelements['Country']).plot.bar(title='Measurements per country')\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd0c8193b20&gt;\n\n\n\n\n\n\n #Draw map\n\n\nm = folium.Map(location=[-3.5, 35.6], tiles=\"stamentoner\", zoom_start=5)\n \nfor _, row in df_geoelements.iterrows():\n    if row[['Latitude', 'Longitude']].notnull().all():\n        folium.Marker([row['Latitude'], \n                       row['Longitude']], \n                      popup=row['SSN']\n                     ).add_to(m)\n\n#m\n\nImage(filename='img/folium.png') \n\n\n\n\n\n\nimputation of missing values\n\ndef replace_missings(data):\n    # this replaces missings with medians\n    # NOTE: mixed string num columns it does not do anything with\n    for cols in data._get_numeric_data().columns:\n        data[cols].fillna(value=data[cols].median(), inplace=True)\n\n\nreplace_missings(df_geoelements)\n\nprint(df_geoelements['Cultivated'].unique())\ndf_geoelements['Cultivated'] = df_geoelements['Cultivated'].fillna('unknown')\nprint(df_geoelements['Cultivated'].unique())\nprint(df_geoelements.isna().sum().sum())\n\n[False 'unknown' True]\n[False 'unknown' True]\n0\n\n\n\n\noutliers\n\ndef detect_outliers(df, n, features):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than n outliers according\n    to the Tukey method.\n    \"\"\"\n    outlier_indices = []\n\n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col], 75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n\n        # outlier step\n        outlier_step = 3 * IQR\n\n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] &lt; Q1 - outlier_step) | (df[col] &gt; Q3 + outlier_step)].index\n\n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n\n    # select observations containing more than 1 outlier\n    outlier_indices = Counter(outlier_indices)        \n  \n\n    return outlier_indices\n\n# detect outliers from list of features\nlof = ['M3 Ca', 'M3 K', 'M3 Al', 'M3 P', 'M3 S', 'PH', 'Psa asand',\n       'Psa asilt', 'Psa aclay', 'Volfr', 'Awc1', 'Lshrinkpct', 'pH', 'Flash2000_N_ppm','Leco_N_ppm',\n       'C % Org', 'ICP OES K mg/kg ', 'ICP OES P mg/kg ', 'P', 'K', 'S', 'Ca',\n       'Mg', 'Cu', 'Cl', 'Zn', 'Fe', 'Mn', 'Mo']#, \nOutliers_to_drop = detect_outliers(df_geoelements, 1, lof)\n\nprint(len(Outliers_to_drop), 'outliers according to Tukey method')\nif len(Outliers_to_drop)&gt;50:\n    print('loss of information would be too high if Tukey method would be applied')\n\n209 outliers according to Tukey method\nloss of information would be too high if Tukey method would be applied\n\n\n\ndf_chem = df_geoelements[lof]\ndf_chem_scaled =((df_chem -df_chem.min())/(df_chem.max()-df_chem.min()))*10\n\n%matplotlib inline\nplt.figure(figsize= (22,6))\nbox_plot_scaled = sns.boxplot( data= df_chem_scaled)\nfig = box_plot_scaled.get_figure()\nplt.xticks(rotation=90)\nplt.ylabel(\"Scaled values\")\nfig.savefig(\"box.png\", dpi= 100)\n\n\n\n\n\ndf_chem.describe()\n\n\n\n\n\n\n\n\nM3 Ca\nM3 K\nM3 Al\nM3 P\nM3 S\nPH\nPsa asand\nPsa asilt\nPsa aclay\nVolfr\n...\nK\nS\nCa\nMg\nCu\nCl\nZn\nFe\nMn\nMo\n\n\n\n\ncount\n467.000000\n467.000000\n467.000000\n467.000000\n467.000000\n467.000000\n467.000000\n467.000000\n467.000000\n467.000000\n...\n467.000000\n467.000000\n467.000000\n467.000000\n467.000000\n467.000000\n467.000000\n467.000000\n467.000000\n467.000000\n\n\nmean\n1605.142957\n186.569448\n802.767263\n10.296321\n23.906516\n6.133593\n84.902406\n8.056587\n7.041254\n1.050843\n...\n11204.939829\n77.834261\n5092.092505\n7004.122270\n12.137473\n190.165096\n22.208351\n22659.693362\n316.650107\n187.781370\n\n\nstd\n2780.256854\n303.097846\n423.564349\n22.179751\n154.640351\n1.151291\n14.436237\n5.750215\n9.775127\n0.161490\n...\n13913.144061\n467.413144\n11339.284556\n5675.704395\n13.737260\n1025.070888\n25.877222\n26454.053657\n423.535962\n48.287469\n\n\nmin\n0.001000\n5.110000\n14.300000\n0.001000\n1.490000\n4.000000\n0.440000\n0.000000\n0.000000\n0.644500\n...\n90.400000\n43.400000\n69.700000\n4032.600000\n0.700000\n19.700000\n0.900000\n729.900000\n7.900000\n184.500000\n\n\n25%\n236.800000\n47.200000\n446.543000\n2.495000\n4.732000\n5.315000\n83.500000\n4.352500\n2.375000\n0.943500\n...\n1917.350000\n45.700000\n344.750000\n5575.000000\n3.900000\n52.550000\n6.600000\n6575.600000\n67.650000\n184.500000\n\n\n50%\n560.700000\n82.200000\n735.486000\n4.495000\n7.530000\n5.950000\n88.525000\n6.710000\n4.595000\n1.050500\n...\n5963.000000\n45.700000\n991.700000\n5575.000000\n8.000000\n84.400000\n15.200000\n15149.100000\n153.900000\n184.500000\n\n\n75%\n1375.400000\n181.900000\n1130.000000\n8.590500\n12.300000\n6.603000\n92.545000\n9.607500\n7.640000\n1.180000\n...\n15435.750000\n45.700000\n3597.000000\n5575.000000\n15.100000\n136.850000\n28.550000\n28039.050000\n377.800000\n184.500000\n\n\nmax\n18510.000000\n3432.000000\n2444.000000\n221.800000\n2728.860000\n9.860000\n100.005000\n35.555000\n81.400000\n1.429500\n...\n106273.400000\n9704.000000\n89497.800000\n51204.800000\n110.300000\n21557.400000\n259.900000\n222256.400000\n3314.100000\n1085.400000\n\n\n\n\n8 rows × 29 columns\n\n\n\n\n# export to csv\ndf_geoelements.to_csv( 'elemental_analysis_dataset.csv',index=False)\n\n\n\n5.2 FTIR\n\ndf_FTIR_reindexed = df_KBR_FTIRspectra.set_index('labda')\nmid_infrared_df = df_FTIR_reindexed.T.reset_index()\nmid_infrared_df = mid_infrared_df.rename(columns={'index': 'SSN'})\n\n\n\nI select only the sample that are in the compositional dataframe “geoelements”\n\n\nThe composition and FTIR dataframes have same sample numbers (SSN) column\n\n\nEach infrared spectrum corresponds to an elemental analysis\n\ncomplete_measurements_list = df_geoelements_reduced.SSN.tolist()\n\n\nmid infrared\n\n\ndf_FTIRdata = mid_infrared_df[mid_infrared_df.SSN.isin(complete_measurements_list)]\nprint(df_FTIRdata.shape)\n#print(df_FTIRdata.isna().sum())\n\n(467, 2543)\n\n\n\ndf_FTIRdata.to_csv( 'middle_infrared_spectra_dataset.csv',index=False)\n\n\nnear infrared\n\n\ndf_FTIR_reindexed1 = df_NIR_FTIRspectra.set_index('labda')\nnear_infrared_df = df_FTIR_reindexed1.T.reset_index()\nnear_infrared_df = near_infrared_df.rename(columns={'index': 'SSN'})\n# I select only the sample that are in the compositional dataframe \"geoelements\"\n\ncomplete_measurements_list = df_geoelements_reduced.SSN.tolist()\n\ndf_FTIRdataM = near_infrared_df[near_infrared_df.SSN.isin(complete_measurements_list)]\nprint(df_FTIRdata.shape)\n#print(df_FTIRdata.isna().sum())\ndf_FTIRdataM.to_csv( 'near_infrared_spectra_dataset.csv',index=False)\n\n(467, 2543)"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Recent posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nknn\n\n\nEfficiency of knn with sample data from scikit-learn\n\n\n\nMay 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlogistic_regression\n\n\nEfficiency of logistic regression with sample data from scikit-learn\n\n\n\nMay 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBig Data Analysis\n\n\nData analysis with Pyspark\n\n\n\nMar 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData analysis for reporting\n\n\n\nMar 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nData analytics workflow\n\n\nData analysis example workbook\n\n\n\nMar 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGroup operations\n\n\nData aggregation, grouping, and pivoting\n\n\n\nMar 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting and Visualization\n\n\nUsing data visualization libraries\n\n\n\nMar 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series\n\n\nA brief overview of Time series\n\n\n\nMar 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModeling libries in python\n\n\nA brief introduction to modeling libraries in Python\n\n\n\nMar 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPandas_2\n\n\nData analysis with Pandas\n\n\n\nFeb 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Structures\n\n\nPython basics\n\n\n\nFeb 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Cleaning\n\n\nPython basics\n\n\n\nFeb 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Wrangling\n\n\nPython basics\n\n\n\nFeb 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumpy_2\n\n\nData analysis with Numpy\n\n\n\nFeb 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFiles (reading and writing)\n\n\nWorking with files\n\n\n\nFeb 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunctions\n\n\nPython basics\n\n\n\nFeb 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Structures\n\n\nPython basics\n\n\n\nFeb 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\nSample notebook to handle any data for EDA\n\n\n\nJan 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning\n\n\n1D and 2D partial dependent plots with RandomForestClassifier and DecisionTreeClassifier\n\n\n\nJan 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeoPandas\n\n\nData analysis with GeoPandas\n\n\n\nJan 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPandas\n\n\nData analysis with Pandas\n\n\n\nDec 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMachine learning\n\n\nbasics\n\n\n\nDec 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPython basics\n\n\n\nDec 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython Coding Rules\n\n\nPython basics\n\n\n\nDec 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with APIs\n\n\nPython basics\n\n\n\nOct 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nDownloading data\n\n\nPython basics\n\n\n\nOct 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nGenerating data (data visualization, representations, etc.)\n\n\nPython basics\n\n\n\nOct 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLists, for loops\n\n\nPython basics\n\n\n\nOct 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIF statements\n\n\nPython basics\n\n\n\nOct 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDictionaries\n\n\nPython basics\n\n\n\nOct 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUser input and while loops\n\n\nPython basics\n\n\n\nOct 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunctions\n\n\nPython basics\n\n\n\nOct 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClasses\n\n\nPython basics\n\n\n\nOct 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFiles and exceptions\n\n\nPython basics\n\n\n\nOct 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTools -ML sample project\n\n\nData Analysis\n\n\n\nJun 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizations with matplotlib, seaborn, ploty, altair\n\n\nVisualizatons\n\n\n\nJun 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSoil Chemistry Dataset\n\n\nExploratory analysis\n\n\n\nJun 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTools - Numpy\n\n\nData Analysis\n\n\n\nJun 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTools - matplotlib\n\n\nData Analysis\n\n\n\nJun 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTools - pandas\n\n\nData Analysis\n\n\n\nJun 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto Computations\n\n\n\nMay 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto Basics\n\n\n\nMay 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTitanic dataset analysis using Pandas and Numpy\n\n\nData Visualization\n\n\n\nMay 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow helpful can AI be in solving the water crisis?\n\n\n\nMay 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCan bamboo help to solve climate crisis?\n\n\n\nMay 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCan AI decide when to water crops?\n\n\n\nMay 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMachine learning in Agriculture\n\n\n\nApr 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIrrigation scheduling with Machine learning\n\n\n\nApr 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDigitaliztion of Agriculture\n\n\n\nApr 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeopandas2\n\n\nData analysis with GeoPandas\n\n\n\nJan 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProximity Analysis\n\n\nData analysis with GeoPandas\n\n\n\nJan 7, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Machine learning in Agriculture/index.html",
    "href": "posts/Machine learning in Agriculture/index.html",
    "title": "Machine learning in Agriculture",
    "section": "",
    "text": "The article “Machine Learning in Agriculture: A Review” provides a comprehensive overview of the current state and potential applications of machine learning in agriculture.\nThe authors first introduce the concept of machine learning and its various techniques, including supervised, unsupervised, and reinforcement learning. They then provide an overview of the different areas in agriculture where machine learning can be applied, such as crop yield prediction, plant disease diagnosis, soil analysis, and livestock management.\nThe article further explores the use of different types of sensors and data collection techniques that can be used to gather data for machine learning applications in agriculture. The authors also discuss the challenges associated with implementing machine learning in agriculture, such as data quality, lack of standardization, and limited computational resources.\nThe authors present several case studies and examples of successful machine learning applications in agriculture, including precision farming, crop monitoring, and yield prediction. The article concludes with a discussion on the future directions and potential impact of machine learning in agriculture, including the development of new tools and technologies to support sustainable agriculture practices and enhance food security."
  },
  {
    "objectID": "posts/ML/EDA_Classification.html",
    "href": "posts/ML/EDA_Classification.html",
    "title": "Data Analysis",
    "section": "",
    "text": "In this exercies, we’ll analyse the bank dataset for exploratory and classification techniques."
  },
  {
    "objectID": "posts/ML/EDA_Classification.html#loading-libraries",
    "href": "posts/ML/EDA_Classification.html#loading-libraries",
    "title": "Data Analysis",
    "section": "2.1 Loading Libraries",
    "text": "2.1 Loading Libraries\n\n# import libraries\n\nimport warnings as wrn\nwrn.filterwarnings('ignore', category = DeprecationWarning) \nwrn.filterwarnings('ignore', category = FutureWarning) \nwrn.filterwarnings('ignore', category = UserWarning) \n\n#import optuna\n#import xgboost as xgb\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as stats\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import accuracy_score, classification_report, mean_absolute_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import LinearSVC\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import make_scorer, accuracy_score, median_absolute_error\n#from imblearn.over_sampling import RandomOverSampler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error, r2_score\n#import lightgbm as lgb\nimport numpy as np\nfrom scipy import stats\n\n\n# reading .csv files\n\ntrain_data = pd.read_csv('train_bank.csv')\ntest_data = pd.read_csv('test.csv')\norignal_data = pd.read_csv('Churn_Modelling.csv')"
  },
  {
    "objectID": "posts/ML/EDA_Classification.html#initial-observations-and-trends",
    "href": "posts/ML/EDA_Classification.html#initial-observations-and-trends",
    "title": "Data Analysis",
    "section": "2.2 Initial Observations and Trends",
    "text": "2.2 Initial Observations and Trends\n\ntrain_data.head()\n\n\n\n\n\n\n\n\nid\nCustomerId\nSurname\nCreditScore\nGeography\nGender\nAge\nTenure\nBalance\nNumOfProducts\nHasCrCard\nIsActiveMember\nEstimatedSalary\nExited\n\n\n\n\n0\n0\n15674932\nOkwudilichukwu\n668\nFrance\nMale\n33.0\n3\n0.00\n2\n1.0\n0.0\n181449.97\n0\n\n\n1\n1\n15749177\nOkwudiliolisa\n627\nFrance\nMale\n33.0\n1\n0.00\n2\n1.0\n1.0\n49503.50\n0\n\n\n2\n2\n15694510\nHsueh\n678\nFrance\nMale\n40.0\n10\n0.00\n2\n1.0\n0.0\n184866.69\n0\n\n\n3\n3\n15741417\nKao\n581\nFrance\nMale\n34.0\n2\n148882.54\n1\n1.0\n1.0\n84560.88\n0\n\n\n4\n4\n15766172\nChiemenam\n716\nSpain\nMale\n33.0\n5\n0.00\n2\n1.0\n1.0\n15068.83\n0\n\n\n\n\n\n\n\n\ntest_data.head()\n\n\n\n\n\n\n\n\nid\nCustomerId\nSurname\nCreditScore\nGeography\nGender\nAge\nTenure\nBalance\nNumOfProducts\nHasCrCard\nIsActiveMember\nEstimatedSalary\n\n\n\n\n0\n165034\n15773898\nLucchese\n586\nFrance\nFemale\n23.0\n2\n0.00\n2\n0.0\n1.0\n160976.75\n\n\n1\n165035\n15782418\nNott\n683\nFrance\nFemale\n46.0\n2\n0.00\n1\n1.0\n0.0\n72549.27\n\n\n2\n165036\n15807120\nK?\n656\nFrance\nFemale\n34.0\n7\n0.00\n2\n1.0\n0.0\n138882.09\n\n\n3\n165037\n15808905\nO'Donnell\n681\nFrance\nMale\n36.0\n8\n0.00\n1\n1.0\n0.0\n113931.57\n\n\n4\n165038\n15607314\nHiggins\n752\nGermany\nMale\n38.0\n10\n121263.62\n1\n1.0\n0.0\n139431.00\n\n\n\n\n\n\n\n\norignal_data.head()\n\n\n\n\n\n\n\n\nRowNumber\nCustomerId\nSurname\nCreditScore\nGeography\nGender\nAge\nTenure\nBalance\nNumOfProducts\nHasCrCard\nIsActiveMember\nEstimatedSalary\nExited\n\n\n\n\n0\n1\n15634602\nHargrave\n619\nFrance\nFemale\n42.0\n2\n0.00\n1\n1.0\n1.0\n101348.88\n1\n\n\n1\n2\n15647311\nHill\n608\nSpain\nFemale\n41.0\n1\n83807.86\n1\n0.0\n1.0\n112542.58\n0\n\n\n2\n3\n15619304\nOnio\n502\nFrance\nFemale\n42.0\n8\n159660.80\n3\n1.0\n0.0\n113931.57\n1\n\n\n3\n4\n15701354\nBoni\n699\nFrance\nFemale\n39.0\n1\n0.00\n2\n0.0\n0.0\n93826.63\n0\n\n\n4\n5\n15737888\nMitchell\n850\nSpain\nFemale\n43.0\n2\n125510.82\n1\nNaN\n1.0\n79084.10\n0\n\n\n\n\n\n\n\n\n# checking the number of rows and columns\n\nnum_train_rows, num_train_columns = train_data.shape\n\nnum_test_rows, num_test_columns = test_data.shape\n\nnum_orignal_rows, num_orignal_columns = orignal_data.shape\n\nprint('Training Data: ')\nprint(f\"Number of Rows: {num_train_rows}\")\nprint(f\"Number of Columns: {num_train_columns}\\n\")\n\nprint('Test Data: ')\nprint(f\"Number of Rows: {num_test_rows}\")\nprint(f\"Number of Columns :{num_test_columns}\\n\")\n\nprint(\"Orignal Data: \")\nprint(f\"Number of Rows: {num_orignal_rows}\")\nprint(f\"Number of Columns: {num_orignal_columns}\")\n\n\nTraining Data: \nNumber of Rows: 165034\nNumber of Columns: 14\n\nTest Data: \nNumber of Rows: 110023\nNumber of Columns :13\n\nOrignal Data: \nNumber of Rows: 10002\nNumber of Columns: 14\n\n\n\n# create a table for missing values, unique values, and data types\n\nmissing_values_train = pd.DataFrame({\n    'Feature': train_data.columns,\n    '[TRAIN] No. of Missing Values' : train_data.isnull().sum().values,\n    '[TRAIN] % of Missing Values' : ((train_data.isnull().sum().values)/len(train_data)*100)\n})\n\nmissing_values_test = pd.DataFrame({\n    'Feature' : test_data.columns,\n    '[TEST] No. of Missing Values' : test_data.isnull().sum().values,\n    '[TEST]% of Missing Values': ((test_data.isnull().sum().values)/len(test_data)*100)\n})\n                                    \nmissing_values_orignal = pd.DataFrame({\n    'Feature' : orignal_data.columns,\n    '[ORIGNAL] No. of Missing Values': orignal_data.isnull().sum().values,\n    '[ORIGNAL] % of Missing Values' : ((orignal_data.isnull().sum().values)/len(orignal_data)*100)\n})\n\n\nunique_values = pd.DataFrame({\n    'Feature': train_data.columns,\n    'No. of Unique Values [FROM TRAIN]' :train_data.nunique().values\n})\n\nfeature_types = pd.DataFrame({\n    'Feature': train_data.columns,\n    'DataType': train_data.dtypes\n})\n\n\nmerged_df = pd.merge(missing_values_train, missing_values_test, on= 'Feature', how= 'left')\nmerged_df = pd.merge(merged_df, missing_values_orignal, on = 'Feature', how = 'left')\nmerged_df = pd.merge(merged_df, unique_values, on= 'Feature', how= 'left')\nmerged_df = pd.merge(merged_df, feature_types, on = 'Feature', how= 'left')\n\nmerged_df\n\n\n\n\n\n\n\n\nFeature\n[TRAIN] No. of Missing Values\n[TRAIN] % of Missing Values\n[TEST] No. of Missing Values\n[TEST]% of Missing Values\n[ORIGNAL] No. of Missing Values\n[ORIGNAL] % of Missing Values\nNo. of Unique Values [FROM TRAIN]\nDataType\n\n\n\n\n0\nid\n0\n0.0\n0.0\n0.0\nNaN\nNaN\n165034\nint64\n\n\n1\nCustomerId\n0\n0.0\n0.0\n0.0\n0.0\n0.000000\n23221\nint64\n\n\n2\nSurname\n0\n0.0\n0.0\n0.0\n0.0\n0.000000\n2797\nobject\n\n\n3\nCreditScore\n0\n0.0\n0.0\n0.0\n0.0\n0.000000\n457\nint64\n\n\n4\nGeography\n0\n0.0\n0.0\n0.0\n1.0\n0.009998\n3\nobject\n\n\n5\nGender\n0\n0.0\n0.0\n0.0\n0.0\n0.000000\n2\nobject\n\n\n6\nAge\n0\n0.0\n0.0\n0.0\n1.0\n0.009998\n71\nfloat64\n\n\n7\nTenure\n0\n0.0\n0.0\n0.0\n0.0\n0.000000\n11\nint64\n\n\n8\nBalance\n0\n0.0\n0.0\n0.0\n0.0\n0.000000\n30075\nfloat64\n\n\n9\nNumOfProducts\n0\n0.0\n0.0\n0.0\n0.0\n0.000000\n4\nint64\n\n\n10\nHasCrCard\n0\n0.0\n0.0\n0.0\n1.0\n0.009998\n2\nfloat64\n\n\n11\nIsActiveMember\n0\n0.0\n0.0\n0.0\n1.0\n0.009998\n2\nfloat64\n\n\n12\nEstimatedSalary\n0\n0.0\n0.0\n0.0\n0.0\n0.000000\n55298\nfloat64\n\n\n13\nExited\n0\n0.0\nNaN\nNaN\n0.0\n0.000000\n2\nint64\n\n\n\n\n\n\n\n\n# count duplicate rows in train_data\ntrain_duplicates = train_data.duplicated().sum()\n\n# count duplicate rows in test_data\ntest_duplicates = test_data.duplicated().sum()\n\n# count duplicate rows in orignal_data\norignal_duplicates = orignal_data.duplicated().sum()\n\n# print results\nprint(f\"Number of duplicate rows in train_data: {train_duplicates}\")\nprint(f\"Number of duplicate rows in test_data: {test_duplicates}\")\nprint(f\"Number of duplicate rows in orignal_data: {orignal_duplicates}\")\n\nNumber of duplicate rows in train_data: 0\nNumber of duplicate rows in test_data: 0\nNumber of duplicate rows in orignal_data: 2\n\n\n\n# description of all numerical columns in the dataset\ntrain_data.describe().T\n#test_data.describe().T\n#orignal_data.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nid\n165034.0\n8.251650e+04\n47641.356500\n0.00\n41258.25\n82516.5\n1.237748e+05\n165033.00\n\n\nCustomerId\n165034.0\n1.569201e+07\n71397.816791\n15565701.00\n15633141.00\n15690169.0\n1.575682e+07\n15815690.00\n\n\nCreditScore\n165034.0\n6.564544e+02\n80.103340\n350.00\n597.00\n659.0\n7.100000e+02\n850.00\n\n\nAge\n165034.0\n3.812589e+01\n8.867205\n18.00\n32.00\n37.0\n4.200000e+01\n92.00\n\n\nTenure\n165034.0\n5.020353e+00\n2.806159\n0.00\n3.00\n5.0\n7.000000e+00\n10.00\n\n\nBalance\n165034.0\n5.547809e+04\n62817.663278\n0.00\n0.00\n0.0\n1.199395e+05\n250898.09\n\n\nNumOfProducts\n165034.0\n1.554455e+00\n0.547154\n1.00\n1.00\n2.0\n2.000000e+00\n4.00\n\n\nHasCrCard\n165034.0\n7.539537e-01\n0.430707\n0.00\n1.00\n1.0\n1.000000e+00\n1.00\n\n\nIsActiveMember\n165034.0\n4.977702e-01\n0.499997\n0.00\n0.00\n0.0\n1.000000e+00\n1.00\n\n\nEstimatedSalary\n165034.0\n1.125748e+05\n50292.865585\n11.58\n74637.57\n117948.0\n1.551525e+05\n199992.48\n\n\nExited\n165034.0\n2.115988e-01\n0.408443\n0.00\n0.00\n0.0\n0.000000e+00\n1.00"
  },
  {
    "objectID": "posts/ML/EDA_Classification.html#numerical-features",
    "href": "posts/ML/EDA_Classification.html#numerical-features",
    "title": "Data Analysis",
    "section": "3.1 Numerical Features",
    "text": "3.1 Numerical Features\n\n# Analysis\n\n# custom color pallete define\ncustom_palette = ['#3498db', '#e74c3c','#2ecc71']\n\n# add 'Dataset' column to distinguish between train and test data\ntrain_data['Dataset'] = 'Train'\ntest_data['Dataset'] = 'Test'\norignal_data['Dataset']= 'Orignal'\n\nvariables = [col for col in train_data.columns if col in numerical_variables]\n\n# function to create and display a row of plots for a single variable\ndef create_variable_plots(variable):\n    sns.set_style('whitegrid')\n    \n    fig, axes = plt.subplots(1,2, figsize= (12, 4))\n    \n    #Box plot\n    plt.subplot(1, 2, 1)\n    sns.boxplot(data = pd.concat([\n        train_data, test_data, orignal_data.dropna()\n    ]), \n    x= variable, y = \"Dataset\", palette= custom_palette)\n    plt.xlabel(variable)\n    plt.title(f\"Box Plot for {variable}\")\n    \n    # Seperate Histograms\n    plt.subplot(1,2,2)\n    sns.histplot(data = train_data, x = variable, color= custom_palette[0], kde= True, bins= 30, label= 'Train')\n    sns.histplot(data = test_data, x= variable, color= custom_palette[1], kde= True, bins= 30, label= 'Test')\n    sns.histplot(data = orignal_data.dropna(), x=variable, color=custom_palette[2], kde=True, bins=30, label=\"Original\")\n    plt.xlabel(variable)\n    plt.ylabel('Frequency')\n    plt.title(f'Histogram for {variable} [TRAIN, TEST and ORIGNAL]')\n    plt.legend()\n    \n    #adjust spacing between subplots\n    plt.tight_layout()\n    \n    # show the plots\n    plt.show()\n\n# perform univariate analysis for each variable\nfor variable in variables:\n    create_variable_plots(variable)\n    \n# drop the 'Dataset' column after analysis\ntrain_data.drop('Dataset', axis=1, inplace = True)\ntest_data.drop('Dataset', axis=1, inplace= True)\norignal_data.drop('Dataset', axis=1, inplace=True)"
  },
  {
    "objectID": "posts/ML/EDA_Classification.html#categorical-features",
    "href": "posts/ML/EDA_Classification.html#categorical-features",
    "title": "Data Analysis",
    "section": "3.2 Categorical features",
    "text": "3.2 Categorical features\n\n# Analysis of all CATEGORICAL features\n\n# Define a custom color palette for categorical features\ncategorical_palette = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6', '#bdc3c7', '#1abc9c', '#f1c40f', '#95a5a6', '#d35400']\n\n# List of categorical variables\ncategorical_variables = [col for col in categorical_variables]\n\n# Function to create and display a row of plots for a single categorical variable\ndef create_categorical_plots(variable):\n    sns.set_style('whitegrid')\n    \n    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n    # Pie Chart\n    plt.subplot(1, 2, 1)\n    train_data[variable].value_counts().plot.pie(autopct='%1.1f%%',\n                                                 colors=categorical_palette, \n                                                 wedgeprops=dict(width=0.3), \n                                                 startangle=140)\n    plt.title(f\"Pie Chart for {variable}\")\n\n    # Bar Graph\n    plt.subplot(1, 2, 2)\n    sns.countplot(data=pd.concat([\n        train_data, test_data, orignal_data.dropna()\n    ]), x=variable, palette=categorical_palette)\n    plt.xlabel(variable)\n    plt.ylabel(\"Count\")\n    plt.title(f\"Bar Graph for {variable} [TRAIN, TEST & ORIGINAL]\")\n\n    # Adjust spacing between subplots\n    plt.tight_layout()\n\n    # Show the plots\n    plt.show()\n\n# Perform univariate analysis for each categorical variable\nfor variable in categorical_variables:\n    create_categorical_plots(variable)"
  },
  {
    "objectID": "posts/ML/EDA_Classification.html#target-features",
    "href": "posts/ML/EDA_Classification.html#target-features",
    "title": "Data Analysis",
    "section": "3.3 Target features",
    "text": "3.3 Target features\n\n# Analysis of TARGET feature\n\n# Define a custom color palette for categorical features\ntarget_palette = ['#3498db', '#e74c3c']\n\nfig, axes = plt.subplots(1, 2, figsize = (12, 4))\n\n# Pie Chart\nplt.subplot(1,2,1)\ntrain_data[target_variable].value_counts().plot.pie(\n    autopct='%1.1f%%', colors= target_palette, \n    wedgeprops=dict(width=0.3), startangle=140\n)\nplt.title(f\"Pie Chart for Target Feature 'Exited'\")\n\n# Bar Graph\nplt.subplot(1,2,2)\nsns.countplot(data=pd.concat([\n    train_data, orignal_data.dropna()\n]),\n              x=target_variable, palette=target_palette)\nplt.xlabel(variable)\nplt.ylabel('Count')\nplt.title(f\"Bar Graph for Target Feature 'Exited'\")\n\n# adjust spacing\nplt.tight_layout()\n\n# show\nplt.show()"
  },
  {
    "objectID": "posts/ML/EDA_Classification.html#bivariate-analysis",
    "href": "posts/ML/EDA_Classification.html#bivariate-analysis",
    "title": "Data Analysis",
    "section": "3.4 Bivariate Analysis",
    "text": "3.4 Bivariate Analysis\n\nvariables = [col for col in train_data.columns if col in numerical_variables]\n\ncat_variables_train = ['NumOfProducts', 'HasCrCard', 'IsActiveMember', 'Tenure', 'Exited']\ncat_variables_test = ['NumOfProducts', 'HasCrCard', 'IsActiveMember', 'Tenure']\n\n# Adding variables to the existing list\ntrain_variables = variables + cat_variables_train\ntest_variables = variables + cat_variables_test\n\n# Calculate correlation matrices for train_data and test_data\ncorr_train = train_data[train_variables].corr()\ncorr_test = test_data[test_variables].corr()\n\n# Create masks for the upper triangle\nmask_train = np.triu(np.ones_like(corr_train, dtype=bool))\nmask_test = np.triu(np.ones_like(corr_test, dtype=bool))\n\n# Set the text size and rotation\nannot_kws = {\"size\": 8, \"rotation\": 45}\n\n# Generate heatmaps for train_data\nplt.figure(figsize=(15, 5))\nplt.subplot(1, 2, 1)\nax_train = sns.heatmap(corr_train, mask=mask_train, cmap='viridis', annot=True,\n                      square=True, linewidths=.5, xticklabels=1, yticklabels=1, annot_kws=annot_kws)\nplt.title('Correlation Heatmap - Train Data')\n\n# Generate heatmaps for test_data\nplt.subplot(1, 2, 2)\nax_test = sns.heatmap(corr_test, mask=mask_test, cmap='viridis', annot=True,\n                     square=True, linewidths=.5, xticklabels=1, yticklabels=1, annot_kws=annot_kws)\nplt.title('Correlation Heatmap - Test Data')\n\n# Adjust layout\nplt.tight_layout()\n\n# Show the plots\nplt.show()"
  },
  {
    "objectID": "posts/ML/ML_project.html",
    "href": "posts/ML/ML_project.html",
    "title": "Tools -ML sample project",
    "section": "",
    "text": "First, let’s import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20.\n\n# Python ≥3.5 is required\nimport sys\nassert sys.version_info &gt;= (3, 5)\n\n# Scikit-Learn ≥0.20 is required\nimport sklearn\nassert sklearn.__version__ &gt;= \"0.20\"\n\n# Common imports\nimport numpy as np\nimport os\n\n# To plot pretty figures\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n\n# Where to save the figures\nPROJECT_ROOT_DIR = \".\"\nCHAPTER_ID = \"end_to_end_project\"\nIMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\nos.makedirs(IMAGES_PATH, exist_ok=True)\n\ndef save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n    print(\"Saving figure\", fig_id)\n    if tight_layout:\n        plt.tight_layout()\n    plt.savefig(path, format=fig_extension, dpi=resolution)"
  },
  {
    "objectID": "posts/ML/ML_project.html#download-the-data",
    "href": "posts/ML/ML_project.html#download-the-data",
    "title": "Tools -ML sample project",
    "section": "Download the Data",
    "text": "Download the Data\n\nimport os\nimport tarfile\nimport urllib.request\n\nDOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\nHOUSING_PATH = os.path.join(\"datasets\", \"housing\")\nHOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n\ndef fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n    if not os.path.isdir(housing_path):\n        os.makedirs(housing_path)\n    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n    urllib.request.urlretrieve(housing_url, tgz_path)\n    housing_tgz = tarfile.open(tgz_path)\n    housing_tgz.extractall(path=housing_path)\n    housing_tgz.close()\n\n\nfetch_housing_data()\n\n\nimport pandas as pd\n\ndef load_housing_data(housing_path=HOUSING_PATH):\n    csv_path = os.path.join(housing_path, \"housing.csv\")\n    return pd.read_csv(csv_path)"
  },
  {
    "objectID": "posts/ML/ML_project.html#take-a-quick-look-at-the-data-structure",
    "href": "posts/ML/ML_project.html#take-a-quick-look-at-the-data-structure",
    "title": "Tools -ML sample project",
    "section": "Take a Quick Look at the Data Structure",
    "text": "Take a Quick Look at the Data Structure\n\nhousing = load_housing_data()\nhousing.head()\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\n\n\n\n\n0\n-122.23\n37.88\n41.0\n880.0\n129.0\n322.0\n126.0\n8.3252\n452600.0\nNEAR BAY\n\n\n1\n-122.22\n37.86\n21.0\n7099.0\n1106.0\n2401.0\n1138.0\n8.3014\n358500.0\nNEAR BAY\n\n\n2\n-122.24\n37.85\n52.0\n1467.0\n190.0\n496.0\n177.0\n7.2574\n352100.0\nNEAR BAY\n\n\n3\n-122.25\n37.85\n52.0\n1274.0\n235.0\n558.0\n219.0\n5.6431\n341300.0\nNEAR BAY\n\n\n4\n-122.25\n37.85\n52.0\n1627.0\n280.0\n565.0\n259.0\n3.8462\n342200.0\nNEAR BAY\n\n\n\n\n\n\n\n\nhousing.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 20640 entries, 0 to 20639\nData columns (total 10 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   longitude           20640 non-null  float64\n 1   latitude            20640 non-null  float64\n 2   housing_median_age  20640 non-null  float64\n 3   total_rooms         20640 non-null  float64\n 4   total_bedrooms      20433 non-null  float64\n 5   population          20640 non-null  float64\n 6   households          20640 non-null  float64\n 7   median_income       20640 non-null  float64\n 8   median_house_value  20640 non-null  float64\n 9   ocean_proximity     20640 non-null  object \ndtypes: float64(9), object(1)\nmemory usage: 1.6+ MB\n\n\n\nhousing[\"ocean_proximity\"].value_counts()\n\n&lt;1H OCEAN     9136\nINLAND        6551\nNEAR OCEAN    2658\nNEAR BAY      2290\nISLAND           5\nName: ocean_proximity, dtype: int64\n\n\n\nhousing.describe()\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\n\n\n\n\ncount\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20433.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n\n\nmean\n-119.569704\n35.631861\n28.639486\n2635.763081\n537.870553\n1425.476744\n499.539680\n3.870671\n206855.816909\n\n\nstd\n2.003532\n2.135952\n12.585558\n2181.615252\n421.385070\n1132.462122\n382.329753\n1.899822\n115395.615874\n\n\nmin\n-124.350000\n32.540000\n1.000000\n2.000000\n1.000000\n3.000000\n1.000000\n0.499900\n14999.000000\n\n\n25%\n-121.800000\n33.930000\n18.000000\n1447.750000\n296.000000\n787.000000\n280.000000\n2.563400\n119600.000000\n\n\n50%\n-118.490000\n34.260000\n29.000000\n2127.000000\n435.000000\n1166.000000\n409.000000\n3.534800\n179700.000000\n\n\n75%\n-118.010000\n37.710000\n37.000000\n3148.000000\n647.000000\n1725.000000\n605.000000\n4.743250\n264725.000000\n\n\nmax\n-114.310000\n41.950000\n52.000000\n39320.000000\n6445.000000\n35682.000000\n6082.000000\n15.000100\n500001.000000\n\n\n\n\n\n\n\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nhousing.hist(bins=50, figsize=(20,15))\nsave_fig(\"attribute_histogram_plots\")\nplt.show()\n\nSaving figure attribute_histogram_plots"
  },
  {
    "objectID": "posts/ML/ML_project.html#create-a-test-set",
    "href": "posts/ML/ML_project.html#create-a-test-set",
    "title": "Tools -ML sample project",
    "section": "Create a Test Set",
    "text": "Create a Test Set\n\n# to make this notebook's output identical at every run\nnp.random.seed(42)\n\n\nimport numpy as np\n\n# For illustration only. Sklearn has train_test_split()\ndef split_train_test(data, test_ratio):\n    shuffled_indices = np.random.permutation(len(data))\n    test_set_size = int(len(data) * test_ratio)\n    test_indices = shuffled_indices[:test_set_size]\n    train_indices = shuffled_indices[test_set_size:]\n    return data.iloc[train_indices], data.iloc[test_indices]\n\n\ntrain_set, test_set = split_train_test(housing, 0.2)\nlen(train_set)\n\n16512\n\n\n\nlen(test_set)\n\n4128\n\n\n\nfrom zlib import crc32\n\ndef test_set_check(identifier, test_ratio):\n    return crc32(np.int64(identifier)) & 0xffffffff &lt; test_ratio * 2**32\n\ndef split_train_test_by_id(data, test_ratio, id_column):\n    ids = data[id_column]\n    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n    return data.loc[~in_test_set], data.loc[in_test_set]\n\nThe implementation of test_set_check() above works fine in both Python 2 and Python 3. In earlier releases, the following implementation was proposed, which supported any hash function, but was much slower and did not support Python 2:\n\nimport hashlib\n\ndef test_set_check(identifier, test_ratio, hash=hashlib.md5):\n    return hash(np.int64(identifier)).digest()[-1] &lt; 256 * test_ratio\n\nIf you want an implementation that supports any hash function and is compatible with both Python 2 and Python 3, here is one:\n\ndef test_set_check(identifier, test_ratio, hash=hashlib.md5):\n    return bytearray(hash(np.int64(identifier)).digest())[-1] &lt; 256 * test_ratio\n\n\nhousing_with_id = housing.reset_index()   # adds an `index` column\ntrain_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")\n\n\nhousing_with_id[\"id\"] = housing[\"longitude\"] * 1000 + housing[\"latitude\"]\ntrain_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"id\")\n\n\ntest_set.head()\n\n\n\n\n\n\n\n\nindex\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\nid\n\n\n\n\n8\n8\n-122.26\n37.84\n42.0\n2555.0\n665.0\n1206.0\n595.0\n2.0804\n226700.0\nNEAR BAY\n-122222.16\n\n\n10\n10\n-122.26\n37.85\n52.0\n2202.0\n434.0\n910.0\n402.0\n3.2031\n281500.0\nNEAR BAY\n-122222.15\n\n\n11\n11\n-122.26\n37.85\n52.0\n3503.0\n752.0\n1504.0\n734.0\n3.2705\n241800.0\nNEAR BAY\n-122222.15\n\n\n12\n12\n-122.26\n37.85\n52.0\n2491.0\n474.0\n1098.0\n468.0\n3.0750\n213500.0\nNEAR BAY\n-122222.15\n\n\n13\n13\n-122.26\n37.84\n52.0\n696.0\n191.0\n345.0\n174.0\n2.6736\n191300.0\nNEAR BAY\n-122222.16\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n\n\ntest_set.head()\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\n\n\n\n\n20046\n-119.01\n36.06\n25.0\n1505.0\nNaN\n1392.0\n359.0\n1.6812\n47700.0\nINLAND\n\n\n3024\n-119.46\n35.14\n30.0\n2943.0\nNaN\n1565.0\n584.0\n2.5313\n45800.0\nINLAND\n\n\n15663\n-122.44\n37.80\n52.0\n3830.0\nNaN\n1310.0\n963.0\n3.4801\n500001.0\nNEAR BAY\n\n\n20484\n-118.72\n34.28\n17.0\n3051.0\nNaN\n1705.0\n495.0\n5.7376\n218600.0\n&lt;1H OCEAN\n\n\n9814\n-121.93\n36.62\n34.0\n2351.0\nNaN\n1063.0\n428.0\n3.7250\n278000.0\nNEAR OCEAN\n\n\n\n\n\n\n\n\nhousing[\"median_income\"].hist()\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7faaf0489250&gt;\n\n\n\n\n\n\nhousing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n                               labels=[1, 2, 3, 4, 5])\n\n\nhousing[\"income_cat\"].value_counts()\n\n3    7236\n2    6581\n4    3639\n5    2362\n1     822\nName: income_cat, dtype: int64\n\n\n\nhousing[\"income_cat\"].hist()\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7faaee3e31d0&gt;\n\n\n\n\n\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n    strat_train_set = housing.loc[train_index]\n    strat_test_set = housing.loc[test_index]\n\n\nstrat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)\n\n3    0.350533\n2    0.318798\n4    0.176357\n5    0.114583\n1    0.039729\nName: income_cat, dtype: float64\n\n\n\nhousing[\"income_cat\"].value_counts() / len(housing)\n\n3    0.350581\n2    0.318847\n4    0.176308\n5    0.114438\n1    0.039826\nName: income_cat, dtype: float64\n\n\n\ndef income_cat_proportions(data):\n    return data[\"income_cat\"].value_counts() / len(data)\n\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n\ncompare_props = pd.DataFrame({\n    \"Overall\": income_cat_proportions(housing),\n    \"Stratified\": income_cat_proportions(strat_test_set),\n    \"Random\": income_cat_proportions(test_set),\n}).sort_index()\ncompare_props[\"Rand. %error\"] = 100 * compare_props[\"Random\"] / compare_props[\"Overall\"] - 100\ncompare_props[\"Strat. %error\"] = 100 * compare_props[\"Stratified\"] / compare_props[\"Overall\"] - 100\n\n\ncompare_props\n\n\n\n\n\n\n\n\nOverall\nStratified\nRandom\nRand. %error\nStrat. %error\n\n\n\n\n1\n0.039826\n0.039729\n0.040213\n0.973236\n-0.243309\n\n\n2\n0.318847\n0.318798\n0.324370\n1.732260\n-0.015195\n\n\n3\n0.350581\n0.350533\n0.358527\n2.266446\n-0.013820\n\n\n4\n0.176308\n0.176357\n0.167393\n-5.056334\n0.027480\n\n\n5\n0.114438\n0.114583\n0.109496\n-4.318374\n0.127011\n\n\n\n\n\n\n\n\nfor set_ in (strat_train_set, strat_test_set):\n    set_.drop(\"income_cat\", axis=1, inplace=True)"
  },
  {
    "objectID": "posts/ML/ML_project.html#visualizing-geographical-data",
    "href": "posts/ML/ML_project.html#visualizing-geographical-data",
    "title": "Tools -ML sample project",
    "section": "Visualizing Geographical Data",
    "text": "Visualizing Geographical Data\n\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")\nsave_fig(\"bad_visualization_plot\")\n\nSaving figure bad_visualization_plot\n\n\n\n\n\n\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)\nsave_fig(\"better_visualization_plot\")\n\nSaving figure better_visualization_plot\n\n\n\n\n\nThe argument sharex=False fixes a display bug (the x-axis values and legend were not displayed). This is a temporary fix (see: https://github.com/pandas-dev/pandas/issues/10611 ). Thanks to Wilmer Arellano for pointing it out.\n\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n             s=housing[\"population\"]/100, label=\"population\", figsize=(10,7),\n             c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n             sharex=False)\nplt.legend()\nsave_fig(\"housing_prices_scatterplot\")\n\nSaving figure housing_prices_scatterplot\n\n\n\n\n\n\n# Download the California image\nimages_path = os.path.join(PROJECT_ROOT_DIR, \"images\", \"end_to_end_project\")\nos.makedirs(images_path, exist_ok=True)\nDOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\nfilename = \"california.png\"\nprint(\"Downloading\", filename)\nurl = DOWNLOAD_ROOT + \"images/end_to_end_project/\" + filename\nurllib.request.urlretrieve(url, os.path.join(images_path, filename))\n\nDownloading california.png\n\n\n('./images/end_to_end_project/california.png',\n &lt;http.client.HTTPMessage at 0x7fd1784e5050&gt;)\n\n\n\nimport matplotlib.image as mpimg\ncalifornia_img=mpimg.imread(os.path.join(images_path, filename))\nax = housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", figsize=(10,7),\n                  s=housing['population']/100, label=\"Population\",\n                  c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"),\n                  colorbar=False, alpha=0.4)\nplt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5,\n           cmap=plt.get_cmap(\"jet\"))\nplt.ylabel(\"Latitude\", fontsize=14)\nplt.xlabel(\"Longitude\", fontsize=14)\n\nprices = housing[\"median_house_value\"]\ntick_values = np.linspace(prices.min(), prices.max(), 11)\ncbar = plt.colorbar(ticks=tick_values/prices.max())\ncbar.ax.set_yticklabels([\"$%dk\"%(round(v/1000)) for v in tick_values], fontsize=14)\ncbar.set_label('Median House Value', fontsize=16)\n\nplt.legend(fontsize=16)\nsave_fig(\"california_housing_prices_plot\")\nplt.show()\n\nSaving figure california_housing_prices_plot"
  },
  {
    "objectID": "posts/ML/ML_project.html#looking-for-correlations",
    "href": "posts/ML/ML_project.html#looking-for-correlations",
    "title": "Tools -ML sample project",
    "section": "Looking for Correlations",
    "text": "Looking for Correlations\n\ncorr_matrix = housing.corr()\n\n\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)\n\nmedian_house_value    1.000000\nmedian_income         0.687160\ntotal_rooms           0.135097\nhousing_median_age    0.114110\nhouseholds            0.064506\ntotal_bedrooms        0.047689\npopulation           -0.026920\nlongitude            -0.047432\nlatitude             -0.142724\nName: median_house_value, dtype: float64\n\n\n\n# from pandas.tools.plotting import scatter_matrix # For older versions of Pandas\nfrom pandas.plotting import scatter_matrix\n\nattributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n              \"housing_median_age\"]\nscatter_matrix(housing[attributes], figsize=(12, 8))\nsave_fig(\"scatter_matrix_plot\")\n\nSaving figure scatter_matrix_plot\n\n\n\n\n\n\nhousing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n             alpha=0.1)\nplt.axis([0, 16, 0, 550000])\nsave_fig(\"income_vs_house_value_scatterplot\")\n\nSaving figure income_vs_house_value_scatterplot"
  },
  {
    "objectID": "posts/ML/ML_project.html#experimenting-with-attribute-combinations",
    "href": "posts/ML/ML_project.html#experimenting-with-attribute-combinations",
    "title": "Tools -ML sample project",
    "section": "Experimenting with Attribute Combinations",
    "text": "Experimenting with Attribute Combinations\n\nhousing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\nhousing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\nhousing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]\n\n\ncorr_matrix = housing.corr()\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)\n\nmedian_house_value          1.000000\nmedian_income               0.687160\nrooms_per_household         0.146285\ntotal_rooms                 0.135097\nhousing_median_age          0.114110\nhouseholds                  0.064506\ntotal_bedrooms              0.047689\npopulation_per_household   -0.021985\npopulation                 -0.026920\nlongitude                  -0.047432\nlatitude                   -0.142724\nbedrooms_per_room          -0.259984\nName: median_house_value, dtype: float64\n\n\n\nhousing.plot(kind=\"scatter\", x=\"rooms_per_household\", y=\"median_house_value\",\n             alpha=0.2)\nplt.axis([0, 5, 0, 520000])\nplt.show()\n\n\n\n\n\nhousing.describe()\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nrooms_per_household\nbedrooms_per_room\npopulation_per_household\n\n\n\n\ncount\n16512.000000\n16512.000000\n16512.000000\n16512.000000\n16354.000000\n16512.000000\n16512.000000\n16512.000000\n16512.000000\n16512.000000\n16354.000000\n16512.000000\n\n\nmean\n-119.575834\n35.639577\n28.653101\n2622.728319\n534.973890\n1419.790819\n497.060380\n3.875589\n206990.920724\n5.440341\n0.212878\n3.096437\n\n\nstd\n2.001860\n2.138058\n12.574726\n2138.458419\n412.699041\n1115.686241\n375.720845\n1.904950\n115703.014830\n2.611712\n0.057379\n11.584826\n\n\nmin\n-124.350000\n32.540000\n1.000000\n6.000000\n2.000000\n3.000000\n2.000000\n0.499900\n14999.000000\n1.130435\n0.100000\n0.692308\n\n\n25%\n-121.800000\n33.940000\n18.000000\n1443.000000\n295.000000\n784.000000\n279.000000\n2.566775\n119800.000000\n4.442040\n0.175304\n2.431287\n\n\n50%\n-118.510000\n34.260000\n29.000000\n2119.500000\n433.000000\n1164.000000\n408.000000\n3.540900\n179500.000000\n5.232284\n0.203031\n2.817653\n\n\n75%\n-118.010000\n37.720000\n37.000000\n3141.000000\n644.000000\n1719.250000\n602.000000\n4.744475\n263900.000000\n6.056361\n0.239831\n3.281420\n\n\nmax\n-114.310000\n41.950000\n52.000000\n39320.000000\n6210.000000\n35682.000000\n5358.000000\n15.000100\n500001.000000\n141.909091\n1.000000\n1243.333333"
  },
  {
    "objectID": "posts/ML/ML_project.html#data-cleaning",
    "href": "posts/ML/ML_project.html#data-cleaning",
    "title": "Tools -ML sample project",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nIn the book 3 options are listed:\nhousing.dropna(subset=[\"total_bedrooms\"])    # option 1\nhousing.drop(\"total_bedrooms\", axis=1)       # option 2\nmedian = housing[\"total_bedrooms\"].median()  # option 3\nhousing[\"total_bedrooms\"].fillna(median, inplace=True)\nTo demonstrate each of them, let’s create a copy of the housing dataset, but keeping only the rows that contain at least one null. Then it will be easier to visualize exactly what each option does:\n\nsample_incomplete_rows = housing[housing.isnull().any(axis=1)].head()\nsample_incomplete_rows\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nocean_proximity\n\n\n\n\n4629\n-118.30\n34.07\n18.0\n3759.0\nNaN\n3296.0\n1462.0\n2.2708\n&lt;1H OCEAN\n\n\n6068\n-117.86\n34.01\n16.0\n4632.0\nNaN\n3038.0\n727.0\n5.1762\n&lt;1H OCEAN\n\n\n17923\n-121.97\n37.35\n30.0\n1955.0\nNaN\n999.0\n386.0\n4.6328\n&lt;1H OCEAN\n\n\n13656\n-117.30\n34.05\n6.0\n2155.0\nNaN\n1039.0\n391.0\n1.6675\nINLAND\n\n\n19252\n-122.79\n38.48\n7.0\n6837.0\nNaN\n3468.0\n1405.0\n3.1662\n&lt;1H OCEAN\n\n\n\n\n\n\n\n\nsample_incomplete_rows.dropna(subset=[\"total_bedrooms\"])    # option 1\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nocean_proximity\n\n\n\n\n\n\n\n\n\n\nsample_incomplete_rows.drop(\"total_bedrooms\", axis=1)       # option 2\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\npopulation\nhouseholds\nmedian_income\nocean_proximity\n\n\n\n\n4629\n-118.30\n34.07\n18.0\n3759.0\n3296.0\n1462.0\n2.2708\n&lt;1H OCEAN\n\n\n6068\n-117.86\n34.01\n16.0\n4632.0\n3038.0\n727.0\n5.1762\n&lt;1H OCEAN\n\n\n17923\n-121.97\n37.35\n30.0\n1955.0\n999.0\n386.0\n4.6328\n&lt;1H OCEAN\n\n\n13656\n-117.30\n34.05\n6.0\n2155.0\n1039.0\n391.0\n1.6675\nINLAND\n\n\n19252\n-122.79\n38.48\n7.0\n6837.0\n3468.0\n1405.0\n3.1662\n&lt;1H OCEAN\n\n\n\n\n\n\n\n\nmedian = housing[\"total_bedrooms\"].median()\nsample_incomplete_rows[\"total_bedrooms\"].fillna(median, inplace=True) # option 3\n\n\nsample_incomplete_rows\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nocean_proximity\n\n\n\n\n4629\n-118.30\n34.07\n18.0\n3759.0\n433.0\n3296.0\n1462.0\n2.2708\n&lt;1H OCEAN\n\n\n6068\n-117.86\n34.01\n16.0\n4632.0\n433.0\n3038.0\n727.0\n5.1762\n&lt;1H OCEAN\n\n\n17923\n-121.97\n37.35\n30.0\n1955.0\n433.0\n999.0\n386.0\n4.6328\n&lt;1H OCEAN\n\n\n13656\n-117.30\n34.05\n6.0\n2155.0\n433.0\n1039.0\n391.0\n1.6675\nINLAND\n\n\n19252\n-122.79\n38.48\n7.0\n6837.0\n433.0\n3468.0\n1405.0\n3.1662\n&lt;1H OCEAN\n\n\n\n\n\n\n\n\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy=\"median\")\n\nRemove the text attribute because median can only be calculated on numerical attributes:\n\nhousing_num = housing.drop(\"ocean_proximity\", axis=1)\n# alternatively: housing_num = housing.select_dtypes(include=[np.number])\n\n\nimputer.fit(housing_num)\n\nSimpleImputer(strategy='median')\n\n\n\nimputer.statistics_\n\narray([-118.51  ,   34.26  ,   29.    , 2119.5   ,  433.    , 1164.    ,\n        408.    ,    3.5409])\n\n\nCheck that this is the same as manually computing the median of each attribute:\n\nhousing_num.median().values\n\narray([-118.51  ,   34.26  ,   29.    , 2119.5   ,  433.    , 1164.    ,\n        408.    ,    3.5409])\n\n\nTransform the training set:\n\nX = imputer.transform(housing_num)\n\n\nhousing_tr = pd.DataFrame(X, columns=housing_num.columns,\n                          index=housing.index)\n\n\nhousing_tr.loc[sample_incomplete_rows.index.values]\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\n\n\n\n\n4629\n-118.30\n34.07\n18.0\n3759.0\n433.0\n3296.0\n1462.0\n2.2708\n\n\n6068\n-117.86\n34.01\n16.0\n4632.0\n433.0\n3038.0\n727.0\n5.1762\n\n\n17923\n-121.97\n37.35\n30.0\n1955.0\n433.0\n999.0\n386.0\n4.6328\n\n\n13656\n-117.30\n34.05\n6.0\n2155.0\n433.0\n1039.0\n391.0\n1.6675\n\n\n19252\n-122.79\n38.48\n7.0\n6837.0\n433.0\n3468.0\n1405.0\n3.1662\n\n\n\n\n\n\n\n\nimputer.strategy\n\n'median'\n\n\n\nhousing_tr = pd.DataFrame(X, columns=housing_num.columns,\n                          index=housing_num.index)\n\n\nhousing_tr.head()\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\n\n\n\n\n17606\n-121.89\n37.29\n38.0\n1568.0\n351.0\n710.0\n339.0\n2.7042\n\n\n18632\n-121.93\n37.05\n14.0\n679.0\n108.0\n306.0\n113.0\n6.4214\n\n\n14650\n-117.20\n32.77\n31.0\n1952.0\n471.0\n936.0\n462.0\n2.8621\n\n\n3230\n-119.61\n36.31\n25.0\n1847.0\n371.0\n1460.0\n353.0\n1.8839\n\n\n3555\n-118.59\n34.23\n17.0\n6592.0\n1525.0\n4459.0\n1463.0\n3.0347"
  },
  {
    "objectID": "posts/ML/ML_project.html#handling-text-and-categorical-attributes",
    "href": "posts/ML/ML_project.html#handling-text-and-categorical-attributes",
    "title": "Tools -ML sample project",
    "section": "Handling Text and Categorical Attributes",
    "text": "Handling Text and Categorical Attributes\nNow let’s preprocess the categorical input feature, ocean_proximity:\n\nhousing_cat = housing[[\"ocean_proximity\"]]\nhousing_cat.head(10)\n\n\n\n\n\n\n\n\nocean_proximity\n\n\n\n\n17606\n&lt;1H OCEAN\n\n\n18632\n&lt;1H OCEAN\n\n\n14650\nNEAR OCEAN\n\n\n3230\nINLAND\n\n\n3555\n&lt;1H OCEAN\n\n\n19480\nINLAND\n\n\n8879\n&lt;1H OCEAN\n\n\n13685\nINLAND\n\n\n4937\n&lt;1H OCEAN\n\n\n4861\n&lt;1H OCEAN\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import OrdinalEncoder\n\nordinal_encoder = OrdinalEncoder()\nhousing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\nhousing_cat_encoded[:10]\n\narray([[0.],\n       [0.],\n       [4.],\n       [1.],\n       [0.],\n       [1.],\n       [0.],\n       [1.],\n       [0.],\n       [0.]])\n\n\n\nordinal_encoder.categories_\n\n[array(['&lt;1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],\n       dtype=object)]\n\n\n\nfrom sklearn.preprocessing import OneHotEncoder\n\ncat_encoder = OneHotEncoder()\nhousing_cat_1hot = cat_encoder.fit_transform(housing_cat)\nhousing_cat_1hot\n\n&lt;16512x5 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 16512 stored elements in Compressed Sparse Row format&gt;\n\n\nBy default, the OneHotEncoder class returns a sparse array, but we can convert it to a dense array if needed by calling the toarray() method:\n\nhousing_cat_1hot.toarray()\n\narray([[1., 0., 0., 0., 0.],\n       [1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1.],\n       ...,\n       [0., 1., 0., 0., 0.],\n       [1., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0.]])\n\n\nAlternatively, you can set sparse=False when creating the OneHotEncoder:\n\ncat_encoder = OneHotEncoder(sparse=False)\nhousing_cat_1hot = cat_encoder.fit_transform(housing_cat)\nhousing_cat_1hot\n\narray([[1., 0., 0., 0., 0.],\n       [1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1.],\n       ...,\n       [0., 1., 0., 0., 0.],\n       [1., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0.]])\n\n\n\ncat_encoder.categories_\n\n[array(['&lt;1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],\n       dtype=object)]"
  },
  {
    "objectID": "posts/ML/ML_project.html#custom-transformers",
    "href": "posts/ML/ML_project.html#custom-transformers",
    "title": "Tools -ML sample project",
    "section": "Custom Transformers",
    "text": "Custom Transformers\nLet’s create a custom transformer to add extra attributes:\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# column index\nrooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bedrooms_per_room=True): # no *args or **kargs\n        self.add_bedrooms_per_room = add_bedrooms_per_room\n    def fit(self, X, y=None):\n        return self  # nothing else to do\n    def transform(self, X):\n        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n        population_per_household = X[:, population_ix] / X[:, households_ix]\n        if self.add_bedrooms_per_room:\n            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n            return np.c_[X, rooms_per_household, population_per_household,\n                         bedrooms_per_room]\n        else:\n            return np.c_[X, rooms_per_household, population_per_household]\n\nattr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\nhousing_extra_attribs = attr_adder.transform(housing.values)\n\nNote that I hard coded the indices (3, 4, 5, 6) for concision and clarity in the book, but it would be much cleaner to get them dynamically, like this:\n\ncol_names = \"total_rooms\", \"total_bedrooms\", \"population\", \"households\"\nrooms_ix, bedrooms_ix, population_ix, households_ix = [\n    housing.columns.get_loc(c) for c in col_names] # get the column indices\n\nAlso, housing_extra_attribs is a NumPy array, we’ve lost the column names (unfortunately, that’s a problem with Scikit-Learn). To recover a DataFrame, you could run this:\n\nhousing_extra_attribs = pd.DataFrame(\n    housing_extra_attribs,\n    columns=list(housing.columns)+[\"rooms_per_household\", \"population_per_household\"],\n    index=housing.index)\nhousing_extra_attribs.head()\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nocean_proximity\nrooms_per_household\npopulation_per_household\n\n\n\n\n17606\n-121.89\n37.29\n38.0\n1568.0\n351.0\n710.0\n339.0\n2.7042\n&lt;1H OCEAN\n4.625369\n2.094395\n\n\n18632\n-121.93\n37.05\n14.0\n679.0\n108.0\n306.0\n113.0\n6.4214\n&lt;1H OCEAN\n6.00885\n2.707965\n\n\n14650\n-117.2\n32.77\n31.0\n1952.0\n471.0\n936.0\n462.0\n2.8621\nNEAR OCEAN\n4.225108\n2.025974\n\n\n3230\n-119.61\n36.31\n25.0\n1847.0\n371.0\n1460.0\n353.0\n1.8839\nINLAND\n5.232295\n4.135977\n\n\n3555\n-118.59\n34.23\n17.0\n6592.0\n1525.0\n4459.0\n1463.0\n3.0347\n&lt;1H OCEAN\n4.50581\n3.047847"
  },
  {
    "objectID": "posts/ML/ML_project.html#transformation-pipelines",
    "href": "posts/ML/ML_project.html#transformation-pipelines",
    "title": "Tools -ML sample project",
    "section": "Transformation Pipelines",
    "text": "Transformation Pipelines\nNow let’s build a pipeline for preprocessing the numerical attributes:\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nnum_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('attribs_adder', CombinedAttributesAdder()),\n        ('std_scaler', StandardScaler()),\n    ])\n\nhousing_num_tr = num_pipeline.fit_transform(housing_num)\n\n\nhousing_num_tr\n\narray([[-1.15604281,  0.77194962,  0.74333089, ..., -0.31205452,\n        -0.08649871,  0.15531753],\n       [-1.17602483,  0.6596948 , -1.1653172 , ...,  0.21768338,\n        -0.03353391, -0.83628902],\n       [ 1.18684903, -1.34218285,  0.18664186, ..., -0.46531516,\n        -0.09240499,  0.4222004 ],\n       ...,\n       [ 1.58648943, -0.72478134, -1.56295222, ...,  0.3469342 ,\n        -0.03055414, -0.52177644],\n       [ 0.78221312, -0.85106801,  0.18664186, ...,  0.02499488,\n         0.06150916, -0.30340741],\n       [-1.43579109,  0.99645926,  1.85670895, ..., -0.22852947,\n        -0.09586294,  0.10180567]])\n\n\n\nfrom sklearn.compose import ColumnTransformer\n\nnum_attribs = list(housing_num)\ncat_attribs = [\"ocean_proximity\"]\n\nfull_pipeline = ColumnTransformer([\n        (\"num\", num_pipeline, num_attribs),\n        (\"cat\", OneHotEncoder(), cat_attribs),\n    ])\n\nhousing_prepared = full_pipeline.fit_transform(housing)\n\n\nhousing_prepared\n\narray([[-1.15604281,  0.77194962,  0.74333089, ...,  0.        ,\n         0.        ,  0.        ],\n       [-1.17602483,  0.6596948 , -1.1653172 , ...,  0.        ,\n         0.        ,  0.        ],\n       [ 1.18684903, -1.34218285,  0.18664186, ...,  0.        ,\n         0.        ,  1.        ],\n       ...,\n       [ 1.58648943, -0.72478134, -1.56295222, ...,  0.        ,\n         0.        ,  0.        ],\n       [ 0.78221312, -0.85106801,  0.18664186, ...,  0.        ,\n         0.        ,  0.        ],\n       [-1.43579109,  0.99645926,  1.85670895, ...,  0.        ,\n         1.        ,  0.        ]])\n\n\n\nhousing_prepared.shape\n\n(16512, 16)\n\n\nFor reference, here is the old solution based on a DataFrameSelector transformer (to just select a subset of the Pandas DataFrame columns), and a FeatureUnion:\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# Create a class to select numerical or categorical columns \nclass OldDataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names].values\n\nNow let’s join all these components into a big pipeline that will preprocess both the numerical and the categorical features:\n\nnum_attribs = list(housing_num)\ncat_attribs = [\"ocean_proximity\"]\n\nold_num_pipeline = Pipeline([\n        ('selector', OldDataFrameSelector(num_attribs)),\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('attribs_adder', CombinedAttributesAdder()),\n        ('std_scaler', StandardScaler()),\n    ])\n\nold_cat_pipeline = Pipeline([\n        ('selector', OldDataFrameSelector(cat_attribs)),\n        ('cat_encoder', OneHotEncoder(sparse=False)),\n    ])\n\n\nfrom sklearn.pipeline import FeatureUnion\n\nold_full_pipeline = FeatureUnion(transformer_list=[\n        (\"num_pipeline\", old_num_pipeline),\n        (\"cat_pipeline\", old_cat_pipeline),\n    ])\n\n\nold_housing_prepared = old_full_pipeline.fit_transform(housing)\nold_housing_prepared\n\narray([[-1.15604281,  0.77194962,  0.74333089, ...,  0.        ,\n         0.        ,  0.        ],\n       [-1.17602483,  0.6596948 , -1.1653172 , ...,  0.        ,\n         0.        ,  0.        ],\n       [ 1.18684903, -1.34218285,  0.18664186, ...,  0.        ,\n         0.        ,  1.        ],\n       ...,\n       [ 1.58648943, -0.72478134, -1.56295222, ...,  0.        ,\n         0.        ,  0.        ],\n       [ 0.78221312, -0.85106801,  0.18664186, ...,  0.        ,\n         0.        ,  0.        ],\n       [-1.43579109,  0.99645926,  1.85670895, ...,  0.        ,\n         1.        ,  0.        ]])\n\n\nThe result is the same as with the ColumnTransformer:\n\nnp.allclose(housing_prepared, old_housing_prepared)\n\nTrue"
  },
  {
    "objectID": "posts/ML/ML_project.html#training-and-evaluating-on-the-training-set",
    "href": "posts/ML/ML_project.html#training-and-evaluating-on-the-training-set",
    "title": "Tools -ML sample project",
    "section": "Training and Evaluating on the Training Set",
    "text": "Training and Evaluating on the Training Set\n\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(housing_prepared, housing_labels)\n\nLinearRegression()\n\n\n\n# let's try the full preprocessing pipeline on a few training instances\nsome_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\n\nprint(\"Predictions:\", lin_reg.predict(some_data_prepared))\n\nPredictions: [210644.60459286 317768.80697211 210956.43331178  59218.98886849\n 189747.55849879]\n\n\nCompare against the actual values:\n\nprint(\"Labels:\", list(some_labels))\n\nLabels: [286600.0, 340600.0, 196900.0, 46300.0, 254500.0]\n\n\n\nsome_data_prepared\n\narray([[-1.15604281,  0.77194962,  0.74333089, -0.49323393, -0.44543821,\n        -0.63621141, -0.42069842, -0.61493744, -0.31205452, -0.08649871,\n         0.15531753,  1.        ,  0.        ,  0.        ,  0.        ,\n         0.        ],\n       [-1.17602483,  0.6596948 , -1.1653172 , -0.90896655, -1.0369278 ,\n        -0.99833135, -1.02222705,  1.33645936,  0.21768338, -0.03353391,\n        -0.83628902,  1.        ,  0.        ,  0.        ,  0.        ,\n         0.        ],\n       [ 1.18684903, -1.34218285,  0.18664186, -0.31365989, -0.15334458,\n        -0.43363936, -0.0933178 , -0.5320456 , -0.46531516, -0.09240499,\n         0.4222004 ,  0.        ,  0.        ,  0.        ,  0.        ,\n         1.        ],\n       [-0.01706767,  0.31357576, -0.29052016, -0.36276217, -0.39675594,\n         0.03604096, -0.38343559, -1.04556555, -0.07966124,  0.08973561,\n        -0.19645314,  0.        ,  1.        ,  0.        ,  0.        ,\n         0.        ],\n       [ 0.49247384, -0.65929936, -0.92673619,  1.85619316,  2.41221109,\n         2.72415407,  2.57097492, -0.44143679, -0.35783383, -0.00419445,\n         0.2699277 ,  1.        ,  0.        ,  0.        ,  0.        ,\n         0.        ]])\n\n\n\nfrom sklearn.metrics import mean_squared_error\n\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse\n\n68628.19819848923\n\n\nNote: since Scikit-Learn 0.22, you can get the RMSE directly by calling the mean_squared_error() function with squared=False.\n\nfrom sklearn.metrics import mean_absolute_error\n\nlin_mae = mean_absolute_error(housing_labels, housing_predictions)\nlin_mae\n\n49439.89599001897\n\n\n\nfrom sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor(random_state=42)\ntree_reg.fit(housing_prepared, housing_labels)\n\nDecisionTreeRegressor(random_state=42)\n\n\n\nhousing_predictions = tree_reg.predict(housing_prepared)\ntree_mse = mean_squared_error(housing_labels, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse\n\n0.0"
  },
  {
    "objectID": "posts/ML/ML_project.html#better-evaluation-using-cross-validation",
    "href": "posts/ML/ML_project.html#better-evaluation-using-cross-validation",
    "title": "Tools -ML sample project",
    "section": "Better Evaluation Using Cross-Validation",
    "text": "Better Evaluation Using Cross-Validation\n\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n                         scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-scores)\n\n\ndef display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())\n\ndisplay_scores(tree_rmse_scores)\n\nScores: [70194.33680785 66855.16363941 72432.58244769 70758.73896782\n 71115.88230639 75585.14172901 70262.86139133 70273.6325285\n 75366.87952553 71231.65726027]\nMean: 71407.68766037929\nStandard deviation: 2439.4345041191004\n\n\n\nlin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n                             scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)\n\nScores: [66782.73843989 66960.118071   70347.95244419 74739.57052552\n 68031.13388938 71193.84183426 64969.63056405 68281.61137997\n 71552.91566558 67665.10082067]\nMean: 69052.46136345083\nStandard deviation: 2731.674001798342\n\n\nNote: we specify n_estimators=100 to be future-proof since the default value is going to change to 100 in Scikit-Learn 0.22 (for simplicity, this is not shown in the book).\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nforest_reg = RandomForestRegressor(n_estimators=100, random_state=42)\nforest_reg.fit(housing_prepared, housing_labels)\n\nRandomForestRegressor(random_state=42)\n\n\n\nhousing_predictions = forest_reg.predict(housing_prepared)\nforest_mse = mean_squared_error(housing_labels, housing_predictions)\nforest_rmse = np.sqrt(forest_mse)\nforest_rmse\n\n18603.515021376355\n\n\n\nfrom sklearn.model_selection import cross_val_score\n\nforest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n                                scoring=\"neg_mean_squared_error\", cv=10)\nforest_rmse_scores = np.sqrt(-forest_scores)\ndisplay_scores(forest_rmse_scores)\n\nScores: [49519.80364233 47461.9115823  50029.02762854 52325.28068953\n 49308.39426421 53446.37892622 48634.8036574  47585.73832311\n 53490.10699751 50021.5852922 ]\nMean: 50182.303100336096\nStandard deviation: 2097.0810550985693\n\n\n\nscores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\npd.Series(np.sqrt(-scores)).describe()\n\ncount       10.000000\nmean     69052.461363\nstd       2879.437224\nmin      64969.630564\n25%      67136.363758\n50%      68156.372635\n75%      70982.369487\nmax      74739.570526\ndtype: float64\n\n\n\nfrom sklearn.svm import SVR\n\nsvm_reg = SVR(kernel=\"linear\")\nsvm_reg.fit(housing_prepared, housing_labels)\nhousing_predictions = svm_reg.predict(housing_prepared)\nsvm_mse = mean_squared_error(housing_labels, housing_predictions)\nsvm_rmse = np.sqrt(svm_mse)\nsvm_rmse\n\n111094.6308539982"
  },
  {
    "objectID": "posts/ML/ML_project.html#grid-search",
    "href": "posts/ML/ML_project.html#grid-search",
    "title": "Tools -ML sample project",
    "section": "Grid Search",
    "text": "Grid Search\n\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    # try 12 (3×4) combinations of hyperparameters\n    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n    # then try 6 (2×3) combinations with bootstrap set as False\n    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n  ]\n\nforest_reg = RandomForestRegressor(random_state=42)\n# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n                           scoring='neg_mean_squared_error',\n                           return_train_score=True)\ngrid_search.fit(housing_prepared, housing_labels)\n\nGridSearchCV(cv=5, estimator=RandomForestRegressor(random_state=42),\n             param_grid=[{'max_features': [2, 4, 6, 8],\n                          'n_estimators': [3, 10, 30]},\n                         {'bootstrap': [False], 'max_features': [2, 3, 4],\n                          'n_estimators': [3, 10]}],\n             return_train_score=True, scoring='neg_mean_squared_error')\n\n\nThe best hyperparameter combination found:\n\ngrid_search.best_params_\n\n{'max_features': 8, 'n_estimators': 30}\n\n\n\ngrid_search.best_estimator_\n\nRandomForestRegressor(max_features=8, n_estimators=30, random_state=42)\n\n\nLet’s look at the score of each hyperparameter combination tested during the grid search:\n\ncvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)\n\n63669.11631261028 {'max_features': 2, 'n_estimators': 3}\n55627.099719926795 {'max_features': 2, 'n_estimators': 10}\n53384.57275149205 {'max_features': 2, 'n_estimators': 30}\n60965.950449450494 {'max_features': 4, 'n_estimators': 3}\n52741.04704299915 {'max_features': 4, 'n_estimators': 10}\n50377.40461678399 {'max_features': 4, 'n_estimators': 30}\n58663.93866579625 {'max_features': 6, 'n_estimators': 3}\n52006.19873526564 {'max_features': 6, 'n_estimators': 10}\n50146.51167415009 {'max_features': 6, 'n_estimators': 30}\n57869.25276169646 {'max_features': 8, 'n_estimators': 3}\n51711.127883959234 {'max_features': 8, 'n_estimators': 10}\n49682.273345071546 {'max_features': 8, 'n_estimators': 30}\n62895.06951262424 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3}\n54658.176157539405 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10}\n59470.40652318466 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3}\n52724.9822587892 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10}\n57490.5691951261 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3}\n51009.495668875716 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}\n\n\n\npd.DataFrame(grid_search.cv_results_)\n\n\n\n\n\n\n\n\nmean_fit_time\nstd_fit_time\nmean_score_time\nstd_score_time\nparam_max_features\nparam_n_estimators\nparam_bootstrap\nparams\nsplit0_test_score\nsplit1_test_score\n...\nmean_test_score\nstd_test_score\nrank_test_score\nsplit0_train_score\nsplit1_train_score\nsplit2_train_score\nsplit3_train_score\nsplit4_train_score\nmean_train_score\nstd_train_score\n\n\n\n\n0\n0.050905\n0.004097\n0.002766\n0.000256\n2\n3\nNaN\n{'max_features': 2, 'n_estimators': 3}\n-3.837622e+09\n-4.147108e+09\n...\n-4.053756e+09\n1.519591e+08\n18\n-1.064113e+09\n-1.105142e+09\n-1.116550e+09\n-1.112342e+09\n-1.129650e+09\n-1.105559e+09\n2.220402e+07\n\n\n1\n0.143706\n0.002170\n0.007205\n0.000304\n2\n10\nNaN\n{'max_features': 2, 'n_estimators': 10}\n-3.047771e+09\n-3.254861e+09\n...\n-3.094374e+09\n1.327062e+08\n11\n-5.927175e+08\n-5.870952e+08\n-5.776964e+08\n-5.716332e+08\n-5.802501e+08\n-5.818785e+08\n7.345821e+06\n\n\n2\n0.410306\n0.004403\n0.019903\n0.000964\n2\n30\nNaN\n{'max_features': 2, 'n_estimators': 30}\n-2.689185e+09\n-3.021086e+09\n...\n-2.849913e+09\n1.626875e+08\n9\n-4.381089e+08\n-4.391272e+08\n-4.371702e+08\n-4.376955e+08\n-4.452654e+08\n-4.394734e+08\n2.966320e+06\n\n\n3\n0.069762\n0.000987\n0.002409\n0.000080\n4\n3\nNaN\n{'max_features': 4, 'n_estimators': 3}\n-3.730181e+09\n-3.786886e+09\n...\n-3.716847e+09\n1.631510e+08\n16\n-9.865163e+08\n-1.012565e+09\n-9.169425e+08\n-1.037400e+09\n-9.707739e+08\n-9.848396e+08\n4.084607e+07\n\n\n4\n0.227188\n0.001444\n0.006829\n0.000090\n4\n10\nNaN\n{'max_features': 4, 'n_estimators': 10}\n-2.666283e+09\n-2.784511e+09\n...\n-2.781618e+09\n1.268607e+08\n8\n-5.097115e+08\n-5.162820e+08\n-4.962893e+08\n-5.436192e+08\n-5.160297e+08\n-5.163863e+08\n1.542862e+07\n\n\n5\n0.711381\n0.038618\n0.020686\n0.001864\n4\n30\nNaN\n{'max_features': 4, 'n_estimators': 30}\n-2.387153e+09\n-2.588448e+09\n...\n-2.537883e+09\n1.214614e+08\n3\n-3.838835e+08\n-3.880268e+08\n-3.790867e+08\n-4.040957e+08\n-3.845520e+08\n-3.879289e+08\n8.571233e+06\n\n\n6\n0.094580\n0.003861\n0.002426\n0.000118\n6\n3\nNaN\n{'max_features': 6, 'n_estimators': 3}\n-3.119657e+09\n-3.586319e+09\n...\n-3.441458e+09\n1.893056e+08\n14\n-9.245343e+08\n-8.886939e+08\n-9.353135e+08\n-9.009801e+08\n-8.624664e+08\n-9.023976e+08\n2.591445e+07\n\n\n7\n0.311034\n0.005247\n0.006980\n0.000283\n6\n10\nNaN\n{'max_features': 6, 'n_estimators': 10}\n-2.549663e+09\n-2.782039e+09\n...\n-2.704645e+09\n1.471569e+08\n6\n-4.980344e+08\n-5.045869e+08\n-4.994664e+08\n-4.990325e+08\n-5.055542e+08\n-5.013349e+08\n3.100456e+06\n\n\n8\n0.979656\n0.048790\n0.021028\n0.001812\n6\n30\nNaN\n{'max_features': 6, 'n_estimators': 30}\n-2.370010e+09\n-2.583638e+09\n...\n-2.514673e+09\n1.285080e+08\n2\n-3.838538e+08\n-3.804711e+08\n-3.805218e+08\n-3.856095e+08\n-3.901917e+08\n-3.841296e+08\n3.617057e+06\n\n\n9\n0.118484\n0.001009\n0.002239\n0.000068\n8\n3\nNaN\n{'max_features': 8, 'n_estimators': 3}\n-3.353504e+09\n-3.348552e+09\n...\n-3.348850e+09\n1.241939e+08\n13\n-9.228123e+08\n-8.553031e+08\n-8.603321e+08\n-8.881964e+08\n-9.151287e+08\n-8.883545e+08\n2.750227e+07\n\n\n10\n0.401726\n0.005465\n0.007028\n0.000345\n8\n10\nNaN\n{'max_features': 8, 'n_estimators': 10}\n-2.571970e+09\n-2.718994e+09\n...\n-2.674041e+09\n1.392777e+08\n5\n-4.932416e+08\n-4.815238e+08\n-4.730979e+08\n-5.155367e+08\n-4.985555e+08\n-4.923911e+08\n1.459294e+07\n\n\n11\n1.236572\n0.036875\n0.019325\n0.000252\n8\n30\nNaN\n{'max_features': 8, 'n_estimators': 30}\n-2.357390e+09\n-2.546640e+09\n...\n-2.468328e+09\n1.091662e+08\n1\n-3.841658e+08\n-3.744500e+08\n-3.773239e+08\n-3.882250e+08\n-3.810005e+08\n-3.810330e+08\n4.871017e+06\n\n\n12\n0.064666\n0.001042\n0.002780\n0.000240\n2\n3\nFalse\n{'bootstrap': False, 'max_features': 2, 'n_est...\n-3.785816e+09\n-4.166012e+09\n...\n-3.955790e+09\n1.900964e+08\n17\n-0.000000e+00\n-0.000000e+00\n-0.000000e+00\n-0.000000e+00\n-0.000000e+00\n0.000000e+00\n0.000000e+00\n\n\n13\n0.213941\n0.000996\n0.007956\n0.000267\n2\n10\nFalse\n{'bootstrap': False, 'max_features': 2, 'n_est...\n-2.810721e+09\n-3.107789e+09\n...\n-2.987516e+09\n1.539234e+08\n10\n-6.056477e-02\n-0.000000e+00\n-0.000000e+00\n-0.000000e+00\n-2.967449e+00\n-6.056027e-01\n1.181156e+00\n\n\n14\n0.090480\n0.003167\n0.002681\n0.000082\n3\n3\nFalse\n{'bootstrap': False, 'max_features': 3, 'n_est...\n-3.618324e+09\n-3.441527e+09\n...\n-3.536729e+09\n7.795057e+07\n15\n-0.000000e+00\n-0.000000e+00\n-0.000000e+00\n-0.000000e+00\n-6.072840e+01\n-1.214568e+01\n2.429136e+01\n\n\n15\n0.286396\n0.004578\n0.008019\n0.000384\n3\n10\nFalse\n{'bootstrap': False, 'max_features': 3, 'n_est...\n-2.757999e+09\n-2.851737e+09\n...\n-2.779924e+09\n6.286720e+07\n7\n-2.089484e+01\n-0.000000e+00\n-0.000000e+00\n-0.000000e+00\n-5.465556e+00\n-5.272080e+00\n8.093117e+00\n\n\n16\n0.109239\n0.002999\n0.003399\n0.001579\n4\n3\nFalse\n{'bootstrap': False, 'max_features': 4, 'n_est...\n-3.134040e+09\n-3.559375e+09\n...\n-3.305166e+09\n1.879165e+08\n12\n-0.000000e+00\n-0.000000e+00\n-0.000000e+00\n-0.000000e+00\n-0.000000e+00\n0.000000e+00\n0.000000e+00\n\n\n17\n0.370459\n0.017424\n0.007863\n0.000056\n4\n10\nFalse\n{'bootstrap': False, 'max_features': 4, 'n_est...\n-2.525578e+09\n-2.710011e+09\n...\n-2.601969e+09\n1.088048e+08\n4\n-0.000000e+00\n-1.514119e-02\n-0.000000e+00\n-0.000000e+00\n-0.000000e+00\n-3.028238e-03\n6.056477e-03\n\n\n\n\n18 rows × 23 columns"
  },
  {
    "objectID": "posts/ML/ML_project.html#randomized-search",
    "href": "posts/ML/ML_project.html#randomized-search",
    "title": "Tools -ML sample project",
    "section": "Randomized Search",
    "text": "Randomized Search\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nparam_distribs = {\n        'n_estimators': randint(low=1, high=200),\n        'max_features': randint(low=1, high=8),\n    }\n\nforest_reg = RandomForestRegressor(random_state=42)\nrnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,\n                                n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)\nrnd_search.fit(housing_prepared, housing_labels)\n\nRandomizedSearchCV(cv=5, estimator=RandomForestRegressor(random_state=42),\n                   param_distributions={'max_features': &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7fd1b96682d0&gt;,\n                                        'n_estimators': &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7fd1b9668b10&gt;},\n                   random_state=42, scoring='neg_mean_squared_error')\n\n\n\ncvres = rnd_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)\n\n49150.70756927707 {'max_features': 7, 'n_estimators': 180}\n51389.889203389284 {'max_features': 5, 'n_estimators': 15}\n50796.155224308866 {'max_features': 3, 'n_estimators': 72}\n50835.13360315349 {'max_features': 5, 'n_estimators': 21}\n49280.9449827171 {'max_features': 7, 'n_estimators': 122}\n50774.90662363929 {'max_features': 3, 'n_estimators': 75}\n50682.78888164288 {'max_features': 3, 'n_estimators': 88}\n49608.99608105296 {'max_features': 5, 'n_estimators': 100}\n50473.61930350219 {'max_features': 3, 'n_estimators': 150}\n64429.84143294435 {'max_features': 5, 'n_estimators': 2}"
  },
  {
    "objectID": "posts/ML/ML_project.html#analyze-the-best-models-and-their-errors",
    "href": "posts/ML/ML_project.html#analyze-the-best-models-and-their-errors",
    "title": "Tools -ML sample project",
    "section": "Analyze the Best Models and Their Errors",
    "text": "Analyze the Best Models and Their Errors\n\nfeature_importances = grid_search.best_estimator_.feature_importances_\nfeature_importances\n\narray([7.33442355e-02, 6.29090705e-02, 4.11437985e-02, 1.46726854e-02,\n       1.41064835e-02, 1.48742809e-02, 1.42575993e-02, 3.66158981e-01,\n       5.64191792e-02, 1.08792957e-01, 5.33510773e-02, 1.03114883e-02,\n       1.64780994e-01, 6.02803867e-05, 1.96041560e-03, 2.85647464e-03])\n\n\n\nextra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n#cat_encoder = cat_pipeline.named_steps[\"cat_encoder\"] # old solution\ncat_encoder = full_pipeline.named_transformers_[\"cat\"]\ncat_one_hot_attribs = list(cat_encoder.categories_[0])\nattributes = num_attribs + extra_attribs + cat_one_hot_attribs\nsorted(zip(feature_importances, attributes), reverse=True)\n\n[(0.36615898061813423, 'median_income'),\n (0.16478099356159054, 'INLAND'),\n (0.10879295677551575, 'pop_per_hhold'),\n (0.07334423551601243, 'longitude'),\n (0.06290907048262032, 'latitude'),\n (0.056419179181954014, 'rooms_per_hhold'),\n (0.053351077347675815, 'bedrooms_per_room'),\n (0.04114379847872964, 'housing_median_age'),\n (0.014874280890402769, 'population'),\n (0.014672685420543239, 'total_rooms'),\n (0.014257599323407808, 'households'),\n (0.014106483453584104, 'total_bedrooms'),\n (0.010311488326303788, '&lt;1H OCEAN'),\n (0.0028564746373201584, 'NEAR OCEAN'),\n (0.0019604155994780706, 'NEAR BAY'),\n (6.0280386727366e-05, 'ISLAND')]"
  },
  {
    "objectID": "posts/ML/ML_project.html#evaluate-your-system-on-the-test-set",
    "href": "posts/ML/ML_project.html#evaluate-your-system-on-the-test-set",
    "title": "Tools -ML sample project",
    "section": "Evaluate Your System on the Test Set",
    "text": "Evaluate Your System on the Test Set\n\nfinal_model = grid_search.best_estimator_\n\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\ny_test = strat_test_set[\"median_house_value\"].copy()\n\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = final_model.predict(X_test_prepared)\n\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)\n\n\nfinal_rmse\n\n47730.22690385927\n\n\nWe can compute a 95% confidence interval for the test RMSE:\n\nfrom scipy import stats\n\nconfidence = 0.95\nsquared_errors = (final_predictions - y_test) ** 2\nnp.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n                         loc=squared_errors.mean(),\n                         scale=stats.sem(squared_errors)))\n\narray([45685.10470776, 49691.25001878])\n\n\nWe could compute the interval manually like this:\n\nm = len(squared_errors)\nmean = squared_errors.mean()\ntscore = stats.t.ppf((1 + confidence) / 2, df=m - 1)\ntmargin = tscore * squared_errors.std(ddof=1) / np.sqrt(m)\nnp.sqrt(mean - tmargin), np.sqrt(mean + tmargin)\n\n(45685.10470776014, 49691.25001877871)\n\n\nAlternatively, we could use a z-scores rather than t-scores:\n\nzscore = stats.norm.ppf((1 + confidence) / 2)\nzmargin = zscore * squared_errors.std(ddof=1) / np.sqrt(m)\nnp.sqrt(mean - zmargin), np.sqrt(mean + zmargin)\n\n(45685.717918136594, 49690.68623889426)"
  },
  {
    "objectID": "posts/ML/ML_project.html#a-full-pipeline-with-both-preparation-and-prediction",
    "href": "posts/ML/ML_project.html#a-full-pipeline-with-both-preparation-and-prediction",
    "title": "Tools -ML sample project",
    "section": "A full pipeline with both preparation and prediction",
    "text": "A full pipeline with both preparation and prediction\n\nfull_pipeline_with_predictor = Pipeline([\n        (\"preparation\", full_pipeline),\n        (\"linear\", LinearRegression())\n    ])\n\nfull_pipeline_with_predictor.fit(housing, housing_labels)\nfull_pipeline_with_predictor.predict(some_data)\n\narray([210644.60459286, 317768.80697211, 210956.43331178,  59218.98886849,\n       189747.55849879])"
  },
  {
    "objectID": "posts/ML/ML_project.html#model-persistence-using-joblib",
    "href": "posts/ML/ML_project.html#model-persistence-using-joblib",
    "title": "Tools -ML sample project",
    "section": "Model persistence using joblib",
    "text": "Model persistence using joblib\n\nmy_model = full_pipeline_with_predictor\n\n\nimport joblib\njoblib.dump(my_model, \"my_model.pkl\") # DIFF\n#...\nmy_model_loaded = joblib.load(\"my_model.pkl\") # DIFF"
  },
  {
    "objectID": "posts/ML/ML_project.html#example-scipy-distributions-for-randomizedsearchcv",
    "href": "posts/ML/ML_project.html#example-scipy-distributions-for-randomizedsearchcv",
    "title": "Tools -ML sample project",
    "section": "Example SciPy distributions for RandomizedSearchCV",
    "text": "Example SciPy distributions for RandomizedSearchCV\n\nfrom scipy.stats import geom, expon\ngeom_distrib=geom(0.5).rvs(10000, random_state=42)\nexpon_distrib=expon(scale=1).rvs(10000, random_state=42)\nplt.hist(geom_distrib, bins=50)\nplt.show()\nplt.hist(expon_distrib, bins=50)\nplt.show()"
  },
  {
    "objectID": "posts/ML/ML_project.html#section",
    "href": "posts/ML/ML_project.html#section",
    "title": "Tools -ML sample project",
    "section": "1.",
    "text": "1.\nQuestion: Try a Support Vector Machine regressor (sklearn.svm.SVR), with various hyperparameters such as kernel=\"linear\" (with various values for the C hyperparameter) or kernel=\"rbf\" (with various values for the C and gamma hyperparameters). Don’t worry about what these hyperparameters mean for now. How does the best SVR predictor perform?\nWarning: the following cell may take close to 30 minutes to run, or more depending on your hardware.\n\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n        {'kernel': ['linear'], 'C': [10., 30., 100., 300., 1000., 3000., 10000., 30000.0]},\n        {'kernel': ['rbf'], 'C': [1.0, 3.0, 10., 30., 100., 300., 1000.0],\n         'gamma': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0]},\n    ]\n\nsvm_reg = SVR()\ngrid_search = GridSearchCV(svm_reg, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=2)\ngrid_search.fit(housing_prepared, housing_labels)\n\nFitting 5 folds for each of 50 candidates, totalling 250 fits\n[CV] C=10.0, kernel=linear ...........................................\n[CV] ............................ C=10.0, kernel=linear, total=   3.9s\n[CV] C=10.0, kernel=linear ...........................................\n[CV] ............................ C=10.0, kernel=linear, total=   3.9s\n[CV] C=10.0, kernel=linear ...........................................\n[CV] ............................ C=10.0, kernel=linear, total=   4.6s\n[CV] C=10.0, kernel=linear ...........................................\n[CV] ............................ C=10.0, kernel=linear, total=   4.2s\n[CV] C=10.0, kernel=linear ...........................................\n[CV] ............................ C=10.0, kernel=linear, total=   4.5s\n[CV] C=30.0, kernel=linear ...........................................\n[CV] ............................ C=30.0, kernel=linear, total=   4.1s\n[CV] C=30.0, kernel=linear ...........................................\n[CV] ............................ C=30.0, kernel=linear, total=   4.2s\n[CV] C=30.0, kernel=linear ...........................................\n[CV] ............................ C=30.0, kernel=linear, total=   4.3s\n[CV] C=30.0, kernel=linear ...........................................\n[CV] ............................ C=30.0, kernel=linear, total=   4.0s\n[CV] C=30.0, kernel=linear ...........................................\n[CV] ............................ C=30.0, kernel=linear, total=   3.9s\n[CV] C=100.0, kernel=linear ..........................................\n[CV] ........................... C=100.0, kernel=linear, total=   3.9s\n[CV] C=100.0, kernel=linear ..........................................\n[CV] ........................... C=100.0, kernel=linear, total=   4.0s\n[CV] C=100.0, kernel=linear ..........................................\n[CV] ........................... C=100.0, kernel=linear, total=   4.0s\n[CV] C=100.0, kernel=linear ..........................................\n[CV] ........................... C=100.0, kernel=linear, total=   4.0s\n[CV] C=100.0, kernel=linear ..........................................\n[CV] ........................... C=100.0, kernel=linear, total=   3.9s\n[CV] C=300.0, kernel=linear ..........................................\n[CV] ........................... C=300.0, kernel=linear, total=   4.1s\n&lt;&lt;434 more lines&gt;&gt;\n[CV] C=1000.0, gamma=0.1, kernel=rbf .................................\n[CV] .................. C=1000.0, gamma=0.1, kernel=rbf, total=   6.7s\n[CV] C=1000.0, gamma=0.1, kernel=rbf .................................\n[CV] .................. C=1000.0, gamma=0.1, kernel=rbf, total=   6.8s\n[CV] C=1000.0, gamma=0.3, kernel=rbf .................................\n[CV] .................. C=1000.0, gamma=0.3, kernel=rbf, total=   6.7s\n[CV] C=1000.0, gamma=0.3, kernel=rbf .................................\n[CV] .................. C=1000.0, gamma=0.3, kernel=rbf, total=   6.7s\n[CV] C=1000.0, gamma=0.3, kernel=rbf .................................\n[CV] .................. C=1000.0, gamma=0.3, kernel=rbf, total=   6.7s\n[CV] C=1000.0, gamma=0.3, kernel=rbf .................................\n[CV] .................. C=1000.0, gamma=0.3, kernel=rbf, total=   6.7s\n[CV] C=1000.0, gamma=0.3, kernel=rbf .................................\n[CV] .................. C=1000.0, gamma=0.3, kernel=rbf, total=   6.7s\n[CV] C=1000.0, gamma=1.0, kernel=rbf .................................\n[CV] .................. C=1000.0, gamma=1.0, kernel=rbf, total=   6.7s\n[CV] C=1000.0, gamma=1.0, kernel=rbf .................................\n[CV] .................. C=1000.0, gamma=1.0, kernel=rbf, total=   6.8s\n[CV] C=1000.0, gamma=1.0, kernel=rbf .................................\n[CV] .................. C=1000.0, gamma=1.0, kernel=rbf, total=   6.7s\n[CV] C=1000.0, gamma=1.0, kernel=rbf .................................\n[CV] .................. C=1000.0, gamma=1.0, kernel=rbf, total=   6.7s\n[CV] C=1000.0, gamma=1.0, kernel=rbf .................................\n[CV] .................. C=1000.0, gamma=1.0, kernel=rbf, total=   6.7s\n[CV] C=1000.0, gamma=3.0, kernel=rbf .................................\n[CV] .................. C=1000.0, gamma=3.0, kernel=rbf, total=   7.4s\n[CV] C=1000.0, gamma=3.0, kernel=rbf .................................\n[CV] .................. C=1000.0, gamma=3.0, kernel=rbf, total=   7.4s\n[CV] C=1000.0, gamma=3.0, kernel=rbf .................................\n[CV] .................. C=1000.0, gamma=3.0, kernel=rbf, total=   7.4s\n[CV] C=1000.0, gamma=3.0, kernel=rbf .................................\n[CV] .................. C=1000.0, gamma=3.0, kernel=rbf, total=   7.4s\n[CV] C=1000.0, gamma=3.0, kernel=rbf .................................\n[CV] .................. C=1000.0, gamma=3.0, kernel=rbf, total=   7.3s\n\n\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.9s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done 250 out of 250 | elapsed: 26.4min finished\n\n\nGridSearchCV(cv=5, estimator=SVR(),\n             param_grid=[{'C': [10.0, 30.0, 100.0, 300.0, 1000.0, 3000.0,\n                                10000.0, 30000.0],\n                          'kernel': ['linear']},\n                         {'C': [1.0, 3.0, 10.0, 30.0, 100.0, 300.0, 1000.0],\n                          'gamma': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0],\n                          'kernel': ['rbf']}],\n             scoring='neg_mean_squared_error', verbose=2)\n\n\nThe best model achieves the following score (evaluated using 5-fold cross validation):\n\nnegative_mse = grid_search.best_score_\nrmse = np.sqrt(-negative_mse)\nrmse\n\n70363.84006944533\n\n\nThat’s much worse than the RandomForestRegressor. Let’s check the best hyperparameters found:\n\ngrid_search.best_params_\n\n{'C': 30000.0, 'kernel': 'linear'}\n\n\nThe linear kernel seems better than the RBF kernel. Notice that the value of C is the maximum tested value. When this happens you definitely want to launch the grid search again with higher values for C (removing the smallest values), because it is likely that higher values of C will be better."
  },
  {
    "objectID": "posts/ML/ML_project.html#section-1",
    "href": "posts/ML/ML_project.html#section-1",
    "title": "Tools -ML sample project",
    "section": "2.",
    "text": "2.\nQuestion: Try replacing GridSearchCV with RandomizedSearchCV.\nWarning: the following cell may take close to 45 minutes to run, or more depending on your hardware.\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import expon, reciprocal\n\n# see https://docs.scipy.org/doc/scipy/reference/stats.html\n# for `expon()` and `reciprocal()` documentation and more probability distribution functions.\n\n# Note: gamma is ignored when kernel is \"linear\"\nparam_distribs = {\n        'kernel': ['linear', 'rbf'],\n        'C': reciprocal(20, 200000),\n        'gamma': expon(scale=1.0),\n    }\n\nsvm_reg = SVR()\nrnd_search = RandomizedSearchCV(svm_reg, param_distributions=param_distribs,\n                                n_iter=50, cv=5, scoring='neg_mean_squared_error',\n                                verbose=2, random_state=42)\nrnd_search.fit(housing_prepared, housing_labels)\n\nFitting 5 folds for each of 50 candidates, totalling 250 fits\n[CV] C=629.782329591372, gamma=3.010121430917521, kernel=linear ......\n[CV]  C=629.782329591372, gamma=3.010121430917521, kernel=linear, total=   4.2s\n[CV] C=629.782329591372, gamma=3.010121430917521, kernel=linear ......\n[CV]  C=629.782329591372, gamma=3.010121430917521, kernel=linear, total=   4.0s\n[CV] C=629.782329591372, gamma=3.010121430917521, kernel=linear ......\n[CV]  C=629.782329591372, gamma=3.010121430917521, kernel=linear, total=   4.5s\n[CV] C=629.782329591372, gamma=3.010121430917521, kernel=linear ......\n[CV]  C=629.782329591372, gamma=3.010121430917521, kernel=linear, total=   4.5s\n[CV] C=629.782329591372, gamma=3.010121430917521, kernel=linear ......\n[CV]  C=629.782329591372, gamma=3.010121430917521, kernel=linear, total=   4.3s\n[CV] C=26290.206464300216, gamma=0.9084469696321253, kernel=rbf ......\n[CV]  C=26290.206464300216, gamma=0.9084469696321253, kernel=rbf, total=   8.6s\n[CV] C=26290.206464300216, gamma=0.9084469696321253, kernel=rbf ......\n[CV]  C=26290.206464300216, gamma=0.9084469696321253, kernel=rbf, total=   9.1s\n[CV] C=26290.206464300216, gamma=0.9084469696321253, kernel=rbf ......\n[CV]  C=26290.206464300216, gamma=0.9084469696321253, kernel=rbf, total=   8.8s\n[CV] C=26290.206464300216, gamma=0.9084469696321253, kernel=rbf ......\n[CV]  C=26290.206464300216, gamma=0.9084469696321253, kernel=rbf, total=   8.9s\n[CV] C=26290.206464300216, gamma=0.9084469696321253, kernel=rbf ......\n[CV]  C=26290.206464300216, gamma=0.9084469696321253, kernel=rbf, total=   9.0s\n[CV] C=84.14107900575871, gamma=0.059838768608680676, kernel=rbf .....\n[CV]  C=84.14107900575871, gamma=0.059838768608680676, kernel=rbf, total=   7.0s\n[CV] C=84.14107900575871, gamma=0.059838768608680676, kernel=rbf .....\n[CV]  C=84.14107900575871, gamma=0.059838768608680676, kernel=rbf, total=   7.0s\n[CV] C=84.14107900575871, gamma=0.059838768608680676, kernel=rbf .....\n[CV]  C=84.14107900575871, gamma=0.059838768608680676, kernel=rbf, total=   6.9s\n[CV] C=84.14107900575871, gamma=0.059838768608680676, kernel=rbf .....\n[CV]  C=84.14107900575871, gamma=0.059838768608680676, kernel=rbf, total=   7.0s\n[CV] C=84.14107900575871, gamma=0.059838768608680676, kernel=rbf .....\n[CV]  C=84.14107900575871, gamma=0.059838768608680676, kernel=rbf, total=   7.0s\n[CV] C=432.37884813148855, gamma=0.15416196746656105, kernel=linear ..\n[CV]  C=432.37884813148855, gamma=0.15416196746656105, kernel=linear, total=   4.6s\n&lt;&lt;434 more lines&gt;&gt;\n[CV] C=61217.04421344494, gamma=1.6279689407405564, kernel=rbf .......\n[CV]  C=61217.04421344494, gamma=1.6279689407405564, kernel=rbf, total=  25.2s\n[CV] C=61217.04421344494, gamma=1.6279689407405564, kernel=rbf .......\n[CV]  C=61217.04421344494, gamma=1.6279689407405564, kernel=rbf, total=  23.2s\n[CV] C=926.9787684096649, gamma=2.147979593060577, kernel=rbf ........\n[CV]  C=926.9787684096649, gamma=2.147979593060577, kernel=rbf, total=   5.7s\n[CV] C=926.9787684096649, gamma=2.147979593060577, kernel=rbf ........\n[CV]  C=926.9787684096649, gamma=2.147979593060577, kernel=rbf, total=   5.7s\n[CV] C=926.9787684096649, gamma=2.147979593060577, kernel=rbf ........\n[CV]  C=926.9787684096649, gamma=2.147979593060577, kernel=rbf, total=   5.7s\n[CV] C=926.9787684096649, gamma=2.147979593060577, kernel=rbf ........\n[CV]  C=926.9787684096649, gamma=2.147979593060577, kernel=rbf, total=   5.8s\n[CV] C=926.9787684096649, gamma=2.147979593060577, kernel=rbf ........\n[CV]  C=926.9787684096649, gamma=2.147979593060577, kernel=rbf, total=   5.6s\n[CV] C=33946.157064934, gamma=2.2642426492862313, kernel=linear ......\n[CV]  C=33946.157064934, gamma=2.2642426492862313, kernel=linear, total=  10.0s\n[CV] C=33946.157064934, gamma=2.2642426492862313, kernel=linear ......\n[CV]  C=33946.157064934, gamma=2.2642426492862313, kernel=linear, total=   9.7s\n[CV] C=33946.157064934, gamma=2.2642426492862313, kernel=linear ......\n[CV]  C=33946.157064934, gamma=2.2642426492862313, kernel=linear, total=   8.9s\n[CV] C=33946.157064934, gamma=2.2642426492862313, kernel=linear ......\n[CV]  C=33946.157064934, gamma=2.2642426492862313, kernel=linear, total=  10.4s\n[CV] C=33946.157064934, gamma=2.2642426492862313, kernel=linear ......\n[CV]  C=33946.157064934, gamma=2.2642426492862313, kernel=linear, total=   9.3s\n[CV] C=84789.82947739525, gamma=0.3176359085304841, kernel=linear ....\n[CV]  C=84789.82947739525, gamma=0.3176359085304841, kernel=linear, total=  25.8s\n[CV] C=84789.82947739525, gamma=0.3176359085304841, kernel=linear ....\n[CV]  C=84789.82947739525, gamma=0.3176359085304841, kernel=linear, total=  18.5s\n[CV] C=84789.82947739525, gamma=0.3176359085304841, kernel=linear ....\n[CV]  C=84789.82947739525, gamma=0.3176359085304841, kernel=linear, total=  28.3s\n[CV] C=84789.82947739525, gamma=0.3176359085304841, kernel=linear ....\n[CV]  C=84789.82947739525, gamma=0.3176359085304841, kernel=linear, total=  20.8s\n[CV] C=84789.82947739525, gamma=0.3176359085304841, kernel=linear ....\n[CV]  C=84789.82947739525, gamma=0.3176359085304841, kernel=linear, total=  15.6s\n\n\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    4.2s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done 250 out of 250 | elapsed: 44.0min finished\n\n\nRandomizedSearchCV(cv=5, estimator=SVR(), n_iter=50,\n                   param_distributions={'C': &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7f9bd002c790&gt;,\n                                        'gamma': &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7f9bd002cb10&gt;,\n                                        'kernel': ['linear', 'rbf']},\n                   random_state=42, scoring='neg_mean_squared_error',\n                   verbose=2)\n\n\nThe best model achieves the following score (evaluated using 5-fold cross validation):\n\nnegative_mse = rnd_search.best_score_\nrmse = np.sqrt(-negative_mse)\nrmse\n\n54767.960710084146\n\n\nNow this is much closer to the performance of the RandomForestRegressor (but not quite there yet). Let’s check the best hyperparameters found:\n\nrnd_search.best_params_\n\n{'C': 157055.10989448498, 'gamma': 0.26497040005002437, 'kernel': 'rbf'}\n\n\nThis time the search found a good set of hyperparameters for the RBF kernel. Randomized search tends to find better hyperparameters than grid search in the same amount of time.\nLet’s look at the exponential distribution we used, with scale=1.0. Note that some samples are much larger or smaller than 1.0, but when you look at the log of the distribution, you can see that most values are actually concentrated roughly in the range of exp(-2) to exp(+2), which is about 0.1 to 7.4.\n\nexpon_distrib = expon(scale=1.)\nsamples = expon_distrib.rvs(10000, random_state=42)\nplt.figure(figsize=(10, 4))\nplt.subplot(121)\nplt.title(\"Exponential distribution (scale=1.0)\")\nplt.hist(samples, bins=50)\nplt.subplot(122)\nplt.title(\"Log of this distribution\")\nplt.hist(np.log(samples), bins=50)\nplt.show()\n\n\n\n\nThe distribution we used for C looks quite different: the scale of the samples is picked from a uniform distribution within a given range, which is why the right graph, which represents the log of the samples, looks roughly constant. This distribution is useful when you don’t have a clue of what the target scale is:\n\nreciprocal_distrib = reciprocal(20, 200000)\nsamples = reciprocal_distrib.rvs(10000, random_state=42)\nplt.figure(figsize=(10, 4))\nplt.subplot(121)\nplt.title(\"Reciprocal distribution (scale=1.0)\")\nplt.hist(samples, bins=50)\nplt.subplot(122)\nplt.title(\"Log of this distribution\")\nplt.hist(np.log(samples), bins=50)\nplt.show()\n\n\n\n\nThe reciprocal distribution is useful when you have no idea what the scale of the hyperparameter should be (indeed, as you can see on the figure on the right, all scales are equally likely, within the given range), whereas the exponential distribution is best when you know (more or less) what the scale of the hyperparameter should be."
  },
  {
    "objectID": "posts/ML/ML_project.html#section-2",
    "href": "posts/ML/ML_project.html#section-2",
    "title": "Tools -ML sample project",
    "section": "3.",
    "text": "3.\nQuestion: Try adding a transformer in the preparation pipeline to select only the most important attributes.\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\ndef indices_of_top_k(arr, k):\n    return np.sort(np.argpartition(np.array(arr), -k)[-k:])\n\nclass TopFeatureSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, feature_importances, k):\n        self.feature_importances = feature_importances\n        self.k = k\n    def fit(self, X, y=None):\n        self.feature_indices_ = indices_of_top_k(self.feature_importances, self.k)\n        return self\n    def transform(self, X):\n        return X[:, self.feature_indices_]\n\nNote: this feature selector assumes that you have already computed the feature importances somehow (for example using a RandomForestRegressor). You may be tempted to compute them directly in the TopFeatureSelector’s fit() method, however this would likely slow down grid/randomized search since the feature importances would have to be computed for every hyperparameter combination (unless you implement some sort of cache).\nLet’s define the number of top features we want to keep:\n\nk = 5\n\nNow let’s look for the indices of the top k features:\n\ntop_k_feature_indices = indices_of_top_k(feature_importances, k)\ntop_k_feature_indices\n\narray([ 0,  1,  7,  9, 12])\n\n\n\nnp.array(attributes)[top_k_feature_indices]\n\narray(['longitude', 'latitude', 'median_income', 'pop_per_hhold',\n       'INLAND'], dtype='&lt;U18')\n\n\nLet’s double check that these are indeed the top k features:\n\nsorted(zip(feature_importances, attributes), reverse=True)[:k]\n\n[(0.36615898061813423, 'median_income'),\n (0.16478099356159054, 'INLAND'),\n (0.10879295677551575, 'pop_per_hhold'),\n (0.07334423551601243, 'longitude'),\n (0.06290907048262032, 'latitude')]\n\n\nLooking good… Now let’s create a new pipeline that runs the previously defined preparation pipeline, and adds top k feature selection:\n\npreparation_and_feature_selection_pipeline = Pipeline([\n    ('preparation', full_pipeline),\n    ('feature_selection', TopFeatureSelector(feature_importances, k))\n])\n\n\nhousing_prepared_top_k_features = preparation_and_feature_selection_pipeline.fit_transform(housing)\n\nLet’s look at the features of the first 3 instances:\n\nhousing_prepared_top_k_features[0:3]\n\narray([[-1.15604281,  0.77194962, -0.61493744, -0.08649871,  0.        ],\n       [-1.17602483,  0.6596948 ,  1.33645936, -0.03353391,  0.        ],\n       [ 1.18684903, -1.34218285, -0.5320456 , -0.09240499,  0.        ]])\n\n\nNow let’s double check that these are indeed the top k features:\n\nhousing_prepared[0:3, top_k_feature_indices]\n\narray([[-1.15604281,  0.77194962, -0.61493744, -0.08649871,  0.        ],\n       [-1.17602483,  0.6596948 ,  1.33645936, -0.03353391,  0.        ],\n       [ 1.18684903, -1.34218285, -0.5320456 , -0.09240499,  0.        ]])\n\n\nWorks great! :)"
  },
  {
    "objectID": "posts/ML/ML_project.html#section-3",
    "href": "posts/ML/ML_project.html#section-3",
    "title": "Tools -ML sample project",
    "section": "4.",
    "text": "4.\nQuestion: Try creating a single pipeline that does the full data preparation plus the final prediction.\n\nprepare_select_and_predict_pipeline = Pipeline([\n    ('preparation', full_pipeline),\n    ('feature_selection', TopFeatureSelector(feature_importances, k)),\n    ('svm_reg', SVR(**rnd_search.best_params_))\n])\n\n\nprepare_select_and_predict_pipeline.fit(housing, housing_labels)\n\nPipeline(steps=[('preparation',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('attribs_adder',\n                                                                   CombinedAttributesAdder()),\n                                                                  ('std_scaler',\n                                                                   StandardScaler())]),\n                                                  ['longitude', 'latitude',\n                                                   'housing_median_age',\n                                                   'total_rooms',\n                                                   'total_bedrooms',\n                                                   'population', 'households',\n                                                   'median_income']),\n                                                 ('cat', OneHotEncoder(...\n                 TopFeatureSelector(feature_importances=array([7.33442355e-02, 6.29090705e-02, 4.11437985e-02, 1.46726854e-02,\n       1.41064835e-02, 1.48742809e-02, 1.42575993e-02, 3.66158981e-01,\n       5.64191792e-02, 1.08792957e-01, 5.33510773e-02, 1.03114883e-02,\n       1.64780994e-01, 6.02803867e-05, 1.96041560e-03, 2.85647464e-03]),\n                                    k=5)),\n                ('svm_reg',\n                 SVR(C=157055.10989448498, gamma=0.26497040005002437))])\n\n\nLet’s try the full pipeline on a few instances:\n\nsome_data = housing.iloc[:4]\nsome_labels = housing_labels.iloc[:4]\n\nprint(\"Predictions:\\t\", prepare_select_and_predict_pipeline.predict(some_data))\nprint(\"Labels:\\t\\t\", list(some_labels))\n\nPredictions:     [203214.28978849 371846.88152572 173295.65441612  47328.3970888 ]\nLabels:      [286600.0, 340600.0, 196900.0, 46300.0]\n\n\nWell, the full pipeline seems to work fine. Of course, the predictions are not fantastic: they would be better if we used the best RandomForestRegressor that we found earlier, rather than the best SVR."
  },
  {
    "objectID": "posts/ML/ML_project.html#section-4",
    "href": "posts/ML/ML_project.html#section-4",
    "title": "Tools -ML sample project",
    "section": "5.",
    "text": "5.\nQuestion: Automatically explore some preparation options using GridSearchCV.\nWarning: the following cell may take close to 45 minutes to run, or more depending on your hardware.\n\nparam_grid = [{\n    'preparation__num__imputer__strategy': ['mean', 'median', 'most_frequent'],\n    'feature_selection__k': list(range(1, len(feature_importances) + 1))\n}]\n\ngrid_search_prep = GridSearchCV(prepare_select_and_predict_pipeline, param_grid, cv=5,\n                                scoring='neg_mean_squared_error', verbose=2)\ngrid_search_prep.fit(housing, housing_labels)\n\nFitting 5 folds for each of 48 candidates, totalling 240 fits\n[CV] feature_selection__k=1, preparation__num__imputer__strategy=mean \n[CV]  feature_selection__k=1, preparation__num__imputer__strategy=mean, total=   4.2s\n[CV] feature_selection__k=1, preparation__num__imputer__strategy=mean \n[CV]  feature_selection__k=1, preparation__num__imputer__strategy=mean, total=   5.2s\n[CV] feature_selection__k=1, preparation__num__imputer__strategy=mean \n[CV]  feature_selection__k=1, preparation__num__imputer__strategy=mean, total=   4.7s\n[CV] feature_selection__k=1, preparation__num__imputer__strategy=mean \n[CV]  feature_selection__k=1, preparation__num__imputer__strategy=mean, total=   4.7s\n[CV] feature_selection__k=1, preparation__num__imputer__strategy=mean \n[CV]  feature_selection__k=1, preparation__num__imputer__strategy=mean, total=   4.8s\n[CV] feature_selection__k=1, preparation__num__imputer__strategy=median \n[CV]  feature_selection__k=1, preparation__num__imputer__strategy=median, total=   5.1s\n[CV] feature_selection__k=1, preparation__num__imputer__strategy=median \n[CV]  feature_selection__k=1, preparation__num__imputer__strategy=median, total=   4.9s\n[CV] feature_selection__k=1, preparation__num__imputer__strategy=median \n[CV]  feature_selection__k=1, preparation__num__imputer__strategy=median, total=   4.7s\n[CV] feature_selection__k=1, preparation__num__imputer__strategy=median \n[CV]  feature_selection__k=1, preparation__num__imputer__strategy=median, total=   4.3s\n[CV] feature_selection__k=1, preparation__num__imputer__strategy=median \n[CV]  feature_selection__k=1, preparation__num__imputer__strategy=median, total=   4.2s\n[CV] feature_selection__k=1, preparation__num__imputer__strategy=most_frequent \n[CV]  feature_selection__k=1, preparation__num__imputer__strategy=most_frequent, total=   4.6s\n[CV] feature_selection__k=1, preparation__num__imputer__strategy=most_frequent \n[CV]  feature_selection__k=1, preparation__num__imputer__strategy=most_frequent, total=   4.3s\n[CV] feature_selection__k=1, preparation__num__imputer__strategy=most_frequent \n[CV]  feature_selection__k=1, preparation__num__imputer__strategy=most_frequent, total=   4.4s\n[CV] feature_selection__k=1, preparation__num__imputer__strategy=most_frequent \n[CV]  feature_selection__k=1, preparation__num__imputer__strategy=most_frequent, total=   4.7s\n[CV] feature_selection__k=1, preparation__num__imputer__strategy=most_frequent \n[CV]  feature_selection__k=1, preparation__num__imputer__strategy=most_frequent, total=   4.8s\n[CV] feature_selection__k=2, preparation__num__imputer__strategy=mean \n[CV]  feature_selection__k=2, preparation__num__imputer__strategy=mean, total=   4.8s\n&lt;&lt;414 more lines&gt;&gt;\n[CV] feature_selection__k=15, preparation__num__imputer__strategy=most_frequent \n[CV]  feature_selection__k=15, preparation__num__imputer__strategy=most_frequent, total=  15.8s\n[CV] feature_selection__k=15, preparation__num__imputer__strategy=most_frequent \n[CV]  feature_selection__k=15, preparation__num__imputer__strategy=most_frequent, total=  19.8s\n[CV] feature_selection__k=16, preparation__num__imputer__strategy=mean \n[CV]  feature_selection__k=16, preparation__num__imputer__strategy=mean, total=  17.9s\n[CV] feature_selection__k=16, preparation__num__imputer__strategy=mean \n[CV]  feature_selection__k=16, preparation__num__imputer__strategy=mean, total=  19.2s\n[CV] feature_selection__k=16, preparation__num__imputer__strategy=mean \n[CV]  feature_selection__k=16, preparation__num__imputer__strategy=mean, total=  18.2s\n[CV] feature_selection__k=16, preparation__num__imputer__strategy=mean \n[CV]  feature_selection__k=16, preparation__num__imputer__strategy=mean, total=  19.1s\n[CV] feature_selection__k=16, preparation__num__imputer__strategy=mean \n[CV]  feature_selection__k=16, preparation__num__imputer__strategy=mean, total=  16.4s\n[CV] feature_selection__k=16, preparation__num__imputer__strategy=median \n[CV]  feature_selection__k=16, preparation__num__imputer__strategy=median, total=  17.9s\n[CV] feature_selection__k=16, preparation__num__imputer__strategy=median \n[CV]  feature_selection__k=16, preparation__num__imputer__strategy=median, total=  19.2s\n[CV] feature_selection__k=16, preparation__num__imputer__strategy=median \n[CV]  feature_selection__k=16, preparation__num__imputer__strategy=median, total=  20.5s\n[CV] feature_selection__k=16, preparation__num__imputer__strategy=median \n[CV]  feature_selection__k=16, preparation__num__imputer__strategy=median, total=  17.1s\n[CV] feature_selection__k=16, preparation__num__imputer__strategy=median \n[CV]  feature_selection__k=16, preparation__num__imputer__strategy=median, total=  20.3s\n[CV] feature_selection__k=16, preparation__num__imputer__strategy=most_frequent \n[CV]  feature_selection__k=16, preparation__num__imputer__strategy=most_frequent, total=  16.7s\n[CV] feature_selection__k=16, preparation__num__imputer__strategy=most_frequent \n[CV]  feature_selection__k=16, preparation__num__imputer__strategy=most_frequent, total=  19.4s\n[CV] feature_selection__k=16, preparation__num__imputer__strategy=most_frequent \n[CV]  feature_selection__k=16, preparation__num__imputer__strategy=most_frequent, total=  17.2s\n[CV] feature_selection__k=16, preparation__num__imputer__strategy=most_frequent \n[CV]  feature_selection__k=16, preparation__num__imputer__strategy=most_frequent, total=  17.5s\n[CV] feature_selection__k=16, preparation__num__imputer__strategy=most_frequent \n[CV]  feature_selection__k=16, preparation__num__imputer__strategy=most_frequent, total=  19.1s\n\n\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    4.2s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done 240 out of 240 | elapsed: 42.3min finished\n\n\nGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('preparation',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         Pipeline(steps=[('imputer',\n                                                                                          SimpleImputer(strategy='median')),\n                                                                                         ('attribs_adder',\n                                                                                          CombinedAttributesAdder()),\n                                                                                         ('std_scaler',\n                                                                                          StandardScaler())]),\n                                                                         ['longitude',\n                                                                          'latitude',\n                                                                          'housing_median_age',\n                                                                          'total_rooms',\n                                                                          'total_bedrooms',\n                                                                          'population',\n                                                                          'households',\n                                                                          'median_inc...\n       5.64191792e-02, 1.08792957e-01, 5.33510773e-02, 1.03114883e-02,\n       1.64780994e-01, 6.02803867e-05, 1.96041560e-03, 2.85647464e-03]),\n                                                           k=5)),\n                                       ('svm_reg',\n                                        SVR(C=157055.10989448498,\n                                            gamma=0.26497040005002437))]),\n             param_grid=[{'feature_selection__k': [1, 2, 3, 4, 5, 6, 7, 8, 9,\n                                                   10, 11, 12, 13, 14, 15, 16],\n                          'preparation__num__imputer__strategy': ['mean',\n                                                                  'median',\n                                                                  'most_frequent']}],\n             scoring='neg_mean_squared_error', verbose=2)\n\n\n\ngrid_search_prep.best_params_\n\n{'feature_selection__k': 15,\n 'preparation__num__imputer__strategy': 'most_frequent'}\n\n\nThe best imputer strategy is most_frequent and apparently almost all features are useful (15 out of 16). The last one (ISLAND) seems to just add some noise.\nCongratulations! You already know quite a lot about Machine Learning. :)"
  },
  {
    "objectID": "posts/numpy/NumPy.html",
    "href": "posts/numpy/NumPy.html",
    "title": "Numpy_2",
    "section": "",
    "text": "numerical python - function description\nndarray- multidimensional array providing fast arithmetic operations\nmathematical functions\ntools for reading/writing\nlinear algebra, random number generation, fourier transformation\n\n\nimport numpy as np\n\n\nmy_arr = np.arange(1000000)\nmy_list = list(range(100000))\n\n\n%timeit my_arr2 = my_arr * 2\n\n3.91 ms ± 154 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n%timeit my_list2 = [x * 2 for x in my_list]\n\n12.3 ms ± 4.77 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\ndata  = np.array([[1.5, -0.1, 3], [0, -3, 6.5]])\n\n\ndata\n\narray([[ 1.5, -0.1,  3. ],\n       [ 0. , -3. ,  6.5]])\n\n\n\ndata * 10\n\narray([[ 15.,  -1.,  30.],\n       [  0., -30.,  65.]])\n\n\n\ndata + data\n\narray([[ 3. , -0.2,  6. ],\n       [ 0. , -6. , 13. ]])\n\n\n\ndata.shape\n\n(2, 3)\n\n\n\ndata.dtype\n\ndtype('float64')\n\n\n\n# creating arrays\n\ndata1 = [6, 7.5, 8, 0, 1]\n\narr1 = np.array(data1)\n\narr1\n\narray([6. , 7.5, 8. , 0. , 1. ])\n\n\n\ndata2 = [[1, 2, 3, 4], [5, 6, 7, 8]]\n\narr2 = np.array(data2)\n\narr2\n\narray([[1, 2, 3, 4],\n       [5, 6, 7, 8]])\n\n\n\narr2.ndim\n\n2\n\n\n\narr2.shape\n\n(2, 4)\n\n\n\narr1.dtype\n\ndtype('float64')\n\n\n\narr2.dtype\n\ndtype('int32')\n\n\n\nnp.zeros(10)\n\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n\n\n\nnp.ones((3, 6))\n\narray([[1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1.]])\n\n\nnp.empty((2, 3, 2))\n\nnp.arange(15)\n\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])\n\n\n\n\n\narr = np.array([1, 2, 3, 4, 5])\n\n\narr.dtype\n\ndtype('int32')\n\n\n\nfloat_arr = arr.astype(np.float64)\n\n\nfloat_arr.dtype\n\ndtype('float64')\n\n\n\nint_array = np.arange(10)\n\n\ncalibers = np.array([.22, .27, .357, .380, .44, .50], dtype = np.float64)\n\nint_array.astype(calibers.dtype)\n\narray([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])\n\n\n\n\n\n\narr = np.array([[1., 2., 3.], [4., 5., 6.]])\n\narr\n\narray([[1., 2., 3.],\n       [4., 5., 6.]])\n\n\n\narr * arr\n\narray([[ 1.,  4.,  9.],\n       [16., 25., 36.]])\n\n\n\narr + arr\n\narray([[ 2.,  4.,  6.],\n       [ 8., 10., 12.]])\n\n\n\n1 / arr\n\narray([[1.        , 0.5       , 0.33333333],\n       [0.25      , 0.2       , 0.16666667]])\n\n\n\narr ** 2\n\narray([[ 1.,  4.,  9.],\n       [16., 25., 36.]])\n\n\n\n# comparions between arrays yield boolean arrays\n\narr2 = np.array([[0, 4, 1], [7, 2., 12]])\n\n\narr2\n\narray([[ 0.,  4.,  1.],\n       [ 7.,  2., 12.]])\n\n\n\narr2 &gt; arr\n\narray([[False,  True, False],\n       [ True, False,  True]])\n\n\n\n\n\n\n\narr = np.arange(10)\n\narr\n\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\narr[5]\n\n5\n\n\n\narr[5:8]\n\narray([5, 6, 7])\n\n\n\narr[5:8]\n\narray([5, 6, 7])\n\n\n\narr\n\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\narr_slice= arr[5:8]\n\n\narr_slice\n\narray([5, 6, 7])\n\n\n\n # changing values\narr_slice[1] = 12345\n\narr\n\narray([    0,     1,     2,     3,     4,     5, 12345,     7,     8,\n           9])\n\n\n\narr_slice\n\narray([    5, 12345,     7])\n\n\n\n# bare slice\n\narr_slice[:] = 64   #assigns all values to the array\n\narr\n\narray([ 0,  1,  2,  3,  4, 64, 64, 64,  8,  9])\n\n\n\narr2d = np.array([[1,2,3], [4,5,6], [7,8,9]])\n\n\narr2d\n\narray([[[1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]]])\n\n\n\narr2d[1]\n\narray([4, 5, 6])\n\n\n\narr2d[0][2]  #first array third element\n\n3\n\n\n\narr2d[0,2]  # same result\n\n3\n\n\n\narr3d = np.array([[[1,2,3], [4,5,6]], [[7,8,9], [10, 11, 12]]])\n\narr3d\n\narray([[[ 1,  2,  3],\n        [ 4,  5,  6]],\n\n       [[ 7,  8,  9],\n        [10, 11, 12]]])\n\n\n\narr3d[0]\n\narray([[1, 2, 3],\n       [4, 5, 6]])\n\n\n\n# scalar and vector arays can be assigned to arr3d[0]\n\nold_values = arr3d[0].copy()\n\narr3d[0] = 42\n\n\narr3d\n\narray([[[42, 42, 42],\n        [42, 42, 42]],\n\n       [[ 7,  8,  9],\n        [10, 11, 12]]])\n\n\n\narr3d[0] = old_values\n\narr3d\n\narray([[[ 1,  2,  3],\n        [ 4,  5,  6]],\n\n       [[ 7,  8,  9],\n        [10, 11, 12]]])\n\n\n\narr3d[1,0]\n\narray([7, 8, 9])\n\n\n\nx = arr3d[1]\n\n\nx\n\narray([[ 7,  8,  9],\n       [10, 11, 12]])\n\n\n\nx[0]\n\narray([7, 8, 9])\n\n\n\n\n\n\narr\n\narray([ 0,  1,  2,  3,  4, 64, 64, 64,  8,  9])\n\n\n\narr[1:6]\n\narray([ 1,  2,  3,  4, 64])\n\n\n\n# slicing a 2d array\n\narr2d\n\narray([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n\n\n\narr2d[:2] #selects the first two rows\n\narray([[1, 2, 3],\n       [4, 5, 6]])\n\n\n\narr2d[:2, 1:] #selects first two rows and last two columns\n\narray([[2, 3],\n       [5, 6]])\n\n\n\nlower_dim_slice = arr2d[1, :2]\nlower_dim_slice\n\narray([4, 5])\n\n\n\nlower_dim_slice.shape\n\n(2,)\n\n\n\narr2d[:2, 2] #dots before selects rows before\n\narray([3, 6])\n\n\n\narr2d[:, :1] #all rows, first column\n\narray([[1],\n       [4],\n       [7]])\n\n\n\n# assigning value to the section\n\narr2d[:2, 1:] = 0\n\n\narr2d\n\narray([[1, 0, 0],\n       [4, 0, 0],\n       [7, 8, 9]])\n\n\n\n\n\n\nnames = np.array(['bob', 'joe', 'will','zhou', 'lu', 'wei' ])\n\nnames\n\narray(['bob', 'joe', 'will', 'zhou', 'lu', 'wei'], dtype='&lt;U4')\n\n\n\ndata = np.array([[4,7], [0,2], [-5, 6], [0, 0], [1, 2], [-12, -4], [3, 4]])\n                 \ndata                \n\narray([[  4,   7],\n       [  0,   2],\n       [ -5,   6],\n       [  0,   0],\n       [  1,   2],\n       [-12,  -4],\n       [  3,   4]])\n\n\n\ndata.shape\n\n(7, 2)\n\n\n\nnames.shape\n\n(6,)\n\n\n\n# let's check how many times wei's name come\n\nnames == 'wei'   #once\n\narray([False, False, False, False, False,  True])\n\n\n\ndata[names == \"wei\"]\n\nIndexError: boolean index did not match indexed array along dimension 0; dimension is 7 but corresponding boolean dimension is 6\n\n\n\n# adding a name so that the dimension becomes 7\nnames = np.append(names, 'rajwinder')\n\n\nnames\n\narray(['bob', 'joe', 'will', 'zhou', 'lu', 'wei', 'rajwinder',\n       'rajwinder'], dtype='&lt;U9')\n\n\n\n# deleting extra\n\nnames = np.delete(names, 7)\n\n\nnames\n\narray(['bob', 'joe', 'will', 'zhou', 'lu', 'wei', 'rajwinder'],\n      dtype='&lt;U9')\n\n\n\ndata[names == 'rajwinder']\n\narray([[3, 4]])\n\n\n\ndata[names == 'zhou']\n\narray([[0, 0]])\n\n\n\n\n\n\nindexing using integers\nindexing gets modified\n\n\narr = np.zeros((8, 4))\n\n\nfor i in range(8):\n    arr[i] = i\n\n\narr\n\narray([[0., 0., 0., 0.],\n       [1., 1., 1., 1.],\n       [2., 2., 2., 2.],\n       [3., 3., 3., 3.],\n       [4., 4., 4., 4.],\n       [5., 5., 5., 5.],\n       [6., 6., 6., 6.],\n       [7., 7., 7., 7.]])\n\n\n\n# selecting rows in particular order\narr[[4,3, 0, 6]]\n\narray([[4., 4., 4., 4.],\n       [3., 3., 3., 3.],\n       [0., 0., 0., 0.],\n       [6., 6., 6., 6.]])\n\n\n\n# using negative indices\narr[[-2, -4, -7]]\n\narray([[6., 6., 6., 6.],\n       [4., 4., 4., 4.],\n       [1., 1., 1., 1.]])\n\n\n\n# multiple array indexing\n\narr3 = np.arange(32).reshape((8, 4))\n\n\narr3\n\narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11],\n       [12, 13, 14, 15],\n       [16, 17, 18, 19],\n       [20, 21, 22, 23],\n       [24, 25, 26, 27],\n       [28, 29, 30, 31]])\n\n\n\n# selecting elements based on rows and columns\n\narr3[[1, 4, 7, 2], [0, 3, 2, 1]]\n\narray([ 4, 19, 30,  9])\n\n\n\n# selecting complete rows and decding sequence of elements\n\narr3[[1, 4, 7, 2]][:, [0, 3, 2, 1]]\n\narray([[ 4,  7,  6,  5],\n       [16, 19, 18, 17],\n       [28, 31, 30, 29],\n       [ 8, 11, 10,  9]])\n\n\n\n\n\n\narr = np.arange(15). reshape(3, 5)\narr\n\narray([[ 0,  1,  2,  3,  4],\n       [ 5,  6,  7,  8,  9],\n       [10, 11, 12, 13, 14]])\n\n\n\narr.T\n\narray([[ 0,  5, 10],\n       [ 1,  6, 11],\n       [ 2,  7, 12],\n       [ 3,  8, 13],\n       [ 4,  9, 14]])\n\n\n\n# used often for matrix computation\n\narr\n\narray([[ 0,  1,  2,  3,  4],\n       [ 5,  6,  7,  8,  9],\n       [10, 11, 12, 13, 14]])\n\n\n\n# multiplied two arrays\nnp.dot(arr.T, arr)\n\narray([[125, 140, 155, 170, 185],\n       [140, 158, 176, 194, 212],\n       [155, 176, 197, 218, 239],\n       [170, 194, 218, 242, 266],\n       [185, 212, 239, 266, 293]])\n\n\n\n# another way to do it\narr.T @ arr\n\narray([[125, 140, 155, 170, 185],\n       [140, 158, 176, 194, 212],\n       [155, 176, 197, 218, 239],\n       [170, 194, 218, 242, 266],\n       [185, 212, 239, 266, 293]])\n\n\n\narr\n\narray([[ 0,  1,  2,  3,  4],\n       [ 5,  6,  7,  8,  9],\n       [10, 11, 12, 13, 14]])\n\n\n\narr.swapaxes(0, 1)  # returns the view without making a copy\n\narray([[ 0,  5, 10],\n       [ 1,  6, 11],\n       [ 2,  7, 12],\n       [ 3,  8, 13],\n       [ 4,  9, 14]])"
  },
  {
    "objectID": "posts/numpy/NumPy.html#numpy",
    "href": "posts/numpy/NumPy.html#numpy",
    "title": "Numpy_2",
    "section": "",
    "text": "numerical python - function description\nndarray- multidimensional array providing fast arithmetic operations\nmathematical functions\ntools for reading/writing\nlinear algebra, random number generation, fourier transformation\n\n\nimport numpy as np\n\n\nmy_arr = np.arange(1000000)\nmy_list = list(range(100000))\n\n\n%timeit my_arr2 = my_arr * 2\n\n3.91 ms ± 154 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n%timeit my_list2 = [x * 2 for x in my_list]\n\n12.3 ms ± 4.77 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\ndata  = np.array([[1.5, -0.1, 3], [0, -3, 6.5]])\n\n\ndata\n\narray([[ 1.5, -0.1,  3. ],\n       [ 0. , -3. ,  6.5]])\n\n\n\ndata * 10\n\narray([[ 15.,  -1.,  30.],\n       [  0., -30.,  65.]])\n\n\n\ndata + data\n\narray([[ 3. , -0.2,  6. ],\n       [ 0. , -6. , 13. ]])\n\n\n\ndata.shape\n\n(2, 3)\n\n\n\ndata.dtype\n\ndtype('float64')\n\n\n\n# creating arrays\n\ndata1 = [6, 7.5, 8, 0, 1]\n\narr1 = np.array(data1)\n\narr1\n\narray([6. , 7.5, 8. , 0. , 1. ])\n\n\n\ndata2 = [[1, 2, 3, 4], [5, 6, 7, 8]]\n\narr2 = np.array(data2)\n\narr2\n\narray([[1, 2, 3, 4],\n       [5, 6, 7, 8]])\n\n\n\narr2.ndim\n\n2\n\n\n\narr2.shape\n\n(2, 4)\n\n\n\narr1.dtype\n\ndtype('float64')\n\n\n\narr2.dtype\n\ndtype('int32')\n\n\n\nnp.zeros(10)\n\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n\n\n\nnp.ones((3, 6))\n\narray([[1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1.]])\n\n\nnp.empty((2, 3, 2))\n\nnp.arange(15)\n\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])\n\n\n\n\n\narr = np.array([1, 2, 3, 4, 5])\n\n\narr.dtype\n\ndtype('int32')\n\n\n\nfloat_arr = arr.astype(np.float64)\n\n\nfloat_arr.dtype\n\ndtype('float64')\n\n\n\nint_array = np.arange(10)\n\n\ncalibers = np.array([.22, .27, .357, .380, .44, .50], dtype = np.float64)\n\nint_array.astype(calibers.dtype)\n\narray([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])\n\n\n\n\n\n\narr = np.array([[1., 2., 3.], [4., 5., 6.]])\n\narr\n\narray([[1., 2., 3.],\n       [4., 5., 6.]])\n\n\n\narr * arr\n\narray([[ 1.,  4.,  9.],\n       [16., 25., 36.]])\n\n\n\narr + arr\n\narray([[ 2.,  4.,  6.],\n       [ 8., 10., 12.]])\n\n\n\n1 / arr\n\narray([[1.        , 0.5       , 0.33333333],\n       [0.25      , 0.2       , 0.16666667]])\n\n\n\narr ** 2\n\narray([[ 1.,  4.,  9.],\n       [16., 25., 36.]])\n\n\n\n# comparions between arrays yield boolean arrays\n\narr2 = np.array([[0, 4, 1], [7, 2., 12]])\n\n\narr2\n\narray([[ 0.,  4.,  1.],\n       [ 7.,  2., 12.]])\n\n\n\narr2 &gt; arr\n\narray([[False,  True, False],\n       [ True, False,  True]])\n\n\n\n\n\n\n\narr = np.arange(10)\n\narr\n\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\narr[5]\n\n5\n\n\n\narr[5:8]\n\narray([5, 6, 7])\n\n\n\narr[5:8]\n\narray([5, 6, 7])\n\n\n\narr\n\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\narr_slice= arr[5:8]\n\n\narr_slice\n\narray([5, 6, 7])\n\n\n\n # changing values\narr_slice[1] = 12345\n\narr\n\narray([    0,     1,     2,     3,     4,     5, 12345,     7,     8,\n           9])\n\n\n\narr_slice\n\narray([    5, 12345,     7])\n\n\n\n# bare slice\n\narr_slice[:] = 64   #assigns all values to the array\n\narr\n\narray([ 0,  1,  2,  3,  4, 64, 64, 64,  8,  9])\n\n\n\narr2d = np.array([[1,2,3], [4,5,6], [7,8,9]])\n\n\narr2d\n\narray([[[1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]]])\n\n\n\narr2d[1]\n\narray([4, 5, 6])\n\n\n\narr2d[0][2]  #first array third element\n\n3\n\n\n\narr2d[0,2]  # same result\n\n3\n\n\n\narr3d = np.array([[[1,2,3], [4,5,6]], [[7,8,9], [10, 11, 12]]])\n\narr3d\n\narray([[[ 1,  2,  3],\n        [ 4,  5,  6]],\n\n       [[ 7,  8,  9],\n        [10, 11, 12]]])\n\n\n\narr3d[0]\n\narray([[1, 2, 3],\n       [4, 5, 6]])\n\n\n\n# scalar and vector arays can be assigned to arr3d[0]\n\nold_values = arr3d[0].copy()\n\narr3d[0] = 42\n\n\narr3d\n\narray([[[42, 42, 42],\n        [42, 42, 42]],\n\n       [[ 7,  8,  9],\n        [10, 11, 12]]])\n\n\n\narr3d[0] = old_values\n\narr3d\n\narray([[[ 1,  2,  3],\n        [ 4,  5,  6]],\n\n       [[ 7,  8,  9],\n        [10, 11, 12]]])\n\n\n\narr3d[1,0]\n\narray([7, 8, 9])\n\n\n\nx = arr3d[1]\n\n\nx\n\narray([[ 7,  8,  9],\n       [10, 11, 12]])\n\n\n\nx[0]\n\narray([7, 8, 9])\n\n\n\n\n\n\narr\n\narray([ 0,  1,  2,  3,  4, 64, 64, 64,  8,  9])\n\n\n\narr[1:6]\n\narray([ 1,  2,  3,  4, 64])\n\n\n\n# slicing a 2d array\n\narr2d\n\narray([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n\n\n\narr2d[:2] #selects the first two rows\n\narray([[1, 2, 3],\n       [4, 5, 6]])\n\n\n\narr2d[:2, 1:] #selects first two rows and last two columns\n\narray([[2, 3],\n       [5, 6]])\n\n\n\nlower_dim_slice = arr2d[1, :2]\nlower_dim_slice\n\narray([4, 5])\n\n\n\nlower_dim_slice.shape\n\n(2,)\n\n\n\narr2d[:2, 2] #dots before selects rows before\n\narray([3, 6])\n\n\n\narr2d[:, :1] #all rows, first column\n\narray([[1],\n       [4],\n       [7]])\n\n\n\n# assigning value to the section\n\narr2d[:2, 1:] = 0\n\n\narr2d\n\narray([[1, 0, 0],\n       [4, 0, 0],\n       [7, 8, 9]])\n\n\n\n\n\n\nnames = np.array(['bob', 'joe', 'will','zhou', 'lu', 'wei' ])\n\nnames\n\narray(['bob', 'joe', 'will', 'zhou', 'lu', 'wei'], dtype='&lt;U4')\n\n\n\ndata = np.array([[4,7], [0,2], [-5, 6], [0, 0], [1, 2], [-12, -4], [3, 4]])\n                 \ndata                \n\narray([[  4,   7],\n       [  0,   2],\n       [ -5,   6],\n       [  0,   0],\n       [  1,   2],\n       [-12,  -4],\n       [  3,   4]])\n\n\n\ndata.shape\n\n(7, 2)\n\n\n\nnames.shape\n\n(6,)\n\n\n\n# let's check how many times wei's name come\n\nnames == 'wei'   #once\n\narray([False, False, False, False, False,  True])\n\n\n\ndata[names == \"wei\"]\n\nIndexError: boolean index did not match indexed array along dimension 0; dimension is 7 but corresponding boolean dimension is 6\n\n\n\n# adding a name so that the dimension becomes 7\nnames = np.append(names, 'rajwinder')\n\n\nnames\n\narray(['bob', 'joe', 'will', 'zhou', 'lu', 'wei', 'rajwinder',\n       'rajwinder'], dtype='&lt;U9')\n\n\n\n# deleting extra\n\nnames = np.delete(names, 7)\n\n\nnames\n\narray(['bob', 'joe', 'will', 'zhou', 'lu', 'wei', 'rajwinder'],\n      dtype='&lt;U9')\n\n\n\ndata[names == 'rajwinder']\n\narray([[3, 4]])\n\n\n\ndata[names == 'zhou']\n\narray([[0, 0]])\n\n\n\n\n\n\nindexing using integers\nindexing gets modified\n\n\narr = np.zeros((8, 4))\n\n\nfor i in range(8):\n    arr[i] = i\n\n\narr\n\narray([[0., 0., 0., 0.],\n       [1., 1., 1., 1.],\n       [2., 2., 2., 2.],\n       [3., 3., 3., 3.],\n       [4., 4., 4., 4.],\n       [5., 5., 5., 5.],\n       [6., 6., 6., 6.],\n       [7., 7., 7., 7.]])\n\n\n\n# selecting rows in particular order\narr[[4,3, 0, 6]]\n\narray([[4., 4., 4., 4.],\n       [3., 3., 3., 3.],\n       [0., 0., 0., 0.],\n       [6., 6., 6., 6.]])\n\n\n\n# using negative indices\narr[[-2, -4, -7]]\n\narray([[6., 6., 6., 6.],\n       [4., 4., 4., 4.],\n       [1., 1., 1., 1.]])\n\n\n\n# multiple array indexing\n\narr3 = np.arange(32).reshape((8, 4))\n\n\narr3\n\narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11],\n       [12, 13, 14, 15],\n       [16, 17, 18, 19],\n       [20, 21, 22, 23],\n       [24, 25, 26, 27],\n       [28, 29, 30, 31]])\n\n\n\n# selecting elements based on rows and columns\n\narr3[[1, 4, 7, 2], [0, 3, 2, 1]]\n\narray([ 4, 19, 30,  9])\n\n\n\n# selecting complete rows and decding sequence of elements\n\narr3[[1, 4, 7, 2]][:, [0, 3, 2, 1]]\n\narray([[ 4,  7,  6,  5],\n       [16, 19, 18, 17],\n       [28, 31, 30, 29],\n       [ 8, 11, 10,  9]])\n\n\n\n\n\n\narr = np.arange(15). reshape(3, 5)\narr\n\narray([[ 0,  1,  2,  3,  4],\n       [ 5,  6,  7,  8,  9],\n       [10, 11, 12, 13, 14]])\n\n\n\narr.T\n\narray([[ 0,  5, 10],\n       [ 1,  6, 11],\n       [ 2,  7, 12],\n       [ 3,  8, 13],\n       [ 4,  9, 14]])\n\n\n\n# used often for matrix computation\n\narr\n\narray([[ 0,  1,  2,  3,  4],\n       [ 5,  6,  7,  8,  9],\n       [10, 11, 12, 13, 14]])\n\n\n\n# multiplied two arrays\nnp.dot(arr.T, arr)\n\narray([[125, 140, 155, 170, 185],\n       [140, 158, 176, 194, 212],\n       [155, 176, 197, 218, 239],\n       [170, 194, 218, 242, 266],\n       [185, 212, 239, 266, 293]])\n\n\n\n# another way to do it\narr.T @ arr\n\narray([[125, 140, 155, 170, 185],\n       [140, 158, 176, 194, 212],\n       [155, 176, 197, 218, 239],\n       [170, 194, 218, 242, 266],\n       [185, 212, 239, 266, 293]])\n\n\n\narr\n\narray([[ 0,  1,  2,  3,  4],\n       [ 5,  6,  7,  8,  9],\n       [10, 11, 12, 13, 14]])\n\n\n\narr.swapaxes(0, 1)  # returns the view without making a copy\n\narray([[ 0,  5, 10],\n       [ 1,  6, 11],\n       [ 2,  7, 12],\n       [ 3,  8, 13],\n       [ 4,  9, 14]])"
  },
  {
    "objectID": "posts/numpy/NumPy.html#pseudorandom-number-generation",
    "href": "posts/numpy/NumPy.html#pseudorandom-number-generation",
    "title": "Numpy_2",
    "section": "Pseudorandom number generation",
    "text": "Pseudorandom number generation\n\nsamples = np.random.standard_normal(size= (4, 4))\n\nsamples\n\narray([[ 0.26762709, -0.62405293,  0.67249719, -0.46023273],\n       [ 0.40611368, -0.01041362,  0.51275103, -1.95844566],\n       [ 0.90884576, -0.28283029,  0.47254105,  2.20649657],\n       [ 0.69228499, -0.31918775, -0.74474035,  0.28790593]])\n\n\n\nrng = np.random.default_rng(seed = 12334)\n\ndata = rng.standard_normal((2,3))\n\n\ntype(rng)\n\nnumpy.random._generator.Generator\n\n\n\nUniversal Functions : Fast Element-Wise Array Functions\n\nimport numpy as np\nfrom random import normalvariate\narr = np.arange(10)\n\n\nnp.sqrt(arr)\n\narray([0.        , 1.        , 1.41421356, 1.73205081, 2.        ,\n       2.23606798, 2.44948974, 2.64575131, 2.82842712, 3.        ])\n\n\n\nnp.exp(arr)\n\narray([1.00000000e+00, 2.71828183e+00, 7.38905610e+00, 2.00855369e+01,\n       5.45981500e+01, 1.48413159e+02, 4.03428793e+02, 1.09663316e+03,\n       2.98095799e+03, 8.10308393e+03])\n\n\n\nx = rng.standard_normal(8)\n\ny = rng.standard_normal(8)\n\nx\n\narray([-0.32357072, -1.8494368 , -1.89739205,  0.04315429,  1.01046514,\n       -0.73625393,  0.46616191, -0.09290374])\n\n\n\ny\n\narray([-0.12705798, -0.64476954, -0.62430977,  0.87432098,  1.55273649,\n       -1.53739177, -0.73752509,  0.41995739])\n\n\n\nnp.maximum(x, y)  #based on element wise comparison\n\narray([-0.12705798, -0.64476954, -0.62430977,  0.87432098,  1.55273649,\n       -0.73625393,  0.46616191,  0.41995739])\n\n\n\narr = rng.standard_normal(7) * 5\n\narr\n\narray([-1.61785359, -9.24718402, -9.48696026,  0.21577147,  5.05232568,\n       -3.68126964,  2.33080955])\n\n\n\nremainder, whole_part = np.modf(arr)\n\n\nremainder\n\narray([-0.61785359, -0.24718402, -0.48696026,  0.21577147,  0.05232568,\n       -0.68126964,  0.33080955])\n\n\n\nwhole_part\n\narray([-1., -9., -9.,  0.,  5., -3.,  2.])\n\n\n\narr\n\narray([-1.61785359, -9.24718402, -9.48696026,  0.21577147,  5.05232568,\n       -3.68126964,  2.33080955])\n\n\n\nout = np.zeros_like(arr)\n\n\nnp.add(arr, 1)\n\narray([-0.61785359, -8.24718402, -8.48696026,  1.21577147,  6.05232568,\n       -2.68126964,  3.33080955])\n\n\n\nnp.add(arr, 1, out= out)\n\narray([-0.61785359, -8.24718402, -8.48696026,  1.21577147,  6.05232568,\n       -2.68126964,  3.33080955])\n\n\n\nout\n\narray([-0.61785359, -8.24718402, -8.48696026,  1.21577147,  6.05232568,\n       -2.68126964,  3.33080955])\n\n\n\n\nArray oriented programming with Arrays\n\nvectorization (faster) than pure Python equivalents\n\n\npoints = np.arange(-5, 5, 0.01) #100 equally spaced points\n\nxs, ys = np.meshgrid(points, points)\n\n# numpy.meshgrid function takes two one-dimensional arrays and produces two two-dimensional matrices\n\n\nys\n\narray([[-5.  , -5.  , -5.  , ..., -5.  , -5.  , -5.  ],\n       [-4.99, -4.99, -4.99, ..., -4.99, -4.99, -4.99],\n       [-4.98, -4.98, -4.98, ..., -4.98, -4.98, -4.98],\n       ...,\n       [ 4.97,  4.97,  4.97, ...,  4.97,  4.97,  4.97],\n       [ 4.98,  4.98,  4.98, ...,  4.98,  4.98,  4.98],\n       [ 4.99,  4.99,  4.99, ...,  4.99,  4.99,  4.99]])\n\n\n\nz = np.sqrt (xs ** 2 + ys ** 2)\n\nz\n\narray([[7.07106781, 7.06400028, 7.05693985, ..., 7.04988652, 7.05693985,\n        7.06400028],\n       [7.06400028, 7.05692568, 7.04985815, ..., 7.04279774, 7.04985815,\n        7.05692568],\n       [7.05693985, 7.04985815, 7.04278354, ..., 7.03571603, 7.04278354,\n        7.04985815],\n       ...,\n       [7.04988652, 7.04279774, 7.03571603, ..., 7.0286414 , 7.03571603,\n        7.04279774],\n       [7.05693985, 7.04985815, 7.04278354, ..., 7.03571603, 7.04278354,\n        7.04985815],\n       [7.06400028, 7.05692568, 7.04985815, ..., 7.04279774, 7.04985815,\n        7.05692568]])\n\n\n\n# visualizations with 2-d arrays\n\nimport matplotlib.pyplot as plt\n\nplt.imshow(z, cmap = plt.cm.gray, extent = [-5, 5, -5, 5])\n\nplt.colorbar()\n\nplt.title(\"Image plot $\\sqrt{x^2 + y^2}$ for a grid of values\")\n\nText(0.5, 1.0, 'Image plot $\\\\sqrt{x^2 + y^2}$ for a grid of values')\n\n\n\n\n\n\nplt.close('all')\n\n\n\nExpressing Conditional Logic as Array Operations\n\nxarr = np.array([1.1, 1.2, 1.3, 1.4, 1.5])\n\nyarr = np.array([2.1, 2.2, 2.3, 2.4, 2.5])\n\ncond = np.array([True, False, True, True, False])\n\n\n# take value form xarr whenever true, otherwise take value from yarr\n\nresult = [(x if c else y)\n         for x, y, c in zip (xarr, yarr, cond)]\n\n\nresult\n\n[1.1, 2.2, 1.3, 1.4, 2.5]\n\n\n\n\nnumpy.where\n\n# numpy.where (replace all positive values with 2 and negative with -2)\n\narr = rng.standard_normal((4, 4))\n\narr\n\narray([[-0.09290374, -0.12705798, -0.64476954, -0.62430977],\n       [ 0.87432098,  1.55273649, -1.53739177, -0.73752509],\n       [ 0.41995739, -0.93658739,  0.62072248,  0.81057914],\n       [-0.21398203,  0.67748945, -1.54002066, -0.9638457 ]])\n\n\n\narr &gt; 0\n\narray([[False, False, False, False],\n       [ True,  True, False, False],\n       [ True, False,  True,  True],\n       [False,  True, False, False]])\n\n\n\nnp.where (arr  &gt; 0, 2, -2)\n\narray([[-2, -2, -2, -2],\n       [ 2,  2, -2, -2],\n       [ 2, -2,  2,  2],\n       [-2,  2, -2, -2]])\n\n\n\n# or set only the positive values to 2\n\nnp.where (arr &gt; 0, 2, arr)\n\narray([[-0.09290374, -0.12705798, -0.64476954, -0.62430977],\n       [ 2.        ,  2.        , -1.53739177, -0.73752509],\n       [ 2.        , -0.93658739,  2.        ,  2.        ],\n       [-0.21398203,  2.        , -1.54002066, -0.9638457 ]])\n\n\n\n\nmathematical and statistical methods\n\narr = rng.standard_normal ((5, 4))\n\narr\n\narray([[-0.64316368, -0.48860061, -1.41271857, -0.10120962],\n       [-0.70385422,  2.41319157, -0.54405393, -0.90339244],\n       [ 0.82712685, -0.62647321, -0.13480887,  0.03956079],\n       [ 0.56044129,  0.34237924, -0.6576538 ,  1.04696188],\n       [ 0.17595271, -1.13639865, -0.54922125,  0.70725439]])\n\n\n\narr.mean()\n\n-0.08943400646176203\n\n\n\nnp.mean(arr)\n\n-0.08943400646176203\n\n\n\narr.sum()\n\n-1.7886801292352406\n\n\n\narr.mean(axis = 1) # columns\n\narray([-0.66142312,  0.06547274,  0.02635139,  0.32303215, -0.2006032 ])\n\n\n\narr.sum(axis = 1)\n\narray([-2.64569248,  0.26189098,  0.10540556,  1.29212861, -0.8024128 ])\n\n\n\narr = np.array([0,1,2,3,4,5,6,7])\n\narr.cumsum()\n\narray([ 0,  1,  3,  6, 10, 15, 21, 28])\n\n\n\narr = np.array([[0,1,2], [3, 4, 5], [6, 7, 8]])\n\n\narr\n\narray([[0, 1, 2],\n       [3, 4, 5],\n       [6, 7, 8]])\n\n\n\n# arr.cumsum(axis = 0 ) computes the cumulative sum along rows\n\n# arr.sumsum (axis= 1) computes the sum along columns\n\narr.cumsum(axis = 0)\n\narray([[ 0,  1,  2],\n       [ 3,  5,  7],\n       [ 9, 12, 15]])\n\n\n\narr.cumsum(axis = 1)\n\narray([[ 0,  1,  3],\n       [ 3,  7, 12],\n       [ 6, 13, 21]])\n\n\n\n\nmethods for boolean arrays\n\narr = rng.standard_normal(100)\n\n(arr&gt; 0).sum()\n\n41\n\n\n\n(arr &lt;= 0).sum()   #all non-po\n\n59\n\n\n\n\nSorting\n\narr = rng.standard_normal(6)\n\narr\n\narray([ 0.81272428, -0.67629236,  0.09344394, -0.20621744,  0.10364886,\n        0.70966403])\n\n\n\narr.sort()\n\n\narr\n\narray([-0.67629236, -0.20621744,  0.09344394,  0.10364886,  0.70966403,\n        0.81272428])\n\n\n\narr = rng.standard_normal((5, 3))\n\narr\n\narray([[-1.58684863, -0.1143117 ,  2.38420916],\n       [-0.64811009,  1.31931176,  0.01123432],\n       [-0.90663373, -0.96531814,  0.46431808],\n       [ 0.52164015, -0.08486576, -0.98397298],\n       [ 0.09054187, -1.08417551, -0.48832961]])\n\n\n\narr.sort (axis = 0)  #sorts the values across columns\n\narr\n\narray([[-1.58684863, -1.08417551, -0.98397298],\n       [-0.96531814, -0.90663373, -0.48832961],\n       [-0.64811009, -0.1143117 ,  0.01123432],\n       [-0.08486576,  0.09054187,  0.46431808],\n       [ 0.52164015,  1.31931176,  2.38420916]])\n\n\n\narr.sort (axis = 1) \n\narr\n\narray([[-1.58684863, -1.08417551, -0.98397298],\n       [-0.96531814, -0.90663373, -0.48832961],\n       [-0.64811009, -0.1143117 ,  0.01123432],\n       [-0.08486576,  0.09054187,  0.46431808],\n       [ 0.52164015,  1.31931176,  2.38420916]])\n\n\n\narr2 = np.array([5, -10, 7, 1, 0, -3])\n\n\nsorted_arr2 = np.sort(arr2)\n\nsorted_arr2\n\narray([-10,  -3,   0,   1,   5,   7])\n\n\n\n\nunique and other set logic\n\nnames\n\narray(['bob', 'joe', 'will', 'zhou', 'lu', 'wei', 'rajwinder'],\n      dtype='&lt;U9')\n\n\n\nnp.unique(names)\n\narray(['bob', 'joe', 'lu', 'rajwinder', 'wei', 'will', 'zhou'],\n      dtype='&lt;U9')\n\n\n\nnp.append(names, 'lu')\n\narray(['bob', 'joe', 'will', 'zhou', 'lu', 'wei', 'rajwinder', 'lu'],\n      dtype='&lt;U9')\n\n\n\n# we've 'lu' twice now, let's see now unique\n\n# sorting done aswell\n\nnp.unique(names)\n\narray(['bob', 'joe', 'lu', 'rajwinder', 'wei', 'will', 'zhou'],\n      dtype='&lt;U9')\n\n\n\n# python alternative\n\nsorted(set(names))\n\n['bob', 'joe', 'lu', 'rajwinder', 'wei', 'will', 'zhou']\n\n\n\n\narray set operations\n\n# numpy.in1d for testing memebership of the values in one array\n\nvalues = np.array([6, 0,0,0,3,2])\n\nnp.in1d(values, [1,2,3])\n\narray([False, False, False, False,  True,  True])\n\n\n\n\nfile input and output\n\narr = np.arange(10)\n\n\nnp.save('some_array', arr)\n\n\nnp.load('some_array.npy')\n\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\n# save multiple arrays using  np.savez\nnp.savez('array_archive.npz', a = arr, b=arr)\n\n\narch = np.load(\"array_archive.npz\")\n\n\narch['b']\n\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\n# saving in compressed format\n\nnp.savez_compressed('arrays_compressed.npz', a= arr, b= arr)"
  },
  {
    "objectID": "posts/numpy/NumPy.html#linear-alzerba",
    "href": "posts/numpy/NumPy.html#linear-alzerba",
    "title": "Numpy_2",
    "section": "Linear Alzerba",
    "text": "Linear Alzerba\n\nx = np.array([[1, 2, 3], [4, 5, 6]])\n\ny = np.array([[6, 23], [-1, 7], [8,9]])\n\nx\n\narray([[1, 2, 3],\n       [4, 5, 6]])\n\n\n\ny\n\narray([[ 6, 23],\n       [-1,  7],\n       [ 8,  9]])\n\n\n\nx.dot(y)\n\narray([[ 28,  64],\n       [ 67, 181]])\n\n\n\n# equivalent to\n\nnp.dot(x, y)\n\narray([[ 28,  64],\n       [ 67, 181]])\n\n\n\n# product of 1d and 2d array\nx @ np.ones(3)\n\narray([ 6., 15.])\n\n\n\n# numpy.linalg (matrix decompositions)\n\nfrom numpy.linalg import inv, qr\n\nX = rng.standard_normal((5, 5))\n\nmat = X.T @ X\n\n\nmat\n\narray([[ 5.79511464, -3.30831545, -2.66542844, -0.61858429, -4.34315368],\n       [-3.30831545,  6.04913293,  1.09484984, -0.88187098,  3.79344801],\n       [-2.66542844,  1.09484984,  3.59693921, -0.10949232,  1.50109261],\n       [-0.61858429, -0.88187098, -0.10949232,  0.68764721,  0.24806815],\n       [-4.34315368,  3.79344801,  1.50109261,  0.24806815,  4.09980802]])\n\n\n\ninv(mat)\n\narray([[ 1.95391205,  0.4259796 ,  0.86161239,  1.99396982,  1.23962108],\n       [ 0.4259796 ,  1.84110512,  0.55359754,  3.43225775, -1.66263314],\n       [ 0.86161239,  0.55359754,  0.79117237,  1.60608307,  0.01366661],\n       [ 1.99396982,  3.43225775,  1.60608307,  8.69084511, -2.17736422],\n       [ 1.23962108, -1.66263314,  0.01366661, -2.17736422,  3.22224774]])\n\n\n\nmat @ inv(mat)\n\narray([[ 1.00000000e+00,  7.37690538e-17,  8.63526934e-17,\n         2.45602532e-16,  5.57110698e-16],\n       [ 3.59505366e-17,  1.00000000e+00, -1.43602651e-16,\n         1.56181454e-15, -8.26684003e-16],\n       [-2.51848975e-16, -9.56323491e-18,  1.00000000e+00,\n        -7.81952475e-16, -4.32942875e-16],\n       [-1.22081410e-16,  5.77266093e-17, -3.23653576e-16,\n         1.00000000e+00, -9.26541377e-18],\n       [-4.93401745e-16,  1.63171237e-15, -7.64319458e-17,\n         1.26774536e-15,  1.00000000e+00]])"
  },
  {
    "objectID": "posts/numpy/NumPy.html#random-walks",
    "href": "posts/numpy/NumPy.html#random-walks",
    "title": "Numpy_2",
    "section": "Random walks",
    "text": "Random walks\n\n# with python\n\nimport random\nposition = 0\nwalk = [position]\nnsteps = 1000\n\nfor _ in range(nsteps):\n    step = 1 if random.randint(0, 1) else -1\n    position += step\n    walk.append (position)\n\n\nplt.plot(walk[:100])\n\n\n\n\n\n# with numpy\n\nnsteps = 1000\n\nrng = np.random.default_rng (seed = 12345)\n\ndraws = rng.integers(0, 2, size= nsteps)\nsteps = np.where(draws == 0, 1, -1)\n\nwalk = steps.cumsum()\n\n\nwalk.min()\n\n-8\n\n\n\nwalk.max()\n\n50\n\n\n\n(np.abs(walk) &gt;= 10).argmax()\n\n155\n\n\n\n# simulting many random walks at once with numpy\n\n\nnwalks = 5000\n\nnsteps = 1000\n\ndraws = rng.integers(0, 2, size = (nwalks, nsteps))\n\nsteps = np.where(draws &gt; 0, 1, -1)\n\nwalks = steps.cumsum(axis = 1)\n\nwalks\n\narray([[  1,   2,   1, ..., -24, -25, -26],\n       [ -1,   0,  -1, ...,  -2,  -1,   0],\n       [  1,   0,   1, ..., -22, -23, -24],\n       ...,\n       [  1,   0,   1, ...,   0,   1,   0],\n       [ -1,  -2,  -3, ...,  78,  77,  78],\n       [  1,   2,   1, ..., -42, -41, -40]])\n\n\n\nwalks.max()\n\n143\n\n\n\nwalks.min()\n\n-125\n\n\n\n# any method to check for details\n\nhits30 = (np.abs(walks) &gt;=30).any(axis = 1)\n\nhits30\n\narray([ True, False,  True, ..., False,  True,  True])\n\n\n\nhits30.sum()\n\n3314\n\n\n\ncrossing_times = (np.abs(walks[hits30]) &gt;= 30).argmax(axis = 1)\n\ncrossing_times\n\narray([897, 187, 607, ..., 497, 363, 337], dtype=int64)\n\n\n\n# average minn"
  },
  {
    "objectID": "posts/pandas/Pandas.html",
    "href": "posts/pandas/Pandas.html",
    "title": "Pandas",
    "section": "",
    "text": "import pandas as pd\n\n2 core objects- - DataFrame - array of inidividual entries (contains row and column)\n\nkeys = ‘column names’, values = list of entries\n\n\nrows = Index\n\n\nSeries- sequence of data values\n\n\ndon’t have any column name\n\n\nrow names defined by index parameter aswell\n\n\n#DataFrame_integer\npd.DataFrame({'Yes' : [390, 233], 'No' : [1,23]})\n\n\n\n\n\n\n\n\nYes\nNo\n\n\n\n\n0\n390\n1\n\n\n1\n233\n23\n\n\n\n\n\n\n\n\n# DataFrame_Strings\npd.DataFrame({'Suzaine': ['I liked chocolate', 'Lets have some fun'], \n              'Marie': ['butterscotch worked fine', 'wow, its raining']},\n            index = ['topic_1', 'topic_2'])\n\n\n\n\n\n\n\n\nSuzaine\nMarie\n\n\n\n\ntopic_1\nI liked chocolate\nbutterscotch worked fine\n\n\ntopic_2\nLets have some fun\nwow, its raining\n\n\n\n\n\n\n\n\n# series\npd.Series([1, 2, 3], \nindex= ['2014_sales', '2015_sales', '2016_sales'], \nname = 'Product A')\n\n2014_sales    1\n2015_sales    2\n2016_sales    3\nName: Product A, dtype: int64\n\n\n\n# example\nDinner = pd.Series(['4 cups', '1 cup', '2 large', '1 can'], \n         index = ['Flour', 'Milk', 'Eggs', 'Spam'], \n         name = 'Dinner')\nprint(Dinner)\n\nFlour     4 cups\nMilk       1 cup\nEggs     2 large\nSpam       1 can\nName: Dinner, dtype: object\n\n\n\n\n\n\nDinner.to_csv(\"Dinner.csv\")\n\n\n\n\n\nreactions = pd.read_csv('Reactions.csv')\nprint(reactions.shape)\n\n(25553, 5)\n\n\n\nprint(reactions.head())\n\n   Unnamed: 0                            Content ID  \\\n0           0  97522e57-d9ab-4bd6-97bf-c24d952602d2   \n1           1  97522e57-d9ab-4bd6-97bf-c24d952602d2   \n2           2  97522e57-d9ab-4bd6-97bf-c24d952602d2   \n3           3  97522e57-d9ab-4bd6-97bf-c24d952602d2   \n4           4  97522e57-d9ab-4bd6-97bf-c24d952602d2   \n\n                                User ID     Type             Datetime  \n0                                   NaN      NaN  2021-04-22 15:17:15  \n1  5d454588-283d-459d-915d-c48a2cb4c27f  disgust  2020-11-07 09:43:50  \n2  92b87fa5-f271-43e0-af66-84fac21052e6  dislike  2021-06-17 12:22:51  \n3  163daa38-8b77-48c9-9af6-37a6c1447ac2   scared  2021-04-18 05:13:58  \n4  34e8add9-0206-47fd-a501-037b994650a2  disgust  2021-01-06 19:13:01"
  },
  {
    "objectID": "posts/pandas/Pandas.html#creating-reading-and-writing",
    "href": "posts/pandas/Pandas.html#creating-reading-and-writing",
    "title": "Pandas",
    "section": "",
    "text": "import pandas as pd\n\n2 core objects- - DataFrame - array of inidividual entries (contains row and column)\n\nkeys = ‘column names’, values = list of entries\n\n\nrows = Index\n\n\nSeries- sequence of data values\n\n\ndon’t have any column name\n\n\nrow names defined by index parameter aswell\n\n\n#DataFrame_integer\npd.DataFrame({'Yes' : [390, 233], 'No' : [1,23]})\n\n\n\n\n\n\n\n\nYes\nNo\n\n\n\n\n0\n390\n1\n\n\n1\n233\n23\n\n\n\n\n\n\n\n\n# DataFrame_Strings\npd.DataFrame({'Suzaine': ['I liked chocolate', 'Lets have some fun'], \n              'Marie': ['butterscotch worked fine', 'wow, its raining']},\n            index = ['topic_1', 'topic_2'])\n\n\n\n\n\n\n\n\nSuzaine\nMarie\n\n\n\n\ntopic_1\nI liked chocolate\nbutterscotch worked fine\n\n\ntopic_2\nLets have some fun\nwow, its raining\n\n\n\n\n\n\n\n\n# series\npd.Series([1, 2, 3], \nindex= ['2014_sales', '2015_sales', '2016_sales'], \nname = 'Product A')\n\n2014_sales    1\n2015_sales    2\n2016_sales    3\nName: Product A, dtype: int64\n\n\n\n# example\nDinner = pd.Series(['4 cups', '1 cup', '2 large', '1 can'], \n         index = ['Flour', 'Milk', 'Eggs', 'Spam'], \n         name = 'Dinner')\nprint(Dinner)\n\nFlour     4 cups\nMilk       1 cup\nEggs     2 large\nSpam       1 can\nName: Dinner, dtype: object\n\n\n\n\n\n\nDinner.to_csv(\"Dinner.csv\")\n\n\n\n\n\nreactions = pd.read_csv('Reactions.csv')\nprint(reactions.shape)\n\n(25553, 5)\n\n\n\nprint(reactions.head())\n\n   Unnamed: 0                            Content ID  \\\n0           0  97522e57-d9ab-4bd6-97bf-c24d952602d2   \n1           1  97522e57-d9ab-4bd6-97bf-c24d952602d2   \n2           2  97522e57-d9ab-4bd6-97bf-c24d952602d2   \n3           3  97522e57-d9ab-4bd6-97bf-c24d952602d2   \n4           4  97522e57-d9ab-4bd6-97bf-c24d952602d2   \n\n                                User ID     Type             Datetime  \n0                                   NaN      NaN  2021-04-22 15:17:15  \n1  5d454588-283d-459d-915d-c48a2cb4c27f  disgust  2020-11-07 09:43:50  \n2  92b87fa5-f271-43e0-af66-84fac21052e6  dislike  2021-06-17 12:22:51  \n3  163daa38-8b77-48c9-9af6-37a6c1447ac2   scared  2021-04-18 05:13:58  \n4  34e8add9-0206-47fd-a501-037b994650a2  disgust  2021-01-06 19:13:01"
  },
  {
    "objectID": "posts/pandas/Pandas.html#indexing-selecting-and-assigning",
    "href": "posts/pandas/Pandas.html#indexing-selecting-and-assigning",
    "title": "Pandas",
    "section": "Indexing, Selecting and Assigning",
    "text": "Indexing, Selecting and Assigning\n\ndata = pd.read_csv(\"winemag-data-130k-v2.csv\")\npd.set_option('display.max_rows', 5)\nprint(data.head())\n\n   Unnamed: 0   country                                        description  \\\n0           0     Italy  Aromas include tropical fruit, broom, brimston...   \n1           1  Portugal  This is ripe and fruity, a wine that is smooth...   \n2           2        US  Tart and snappy, the flavors of lime flesh and...   \n3           3        US  Pineapple rind, lemon pith and orange blossom ...   \n4           4        US  Much like the regular bottling from 2012, this...   \n\n                          designation  points  price           province  \\\n0                        Vulkà Bianco      87    NaN  Sicily & Sardinia   \n1                            Avidagos      87   15.0              Douro   \n2                                 NaN      87   14.0             Oregon   \n3                Reserve Late Harvest      87   13.0           Michigan   \n4  Vintner's Reserve Wild Child Block      87   65.0             Oregon   \n\n              region_1           region_2         taster_name  \\\n0                 Etna                NaN       Kerin O’Keefe   \n1                  NaN                NaN          Roger Voss   \n2    Willamette Valley  Willamette Valley        Paul Gregutt   \n3  Lake Michigan Shore                NaN  Alexander Peartree   \n4    Willamette Valley  Willamette Valley        Paul Gregutt   \n\n  taster_twitter_handle                                              title  \\\n0          @kerinokeefe                  Nicosia 2013 Vulkà Bianco  (Etna)   \n1            @vossroger      Quinta dos Avidagos 2011 Avidagos Red (Douro)   \n2           @paulgwine       Rainstorm 2013 Pinot Gris (Willamette Valley)   \n3                   NaN  St. Julian 2013 Reserve Late Harvest Riesling ...   \n4           @paulgwine   Sweet Cheeks 2012 Vintner's Reserve Wild Child...   \n\n          variety               winery  \n0     White Blend              Nicosia  \n1  Portuguese Red  Quinta dos Avidagos  \n2      Pinot Gris            Rainstorm  \n3        Riesling           St. Julian  \n4      Pinot Noir         Sweet Cheeks  \n\n\n\nprint(data.columns)\n\nIndex(['Unnamed: 0', 'country', 'description', 'designation', 'points',\n       'price', 'province', 'region_1', 'region_2', 'taster_name',\n       'taster_twitter_handle', 'title', 'variety', 'winery'],\n      dtype='object')\n\n\n\nprint(data.country)\n\n0            Italy\n1         Portugal\n            ...   \n129969      France\n129970      France\nName: country, Length: 129971, dtype: object\n\n\n\nprint(data['country'])  #handles reserved characters\n\n0            Italy\n1         Portugal\n            ...   \n129969      France\n129970      France\nName: country, Length: 129971, dtype: object\n\n\n\nprint(data['country'][4])\n\nUS\n\n\n\nIndexing\n\nindex based or numerical position based (.iloc operator used)\n\n   - python's std. library appraoch (0:10 selects 0, 1, ...9)\n\nlabel based or value based (.loc operator used)\n\n    -indexes inclusively. So 0:10 will select entries 0,...,10\n\n# selecting first row\ndata.iloc[0]\n\nUnnamed: 0              0\ncountry             Italy\n                 ...     \nvariety       White Blend\nwinery            Nicosia\nName: 0, Length: 14, dtype: object\n\n\n\ndata.iloc[:3, 1]\n\n0       Italy\n1    Portugal\n2          US\nName: country, dtype: object\n\n\n\ndata.iloc[-5:] #selecting last 5 rows, plus all columns\n\n\n\n\n\n\n\n\nUnnamed: 0\ncountry\ndescription\ndesignation\npoints\nprice\nprovince\nregion_1\nregion_2\ntaster_name\ntaster_twitter_handle\ntitle\nvariety\nwinery\n\n\n\n\n129966\n129966\nGermany\nNotes of honeysuckle and cantaloupe sweeten th...\nBrauneberger Juffer-Sonnenuhr Spätlese\n90\n28.0\nMosel\nNaN\nNaN\nAnna Lee C. Iijima\nNaN\nDr. H. Thanisch (Erben Müller-Burggraef) 2013 ...\nRiesling\nDr. H. Thanisch (Erben Müller-Burggraef)\n\n\n129967\n129967\nUS\nCitation is given as much as a decade of bottl...\nNaN\n90\n75.0\nOregon\nOregon\nOregon Other\nPaul Gregutt\n@paulgwine\nCitation 2004 Pinot Noir (Oregon)\nPinot Noir\nCitation\n\n\n129968\n129968\nFrance\nWell-drained gravel soil gives this wine its c...\nKritt\n90\n30.0\nAlsace\nAlsace\nNaN\nRoger Voss\n@vossroger\nDomaine Gresser 2013 Kritt Gewurztraminer (Als...\nGewürztraminer\nDomaine Gresser\n\n\n129969\n129969\nFrance\nA dry style of Pinot Gris, this is crisp with ...\nNaN\n90\n32.0\nAlsace\nAlsace\nNaN\nRoger Voss\n@vossroger\nDomaine Marcel Deiss 2012 Pinot Gris (Alsace)\nPinot Gris\nDomaine Marcel Deiss\n\n\n129970\n129970\nFrance\nBig, rich and off-dry, this is powered by inte...\nLieu-dit Harth Cuvée Caroline\n90\n21.0\nAlsace\nAlsace\nNaN\nRoger Voss\n@vossroger\nDomaine Schoffit 2012 Lieu-dit Harth Cuvée Car...\nGewürztraminer\nDomaine Schoffit\n\n\n\n\n\n\n\n\ndata.loc[:, ['taster_name', 'variety', 'winery']]\n\n\n\n\n\n\n\n\ntaster_name\nvariety\nwinery\n\n\n\n\n0\nKerin O’Keefe\nWhite Blend\nNicosia\n\n\n1\nRoger Voss\nPortuguese Red\nQuinta dos Avidagos\n\n\n...\n...\n...\n...\n\n\n129969\nRoger Voss\nPinot Gris\nDomaine Marcel Deiss\n\n\n129970\nRoger Voss\nGewürztraminer\nDomaine Schoffit\n\n\n\n\n129971 rows × 3 columns\n\n\n\n\n\nManipulating the index\n\ndata.set_index('title') #now first column is title\n\n\n\n\n\n\n\n\nUnnamed: 0\ncountry\ndescription\ndesignation\npoints\nprice\nprovince\nregion_1\nregion_2\ntaster_name\ntaster_twitter_handle\nvariety\nwinery\n\n\ntitle\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNicosia 2013 Vulkà Bianco (Etna)\n0\nItaly\nAromas include tropical fruit, broom, brimston...\nVulkà Bianco\n87\nNaN\nSicily & Sardinia\nEtna\nNaN\nKerin O’Keefe\n@kerinokeefe\nWhite Blend\nNicosia\n\n\nQuinta dos Avidagos 2011 Avidagos Red (Douro)\n1\nPortugal\nThis is ripe and fruity, a wine that is smooth...\nAvidagos\n87\n15.0\nDouro\nNaN\nNaN\nRoger Voss\n@vossroger\nPortuguese Red\nQuinta dos Avidagos\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nDomaine Marcel Deiss 2012 Pinot Gris (Alsace)\n129969\nFrance\nA dry style of Pinot Gris, this is crisp with ...\nNaN\n90\n32.0\nAlsace\nAlsace\nNaN\nRoger Voss\n@vossroger\nPinot Gris\nDomaine Marcel Deiss\n\n\nDomaine Schoffit 2012 Lieu-dit Harth Cuvée Caroline Gewurztraminer (Alsace)\n129970\nFrance\nBig, rich and off-dry, this is powered by inte...\nLieu-dit Harth Cuvée Caroline\n90\n21.0\nAlsace\nAlsace\nNaN\nRoger Voss\n@vossroger\nGewürztraminer\nDomaine Schoffit\n\n\n\n\n129971 rows × 13 columns\n\n\n\n\n# conditional selection\n# selects data with US in columns names for countries\ndata.loc[data.country == 'US']\n\n\n\n\n\n\n\n\nUnnamed: 0\ncountry\ndescription\ndesignation\npoints\nprice\nprovince\nregion_1\nregion_2\ntaster_name\ntaster_twitter_handle\ntitle\nvariety\nwinery\n\n\n\n\n2\n2\nUS\nTart and snappy, the flavors of lime flesh and...\nNaN\n87\n14.0\nOregon\nWillamette Valley\nWillamette Valley\nPaul Gregutt\n@paulgwine\nRainstorm 2013 Pinot Gris (Willamette Valley)\nPinot Gris\nRainstorm\n\n\n3\n3\nUS\nPineapple rind, lemon pith and orange blossom ...\nReserve Late Harvest\n87\n13.0\nMichigan\nLake Michigan Shore\nNaN\nAlexander Peartree\nNaN\nSt. Julian 2013 Reserve Late Harvest Riesling ...\nRiesling\nSt. Julian\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n129952\n129952\nUS\nThis Zinfandel from the eastern section of Nap...\nNaN\n90\n22.0\nCalifornia\nChiles Valley\nNapa\nVirginie Boone\n@vboone\nHoudini 2011 Zinfandel (Chiles Valley)\nZinfandel\nHoudini\n\n\n129967\n129967\nUS\nCitation is given as much as a decade of bottl...\nNaN\n90\n75.0\nOregon\nOregon\nOregon Other\nPaul Gregutt\n@paulgwine\nCitation 2004 Pinot Noir (Oregon)\nPinot Noir\nCitation\n\n\n\n\n54504 rows × 14 columns\n\n\n\n\n# selecting particular rows\nindices = [1, 2, 3, 5, 8]\nsample_rows = data.loc[indices]\nprint(sample_rows)\n\n   Unnamed: 0   country                                        description  \\\n1           1  Portugal  This is ripe and fruity, a wine that is smooth...   \n2           2        US  Tart and snappy, the flavors of lime flesh and...   \n3           3        US  Pineapple rind, lemon pith and orange blossom ...   \n5           5     Spain  Blackberry and raspberry aromas show a typical...   \n8           8   Germany  Savory dried thyme notes accent sunnier flavor...   \n\n            designation  points  price        province             region_1  \\\n1              Avidagos      50   15.0           Douro                  NaN   \n2                   NaN      50   14.0          Oregon    Willamette Valley   \n3  Reserve Late Harvest      50   13.0        Michigan  Lake Michigan Shore   \n5          Ars In Vitro      50   15.0  Northern Spain              Navarra   \n8                 Shine      50   12.0     Rheinhessen                  NaN   \n\n            region_2         taster_name taster_twitter_handle  \\\n1                NaN          Roger Voss            @vossroger   \n2  Willamette Valley        Paul Gregutt           @paulgwine    \n3                NaN  Alexander Peartree                   NaN   \n5                NaN   Michael Schachner           @wineschach   \n8                NaN  Anna Lee C. Iijima                   NaN   \n\n                                               title             variety  \\\n1      Quinta dos Avidagos 2011 Avidagos Red (Douro)      Portuguese Red   \n2      Rainstorm 2013 Pinot Gris (Willamette Valley)          Pinot Gris   \n3  St. Julian 2013 Reserve Late Harvest Riesling ...            Riesling   \n5  Tandem 2011 Ars In Vitro Tempranillo-Merlot (N...  Tempranillo-Merlot   \n8  Heinz Eifel 2013 Shine Gewürztraminer (Rheinhe...      Gewürztraminer   \n\n                winery  \n1  Quinta dos Avidagos  \n2            Rainstorm  \n3           St. Julian  \n5               Tandem  \n8          Heinz Eifel  \n\n\n\n# selecting costly wines from US\ndata.loc[(data.country == 'US') & (data.price &gt;= 75)]\n\n\n\n\n\n\n\n\nUnnamed: 0\ncountry\ndescription\ndesignation\npoints\nprice\nprovince\nregion_1\nregion_2\ntaster_name\ntaster_twitter_handle\ntitle\nvariety\nwinery\n\n\n\n\n60\n60\nUS\nSyrupy and dense, this wine is jammy in plum a...\nEstate\n86\n100.0\nCalifornia\nNapa Valley\nNapa\nVirginie Boone\n@vboone\nOkapi 2013 Estate Cabernet Sauvignon (Napa Val...\nCabernet Sauvignon\nOkapi\n\n\n73\n73\nUS\nJuicy plum, raspberry and pencil lead lead the...\nBella Vetta Vineyard\n86\n75.0\nCalifornia\nHowell Mountain\nNapa\nVirginie Boone\n@vboone\nHindsight 2013 Bella Vetta Vineyard Cabernet S...\nCabernet Sauvignon\nHindsight\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n129919\n129919\nUS\nThis ripe, rich, almost decadently thick wine ...\nReserve\n91\n105.0\nWashington\nWalla Walla Valley (WA)\nColumbia Valley\nPaul Gregutt\n@paulgwine\nNicholas Cole Cellars 2004 Reserve Red (Walla ...\nRed Blend\nNicholas Cole Cellars\n\n\n129967\n129967\nUS\nCitation is given as much as a decade of bottl...\nNaN\n90\n75.0\nOregon\nOregon\nOregon Other\nPaul Gregutt\n@paulgwine\nCitation 2004 Pinot Noir (Oregon)\nPinot Noir\nCitation\n\n\n\n\n3629 rows × 14 columns\n\n\n\n\n# wines from Australia and New Zealand \ndata.loc[\n    (data.country.isin(['Australia', 'New Zealand']))\n]\n\n\n\n\n\n\n\n\nUnnamed: 0\ncountry\ndescription\ndesignation\npoints\nprice\nprovince\nregion_1\nregion_2\ntaster_name\ntaster_twitter_handle\ntitle\nvariety\nwinery\n\n\n\n\n77\n77\nAustralia\nThis medium-bodied Chardonnay features aromas ...\nMade With Organic Grapes\n50\n18.0\nSouth Australia\nSouth Australia\nNaN\nJoe Czerwinski\n@JoeCz\nYalumba 2016 Made With Organic Grapes Chardonn...\nChardonnay\nYalumba\n\n\n83\n83\nAustralia\nPale copper in hue, this wine exudes passion f...\nJester Sangiovese\n50\n20.0\nSouth Australia\nMcLaren Vale\nNaN\nJoe Czerwinski\n@JoeCz\nMitolo 2016 Jester Sangiovese Rosé (McLaren Vale)\nRosé\nMitolo\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n129956\n129956\nNew Zealand\nThe blend is 44% Merlot, 33% Cabernet Sauvigno...\nGimblett Gravels Merlot-Cabernet Sauvignon-Malbec\n50\n19.0\nHawke's Bay\nNaN\nNaN\nJoe Czerwinski\n@JoeCz\nEsk Valley 2011 Gimblett Gravels Merlot-Cabern...\nBordeaux-style Red Blend\nEsk Valley\n\n\n129958\n129958\nNew Zealand\nThis blend of Cabernet Sauvignon-Merlot and Ca...\nIrongate\n50\n35.0\nHawke's Bay\nNaN\nNaN\nJoe Czerwinski\n@JoeCz\nBabich 2010 Irongate Red (Hawke's Bay)\nBordeaux-style Red Blend\nBabich\n\n\n\n\n3748 rows × 14 columns\n\n\n\n\n# selecting rows and columns\ncolumns = ['price', 'region_1', 'region_2']\nrows = [1, 10, 100]\ndf = data.loc[rows, columns]\nprint(df)\n\n     price      region_1      region_2\n1     15.0           NaN           NaN\n10    19.0   Napa Valley          Napa\n100   18.0  Finger Lakes  Finger Lakes\n\n\n\n# selecting notnull values\ndata.loc[data.price.notnull()]\n\n\n\n\n\n\n\n\nUnnamed: 0\ncountry\ndescription\ndesignation\npoints\nprice\nprovince\nregion_1\nregion_2\ntaster_name\ntaster_twitter_handle\ntitle\nvariety\nwinery\n\n\n\n\n1\n1\nPortugal\nThis is ripe and fruity, a wine that is smooth...\nAvidagos\n87\n15.0\nDouro\nNaN\nNaN\nRoger Voss\n@vossroger\nQuinta dos Avidagos 2011 Avidagos Red (Douro)\nPortuguese Red\nQuinta dos Avidagos\n\n\n2\n2\nUS\nTart and snappy, the flavors of lime flesh and...\nNaN\n87\n14.0\nOregon\nWillamette Valley\nWillamette Valley\nPaul Gregutt\n@paulgwine\nRainstorm 2013 Pinot Gris (Willamette Valley)\nPinot Gris\nRainstorm\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n129969\n129969\nFrance\nA dry style of Pinot Gris, this is crisp with ...\nNaN\n90\n32.0\nAlsace\nAlsace\nNaN\nRoger Voss\n@vossroger\nDomaine Marcel Deiss 2012 Pinot Gris (Alsace)\nPinot Gris\nDomaine Marcel Deiss\n\n\n129970\n129970\nFrance\nBig, rich and off-dry, this is powered by inte...\nLieu-dit Harth Cuvée Caroline\n90\n21.0\nAlsace\nAlsace\nNaN\nRoger Voss\n@vossroger\nDomaine Schoffit 2012 Lieu-dit Harth Cuvée Car...\nGewürztraminer\nDomaine Schoffit\n\n\n\n\n120975 rows × 14 columns"
  },
  {
    "objectID": "posts/pandas/Pandas.html#assigning-data",
    "href": "posts/pandas/Pandas.html#assigning-data",
    "title": "Pandas",
    "section": "Assigning data",
    "text": "Assigning data\n\ndata['points'] = 50\nprint(data['points'])\n\n0         50\n1         50\n          ..\n129969    50\n129970    50\nName: points, Length: 129971, dtype: int64"
  },
  {
    "objectID": "posts/pandas/Pandas.html#renaming-and-combining",
    "href": "posts/pandas/Pandas.html#renaming-and-combining",
    "title": "Pandas",
    "section": "Renaming and Combining",
    "text": "Renaming and Combining\n\n# renaming columns\nprint(data.rename(columns={'points' : 'score'}))\n\n        Unnamed: 0   country  \\\n0                0     Italy   \n1                1  Portugal   \n...            ...       ...   \n129969      129969    France   \n129970      129970    France   \n\n                                              description  \\\n0       Aromas include tropical fruit, broom, brimston...   \n1       This is ripe and fruity, a wine that is smooth...   \n...                                                   ...   \n129969  A dry style of Pinot Gris, this is crisp with ...   \n129970  Big, rich and off-dry, this is powered by inte...   \n\n                          designation  score  price           province  \\\n0                        Vulkà Bianco     87    NaN  Sicily & Sardinia   \n1                            Avidagos     87   15.0              Douro   \n...                               ...    ...    ...                ...   \n129969                            NaN     90   32.0             Alsace   \n129970  Lieu-dit Harth Cuvée Caroline     90   21.0             Alsace   \n\n       region_1 region_2    taster_name taster_twitter_handle  \\\n0          Etna      NaN  Kerin O’Keefe          @kerinokeefe   \n1           NaN      NaN     Roger Voss            @vossroger   \n...         ...      ...            ...                   ...   \n129969   Alsace      NaN     Roger Voss            @vossroger   \n129970   Alsace      NaN     Roger Voss            @vossroger   \n\n                                                    title         variety  \\\n0                       Nicosia 2013 Vulkà Bianco  (Etna)     White Blend   \n1           Quinta dos Avidagos 2011 Avidagos Red (Douro)  Portuguese Red   \n...                                                   ...             ...   \n129969      Domaine Marcel Deiss 2012 Pinot Gris (Alsace)      Pinot Gris   \n129970  Domaine Schoffit 2012 Lieu-dit Harth Cuvée Car...  Gewürztraminer   \n\n                      winery  \n0                    Nicosia  \n1        Quinta dos Avidagos  \n...                      ...  \n129969  Domaine Marcel Deiss  \n129970      Domaine Schoffit  \n\n[129971 rows x 14 columns]\n\n\n\n# renaming indexes\nprint(data.rename(index={0:'first_entry', 1: 'second_entry'}))\n\n              Unnamed: 0   country  \\\nfirst_entry            0     Italy   \nsecond_entry           1  Portugal   \n...                  ...       ...   \n129969            129969    France   \n129970            129970    France   \n\n                                                    description  \\\nfirst_entry   Aromas include tropical fruit, broom, brimston...   \nsecond_entry  This is ripe and fruity, a wine that is smooth...   \n...                                                         ...   \n129969        A dry style of Pinot Gris, this is crisp with ...   \n129970        Big, rich and off-dry, this is powered by inte...   \n\n                                designation  points  price           province  \\\nfirst_entry                    Vulkà Bianco      87    NaN  Sicily & Sardinia   \nsecond_entry                       Avidagos      87   15.0              Douro   \n...                                     ...     ...    ...                ...   \n129969                                  NaN      90   32.0             Alsace   \n129970        Lieu-dit Harth Cuvée Caroline      90   21.0             Alsace   \n\n             region_1 region_2    taster_name taster_twitter_handle  \\\nfirst_entry      Etna      NaN  Kerin O’Keefe          @kerinokeefe   \nsecond_entry      NaN      NaN     Roger Voss            @vossroger   \n...               ...      ...            ...                   ...   \n129969         Alsace      NaN     Roger Voss            @vossroger   \n129970         Alsace      NaN     Roger Voss            @vossroger   \n\n                                                          title  \\\nfirst_entry                   Nicosia 2013 Vulkà Bianco  (Etna)   \nsecond_entry      Quinta dos Avidagos 2011 Avidagos Red (Douro)   \n...                                                         ...   \n129969            Domaine Marcel Deiss 2012 Pinot Gris (Alsace)   \n129970        Domaine Schoffit 2012 Lieu-dit Harth Cuvée Car...   \n\n                     variety                winery  \nfirst_entry      White Blend               Nicosia  \nsecond_entry  Portuguese Red   Quinta dos Avidagos  \n...                      ...                   ...  \n129969            Pinot Gris  Domaine Marcel Deiss  \n129970        Gewürztraminer      Domaine Schoffit  \n\n[129971 rows x 14 columns]\n\n\n\n# renaming axis\ndata.rename_axis (\"wines\", axis = 'rows').rename_axis('fields', axis = 'columns')\n\n\n\n\n\n\n\nfields\nUnnamed: 0\ncountry\ndescription\ndesignation\npoints\nprice\nprovince\nregion_1\nregion_2\ntaster_name\ntaster_twitter_handle\ntitle\nvariety\nwinery\n\n\nwines\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n0\nItaly\nAromas include tropical fruit, broom, brimston...\nVulkà Bianco\n87\nNaN\nSicily & Sardinia\nEtna\nNaN\nKerin O’Keefe\n@kerinokeefe\nNicosia 2013 Vulkà Bianco (Etna)\nWhite Blend\nNicosia\n\n\n1\n1\nPortugal\nThis is ripe and fruity, a wine that is smooth...\nAvidagos\n87\n15.0\nDouro\nNaN\nNaN\nRoger Voss\n@vossroger\nQuinta dos Avidagos 2011 Avidagos Red (Douro)\nPortuguese Red\nQuinta dos Avidagos\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n129969\n129969\nFrance\nA dry style of Pinot Gris, this is crisp with ...\nNaN\n90\n32.0\nAlsace\nAlsace\nNaN\nRoger Voss\n@vossroger\nDomaine Marcel Deiss 2012 Pinot Gris (Alsace)\nPinot Gris\nDomaine Marcel Deiss\n\n\n129970\n129970\nFrance\nBig, rich and off-dry, this is powered by inte...\nLieu-dit Harth Cuvée Caroline\n90\n21.0\nAlsace\nAlsace\nNaN\nRoger Voss\n@vossroger\nDomaine Schoffit 2012 Lieu-dit Harth Cuvée Car...\nGewürztraminer\nDomaine Schoffit\n\n\n\n\n129971 rows × 14 columns\n\n\n\n\n# combining with concat(), join(), and merge()\nfile1 = 'CAvideos.csv'\nCAdata = pd.read_csv(file1)\nCAdata\n\n\n\n\n\n\n\n\nvideo_id\ntrending_date\ntitle\nchannel_title\ncategory_id\npublish_time\ntags\nviews\nlikes\ndislikes\ncomment_count\nthumbnail_link\ncomments_disabled\nratings_disabled\nvideo_error_or_removed\ndescription\n\n\n\n\n0\nn1WpP7iowLc\n17.14.11\nEminem - Walk On Water (Audio) ft. Beyoncé\nEminemVEVO\n10\n2017-11-10T17:00:03.000Z\nEminem|\"Walk\"|\"On\"|\"Water\"|\"Aftermath/Shady/In...\n17158579\n787425\n43420\n125882\nhttps://i.ytimg.com/vi/n1WpP7iowLc/default.jpg\nFalse\nFalse\nFalse\nEminem's new track Walk on Water ft. Beyoncé i...\n\n\n1\n0dBIkQ4Mz1M\n17.14.11\nPLUSH - Bad Unboxing Fan Mail\niDubbbzTV\n23\n2017-11-13T17:00:00.000Z\nplush|\"bad unboxing\"|\"unboxing\"|\"fan mail\"|\"id...\n1014651\n127794\n1688\n13030\nhttps://i.ytimg.com/vi/0dBIkQ4Mz1M/default.jpg\nFalse\nFalse\nFalse\nSTill got a lot of packages. Probably will las...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n40879\nlbMKLzQ4cNQ\n18.14.06\nTrump Advisor Grovels To Trudeau\nThe Young Turks\n25\n2018-06-13T04:00:05.000Z\n180612__TB02SorryExcuse|\"News\"|\"Politics\"|\"The...\n115225\n2115\n182\n1672\nhttps://i.ytimg.com/vi/lbMKLzQ4cNQ/default.jpg\nFalse\nFalse\nFalse\nPeter Navarro isn’t talking so tough now. Ana ...\n\n\n40880\nPOTgw38-m58\n18.14.06\n【完整版】遇到恐怖情人該怎麼辦？2018.06.13小明星大跟班\n我愛小明星大跟班\n24\n2018-06-13T16:00:03.000Z\n吳宗憲|\"吳姍儒\"|\"小明星大跟班\"|\"Sandy\"|\"Jacky wu\"|\"憲哥\"|\"中天...\n107392\n300\n62\n251\nhttps://i.ytimg.com/vi/POTgw38-m58/default.jpg\nFalse\nFalse\nFalse\n藝人：李妍瑾、玉兔、班傑、LaLa、小優、少少專家：陳筱屏(律師)、Wendy(心理師)、羅...\n\n\n\n\n40881 rows × 16 columns\n\n\n\n\nfile2 = 'FRvideos.csv'\nFRdata = pd.read_csv(file2)\nFRdata\n\n\n\n\n\n\n\n\nvideo_id\ntrending_date\ntitle\nchannel_title\ncategory_id\npublish_time\ntags\nviews\nlikes\ndislikes\ncomment_count\nthumbnail_link\ncomments_disabled\nratings_disabled\nvideo_error_or_removed\ndescription\n\n\n\n\n0\nRo6eob0LrCY\n17.14.11\nMalika LePen : Femme de Gauche - Trailer\nLe Raptor Dissident\n24\n2017-11-13T17:32:55.000Z\nRaptor\"|\"Dissident\"|\"Expliquez\"|\"moi\"|\"cette\"|...\n212702\n29282\n1108\n3817\nhttps://i.ytimg.com/vi/Ro6eob0LrCY/default.jpg\nFalse\nFalse\nFalse\nDimanche.\\n18h30.\\nSoyez présents pour la vidé...\n\n\n1\nYo84eqYwP98\n17.14.11\nLA PIRE PARTIE ft Le Rire Jaune, Pierre Croce,...\nLe Labo\n24\n2017-11-12T15:00:02.000Z\n[none]\n432721\n14053\n576\n1161\nhttps://i.ytimg.com/vi/Yo84eqYwP98/default.jpg\nFalse\nFalse\nFalse\nLe jeu de société: https://goo.gl/hhG1Ta\\n\\nGa...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n40722\nNlxE_QQMRzg\n18.14.06\nՆռան հատիկ, Սերիա 192 / Pomegranate seed / Nra...\nPanArmenian TV\n1\n2018-06-13T18:30:00.000Z\nՆռան հատիկ|\"Սերիա 192\"|\"Pomegranate seed\"|\"Nra...\n78117\n244\n74\n46\nhttps://i.ytimg.com/vi/NlxE_QQMRzg/default.jpg\nFalse\nFalse\nFalse\nFollow Armenia TV on social platforms:Instagra...\n\n\n40723\n_LgKglfnqlc\n18.14.06\nMandoumbé ak Koor Gui 2018 Episode 28\nYesdakar\n24\n2018-06-13T19:45:14.000Z\nramadan2018|\"koorgui\"|\"Mandoumbé\"\n46604\n947\n37\n127\nhttps://i.ytimg.com/vi/_LgKglfnqlc/default.jpg\nFalse\nFalse\nFalse\nNaN\n\n\n\n\n40724 rows × 16 columns\n\n\n\n\n# joining\nleft = CAdata.set_index(['title', 'trending_date'])\nright = FRdata.set_index(['title', 'trending_date'])\n\nleft.join(right, lsuffix= '_CAN', rsuffix = '_FR')\n\n\n\n\n\n\n\n\n\nvideo_id_CAN\nchannel_title_CAN\ncategory_id_CAN\npublish_time_CAN\ntags_CAN\nviews_CAN\nlikes_CAN\ndislikes_CAN\ncomment_count_CAN\nthumbnail_link_CAN\n...\ntags_FR\nviews_FR\nlikes_FR\ndislikes_FR\ncomment_count_FR\nthumbnail_link_FR\ncomments_disabled_FR\nratings_disabled_FR\nvideo_error_or_removed_FR\ndescription_FR\n\n\ntitle\ntrending_date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n!! THIS VIDEO IS NOTHING BUT PAIN !! | Getting Over It - Part 7\n18.04.01\nPNn8sECd7io\nMarkiplier\n20\n2018-01-03T19:33:53.000Z\ngetting over it|\"markiplier\"|\"funny moments\"|\"...\n835930\n47058\n1023\n8250\nhttps://i.ytimg.com/vi/PNn8sECd7io/default.jpg\n...\ngetting over it\"|\"markiplier\"|\"funny moments\"|...\n835930.0\n47058.0\n1023.0\n8250.0\nhttps://i.ytimg.com/vi/PNn8sECd7io/default.jpg\nFalse\nFalse\nFalse\nGetting Over It continues with RAGE BEYOND ALL...\n\n\n#1 Fortnite World Rank - 2,323 Solo Wins!\n18.09.03\nDvPW66IFhMI\nAlexRamiGaming\n20\n2018-03-09T07:15:52.000Z\nPS4 Battle Royale|\"PS4 Pro Battle Royale\"|\"Bat...\n212838\n5199\n542\n11\nhttps://i.ytimg.com/vi/DvPW66IFhMI/default.jpg\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n🚨 BREAKING NEWS 🔴 Raja Live all Slot Channels Welcome 🎰\n18.07.05\nWt9Gkpmbt44\nTheBigJackpot\n24\n2018-05-07T06:58:59.000Z\nSlot Machine|\"win\"|\"Gambling\"|\"Big Win\"|\"raja\"...\n28973\n2167\n175\n10\nhttps://i.ytimg.com/vi/Wt9Gkpmbt44/default.jpg\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n🚨Active Shooter at YouTube Headquarters - LIVE BREAKING NEWS COVERAGE\n18.04.04\nAz72jrKbANA\nRight Side Broadcasting Network\n25\n2018-04-03T23:12:37.000Z\nYouTube shooter|\"YouTube active shooter\"|\"acti...\n103513\n1722\n181\n76\nhttps://i.ytimg.com/vi/Az72jrKbANA/default.jpg\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n40881 rows × 28 columns"
  },
  {
    "objectID": "posts/pandas/Pandas.html#summary-functions-and-maps",
    "href": "posts/pandas/Pandas.html#summary-functions-and-maps",
    "title": "Pandas",
    "section": "Summary Functions and Maps",
    "text": "Summary Functions and Maps\n\n# some of the summary functions include- describe, mean, unique, value_counts\nprint(data.columns)\n\nIndex(['Unnamed: 0', 'country', 'description', 'designation', 'points',\n       'price', 'province', 'region_1', 'region_2', 'taster_name',\n       'taster_twitter_handle', 'title', 'variety', 'winery'],\n      dtype='object')\n\n\n\nprint(data.points.describe())\n\ncount    129971.000000\nmean         88.447138\n             ...      \n75%          91.000000\nmax         100.000000\nName: points, Length: 8, dtype: float64\n\n\n\n# to see the list of unique values\nprint(data.taster_name.unique)\n\n&lt;bound method Series.unique of 0         Kerin O’Keefe\n1            Roger Voss\n              ...      \n129969       Roger Voss\n129970       Roger Voss\nName: taster_name, Length: 129971, dtype: object&gt;\n\n\n\nprint(data.taster_name.value_counts)\n\n&lt;bound method IndexOpsMixin.value_counts of 0         Kerin O’Keefe\n1            Roger Voss\n              ...      \n129969       Roger Voss\n129970       Roger Voss\nName: taster_name, Length: 129971, dtype: object&gt;\n\n\n\n# best_bargain_wine- wine with the highest points-to-price ratio\nbargain_idx = (data.points / data.price).idxmax()\nbargain_wine = data.loc[bargain_idx, 'title']\nprint(bargain_wine)\n\nBandit NV Merlot (California)\n\n\n\nMaps\n\ntakes one set of values and ‘maps’ them to another set of values\nexample usage - remean the scores of wines received to 0\nuse apply if you wish to call custom method on each row\n\n\nreview_points_mean = data.points.mean()\ndata.points.map(lambda p:p - review_points_mean)\n\n0        -1.447138\n1        -1.447138\n            ...   \n129969    1.552862\n129970    1.552862\nName: points, Length: 129971, dtype: float64\n\n\n\ndata_points_mean = data.points.mean()\ndata.points.map(lambda p:p - data_points_mean)\n\n0        -1.447138\n1        -1.447138\n            ...   \n129969    1.552862\n129970    1.552862\nName: points, Length: 129971, dtype: float64\n\n\n\n# create descriptor_counts from description for 'tropical' and 'fruity'\nn_tropical = data.description.map(lambda desc:'tropical' in desc).sum() \n# desc signifies description\nn_fruity = data.description.map(lambda desc:'fruity' in desc).sum()\ndescriptor_counts = pd.Series([n_tropical, n_fruity], index= ['tropical', 'fruity'])\nprint(descriptor_counts)\n\ntropical    3607\nfruity      9090\ndtype: int64\n\n\n\nsimplify with star ratings\n\n95 and above = 3 stars\nbetween 85 and 95 = 2 stars\nless than 85 = 1 star\nplus, any wines from Canada should get 3 stars\n\n\nprint(data.columns)\n\nIndex(['Unnamed: 0', 'country', 'description', 'designation', 'points',\n       'price', 'province', 'region_1', 'region_2', 'taster_name',\n       'taster_twitter_handle', 'title', 'variety', 'winery'],\n      dtype='object')\n\n\n\n# categorizing using map for points\ncat = data.points.map(lambda \n                      p:'three_stars' if  p&gt;=95\n                      else 'two stars' if p &gt;= 85 \n                      else 'one star')\n\n#count\nstar_rating = cat.value_counts()\n\nprint(star_rating)\n\npoints\ntwo stars      115125\none star        12430\nthree_stars      2416\nName: count, dtype: int64\n\n\n\n# categorizing using apply for points and Country\ncat2 = data.apply(lambda row: \n                    'three stars' if (row['points'] &gt;= 95 or row['country'] == 'Canada')\n                   else 'two stars' if (row['points'] &gt;= 85)\n                   else 'one star', axis = 1)\nstar_rating2 = cat2.value_counts()\nprint(star_rating2)\n\ntwo stars      114877\none star        12421\nthree stars      2673\nName: count, dtype: int64\n\n\n\n# simple way without mapping\ndef stars(row):\n    if row.country == 'Canada':\n        return 3\n    elif row.points &gt;= 95:\n        return 3\n    elif row.points &gt;= 85:\n        return 2\n    else:\n        return 1\n    \n    \nstar_ratings = data.apply(stars, axis = 'columns')\nprint(star_ratings)\n\n0         2\n1         2\n         ..\n129969    2\n129970    2\nLength: 129971, dtype: int64\n\n\n\ndef data_points(row):\n    row.points = row.points - data_points_mean\n    return row\n\ndata.apply(data_points, axis = 'columns')\n\n\n\n\n\n\n\n\nUnnamed: 0\ncountry\ndescription\ndesignation\npoints\nprice\nprovince\nregion_1\nregion_2\ntaster_name\ntaster_twitter_handle\ntitle\nvariety\nwinery\n\n\n\n\n0\n0\nItaly\nAromas include tropical fruit, broom, brimston...\nVulkà Bianco\n-1.447138\nNaN\nSicily & Sardinia\nEtna\nNaN\nKerin O’Keefe\n@kerinokeefe\nNicosia 2013 Vulkà Bianco (Etna)\nWhite Blend\nNicosia\n\n\n1\n1\nPortugal\nThis is ripe and fruity, a wine that is smooth...\nAvidagos\n-1.447138\n15.0\nDouro\nNaN\nNaN\nRoger Voss\n@vossroger\nQuinta dos Avidagos 2011 Avidagos Red (Douro)\nPortuguese Red\nQuinta dos Avidagos\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n129969\n129969\nFrance\nA dry style of Pinot Gris, this is crisp with ...\nNaN\n1.552862\n32.0\nAlsace\nAlsace\nNaN\nRoger Voss\n@vossroger\nDomaine Marcel Deiss 2012 Pinot Gris (Alsace)\nPinot Gris\nDomaine Marcel Deiss\n\n\n129970\n129970\nFrance\nBig, rich and off-dry, this is powered by inte...\nLieu-dit Harth Cuvée Caroline\n1.552862\n21.0\nAlsace\nAlsace\nNaN\nRoger Voss\n@vossroger\nDomaine Schoffit 2012 Lieu-dit Harth Cuvée Car...\nGewürztraminer\nDomaine Schoffit\n\n\n\n\n129971 rows × 14 columns\n\n\n\n\ndata.head(1)\n\n\n\n\n\n\n\n\nUnnamed: 0\ncountry\ndescription\ndesignation\npoints\nprice\nprovince\nregion_1\nregion_2\ntaster_name\ntaster_twitter_handle\ntitle\nvariety\nwinery\n\n\n\n\n0\n0\nItaly\nAromas include tropical fruit, broom, brimston...\nVulkà Bianco\n87\nNaN\nSicily & Sardinia\nEtna\nNaN\nKerin O’Keefe\n@kerinokeefe\nNicosia 2013 Vulkà Bianco (Etna)\nWhite Blend\nNicosia\n\n\n\n\n\n\n\n\noperation (below) between a lot of values on the left-hand side &gt; and a single value on the right-hand side (the mean value).\n\n\ndata_points_mean = data.points.mean()\ndata.points - data_points_mean\n\n0        -1.447138\n1        -1.447138\n            ...   \n129969    1.552862\n129970    1.552862\nName: points, Length: 129971, dtype: float64\n\n\n\ndata.country + \"-\" + data.region_1\n\n0            Italy-Etna\n1                   NaN\n              ...      \n129969    France-Alsace\n129970    France-Alsace\nLength: 129971, dtype: object"
  },
  {
    "objectID": "posts/pandas/Pandas.html#grouping-and-sorting",
    "href": "posts/pandas/Pandas.html#grouping-and-sorting",
    "title": "Pandas",
    "section": "Grouping and Sorting",
    "text": "Grouping and Sorting\n\nuse groupby to group data\n\n\napply() method can fetch us the data that matches the group\n\n\n# groupwise analysis\ndata.groupby('points').points.count()\n\npoints\n80     397\n81     692\n      ... \n99      33\n100     19\nName: points, Length: 21, dtype: int64\n\n\n\n# ascending or descending order \ndata.groupby('points').price.min()\n\npoints\n80      5.0\n81      5.0\n       ... \n99     44.0\n100    80.0\nName: price, Length: 21, dtype: float64\n\n\n\n#grouping in countries and sorting \ndata.groupby(['country', 'province']).apply(lambda df:df.loc[df.points.idxmax()])\n\n\n\n\n\n\n\n\n\nUnnamed: 0\ncountry\ndescription\ndesignation\npoints\nprice\nprovince\nregion_1\nregion_2\ntaster_name\ntaster_twitter_handle\ntitle\nvariety\nwinery\n\n\ncountry\nprovince\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArgentina\nMendoza Province\n82754\nArgentina\nIf the color doesn't tell the full story, the ...\nNicasia Vineyard\n97\n120.0\nMendoza Province\nMendoza\nNaN\nMichael Schachner\n@wineschach\nBodega Catena Zapata 2006 Nicasia Vineyard Mal...\nMalbec\nBodega Catena Zapata\n\n\nOther\n78303\nArgentina\nTake note, this could be the best wine Colomé ...\nReserva\n95\n90.0\nOther\nSalta\nNaN\nMichael Schachner\n@wineschach\nColomé 2010 Reserva Malbec (Salta)\nMalbec\nColomé\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nUruguay\nSan Jose\n39898\nUruguay\nBaked, sweet, heavy aromas turn earthy with ti...\nEl Preciado Gran Reserva\n87\n50.0\nSan Jose\nNaN\nNaN\nMichael Schachner\n@wineschach\nCastillo Viejo 2005 El Preciado Gran Reserva R...\nRed Blend\nCastillo Viejo\n\n\nUruguay\n39361\nUruguay\nCherry and berry aromas are ripe, healthy and ...\nBlend 002 Limited Edition\n91\n22.0\nUruguay\nNaN\nNaN\nMichael Schachner\n@wineschach\nNarbona NV Blend 002 Limited Edition Tannat-Ca...\nTannat-Cabernet Franc\nNarbona\n\n\n\n\n425 rows × 14 columns\n\n\n\n\nhelp(pd.Series.idxmax)\n\nHelp on function idxmax in module pandas.core.series:\n\nidxmax(self, axis: 'Axis' = 0, skipna: 'bool' = True, *args, **kwargs) -&gt; 'Hashable'\n    Return the row label of the maximum value.\n    \n    If multiple values equal the maximum, the first row label with that\n    value is returned.\n    \n    Parameters\n    ----------\n    axis : {0 or 'index'}\n        Unused. Parameter needed for compatibility with DataFrame.\n    skipna : bool, default True\n        Exclude NA/null values. If the entire Series is NA, the result\n        will be NA.\n    *args, **kwargs\n        Additional arguments and keywords have no effect but might be\n        accepted for compatibility with NumPy.\n    \n    Returns\n    -------\n    Index\n        Label of the maximum value.\n    \n    Raises\n    ------\n    ValueError\n        If the Series is empty.\n    \n    See Also\n    --------\n    numpy.argmax : Return indices of the maximum values\n        along the given axis.\n    DataFrame.idxmax : Return index of first occurrence of maximum\n        over requested axis.\n    Series.idxmin : Return index *label* of the first occurrence\n        of minimum of values.\n    \n    Notes\n    -----\n    This method is the Series version of ``ndarray.argmax``. This method\n    returns the label of the maximum, while ``ndarray.argmax`` returns\n    the position. To get the position, use ``series.values.argmax()``.\n    \n    Examples\n    --------\n    &gt;&gt;&gt; s = pd.Series(data=[1, None, 4, 3, 4],\n    ...               index=['A', 'B', 'C', 'D', 'E'])\n    &gt;&gt;&gt; s\n    A    1.0\n    B    NaN\n    C    4.0\n    D    3.0\n    E    4.0\n    dtype: float64\n    \n    &gt;&gt;&gt; s.idxmax()\n    'C'\n    \n    If `skipna` is False and there is an NA value in the data,\n    the function returns ``nan``.\n    \n    &gt;&gt;&gt; s.idxmax(skipna=False)\n    nan\n\n\n\n\ndata.groupby(['country']).price.agg([len, 'min', 'max'])\n\n\n\n\n\n\n\n\nlen\nmin\nmax\n\n\ncountry\n\n\n\n\n\n\n\nArgentina\n3800\n4.0\n230.0\n\n\nArmenia\n2\n14.0\n15.0\n\n\n...\n...\n...\n...\n\n\nUkraine\n14\n6.0\n13.0\n\n\nUruguay\n109\n10.0\n130.0\n\n\n\n\n43 rows × 3 columns\n\n\n\n\nMulti-indexes\n\ncan help to convert to regular index\n\n\ncountries_reviewed = data.groupby(['country', 'province']).description.agg([len])\nprint(countries_reviewed)\n\n                             len\ncountry   province              \nArgentina Mendoza Province  3264\n          Other              536\n...                          ...\nUruguay   San Jose             3\n          Uruguay             24\n\n[425 rows x 1 columns]\n\n\n\nmi = countries_reviewed.index\ntype(mi)\n\npandas.core.indexes.multi.MultiIndex\n\n\n\ncountries_reviewed.reset_index()\n\n\n\n\n\n\n\n\ncountry\nprovince\nlen\n\n\n\n\n0\nArgentina\nMendoza Province\n3264\n\n\n1\nArgentina\nOther\n536\n\n\n...\n...\n...\n...\n\n\n423\nUruguay\nSan Jose\n3\n\n\n424\nUruguay\nUruguay\n24\n\n\n\n\n425 rows × 3 columns\n\n\n\n\n# create a series of price and points. sort values by price (ascending)\nrating = data.groupby('price')['points'].max().sort_index()\nprint(rating)\n\nprice\n4.0       86\n5.0       87\n          ..\n2500.0    96\n3300.0    88\nName: points, Length: 390, dtype: int64\n\n\n\ndf = data.groupby('variety').price.agg('max', 'min')\nprint(df)\n\nvariety\nAbouriou       75.0\nAgiorgitiko    66.0\n               ... \nÇalkarası      19.0\nŽilavka        15.0\nName: price, Length: 707, dtype: float64\n\n\n\n\nSorting\n\n#ascending by defalt\ncountries_reviewed = countries_reviewed.reset_index()\ncountries_reviewed.sort_values(by= 'len')\n\n\n\n\n\n\n\n\ncountry\nprovince\nlen\n\n\n\n\n179\nGreece\nMuscat of Kefallonian\n1\n\n\n192\nGreece\nSterea Ellada\n1\n\n\n...\n...\n...\n...\n\n\n415\nUS\nWashington\n8639\n\n\n392\nUS\nCalifornia\n36247\n\n\n\n\n425 rows × 3 columns\n\n\n\n\n# descending \ncountries_reviewed.sort_values(by= 'len', ascending= False)\n\n\n\n\n\n\n\n\ncountry\nprovince\nlen\n\n\n\n\n392\nUS\nCalifornia\n36247\n\n\n415\nUS\nWashington\n8639\n\n\n...\n...\n...\n...\n\n\n63\nChile\nCoelemu\n1\n\n\n149\nGreece\nBeotia\n1\n\n\n\n\n425 rows × 3 columns\n\n\n\n\n# sorting index_values\ncountries_reviewed.sort_index()\n\n\n\n\n\n\n\n\ncountry\nprovince\nlen\n\n\n\n\n0\nArgentina\nMendoza Province\n3264\n\n\n1\nArgentina\nOther\n536\n\n\n...\n...\n...\n...\n\n\n423\nUruguay\nSan Jose\n3\n\n\n424\nUruguay\nUruguay\n24\n\n\n\n\n425 rows × 3 columns\n\n\n\n\n# sorting more than one column\ncountries_reviewed.sort_values(by=['country', 'len'])\n\n\n\n\n\n\n\n\ncountry\nprovince\nlen\n\n\n\n\n1\nArgentina\nOther\n536\n\n\n0\nArgentina\nMendoza Province\n3264\n\n\n...\n...\n...\n...\n\n\n424\nUruguay\nUruguay\n24\n\n\n419\nUruguay\nCanelones\n43\n\n\n\n\n425 rows × 3 columns"
  },
  {
    "objectID": "posts/pandas/Pandas.html#data-types-and-missing-data",
    "href": "posts/pandas/Pandas.html#data-types-and-missing-data",
    "title": "Pandas",
    "section": "Data Types and Missing Data",
    "text": "Data Types and Missing Data\n\nmissing values are given the value NaN - ‘Not a Number’- float64 dtype\n\n\n# find the data type\ndata.price.dtype\n\ndtype('float64')\n\n\n\n# for every column\nprint(data.dtypes)\n\nUnnamed: 0     int64\ncountry       object\n               ...  \nvariety       object\nwinery        object\nLength: 14, dtype: object\n\n\n\n# transform data type\ndata.points.astype('float64')\n\n0         87.0\n1         87.0\n          ... \n129969    90.0\n129970    90.0\nName: points, Length: 129971, dtype: float64\n\n\n\n# finding values in country by NaN\ndata[pd.isnull(data.country)]\n\n\n\n\n\n\n\n\nUnnamed: 0\ncountry\ndescription\ndesignation\npoints\nprice\nprovince\nregion_1\nregion_2\ntaster_name\ntaster_twitter_handle\ntitle\nvariety\nwinery\n\n\n\n\n913\n913\nNaN\nAmber in color, this wine has aromas of peach ...\nAsureti Valley\n87\n30.0\nNaN\nNaN\nNaN\nMike DeSimone\n@worldwineguys\nGotsa Family Wines 2014 Asureti Valley Chinuri\nChinuri\nGotsa Family Wines\n\n\n3131\n3131\nNaN\nSoft, fruity and juicy, this is a pleasant, si...\nPartager\n83\nNaN\nNaN\nNaN\nNaN\nRoger Voss\n@vossroger\nBarton & Guestier NV Partager Red\nRed Blend\nBarton & Guestier\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n129590\n129590\nNaN\nA blend of 60% Syrah, 30% Cabernet Sauvignon a...\nShah\n90\n30.0\nNaN\nNaN\nNaN\nMike DeSimone\n@worldwineguys\nBüyülübağ 2012 Shah Red\nRed Blend\nBüyülübağ\n\n\n129900\n129900\nNaN\nThis wine offers a delightful bouquet of black...\nNaN\n91\n32.0\nNaN\nNaN\nNaN\nMike DeSimone\n@worldwineguys\nPsagot 2014 Merlot\nMerlot\nPsagot\n\n\n\n\n63 rows × 14 columns\n\n\n\n\n# replacing missing values\ndata.country.fillna('Unknown')\n\n0            Italy\n1         Portugal\n            ...   \n129969      France\n129970      France\nName: country, Length: 129971, dtype: object\n\n\n\n# replacing ('what?','bywhat?')\ndata.price.replace('NaN', '@Unknown')\n\n0          NaN\n1         15.0\n          ... \n129969    32.0\n129970    21.0\nName: price, Length: 129971, dtype: float64\n\n\n\n# missing price values and count them\ndata.price.isnull().sum()\n\n8996\n\n\n\n# arrange region_1 in ascending order of values\ndata.region_1.fillna('Unkown').value_counts().sort_values(ascending= False)\n\nregion_1\nUnkown         21247\nNapa Valley     4480\n               ...  \nGeelong            1\nPaestum            1\nName: count, Length: 1230, dtype: int64"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html",
    "href": "posts/pandas/tools-pandas.html",
    "title": "Tools - pandas",
    "section": "",
    "text": "Tools - pandas\nThe pandas library provides high-performance, easy-to-use data structures and data analysis tools. The main data structure is the DataFrame, which you can think of as an in-memory 2D table (like a spreadsheet, with column names and row labels). Many features available in Excel are available programmatically, such as creating pivot tables, computing columns based on other columns, plotting graphs, etc. You can also group rows by column value, or join tables much like in SQL. Pandas is also great at handling time series.\nThis notebook follows the fastai style conventions.\nPrerequisites: * NumPy – if you are not familiar with NumPy, we recommend that you go through the NumPy tutorial now."
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#creating-a-series",
    "href": "posts/pandas/tools-pandas.html#creating-a-series",
    "title": "Tools - pandas",
    "section": "Creating a Series",
    "text": "Creating a Series\nLet’s start by creating our first Series object!\n\n\nCode\ns = pd.Series([2,-1,3,5])\ns\n\n\n0    2\n1   -1\n2    3\n3    5\ndtype: int64"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#similar-to-a-1d-ndarray",
    "href": "posts/pandas/tools-pandas.html#similar-to-a-1d-ndarray",
    "title": "Tools - pandas",
    "section": "Similar to a 1D ndarray",
    "text": "Similar to a 1D ndarray\nSeries objects behave much like one-dimensional NumPy ndarrays, and you can often pass them as parameters to NumPy functions:\n\n\nCode\nimport numpy as np\nnp.exp(s)\n\n\n0      7.389056\n1      0.367879\n2     20.085537\n3    148.413159\ndtype: float64\n\n\nArithmetic operations on Series are also possible, and they apply elementwise, just like for ndarrays:\n\n\nCode\ns + [1000,2000,3000,4000]\n\n\n0    1002\n1    1999\n2    3003\n3    4005\ndtype: int64\n\n\nSimilar to NumPy, if you add a single number to a Series, that number is added to all items in the Series. This is called * broadcasting*:\n\n\nCode\ns + 1000\n\n\n0    1002\n1     999\n2    1003\n3    1005\ndtype: int64\n\n\nThe same is true for all binary operations such as * or /, and even conditional operations:\n\n\nCode\ns &lt; 0\n\n\n0    False\n1     True\n2    False\n3    False\ndtype: bool"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#index-labels",
    "href": "posts/pandas/tools-pandas.html#index-labels",
    "title": "Tools - pandas",
    "section": "Index labels",
    "text": "Index labels\nEach item in a Series object has a unique identifier called the index label. By default, it is simply the rank of the item in the Series (starting at 0) but you can also set the index labels manually:\n\n\nCode\ns2 = pd.Series([68, 83, 112, 68], index=[\"alice\", \"bob\", \"charles\", \"darwin\"])\ns2\n\n\nalice       68\nbob         83\ncharles    112\ndarwin      68\ndtype: int64\n\n\nYou can then use the Series just like a dict:\n\n\nCode\ns2[\"bob\"]\n\n\n83\n\n\nYou can still access the items by integer location, like in a regular array:\n\n\nCode\ns2[1]\n\n\n83\n\n\nTo make it clear when you are accessing by label or by integer location, it is recommended to always use the loc attribute when accessing by label, and the iloc attribute when accessing by integer location:\n\n\nCode\ns2.loc[\"bob\"]\n\n\n83\n\n\n\n\nCode\ns2.iloc[1]\n\n\n83\n\n\nSlicing a Series also slices the index labels:\n\n\nCode\ns2.iloc[1:3]\n\n\nbob         83\ncharles    112\ndtype: int64\n\n\nThis can lead to unexpected results when using the default numeric labels, so be careful:\n\n\nCode\nsurprise = pd.Series([1000, 1001, 1002, 1003])\nsurprise\n\n\n0    1000\n1    1001\n2    1002\n3    1003\ndtype: int64\n\n\n\n\nCode\nsurprise_slice = surprise[2:]\nsurprise_slice\n\n\n2    1002\n3    1003\ndtype: int64\n\n\nOh look! The first element has index label 2. The element with index label 0 is absent from the slice:\n\n\nCode\ntry:\n    surprise_slice[0]\nexcept KeyError as e:\n    print(\"Key error:\", e)\n\n\nKey error: 0\n\n\nBut remember that you can access elements by integer location using the iloc attribute. This illustrates another reason why it’s always better to use loc and iloc to access Series objects:\n\n\nCode\nsurprise_slice.iloc[0]\n\n\n1002"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#init-from-dict",
    "href": "posts/pandas/tools-pandas.html#init-from-dict",
    "title": "Tools - pandas",
    "section": "Init from dict",
    "text": "Init from dict\nYou can create a Series object from a dict. The keys will be used as index labels:\n\n\nCode\nweights = {\"alice\": 68, \"bob\": 83, \"colin\": 86, \"darwin\": 68}\ns3 = pd.Series(weights)\ns3\n\n\nalice     68\nbob       83\ncolin     86\ndarwin    68\ndtype: int64\n\n\nYou can control which elements you want to include in the Series and in what order by explicitly specifying the desired index:\n\n\nCode\ns4 = pd.Series(weights, index = [\"colin\", \"alice\"])\ns4\n\n\ncolin    86\nalice    68\ndtype: int64"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#automatic-alignment",
    "href": "posts/pandas/tools-pandas.html#automatic-alignment",
    "title": "Tools - pandas",
    "section": "Automatic alignment",
    "text": "Automatic alignment\nWhen an operation involves multiple Series objects, pandas automatically aligns items by matching index labels.\n\n\nCode\nprint(s2.keys())\nprint(s3.keys())\n\ns2 + s3\n\n\nIndex(['alice', 'bob', 'charles', 'darwin'], dtype='object')\nIndex(['alice', 'bob', 'colin', 'darwin'], dtype='object')\n\n\nalice      136.0\nbob        166.0\ncharles      NaN\ncolin        NaN\ndarwin     136.0\ndtype: float64\n\n\nThe resulting Series contains the union of index labels from s2 and s3. Since \"colin\" is missing from s2 and \"charles\" is missing from s3, these items have a NaN result value. (ie. Not-a-Number means missing).\nAutomatic alignment is very handy when working with data that may come from various sources with varying structure and missing items. But if you forget to set the right index labels, you can have surprising results:\n\n\nCode\ns5 = pd.Series([1000,1000,1000,1000])\nprint(\"s2 =\", s2.values)\nprint(\"s5 =\", s5.values)\n\ns2 + s5\n\n\ns2 = [ 68  83 112  68]\ns5 = [1000 1000 1000 1000]\n\n\nalice     NaN\nbob       NaN\ncharles   NaN\ndarwin    NaN\n0         NaN\n1         NaN\n2         NaN\n3         NaN\ndtype: float64\n\n\nPandas could not align the Series, since their labels do not match at all, hence the full NaN result."
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#init-with-a-scalar",
    "href": "posts/pandas/tools-pandas.html#init-with-a-scalar",
    "title": "Tools - pandas",
    "section": "Init with a scalar",
    "text": "Init with a scalar\nYou can also initialize a Series object using a scalar and a list of index labels: all items will be set to the scalar.\n\n\nCode\nmeaning = pd.Series(42, [\"life\", \"universe\", \"everything\"])\nmeaning\n\n\nlife          42\nuniverse      42\neverything    42\ndtype: int64"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#series-name",
    "href": "posts/pandas/tools-pandas.html#series-name",
    "title": "Tools - pandas",
    "section": "Series name",
    "text": "Series name\nA Series can have a name:\n\n\nCode\ns6 = pd.Series([83, 68], index=[\"bob\", \"alice\"], name=\"weights\")\ns6\n\n\nbob      83\nalice    68\nName: weights, dtype: int64"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#plotting-a-series",
    "href": "posts/pandas/tools-pandas.html#plotting-a-series",
    "title": "Tools - pandas",
    "section": "Plotting a Series",
    "text": "Plotting a Series\nPandas makes it easy to plot Series data using matplotlib (for more details on matplotlib, check out the matplotlib tutorial). Just import matplotlib and call the plot() method:\n\n\nCode\n%matplotlib inline\nimport matplotlib.pyplot as plt\ntemperatures = [4.4,5.1,6.1,6.2,6.1,6.1,5.7,5.2,4.7,4.1,3.9,3.5]\ns7 = pd.Series(temperatures, name=\"Temperature\")\ns7.plot()\nplt.show()\n\n\n\n\n\nThere are many options for plotting your data. It is not necessary to list them all here: if you need a particular type of plot (histograms, pie charts, etc.), just look for it in the excellent Visualization section of pandas’ documentation, and look at the example code."
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#time-range",
    "href": "posts/pandas/tools-pandas.html#time-range",
    "title": "Tools - pandas",
    "section": "Time range",
    "text": "Time range\nLet’s start by creating a time series using pd.date_range(). This returns a DatetimeIndex containing one datetime per hour for 12 hours starting on October 29th 2016 at 5:30pm.\n\n\nCode\ndates = pd.date_range('2016/10/29 5:30pm', periods=12, freq='H')\ndates\n\n\nDatetimeIndex(['2016-10-29 17:30:00', '2016-10-29 18:30:00',\n               '2016-10-29 19:30:00', '2016-10-29 20:30:00',\n               '2016-10-29 21:30:00', '2016-10-29 22:30:00',\n               '2016-10-29 23:30:00', '2016-10-30 00:30:00',\n               '2016-10-30 01:30:00', '2016-10-30 02:30:00',\n               '2016-10-30 03:30:00', '2016-10-30 04:30:00'],\n              dtype='datetime64[ns]', freq='H')\n\n\nThis DatetimeIndex may be used as an index in a Series:\n\n\nCode\ntemp_series = pd.Series(temperatures, dates)\ntemp_series\n\n\n2016-10-29 17:30:00    4.4\n2016-10-29 18:30:00    5.1\n2016-10-29 19:30:00    6.1\n2016-10-29 20:30:00    6.2\n2016-10-29 21:30:00    6.1\n2016-10-29 22:30:00    6.1\n2016-10-29 23:30:00    5.7\n2016-10-30 00:30:00    5.2\n2016-10-30 01:30:00    4.7\n2016-10-30 02:30:00    4.1\n2016-10-30 03:30:00    3.9\n2016-10-30 04:30:00    3.5\nFreq: H, dtype: float64\n\n\nLet’s plot this series:\n\n\nCode\ntemp_series.plot(kind=\"bar\")\n\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#resampling",
    "href": "posts/pandas/tools-pandas.html#resampling",
    "title": "Tools - pandas",
    "section": "Resampling",
    "text": "Resampling\nPandas lets us resample a time series very simply. Just call the resample() method and specify a new frequency:\n\n\nCode\ntemp_series_freq_2H = temp_series.resample(\"2H\")\ntemp_series_freq_2H\n\n\n&lt;pandas.core.resample.DatetimeIndexResampler object at 0x7f69a91f0190&gt;\n\n\nThe resampling operation is actually a deferred operation, which is why we did not get a Series object, but a DatetimeIndexResampler object instead. To actually perform the resampling operation, we can simply call the mean() method: Pandas will compute the mean of every pair of consecutive hours:\n\n\nCode\ntemp_series_freq_2H = temp_series_freq_2H.mean()\n\n\nLet’s plot the result:\n\n\nCode\ntemp_series_freq_2H.plot(kind=\"bar\")\nplt.show()\n\n\n\n\n\nNote how the values have automatically been aggregated into 2-hour periods. If we look at the 6-8pm period, for example, we had a value of 5.1 at 6:30pm, and 6.1 at 7:30pm. After resampling, we just have one value of 5.6, which is the mean of 5.1 and 6.1. Rather than computing the mean, we could have used any other aggregation function, for example we can decide to keep the minimum value of each period:\n\n\nCode\ntemp_series_freq_2H = temp_series.resample(\"2H\").min()\ntemp_series_freq_2H\n\n\n2016-10-29 16:00:00    4.4\n2016-10-29 18:00:00    5.1\n2016-10-29 20:00:00    6.1\n2016-10-29 22:00:00    5.7\n2016-10-30 00:00:00    4.7\n2016-10-30 02:00:00    3.9\n2016-10-30 04:00:00    3.5\nFreq: 2H, dtype: float64\n\n\nOr, equivalently, we could use the apply() method instead:\n\n\nCode\ntemp_series_freq_2H = temp_series.resample(\"2H\").apply(np.min)\ntemp_series_freq_2H\n\n\n2016-10-29 16:00:00    4.4\n2016-10-29 18:00:00    5.1\n2016-10-29 20:00:00    6.1\n2016-10-29 22:00:00    5.7\n2016-10-30 00:00:00    4.7\n2016-10-30 02:00:00    3.9\n2016-10-30 04:00:00    3.5\nFreq: 2H, dtype: float64"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#upsampling-and-interpolation",
    "href": "posts/pandas/tools-pandas.html#upsampling-and-interpolation",
    "title": "Tools - pandas",
    "section": "Upsampling and interpolation",
    "text": "Upsampling and interpolation\nThis was an example of downsampling. We can also upsample (ie. increase the frequency), but this creates holes in our data:\n\n\nCode\ntemp_series_freq_15min = temp_series.resample(\"15Min\").mean()\ntemp_series_freq_15min.head(n=10) # `head` displays the top n values\n\n\n2016-10-29 17:30:00    4.4\n2016-10-29 17:45:00    NaN\n2016-10-29 18:00:00    NaN\n2016-10-29 18:15:00    NaN\n2016-10-29 18:30:00    5.1\n2016-10-29 18:45:00    NaN\n2016-10-29 19:00:00    NaN\n2016-10-29 19:15:00    NaN\n2016-10-29 19:30:00    6.1\n2016-10-29 19:45:00    NaN\nFreq: 15T, dtype: float64\n\n\nOne solution is to fill the gaps by interpolating. We just call the interpolate() method. The default is to use linear interpolation, but we can also select another method, such as cubic interpolation:\n\n\nCode\ntemp_series_freq_15min = temp_series.resample(\"15Min\").interpolate(method=\"cubic\")\ntemp_series_freq_15min.head(n=10)\n\n\n2016-10-29 17:30:00    4.400000\n2016-10-29 17:45:00    4.452911\n2016-10-29 18:00:00    4.605113\n2016-10-29 18:15:00    4.829758\n2016-10-29 18:30:00    5.100000\n2016-10-29 18:45:00    5.388992\n2016-10-29 19:00:00    5.669887\n2016-10-29 19:15:00    5.915839\n2016-10-29 19:30:00    6.100000\n2016-10-29 19:45:00    6.203621\nFreq: 15T, dtype: float64\n\n\n\n\nCode\ntemp_series.plot(label=\"Period: 1 hour\")\ntemp_series_freq_15min.plot(label=\"Period: 15 minutes\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#timezones",
    "href": "posts/pandas/tools-pandas.html#timezones",
    "title": "Tools - pandas",
    "section": "Timezones",
    "text": "Timezones\nBy default datetimes are naive: they are not aware of timezones, so 2016-10-30 02:30 might mean October 30th 2016 at 2:30am in Paris or in New York. We can make datetimes timezone aware by calling the tz_localize() method:\n\n\nCode\ntemp_series_ny = temp_series.tz_localize(\"America/New_York\")\ntemp_series_ny\n\n\n2016-10-29 17:30:00-04:00    4.4\n2016-10-29 18:30:00-04:00    5.1\n2016-10-29 19:30:00-04:00    6.1\n2016-10-29 20:30:00-04:00    6.2\n2016-10-29 21:30:00-04:00    6.1\n2016-10-29 22:30:00-04:00    6.1\n2016-10-29 23:30:00-04:00    5.7\n2016-10-30 00:30:00-04:00    5.2\n2016-10-30 01:30:00-04:00    4.7\n2016-10-30 02:30:00-04:00    4.1\n2016-10-30 03:30:00-04:00    3.9\n2016-10-30 04:30:00-04:00    3.5\ndtype: float64\n\n\nNote that -04:00 is now appended to all the datetimes. This means that these datetimes refer to UTC - 4 hours.\nWe can convert these datetimes to Paris time like this:\n\n\nCode\ntemp_series_paris = temp_series_ny.tz_convert(\"Europe/Paris\")\ntemp_series_paris\n\n\n2016-10-29 23:30:00+02:00    4.4\n2016-10-30 00:30:00+02:00    5.1\n2016-10-30 01:30:00+02:00    6.1\n2016-10-30 02:30:00+02:00    6.2\n2016-10-30 02:30:00+01:00    6.1\n2016-10-30 03:30:00+01:00    6.1\n2016-10-30 04:30:00+01:00    5.7\n2016-10-30 05:30:00+01:00    5.2\n2016-10-30 06:30:00+01:00    4.7\n2016-10-30 07:30:00+01:00    4.1\n2016-10-30 08:30:00+01:00    3.9\n2016-10-30 09:30:00+01:00    3.5\ndtype: float64\n\n\nYou may have noticed that the UTC offset changes from +02:00 to +01:00: this is because France switches to winter time at 3am that particular night (time goes back to 2am). Notice that 2:30am occurs twice! Let’s go back to a naive representation (if you log some data hourly using local time, without storing the timezone, you might get something like this):\n\n\nCode\ntemp_series_paris_naive = temp_series_paris.tz_localize(None)\ntemp_series_paris_naive\n\n\n2016-10-29 23:30:00    4.4\n2016-10-30 00:30:00    5.1\n2016-10-30 01:30:00    6.1\n2016-10-30 02:30:00    6.2\n2016-10-30 02:30:00    6.1\n2016-10-30 03:30:00    6.1\n2016-10-30 04:30:00    5.7\n2016-10-30 05:30:00    5.2\n2016-10-30 06:30:00    4.7\n2016-10-30 07:30:00    4.1\n2016-10-30 08:30:00    3.9\n2016-10-30 09:30:00    3.5\ndtype: float64\n\n\nNow 02:30 is really ambiguous. If we try to localize these naive datetimes to the Paris timezone, we get an error:\n\n\nCode\ntry:\n    temp_series_paris_naive.tz_localize(\"Europe/Paris\")\nexcept Exception as e:\n    print(type(e))\n    print(e)\n\n\n&lt;class 'pytz.exceptions.AmbiguousTimeError'&gt;\nCannot infer dst time from 2016-10-30 02:30:00, try using the 'ambiguous' argument\n\n\nFortunately using the ambiguous argument we can tell pandas to infer the right DST (Daylight Saving Time) based on the order of the ambiguous timestamps:\n\n\nCode\ntemp_series_paris_naive.tz_localize(\"Europe/Paris\", ambiguous=\"infer\")\n\n\n2016-10-29 23:30:00+02:00    4.4\n2016-10-30 00:30:00+02:00    5.1\n2016-10-30 01:30:00+02:00    6.1\n2016-10-30 02:30:00+02:00    6.2\n2016-10-30 02:30:00+01:00    6.1\n2016-10-30 03:30:00+01:00    6.1\n2016-10-30 04:30:00+01:00    5.7\n2016-10-30 05:30:00+01:00    5.2\n2016-10-30 06:30:00+01:00    4.7\n2016-10-30 07:30:00+01:00    4.1\n2016-10-30 08:30:00+01:00    3.9\n2016-10-30 09:30:00+01:00    3.5\ndtype: float64"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#periods",
    "href": "posts/pandas/tools-pandas.html#periods",
    "title": "Tools - pandas",
    "section": "Periods",
    "text": "Periods\nThe pd.period_range() function returns a PeriodIndex instead of a DatetimeIndex. For example, let’s get all quarters in 2016 and 2017:\n\n\nCode\nquarters = pd.period_range('2016Q1', periods=8, freq='Q')\nquarters\n\n\nPeriodIndex(['2016Q1', '2016Q2', '2016Q3', '2016Q4', '2017Q1', '2017Q2',\n             '2017Q3', '2017Q4'],\n            dtype='period[Q-DEC]')\n\n\nAdding a number N to a PeriodIndex shifts the periods by N times the PeriodIndex’s frequency:\n\n\nCode\nquarters + 3\n\n\nPeriodIndex(['2016Q4', '2017Q1', '2017Q2', '2017Q3', '2017Q4', '2018Q1',\n             '2018Q2', '2018Q3'],\n            dtype='period[Q-DEC]')\n\n\nThe asfreq() method lets us change the frequency of the PeriodIndex. All periods are lengthened or shortened accordingly. For example, let’s convert all the quarterly periods to monthly periods (zooming in):\n\n\nCode\nquarters.asfreq(\"M\")\n\n\nPeriodIndex(['2016-03', '2016-06', '2016-09', '2016-12', '2017-03', '2017-06',\n             '2017-09', '2017-12'],\n            dtype='period[M]')\n\n\nBy default, the asfreq zooms on the end of each period. We can tell it to zoom on the start of each period instead:\n\n\nCode\nquarters.asfreq(\"M\", how=\"start\")\n\n\nPeriodIndex(['2016-01', '2016-04', '2016-07', '2016-10', '2017-01', '2017-04',\n             '2017-07', '2017-10'],\n            dtype='period[M]')\n\n\nAnd we can zoom out:\n\n\nCode\nquarters.asfreq(\"A\")\n\n\nPeriodIndex(['2016', '2016', '2016', '2016', '2017', '2017', '2017', '2017'], dtype='period[A-DEC]')\n\n\nOf course we can create a Series with a PeriodIndex:\n\n\nCode\nquarterly_revenue = pd.Series([300, 320, 290, 390, 320, 360, 310, 410], index = quarters)\nquarterly_revenue\n\n\n2016Q1    300\n2016Q2    320\n2016Q3    290\n2016Q4    390\n2017Q1    320\n2017Q2    360\n2017Q3    310\n2017Q4    410\nFreq: Q-DEC, dtype: int64\n\n\n\n\nCode\nquarterly_revenue.plot(kind=\"line\")\nplt.show()\n\n\n\n\n\nWe can convert periods to timestamps by calling to_timestamp. By default this will give us the first day of each period, but by setting how and freq, we can get the last hour of each period:\n\n\nCode\nlast_hours = quarterly_revenue.to_timestamp(how=\"end\", freq=\"H\")\nlast_hours\n\n\n2016-03-31 23:59:59.999999999    300\n2016-06-30 23:59:59.999999999    320\n2016-09-30 23:59:59.999999999    290\n2016-12-31 23:59:59.999999999    390\n2017-03-31 23:59:59.999999999    320\n2017-06-30 23:59:59.999999999    360\n2017-09-30 23:59:59.999999999    310\n2017-12-31 23:59:59.999999999    410\ndtype: int64\n\n\nAnd back to periods by calling to_period:\n\n\nCode\nlast_hours.to_period()\n\n\n2016Q1    300\n2016Q2    320\n2016Q3    290\n2016Q4    390\n2017Q1    320\n2017Q2    360\n2017Q3    310\n2017Q4    410\nFreq: Q-DEC, dtype: int64\n\n\nPandas also provides many other time-related functions that we recommend you check out in the documentation. To whet your appetite, here is one way to get the last business day of each month in 2016, at 9am:\n\n\nCode\nmonths_2016 = pd.period_range(\"2016\", periods=12, freq=\"M\")\none_day_after_last_days = months_2016.asfreq(\"D\") + 1\nlast_bdays = one_day_after_last_days.to_timestamp() - pd.tseries.offsets.BDay()\nlast_bdays.to_period(\"H\") + 9\n\n\nPeriodIndex(['2016-01-29 09:00', '2016-02-29 09:00', '2016-03-31 09:00',\n             '2016-04-29 09:00', '2016-05-31 09:00', '2016-06-30 09:00',\n             '2016-07-29 09:00', '2016-08-31 09:00', '2016-09-30 09:00',\n             '2016-10-31 09:00', '2016-11-30 09:00', '2016-12-30 09:00'],\n            dtype='period[H]')"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#creating-a-dataframe",
    "href": "posts/pandas/tools-pandas.html#creating-a-dataframe",
    "title": "Tools - pandas",
    "section": "Creating a DataFrame",
    "text": "Creating a DataFrame\nYou can create a DataFrame by passing a dictionary of Series objects:\n\n\nCode\npeople_dict = {\n    \"weight\": pd.Series([68, 83, 112], index=[\"alice\", \"bob\", \"charles\"]),\n    \"birthyear\": pd.Series([1984, 1985, 1992], index=[\"bob\", \"alice\", \"charles\"], name=\"year\"),\n    \"children\": pd.Series([0, 3], index=[\"charles\", \"bob\"]),\n    \"hobby\": pd.Series([\"Biking\", \"Dancing\"], index=[\"alice\", \"bob\"]),\n}\npeople = pd.DataFrame(people_dict)\npeople\n\n\n\n\n\n\n\n\n\nweight\nbirthyear\nchildren\nhobby\n\n\n\n\nalice\n68\n1985\nNaN\nBiking\n\n\nbob\n83\n1984\n3.0\nDancing\n\n\ncharles\n112\n1992\n0.0\nNaN\n\n\n\n\n\n\n\nA few things to note: * the Series were automatically aligned based on their index, * missing values are represented as NaN, * Series names are ignored (the name \"year\" was dropped), * DataFrames are displayed nicely in Jupyter notebooks, woohoo!\nYou can access columns pretty much as you would expect. They are returned as Series objects:\n\n\nCode\npeople[\"birthyear\"]\n\n\nalice      1985\nbob        1984\ncharles    1992\nName: birthyear, dtype: int64\n\n\nYou can also get multiple columns at once:\n\n\nCode\npeople[[\"birthyear\", \"hobby\"]]\n\n\n\n\n\n\n\n\n\nbirthyear\nhobby\n\n\n\n\nalice\n1985\nBiking\n\n\nbob\n1984\nDancing\n\n\ncharles\n1992\nNaN\n\n\n\n\n\n\n\nIf you pass a list of columns and/or index row labels to the DataFrame constructor, it will guarantee that these columns and/or rows will exist, in that order, and no other column/row will exist. For example:\n\n\nCode\nd2 = pd.DataFrame(\n        people_dict,\n        columns=[\"birthyear\", \"weight\", \"height\"],\n        index=[\"bob\", \"alice\", \"eugene\"]\n     )\nd2\n\n\n\n\n\n\n\n\n\nbirthyear\nweight\nheight\n\n\n\n\nbob\n1984.0\n83.0\nNaN\n\n\nalice\n1985.0\n68.0\nNaN\n\n\neugene\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\nAnother convenient way to create a DataFrame is to pass all the values to the constructor as an ndarray, or a list of lists, and specify the column names and row index labels separately:\n\n\nCode\nvalues = [\n            [1985, np.nan, \"Biking\",   68],\n            [1984, 3,      \"Dancing\",  83],\n            [1992, 0,      np.nan,    112]\n         ]\nd3 = pd.DataFrame(\n        values,\n        columns=[\"birthyear\", \"children\", \"hobby\", \"weight\"],\n        index=[\"alice\", \"bob\", \"charles\"]\n     )\nd3\n\n\n\n\n\n\n\n\n\nbirthyear\nchildren\nhobby\nweight\n\n\n\n\nalice\n1985\nNaN\nBiking\n68\n\n\nbob\n1984\n3.0\nDancing\n83\n\n\ncharles\n1992\n0.0\nNaN\n112\n\n\n\n\n\n\n\nTo specify missing values, you can either use np.nan or NumPy’s masked arrays:\n\n\nCode\nmasked_array = np.ma.asarray(values, dtype=np.object)\nmasked_array[(0, 2), (1, 2)] = np.ma.masked\nd3 = pd.DataFrame(\n        masked_array,\n        columns=[\"birthyear\", \"children\", \"hobby\", \"weight\"],\n        index=[\"alice\", \"bob\", \"charles\"]\n     )\nd3\n\n\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  \"\"\"Entry point for launching an IPython kernel.\n\n\n\n\n\n\n\n\n\nbirthyear\nchildren\nhobby\nweight\n\n\n\n\nalice\n1985\nNaN\nBiking\n68\n\n\nbob\n1984\n3\nDancing\n83\n\n\ncharles\n1992\n0\nNaN\n112\n\n\n\n\n\n\n\nInstead of an ndarray, you can also pass a DataFrame object:\n\n\nCode\nd4 = pd.DataFrame(\n         d3,\n         columns=[\"hobby\", \"children\"],\n         index=[\"alice\", \"bob\"]\n     )\nd4\n\n\n\n\n\n\n\n\n\nhobby\nchildren\n\n\n\n\nalice\nBiking\nNaN\n\n\nbob\nDancing\n3\n\n\n\n\n\n\n\nIt is also possible to create a DataFrame with a dictionary (or list) of dictionaries (or list):\n\n\nCode\npeople = pd.DataFrame({\n    \"birthyear\": {\"alice\":1985, \"bob\": 1984, \"charles\": 1992},\n    \"hobby\": {\"alice\":\"Biking\", \"bob\": \"Dancing\"},\n    \"weight\": {\"alice\":68, \"bob\": 83, \"charles\": 112},\n    \"children\": {\"bob\": 3, \"charles\": 0}\n})\npeople\n\n\n\n\n\n\n\n\n\nbirthyear\nhobby\nweight\nchildren\n\n\n\n\nalice\n1985\nBiking\n68\nNaN\n\n\nbob\n1984\nDancing\n83\n3.0\n\n\ncharles\n1992\nNaN\n112\n0.0"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#multi-indexing",
    "href": "posts/pandas/tools-pandas.html#multi-indexing",
    "title": "Tools - pandas",
    "section": "Multi-indexing",
    "text": "Multi-indexing\nIf all columns are tuples of the same size, then they are understood as a multi-index. The same goes for row index labels. For example:\n\n\nCode\nd5 = pd.DataFrame(\n  {\n    (\"public\", \"birthyear\"):\n        {(\"Paris\",\"alice\"):1985, (\"Paris\",\"bob\"): 1984, (\"London\",\"charles\"): 1992},\n    (\"public\", \"hobby\"):\n        {(\"Paris\",\"alice\"):\"Biking\", (\"Paris\",\"bob\"): \"Dancing\"},\n    (\"private\", \"weight\"):\n        {(\"Paris\",\"alice\"):68, (\"Paris\",\"bob\"): 83, (\"London\",\"charles\"): 112},\n    (\"private\", \"children\"):\n        {(\"Paris\", \"alice\"):np.nan, (\"Paris\",\"bob\"): 3, (\"London\",\"charles\"): 0}\n  }\n)\nd5\n\n\n\n\n\n\n\n\n\n\npublic\nprivate\n\n\n\n\nbirthyear\nhobby\nweight\nchildren\n\n\n\n\nParis\nalice\n1985\nBiking\n68\nNaN\n\n\nbob\n1984\nDancing\n83\n3.0\n\n\nLondon\ncharles\n1992\nNaN\n112\n0.0\n\n\n\n\n\n\n\nYou can now get a DataFrame containing all the \"public\" columns very simply:\n\n\nCode\nd5[\"public\"]\n\n\n\n\n\n\n\n\n\n\nbirthyear\nhobby\n\n\n\n\nParis\nalice\n1985\nBiking\n\n\nbob\n1984\nDancing\n\n\nLondon\ncharles\n1992\nNaN\n\n\n\n\n\n\n\n\n\nCode\nd5[\"public\", \"hobby\"]  # Same result as d5[\"public\"][\"hobby\"]\n\n\nParis   alice       Biking\n        bob        Dancing\nLondon  charles        NaN\nName: (public, hobby), dtype: object"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#dropping-a-level",
    "href": "posts/pandas/tools-pandas.html#dropping-a-level",
    "title": "Tools - pandas",
    "section": "Dropping a level",
    "text": "Dropping a level\nLet’s look at d5 again:\n\n\nCode\nd5\n\n\n\n\n\n\n\n\n\n\npublic\nprivate\n\n\n\n\nbirthyear\nhobby\nweight\nchildren\n\n\n\n\nParis\nalice\n1985\nBiking\n68\nNaN\n\n\nbob\n1984\nDancing\n83\n3.0\n\n\nLondon\ncharles\n1992\nNaN\n112\n0.0\n\n\n\n\n\n\n\nThere are two levels of columns, and two levels of indices. We can drop a column level by calling droplevel() (the same goes for indices):\n\n\nCode\nd5.columns = d5.columns.droplevel(level = 0)\nd5\n\n\n\n\n\n\n\n\n\n\nbirthyear\nhobby\nweight\nchildren\n\n\n\n\nParis\nalice\n1985\nBiking\n68\nNaN\n\n\nbob\n1984\nDancing\n83\n3.0\n\n\nLondon\ncharles\n1992\nNaN\n112\n0.0"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#transposing",
    "href": "posts/pandas/tools-pandas.html#transposing",
    "title": "Tools - pandas",
    "section": "Transposing",
    "text": "Transposing\nYou can swap columns and indices using the T attribute:\n\n\nCode\nd6 = d5.T\nd6\n\n\n\n\n\n\n\n\n\nParis\nLondon\n\n\n\nalice\nbob\ncharles\n\n\n\n\nbirthyear\n1985\n1984\n1992\n\n\nhobby\nBiking\nDancing\nNaN\n\n\nweight\n68\n83\n112\n\n\nchildren\nNaN\n3.0\n0.0"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#stacking-and-unstacking-levels",
    "href": "posts/pandas/tools-pandas.html#stacking-and-unstacking-levels",
    "title": "Tools - pandas",
    "section": "Stacking and unstacking levels",
    "text": "Stacking and unstacking levels\nCalling the stack() method will push the lowest column level after the lowest index:\n\n\nCode\nd7 = d6.stack()\nd7\n\n\n\n\n\n\n\n\n\n\nLondon\nParis\n\n\n\n\nbirthyear\nalice\nNaN\n1985\n\n\nbob\nNaN\n1984\n\n\ncharles\n1992\nNaN\n\n\nhobby\nalice\nNaN\nBiking\n\n\nbob\nNaN\nDancing\n\n\nweight\nalice\nNaN\n68\n\n\nbob\nNaN\n83\n\n\ncharles\n112\nNaN\n\n\nchildren\nbob\nNaN\n3.0\n\n\ncharles\n0.0\nNaN\n\n\n\n\n\n\n\nNote that many NaN values appeared. This makes sense because many new combinations did not exist before (eg. there was no bob in London).\nCalling unstack() will do the reverse, once again creating many NaN values.\n\n\nCode\nd8 = d7.unstack()\nd8\n\n\n\n\n\n\n\n\n\nLondon\nParis\n\n\n\nalice\nbob\ncharles\nalice\nbob\ncharles\n\n\n\n\nbirthyear\nNaN\nNaN\n1992\n1985\n1984\nNaN\n\n\nchildren\nNaN\nNaN\n0.0\nNaN\n3.0\nNaN\n\n\nhobby\nNaN\nNaN\nNaN\nBiking\nDancing\nNaN\n\n\nweight\nNaN\nNaN\n112\n68\n83\nNaN\n\n\n\n\n\n\n\nIf we call unstack again, we end up with a Series object:\n\n\nCode\nd9 = d8.unstack()\nd9\n\n\nLondon  alice    birthyear        NaN\n                 children         NaN\n                 hobby            NaN\n                 weight           NaN\n        bob      birthyear        NaN\n                 children         NaN\n                 hobby            NaN\n                 weight           NaN\n        charles  birthyear       1992\n                 children         0.0\n                 hobby            NaN\n                 weight           112\nParis   alice    birthyear       1985\n                 children         NaN\n                 hobby         Biking\n                 weight            68\n        bob      birthyear       1984\n                 children         3.0\n                 hobby        Dancing\n                 weight            83\n        charles  birthyear        NaN\n                 children         NaN\n                 hobby            NaN\n                 weight           NaN\ndtype: object\n\n\nThe stack() and unstack() methods let you select the level to stack/unstack. You can even stack/unstack multiple levels at once:\n\n\nCode\nd10 = d9.unstack(level = (0,1))\nd10\n\n\n\n\n\n\n\n\n\nLondon\nParis\n\n\n\nalice\nbob\ncharles\nalice\nbob\ncharles\n\n\n\n\nbirthyear\nNaN\nNaN\n1992\n1985\n1984\nNaN\n\n\nchildren\nNaN\nNaN\n0.0\nNaN\n3.0\nNaN\n\n\nhobby\nNaN\nNaN\nNaN\nBiking\nDancing\nNaN\n\n\nweight\nNaN\nNaN\n112\n68\n83\nNaN"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#most-methods-return-modified-copies",
    "href": "posts/pandas/tools-pandas.html#most-methods-return-modified-copies",
    "title": "Tools - pandas",
    "section": "Most methods return modified copies",
    "text": "Most methods return modified copies\nAs you may have noticed, the stack() and unstack() methods do not modify the object they apply to. Instead, they work on a copy and return that copy. This is true of most methods in pandas."
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#accessing-rows",
    "href": "posts/pandas/tools-pandas.html#accessing-rows",
    "title": "Tools - pandas",
    "section": "Accessing rows",
    "text": "Accessing rows\nLet’s go back to the people DataFrame:\n\n\nCode\npeople\n\n\n\n\n\n\n\n\n\nbirthyear\nhobby\nweight\nchildren\n\n\n\n\nalice\n1985\nBiking\n68\nNaN\n\n\nbob\n1984\nDancing\n83\n3.0\n\n\ncharles\n1992\nNaN\n112\n0.0\n\n\n\n\n\n\n\nThe loc attribute lets you access rows instead of columns. The result is a Series object in which the DataFrame’s column names are mapped to row index labels:\n\n\nCode\npeople.loc[\"charles\"]\n\n\nbirthyear    1992\nhobby         NaN\nweight        112\nchildren      0.0\nName: charles, dtype: object\n\n\nYou can also access rows by integer location using the iloc attribute:\n\n\nCode\npeople.iloc[2]\n\n\nbirthyear    1992\nhobby         NaN\nweight        112\nchildren      0.0\nName: charles, dtype: object\n\n\nYou can also get a slice of rows, and this returns a DataFrame object:\n\n\nCode\npeople.iloc[1:3]\n\n\n\n\n\n\n\n\n\nbirthyear\nhobby\nweight\nchildren\n\n\n\n\nbob\n1984\nDancing\n83\n3.0\n\n\ncharles\n1992\nNaN\n112\n0.0\n\n\n\n\n\n\n\nFinally, you can pass a boolean array to get the matching rows:\n\n\nCode\npeople[np.array([True, False, True])]\n\n\n\n\n\n\n\n\n\nbirthyear\nhobby\nweight\nchildren\n\n\n\n\nalice\n1985\nBiking\n68\nNaN\n\n\ncharles\n1992\nNaN\n112\n0.0\n\n\n\n\n\n\n\nThis is most useful when combined with boolean expressions:\n\n\nCode\npeople[people[\"birthyear\"] &lt; 1990]\n\n\n\n\n\n\n\n\n\nbirthyear\nhobby\nweight\nchildren\n\n\n\n\nalice\n1985\nBiking\n68\nNaN\n\n\nbob\n1984\nDancing\n83\n3.0"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#adding-and-removing-columns",
    "href": "posts/pandas/tools-pandas.html#adding-and-removing-columns",
    "title": "Tools - pandas",
    "section": "Adding and removing columns",
    "text": "Adding and removing columns\nYou can generally treat DataFrame objects like dictionaries of Series, so the following work fine:\n\n\nCode\npeople\n\n\n\n\n\n\n\n\n\nbirthyear\nhobby\nweight\nchildren\n\n\n\n\nalice\n1985\nBiking\n68\nNaN\n\n\nbob\n1984\nDancing\n83\n3.0\n\n\ncharles\n1992\nNaN\n112\n0.0\n\n\n\n\n\n\n\n\n\nCode\npeople[\"age\"] = 2018 - people[\"birthyear\"]  # adds a new column \"age\"\npeople[\"over 30\"] = people[\"age\"] &gt; 30      # adds another column \"over 30\"\nbirthyears = people.pop(\"birthyear\")\ndel people[\"children\"]\n\npeople\n\n\n\n\n\n\n\n\n\nhobby\nweight\nage\nover 30\n\n\n\n\nalice\nBiking\n68\n33\nTrue\n\n\nbob\nDancing\n83\n34\nTrue\n\n\ncharles\nNaN\n112\n26\nFalse\n\n\n\n\n\n\n\n\n\nCode\nbirthyears\n\n\nalice      1985\nbob        1984\ncharles    1992\nName: birthyear, dtype: int64\n\n\nWhen you add a new colum, it must have the same number of rows. Missing rows are filled with NaN, and extra rows are ignored:\n\n\nCode\npeople[\"pets\"] = pd.Series({\"bob\": 0, \"charles\": 5, \"eugene\":1})  # alice is missing, eugene is ignored\npeople\n\n\n\n\n\n\n\n\n\nhobby\nweight\nage\nover 30\npets\n\n\n\n\nalice\nBiking\n68\n33\nTrue\nNaN\n\n\nbob\nDancing\n83\n34\nTrue\n0.0\n\n\ncharles\nNaN\n112\n26\nFalse\n5.0\n\n\n\n\n\n\n\nWhen adding a new column, it is added at the end (on the right) by default. You can also insert a column anywhere else using the insert() method:\n\n\nCode\npeople.insert(1, \"height\", [172, 181, 185])\npeople\n\n\n\n\n\n\n\n\n\nhobby\nheight\nweight\nage\nover 30\npets\n\n\n\n\nalice\nBiking\n172\n68\n33\nTrue\nNaN\n\n\nbob\nDancing\n181\n83\n34\nTrue\n0.0\n\n\ncharles\nNaN\n185\n112\n26\nFalse\n5.0"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#assigning-new-columns",
    "href": "posts/pandas/tools-pandas.html#assigning-new-columns",
    "title": "Tools - pandas",
    "section": "Assigning new columns",
    "text": "Assigning new columns\nYou can also create new columns by calling the assign() method. Note that this returns a new DataFrame object, the original is not modified:\n\n\nCode\npeople.assign(\n    body_mass_index = people[\"weight\"] / (people[\"height\"] / 100) ** 2,\n    has_pets = people[\"pets\"] &gt; 0\n)\n\n\n\n\n\n\n\n\n\nhobby\nheight\nweight\nage\nover 30\npets\nbody_mass_index\nhas_pets\n\n\n\n\nalice\nBiking\n172\n68\n33\nTrue\nNaN\n22.985398\nFalse\n\n\nbob\nDancing\n181\n83\n34\nTrue\n0.0\n25.335002\nFalse\n\n\ncharles\nNaN\n185\n112\n26\nFalse\n5.0\n32.724617\nTrue\n\n\n\n\n\n\n\nNote that you cannot access columns created within the same assignment:\n\n\nCode\ntry:\n    people.assign(\n        body_mass_index = people[\"weight\"] / (people[\"height\"] / 100) ** 2,\n        overweight = people[\"body_mass_index\"] &gt; 25\n    )\nexcept KeyError as e:\n    print(\"Key error:\", e)\n\n\nKey error: 'body_mass_index'\n\n\nThe solution is to split this assignment in two consecutive assignments:\n\n\nCode\nd6 = people.assign(body_mass_index = people[\"weight\"] / (people[\"height\"] / 100) ** 2)\nd6.assign(overweight = d6[\"body_mass_index\"] &gt; 25)\n\n\n\n\n\n\n\n\n\nhobby\nheight\nweight\nage\nover 30\npets\nbody_mass_index\noverweight\n\n\n\n\nalice\nBiking\n172\n68\n33\nTrue\nNaN\n22.985398\nFalse\n\n\nbob\nDancing\n181\n83\n34\nTrue\n0.0\n25.335002\nTrue\n\n\ncharles\nNaN\n185\n112\n26\nFalse\n5.0\n32.724617\nTrue\n\n\n\n\n\n\n\nHaving to create a temporary variable d6 is not very convenient. You may want to just chain the assigment calls, but it does not work because the people object is not actually modified by the first assignment:\n\n\nCode\ntry:\n    (people\n         .assign(body_mass_index = people[\"weight\"] / (people[\"height\"] / 100) ** 2)\n         .assign(overweight = people[\"body_mass_index\"] &gt; 25)\n    )\nexcept KeyError as e:\n    print(\"Key error:\", e)\n\n\nKey error: 'body_mass_index'\n\n\nBut fear not, there is a simple solution. You can pass a function to the assign() method (typically a lambda function), and this function will be called with the DataFrame as a parameter:\n\n\nCode\n(people\n     .assign(body_mass_index = lambda df: df[\"weight\"] / (df[\"height\"] / 100) ** 2)\n     .assign(overweight = lambda df: df[\"body_mass_index\"] &gt; 25)\n)\n\n\n\n\n\n\n\n\n\nhobby\nheight\nweight\nage\nover 30\npets\nbody_mass_index\noverweight\n\n\n\n\nalice\nBiking\n172\n68\n33\nTrue\nNaN\n22.985398\nFalse\n\n\nbob\nDancing\n181\n83\n34\nTrue\n0.0\n25.335002\nTrue\n\n\ncharles\nNaN\n185\n112\n26\nFalse\n5.0\n32.724617\nTrue\n\n\n\n\n\n\n\nProblem solved!"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#evaluating-an-expression",
    "href": "posts/pandas/tools-pandas.html#evaluating-an-expression",
    "title": "Tools - pandas",
    "section": "Evaluating an expression",
    "text": "Evaluating an expression\nA great feature supported by pandas is expression evaluation. This relies on the numexpr library which must be installed.\n\n\nCode\npeople.eval(\"weight / (height/100) ** 2 &gt; 25\")\n\n\nalice      False\nbob         True\ncharles     True\ndtype: bool\n\n\nAssignment expressions are also supported. Let’s set inplace=True to directly modify the DataFrame rather than getting a modified copy:\n\n\nCode\npeople.eval(\"body_mass_index = weight / (height/100) ** 2\", inplace=True)\npeople\n\n\n\n\n\n\n\n\n\nhobby\nheight\nweight\nage\nover 30\npets\nbody_mass_index\n\n\n\n\nalice\nBiking\n172\n68\n33\nTrue\nNaN\n22.985398\n\n\nbob\nDancing\n181\n83\n34\nTrue\n0.0\n25.335002\n\n\ncharles\nNaN\n185\n112\n26\nFalse\n5.0\n32.724617\n\n\n\n\n\n\n\nYou can use a local or global variable in an expression by prefixing it with '@':\n\n\nCode\noverweight_threshold = 30\npeople.eval(\"overweight = body_mass_index &gt; @overweight_threshold\", inplace=True)\npeople\n\n\n\n\n\n\n\n\n\nhobby\nheight\nweight\nage\nover 30\npets\nbody_mass_index\noverweight\n\n\n\n\nalice\nBiking\n172\n68\n33\nTrue\nNaN\n22.985398\nFalse\n\n\nbob\nDancing\n181\n83\n34\nTrue\n0.0\n25.335002\nFalse\n\n\ncharles\nNaN\n185\n112\n26\nFalse\n5.0\n32.724617\nTrue"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#querying-a-dataframe",
    "href": "posts/pandas/tools-pandas.html#querying-a-dataframe",
    "title": "Tools - pandas",
    "section": "Querying a DataFrame",
    "text": "Querying a DataFrame\nThe query() method lets you filter a DataFrame based on a query expression:\n\n\nCode\npeople.query(\"age &gt; 30 and pets == 0\")\n\n\n\n\n\n\n\n\n\nhobby\nheight\nweight\nage\nover 30\npets\nbody_mass_index\noverweight\n\n\n\n\nbob\nDancing\n181\n83\n34\nTrue\n0.0\n25.335002\nFalse"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#sorting-a-dataframe",
    "href": "posts/pandas/tools-pandas.html#sorting-a-dataframe",
    "title": "Tools - pandas",
    "section": "Sorting a DataFrame",
    "text": "Sorting a DataFrame\nYou can sort a DataFrame by calling its sort_index method. By default it sorts the rows by their index label, in ascending order, but let’s reverse the order:\n\n\nCode\npeople.sort_index(ascending=False)\n\n\n\n\n\n\n\n\n\nhobby\nheight\nweight\nage\nover 30\npets\nbody_mass_index\noverweight\n\n\n\n\ncharles\nNaN\n185\n112\n26\nFalse\n5.0\n32.724617\nTrue\n\n\nbob\nDancing\n181\n83\n34\nTrue\n0.0\n25.335002\nFalse\n\n\nalice\nBiking\n172\n68\n33\nTrue\nNaN\n22.985398\nFalse\n\n\n\n\n\n\n\nNote that sort_index returned a sorted copy of the DataFrame. To modify people directly, we can set the inplace argument to True. Also, we can sort the columns instead of the rows by setting axis=1:\n\n\nCode\npeople.sort_index(axis=1, inplace=True)\npeople\n\n\n\n\n\n\n\n\n\nage\nbody_mass_index\nheight\nhobby\nover 30\noverweight\npets\nweight\n\n\n\n\nalice\n33\n22.985398\n172\nBiking\nTrue\nFalse\nNaN\n68\n\n\nbob\n34\n25.335002\n181\nDancing\nTrue\nFalse\n0.0\n83\n\n\ncharles\n26\n32.724617\n185\nNaN\nFalse\nTrue\n5.0\n112\n\n\n\n\n\n\n\nTo sort the DataFrame by the values instead of the labels, we can use sort_values and specify the column to sort by:\n\n\nCode\npeople.sort_values(by=\"age\", inplace=True)\npeople\n\n\n\n\n\n\n\n\n\nage\nbody_mass_index\nheight\nhobby\nover 30\noverweight\npets\nweight\n\n\n\n\ncharles\n26\n32.724617\n185\nNaN\nFalse\nTrue\n5.0\n112\n\n\nalice\n33\n22.985398\n172\nBiking\nTrue\nFalse\nNaN\n68\n\n\nbob\n34\n25.335002\n181\nDancing\nTrue\nFalse\n0.0\n83"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#plotting-a-dataframe",
    "href": "posts/pandas/tools-pandas.html#plotting-a-dataframe",
    "title": "Tools - pandas",
    "section": "Plotting a DataFrame",
    "text": "Plotting a DataFrame\nJust like for Series, pandas makes it easy to draw nice graphs based on a DataFrame.\nFor example, it is trivial to create a line plot from a DataFrame’s data by calling its plot method:\n\n\nCode\npeople.plot(kind = \"line\", x = \"body_mass_index\", y = [\"height\", \"weight\"])\nplt.show()\n\n\n\n\n\nYou can pass extra arguments supported by matplotlib’s functions. For example, we can create scatterplot and pass it a list of sizes using the s argument of matplotlib’s scatter() function:\n\n\nCode\npeople.plot(kind = \"scatter\", x = \"height\", y = \"weight\", s=[40, 120, 200])\nplt.show()\n\n\n\n\n\nAgain, there are way too many options to list here: the best option is to scroll through the Visualization page in pandas’ documentation, find the plot you are interested in and look at the example code."
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#operations-on-dataframes",
    "href": "posts/pandas/tools-pandas.html#operations-on-dataframes",
    "title": "Tools - pandas",
    "section": "Operations on DataFrames",
    "text": "Operations on DataFrames\nAlthough DataFrames do not try to mimick NumPy arrays, there are a few similarities. Let’s create a DataFrame to demonstrate this:\n\n\nCode\ngrades_array = np.array([[8,8,9],[10,9,9],[4, 8, 2], [9, 10, 10]])\ngrades = pd.DataFrame(grades_array, columns=[\"sep\", \"oct\", \"nov\"], index=[\"alice\",\"bob\",\"charles\",\"darwin\"])\ngrades\n\n\n\n\n\n\n\n\n\nsep\noct\nnov\n\n\n\n\nalice\n8\n8\n9\n\n\nbob\n10\n9\n9\n\n\ncharles\n4\n8\n2\n\n\ndarwin\n9\n10\n10\n\n\n\n\n\n\n\nYou can apply NumPy mathematical functions on a DataFrame: the function is applied to all values:\n\n\nCode\nnp.sqrt(grades)\n\n\n\n\n\n\n\n\n\nsep\noct\nnov\n\n\n\n\nalice\n2.828427\n2.828427\n3.000000\n\n\nbob\n3.162278\n3.000000\n3.000000\n\n\ncharles\n2.000000\n2.828427\n1.414214\n\n\ndarwin\n3.000000\n3.162278\n3.162278\n\n\n\n\n\n\n\nSimilarly, adding a single value to a DataFrame will add that value to all elements in the DataFrame. This is called broadcasting:\n\n\nCode\ngrades + 1\n\n\n\n\n\n\n\n\n\nsep\noct\nnov\n\n\n\n\nalice\n9\n9\n10\n\n\nbob\n11\n10\n10\n\n\ncharles\n5\n9\n3\n\n\ndarwin\n10\n11\n11\n\n\n\n\n\n\n\nOf course, the same is true for all other binary operations, including arithmetic (*,/,**…) and conditional (&gt;, ==…) operations:\n\n\nCode\ngrades &gt;= 5\n\n\n\n\n\n\n\n\n\nsep\noct\nnov\n\n\n\n\nalice\nTrue\nTrue\nTrue\n\n\nbob\nTrue\nTrue\nTrue\n\n\ncharles\nFalse\nTrue\nFalse\n\n\ndarwin\nTrue\nTrue\nTrue\n\n\n\n\n\n\n\nAggregation operations, such as computing the max, the sum or the mean of a DataFrame, apply to each column, and you get back a Series object:\n\n\nCode\ngrades.mean()\n\n\nsep    7.75\noct    8.75\nnov    7.50\ndtype: float64\n\n\nThe all method is also an aggregation operation: it checks whether all values are True or not. Let’s see during which months all students got a grade greater than 5:\n\n\nCode\n(grades &gt; 5).all()\n\n\nsep    False\noct     True\nnov    False\ndtype: bool\n\n\nMost of these functions take an optional axis parameter which lets you specify along which axis of the DataFrame you want the operation executed. The default is axis=0, meaning that the operation is executed vertically (on each column). You can set axis=1 to execute the operation horizontally (on each row). For example, let’s find out which students had all grades greater than 5:\n\n\nCode\n(grades &gt; 5).all(axis = 1)\n\n\nalice       True\nbob         True\ncharles    False\ndarwin      True\ndtype: bool\n\n\nThe any method returns True if any value is True. Let’s see who got at least one grade 10:\n\n\nCode\n(grades == 10).any(axis = 1)\n\n\nalice      False\nbob         True\ncharles    False\ndarwin      True\ndtype: bool\n\n\nIf you add a Series object to a DataFrame (or execute any other binary operation), pandas attempts to broadcast the operation to all rows in the DataFrame. This only works if the Series has the same size as the DataFrames rows. For example, let’s subtract the mean of the DataFrame (a Series object) from the DataFrame:\n\n\nCode\ngrades - grades.mean()  # equivalent to: grades - [7.75, 8.75, 7.50]\n\n\n\n\n\n\n\n\n\nsep\noct\nnov\n\n\n\n\nalice\n0.25\n-0.75\n1.5\n\n\nbob\n2.25\n0.25\n1.5\n\n\ncharles\n-3.75\n-0.75\n-5.5\n\n\ndarwin\n1.25\n1.25\n2.5\n\n\n\n\n\n\n\nWe subtracted 7.75 from all September grades, 8.75 from October grades and 7.50 from November grades. It is equivalent to subtracting this DataFrame:\n\n\nCode\npd.DataFrame([[7.75, 8.75, 7.50]]*4, index=grades.index, columns=grades.columns)\n\n\n\n\n\n\n\n\n\nsep\noct\nnov\n\n\n\n\nalice\n7.75\n8.75\n7.5\n\n\nbob\n7.75\n8.75\n7.5\n\n\ncharles\n7.75\n8.75\n7.5\n\n\ndarwin\n7.75\n8.75\n7.5\n\n\n\n\n\n\n\nIf you want to subtract the global mean from every grade, here is one way to do it:\n\n\nCode\ngrades - grades.values.mean() # subtracts the global mean (8.00) from all grades\n\n\n\n\n\n\n\n\n\nsep\noct\nnov\n\n\n\n\nalice\n0.0\n0.0\n1.0\n\n\nbob\n2.0\n1.0\n1.0\n\n\ncharles\n-4.0\n0.0\n-6.0\n\n\ndarwin\n1.0\n2.0\n2.0"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#automatic-alignment-1",
    "href": "posts/pandas/tools-pandas.html#automatic-alignment-1",
    "title": "Tools - pandas",
    "section": "Automatic alignment",
    "text": "Automatic alignment\nSimilar to Series, when operating on multiple DataFrames, pandas automatically aligns them by row index label, but also by column names. Let’s create a DataFrame with bonus points for each person from October to December:\n\n\nCode\nbonus_array = np.array([[0,np.nan,2],[np.nan,1,0],[0, 1, 0], [3, 3, 0]])\nbonus_points = pd.DataFrame(bonus_array, columns=[\"oct\", \"nov\", \"dec\"], index=[\"bob\",\"colin\", \"darwin\", \"charles\"])\nbonus_points\n\n\n\n\n\n\n\n\n\noct\nnov\ndec\n\n\n\n\nbob\n0.0\nNaN\n2.0\n\n\ncolin\nNaN\n1.0\n0.0\n\n\ndarwin\n0.0\n1.0\n0.0\n\n\ncharles\n3.0\n3.0\n0.0\n\n\n\n\n\n\n\n\n\nCode\ngrades + bonus_points\n\n\n\n\n\n\n\n\n\ndec\nnov\noct\nsep\n\n\n\n\nalice\nNaN\nNaN\nNaN\nNaN\n\n\nbob\nNaN\nNaN\n9.0\nNaN\n\n\ncharles\nNaN\n5.0\n11.0\nNaN\n\n\ncolin\nNaN\nNaN\nNaN\nNaN\n\n\ndarwin\nNaN\n11.0\n10.0\nNaN\n\n\n\n\n\n\n\nLooks like the addition worked in some cases but way too many elements are now empty. That’s because when aligning the DataFrames, some columns and rows were only present on one side, and thus they were considered missing on the other side (NaN). Then adding NaN to a number results in NaN, hence the result."
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#handling-missing-data",
    "href": "posts/pandas/tools-pandas.html#handling-missing-data",
    "title": "Tools - pandas",
    "section": "Handling missing data",
    "text": "Handling missing data\nDealing with missing data is a frequent task when working with real life data. Pandas offers a few tools to handle missing data.\nLet’s try to fix the problem above. For example, we can decide that missing data should result in a zero, instead of NaN. We can replace all NaN values by a any value using the fillna() method:\n\n\nCode\n(grades + bonus_points).fillna(0)\n\n\n\n\n\n\n\n\n\ndec\nnov\noct\nsep\n\n\n\n\nalice\n0.0\n0.0\n0.0\n0.0\n\n\nbob\n0.0\n0.0\n9.0\n0.0\n\n\ncharles\n0.0\n5.0\n11.0\n0.0\n\n\ncolin\n0.0\n0.0\n0.0\n0.0\n\n\ndarwin\n0.0\n11.0\n10.0\n0.0\n\n\n\n\n\n\n\nIt’s a bit unfair that we’re setting grades to zero in September, though. Perhaps we should decide that missing grades are missing grades, but missing bonus points should be replaced by zeros:\n\n\nCode\nfixed_bonus_points = bonus_points.fillna(0)\nfixed_bonus_points.insert(0, \"sep\", 0)\nfixed_bonus_points.loc[\"alice\"] = 0\ngrades + fixed_bonus_points\n\n\n\n\n\n\n\n\n\ndec\nnov\noct\nsep\n\n\n\n\nalice\nNaN\n9.0\n8.0\n8.0\n\n\nbob\nNaN\n9.0\n9.0\n10.0\n\n\ncharles\nNaN\n5.0\n11.0\n4.0\n\n\ncolin\nNaN\nNaN\nNaN\nNaN\n\n\ndarwin\nNaN\n11.0\n10.0\n9.0\n\n\n\n\n\n\n\nThat’s much better: although we made up some data, we have not been too unfair.\nAnother way to handle missing data is to interpolate. Let’s look at the bonus_points DataFrame again:\n\n\nCode\nbonus_points\n\n\n\n\n\n\n\n\n\noct\nnov\ndec\n\n\n\n\nbob\n0.0\nNaN\n2.0\n\n\ncolin\nNaN\n1.0\n0.0\n\n\ndarwin\n0.0\n1.0\n0.0\n\n\ncharles\n3.0\n3.0\n0.0\n\n\n\n\n\n\n\nNow let’s call the interpolate method. By default, it interpolates vertically (axis=0), so let’s tell it to interpolate horizontally (axis=1).\n\n\nCode\nbonus_points.interpolate(axis=1)\n\n\n\n\n\n\n\n\n\noct\nnov\ndec\n\n\n\n\nbob\n0.0\n1.0\n2.0\n\n\ncolin\nNaN\n1.0\n0.0\n\n\ndarwin\n0.0\n1.0\n0.0\n\n\ncharles\n3.0\n3.0\n0.0\n\n\n\n\n\n\n\nBob had 0 bonus points in October, and 2 in December. When we interpolate for November, we get the mean: 1 bonus point. Colin had 1 bonus point in November, but we do not know how many bonus points he had in September, so we cannot interpolate, this is why there is still a missing value in October after interpolation. To fix this, we can set the September bonus points to 0 before interpolation.\n\n\nCode\nbetter_bonus_points = bonus_points.copy()\nbetter_bonus_points.insert(0, \"sep\", 0)\nbetter_bonus_points.loc[\"alice\"] = 0\nbetter_bonus_points = better_bonus_points.interpolate(axis=1)\nbetter_bonus_points\n\n\n\n\n\n\n\n\n\nsep\noct\nnov\ndec\n\n\n\n\nbob\n0.0\n0.0\n1.0\n2.0\n\n\ncolin\n0.0\n0.5\n1.0\n0.0\n\n\ndarwin\n0.0\n0.0\n1.0\n0.0\n\n\ncharles\n0.0\n3.0\n3.0\n0.0\n\n\nalice\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\nGreat, now we have reasonable bonus points everywhere. Let’s find out the final grades:\n\n\nCode\ngrades + better_bonus_points\n\n\n\n\n\n\n\n\n\ndec\nnov\noct\nsep\n\n\n\n\nalice\nNaN\n9.0\n8.0\n8.0\n\n\nbob\nNaN\n10.0\n9.0\n10.0\n\n\ncharles\nNaN\n5.0\n11.0\n4.0\n\n\ncolin\nNaN\nNaN\nNaN\nNaN\n\n\ndarwin\nNaN\n11.0\n10.0\n9.0\n\n\n\n\n\n\n\nIt is slightly annoying that the September column ends up on the right. This is because the DataFrames we are adding do not have the exact same columns (the grades DataFrame is missing the \"dec\" column), so to make things predictable, pandas orders the final columns alphabetically. To fix this, we can simply add the missing column before adding:\n\n\nCode\ngrades[\"dec\"] = np.nan\nfinal_grades = grades + better_bonus_points\nfinal_grades\n\n\n\n\n\n\n\n\n\nsep\noct\nnov\ndec\n\n\n\n\nalice\n8.0\n8.0\n9.0\nNaN\n\n\nbob\n10.0\n9.0\n10.0\nNaN\n\n\ncharles\n4.0\n11.0\n5.0\nNaN\n\n\ncolin\nNaN\nNaN\nNaN\nNaN\n\n\ndarwin\n9.0\n10.0\n11.0\nNaN\n\n\n\n\n\n\n\nThere’s not much we can do about December and Colin: it’s bad enough that we are making up bonus points, but we can’t reasonably make up grades (well I guess some teachers probably do). So let’s call the dropna() method to get rid of rows that are full of NaNs:\n\n\nCode\nfinal_grades_clean = final_grades.dropna(how=\"all\")\nfinal_grades_clean\n\n\n\n\n\n\n\n\n\nsep\noct\nnov\ndec\n\n\n\n\nalice\n8.0\n8.0\n9.0\nNaN\n\n\nbob\n10.0\n9.0\n10.0\nNaN\n\n\ncharles\n4.0\n11.0\n5.0\nNaN\n\n\ndarwin\n9.0\n10.0\n11.0\nNaN\n\n\n\n\n\n\n\nNow let’s remove columns that are full of NaNs by setting the axis argument to 1:\n\n\nCode\nfinal_grades_clean = final_grades_clean.dropna(axis=1, how=\"all\")\nfinal_grades_clean\n\n\n\n\n\n\n\n\n\nsep\noct\nnov\n\n\n\n\nalice\n8.0\n8.0\n9.0\n\n\nbob\n10.0\n9.0\n10.0\n\n\ncharles\n4.0\n11.0\n5.0\n\n\ndarwin\n9.0\n10.0\n11.0"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#aggregating-with-groupby",
    "href": "posts/pandas/tools-pandas.html#aggregating-with-groupby",
    "title": "Tools - pandas",
    "section": "Aggregating with groupby",
    "text": "Aggregating with groupby\nSimilar to the SQL language, pandas allows grouping your data into groups to run calculations over each group.\nFirst, let’s add some extra data about each person so we can group them, and let’s go back to the final_grades DataFrame so we can see how NaN values are handled:\n\n\nCode\nfinal_grades[\"hobby\"] = [\"Biking\", \"Dancing\", np.nan, \"Dancing\", \"Biking\"]\nfinal_grades\n\n\n\n\n\n\n\n\n\nsep\noct\nnov\ndec\nhobby\n\n\n\n\nalice\n8.0\n8.0\n9.0\nNaN\nBiking\n\n\nbob\n10.0\n9.0\n10.0\nNaN\nDancing\n\n\ncharles\n4.0\n11.0\n5.0\nNaN\nNaN\n\n\ncolin\nNaN\nNaN\nNaN\nNaN\nDancing\n\n\ndarwin\n9.0\n10.0\n11.0\nNaN\nBiking\n\n\n\n\n\n\n\nNow let’s group data in this DataFrame by hobby:\n\n\nCode\ngrouped_grades = final_grades.groupby(\"hobby\")\ngrouped_grades\n\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f697dab7310&gt;\n\n\nWe are ready to compute the average grade per hobby:\n\n\nCode\ngrouped_grades.mean()\n\n\n\n\n\n\n\n\n\nsep\noct\nnov\ndec\n\n\nhobby\n\n\n\n\n\n\n\n\nBiking\n8.5\n9.0\n10.0\nNaN\n\n\nDancing\n10.0\n9.0\n10.0\nNaN\n\n\n\n\n\n\n\nThat was easy! Note that the NaN values have simply been skipped when computing the means."
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#pivot-tables",
    "href": "posts/pandas/tools-pandas.html#pivot-tables",
    "title": "Tools - pandas",
    "section": "Pivot tables",
    "text": "Pivot tables\nPandas supports spreadsheet-like pivot tables that allow quick data summarization. To illustrate this, let’s create a simple DataFrame:\n\n\nCode\nbonus_points\n\n\n\n\n\n\n\n\n\noct\nnov\ndec\n\n\n\n\nbob\n0.0\nNaN\n2.0\n\n\ncolin\nNaN\n1.0\n0.0\n\n\ndarwin\n0.0\n1.0\n0.0\n\n\ncharles\n3.0\n3.0\n0.0\n\n\n\n\n\n\n\n\n\nCode\nmore_grades = final_grades_clean.stack().reset_index()\nmore_grades.columns = [\"name\", \"month\", \"grade\"]\nmore_grades[\"bonus\"] = [np.nan, np.nan, np.nan, 0, np.nan, 2, 3, 3, 0, 0, 1, 0]\nmore_grades\n\n\n\n\n\n\n\n\n\nname\nmonth\ngrade\nbonus\n\n\n\n\n0\nalice\nsep\n8.0\nNaN\n\n\n1\nalice\noct\n8.0\nNaN\n\n\n2\nalice\nnov\n9.0\nNaN\n\n\n3\nbob\nsep\n10.0\n0.0\n\n\n4\nbob\noct\n9.0\nNaN\n\n\n5\nbob\nnov\n10.0\n2.0\n\n\n6\ncharles\nsep\n4.0\n3.0\n\n\n7\ncharles\noct\n11.0\n3.0\n\n\n8\ncharles\nnov\n5.0\n0.0\n\n\n9\ndarwin\nsep\n9.0\n0.0\n\n\n10\ndarwin\noct\n10.0\n1.0\n\n\n11\ndarwin\nnov\n11.0\n0.0\n\n\n\n\n\n\n\nNow we can call the pd.pivot_table() function for this DataFrame, asking to group by the name column. By default, pivot_table() computes the mean of each numeric column:\n\n\nCode\npd.pivot_table(more_grades, index=\"name\")\n\n\n\n\n\n\n\n\n\nbonus\ngrade\n\n\nname\n\n\n\n\n\n\nalice\nNaN\n8.333333\n\n\nbob\n1.000000\n9.666667\n\n\ncharles\n2.000000\n6.666667\n\n\ndarwin\n0.333333\n10.000000\n\n\n\n\n\n\n\nWe can change the aggregation function by setting the aggfunc argument, and we can also specify the list of columns whose values will be aggregated:\n\n\nCode\npd.pivot_table(more_grades, index=\"name\", values=[\"grade\",\"bonus\"], aggfunc=np.max)\n\n\n\n\n\n\n\n\n\nbonus\ngrade\n\n\nname\n\n\n\n\n\n\nalice\nNaN\n9.0\n\n\nbob\n2.0\n10.0\n\n\ncharles\n3.0\n11.0\n\n\ndarwin\n1.0\n11.0\n\n\n\n\n\n\n\nWe can also specify the columns to aggregate over horizontally, and request the grand totals for each row and column by setting margins=True:\n\n\nCode\npd.pivot_table(more_grades, index=\"name\", values=\"grade\", columns=\"month\", margins=True)\n\n\n\n\n\n\n\n\nmonth\nnov\noct\nsep\nAll\n\n\nname\n\n\n\n\n\n\n\n\nalice\n9.00\n8.0\n8.00\n8.333333\n\n\nbob\n10.00\n9.0\n10.00\n9.666667\n\n\ncharles\n5.00\n11.0\n4.00\n6.666667\n\n\ndarwin\n11.00\n10.0\n9.00\n10.000000\n\n\nAll\n8.75\n9.5\n7.75\n8.666667\n\n\n\n\n\n\n\nFinally, we can specify multiple index or column names, and pandas will create multi-level indices:\n\n\nCode\npd.pivot_table(more_grades, index=(\"name\", \"month\"), margins=True)\n\n\n\n\n\n\n\n\n\n\nbonus\ngrade\n\n\nname\nmonth\n\n\n\n\n\n\nalice\nnov\nNaN\n9.00\n\n\noct\nNaN\n8.00\n\n\nsep\nNaN\n8.00\n\n\nbob\nnov\n2.000\n10.00\n\n\noct\nNaN\n9.00\n\n\nsep\n0.000\n10.00\n\n\ncharles\nnov\n0.000\n5.00\n\n\noct\n3.000\n11.00\n\n\nsep\n3.000\n4.00\n\n\ndarwin\nnov\n0.000\n11.00\n\n\noct\n1.000\n10.00\n\n\nsep\n0.000\n9.00\n\n\nAll\n\n1.125\n8.75"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#overview-functions",
    "href": "posts/pandas/tools-pandas.html#overview-functions",
    "title": "Tools - pandas",
    "section": "Overview functions",
    "text": "Overview functions\nWhen dealing with large DataFrames, it is useful to get a quick overview of its content. Pandas offers a few functions for this. First, let’s create a large DataFrame with a mix of numeric values, missing values and text values. Notice how Jupyter displays only the corners of the DataFrame:\n\n\nCode\nmuch_data = np.fromfunction(lambda x,y: (x+y*y)%17*11, (10000, 26))\nlarge_df = pd.DataFrame(much_data, columns=list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"))\nlarge_df[large_df % 16 == 0] = np.nan\nlarge_df.insert(3,\"some_text\", \"Blabla\")\nlarge_df\n\n\n\n\n\n\n\n\n\nA\nB\nC\nsome_text\nD\nE\nF\nG\nH\nI\n...\nQ\nR\nS\nT\nU\nV\nW\nX\nY\nZ\n\n\n\n\n0\nNaN\n11.0\n44.0\nBlabla\n99.0\nNaN\n88.0\n22.0\n165.0\n143.0\n...\n11.0\nNaN\n11.0\n44.0\n99.0\nNaN\n88.0\n22.0\n165.0\n143.0\n\n\n1\n11.0\n22.0\n55.0\nBlabla\n110.0\nNaN\n99.0\n33.0\nNaN\n154.0\n...\n22.0\n11.0\n22.0\n55.0\n110.0\nNaN\n99.0\n33.0\nNaN\n154.0\n\n\n2\n22.0\n33.0\n66.0\nBlabla\n121.0\n11.0\n110.0\n44.0\nNaN\n165.0\n...\n33.0\n22.0\n33.0\n66.0\n121.0\n11.0\n110.0\n44.0\nNaN\n165.0\n\n\n3\n33.0\n44.0\n77.0\nBlabla\n132.0\n22.0\n121.0\n55.0\n11.0\nNaN\n...\n44.0\n33.0\n44.0\n77.0\n132.0\n22.0\n121.0\n55.0\n11.0\nNaN\n\n\n4\n44.0\n55.0\n88.0\nBlabla\n143.0\n33.0\n132.0\n66.0\n22.0\nNaN\n...\n55.0\n44.0\n55.0\n88.0\n143.0\n33.0\n132.0\n66.0\n22.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9995\nNaN\nNaN\n33.0\nBlabla\n88.0\n165.0\n77.0\n11.0\n154.0\n132.0\n...\nNaN\nNaN\nNaN\n33.0\n88.0\n165.0\n77.0\n11.0\n154.0\n132.0\n\n\n9996\nNaN\n11.0\n44.0\nBlabla\n99.0\nNaN\n88.0\n22.0\n165.0\n143.0\n...\n11.0\nNaN\n11.0\n44.0\n99.0\nNaN\n88.0\n22.0\n165.0\n143.0\n\n\n9997\n11.0\n22.0\n55.0\nBlabla\n110.0\nNaN\n99.0\n33.0\nNaN\n154.0\n...\n22.0\n11.0\n22.0\n55.0\n110.0\nNaN\n99.0\n33.0\nNaN\n154.0\n\n\n9998\n22.0\n33.0\n66.0\nBlabla\n121.0\n11.0\n110.0\n44.0\nNaN\n165.0\n...\n33.0\n22.0\n33.0\n66.0\n121.0\n11.0\n110.0\n44.0\nNaN\n165.0\n\n\n9999\n33.0\n44.0\n77.0\nBlabla\n132.0\n22.0\n121.0\n55.0\n11.0\nNaN\n...\n44.0\n33.0\n44.0\n77.0\n132.0\n22.0\n121.0\n55.0\n11.0\nNaN\n\n\n\n\n10000 rows × 27 columns\n\n\n\nThe head() method returns the top 5 rows:\n\n\nCode\nlarge_df.head()\n\n\n\n\n\n\n\n\n\nA\nB\nC\nsome_text\nD\nE\nF\nG\nH\nI\n...\nQ\nR\nS\nT\nU\nV\nW\nX\nY\nZ\n\n\n\n\n0\nNaN\n11.0\n44.0\nBlabla\n99.0\nNaN\n88.0\n22.0\n165.0\n143.0\n...\n11.0\nNaN\n11.0\n44.0\n99.0\nNaN\n88.0\n22.0\n165.0\n143.0\n\n\n1\n11.0\n22.0\n55.0\nBlabla\n110.0\nNaN\n99.0\n33.0\nNaN\n154.0\n...\n22.0\n11.0\n22.0\n55.0\n110.0\nNaN\n99.0\n33.0\nNaN\n154.0\n\n\n2\n22.0\n33.0\n66.0\nBlabla\n121.0\n11.0\n110.0\n44.0\nNaN\n165.0\n...\n33.0\n22.0\n33.0\n66.0\n121.0\n11.0\n110.0\n44.0\nNaN\n165.0\n\n\n3\n33.0\n44.0\n77.0\nBlabla\n132.0\n22.0\n121.0\n55.0\n11.0\nNaN\n...\n44.0\n33.0\n44.0\n77.0\n132.0\n22.0\n121.0\n55.0\n11.0\nNaN\n\n\n4\n44.0\n55.0\n88.0\nBlabla\n143.0\n33.0\n132.0\n66.0\n22.0\nNaN\n...\n55.0\n44.0\n55.0\n88.0\n143.0\n33.0\n132.0\n66.0\n22.0\nNaN\n\n\n\n\n5 rows × 27 columns\n\n\n\nOf course there’s also a tail() function to view the bottom 5 rows. You can pass the number of rows you want:\n\n\nCode\nlarge_df.tail(n=2)\n\n\n\n\n\n\n\n\n\nA\nB\nC\nsome_text\nD\nE\nF\nG\nH\nI\n...\nQ\nR\nS\nT\nU\nV\nW\nX\nY\nZ\n\n\n\n\n9998\n22.0\n33.0\n66.0\nBlabla\n121.0\n11.0\n110.0\n44.0\nNaN\n165.0\n...\n33.0\n22.0\n33.0\n66.0\n121.0\n11.0\n110.0\n44.0\nNaN\n165.0\n\n\n9999\n33.0\n44.0\n77.0\nBlabla\n132.0\n22.0\n121.0\n55.0\n11.0\nNaN\n...\n44.0\n33.0\n44.0\n77.0\n132.0\n22.0\n121.0\n55.0\n11.0\nNaN\n\n\n\n\n2 rows × 27 columns\n\n\n\nThe info() method prints out a summary of each columns contents:\n\n\nCode\nlarge_df.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 10000 entries, 0 to 9999\nData columns (total 27 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   A          8823 non-null   float64\n 1   B          8824 non-null   float64\n 2   C          8824 non-null   float64\n 3   some_text  10000 non-null  object \n 4   D          8824 non-null   float64\n 5   E          8822 non-null   float64\n 6   F          8824 non-null   float64\n 7   G          8824 non-null   float64\n 8   H          8822 non-null   float64\n 9   I          8823 non-null   float64\n 10  J          8823 non-null   float64\n 11  K          8822 non-null   float64\n 12  L          8824 non-null   float64\n 13  M          8824 non-null   float64\n 14  N          8822 non-null   float64\n 15  O          8824 non-null   float64\n 16  P          8824 non-null   float64\n 17  Q          8824 non-null   float64\n 18  R          8823 non-null   float64\n 19  S          8824 non-null   float64\n 20  T          8824 non-null   float64\n 21  U          8824 non-null   float64\n 22  V          8822 non-null   float64\n 23  W          8824 non-null   float64\n 24  X          8824 non-null   float64\n 25  Y          8822 non-null   float64\n 26  Z          8823 non-null   float64\ndtypes: float64(26), object(1)\nmemory usage: 2.1+ MB\n\n\nFinally, the describe() method gives a nice overview of the main aggregated values over each column: * count: number of non-null (not NaN) values * mean: mean of non-null values * std: standard deviation of non-null values * min: minimum of non-null values * 25%, 50%, 75%: 25th, 50th and 75th percentile of non-null values * max: maximum of non-null values\n\n\nCode\nlarge_df.describe()\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\n...\nQ\nR\nS\nT\nU\nV\nW\nX\nY\nZ\n\n\n\n\ncount\n8823.000000\n8824.000000\n8824.000000\n8824.000000\n8822.000000\n8824.000000\n8824.000000\n8822.000000\n8823.000000\n8823.000000\n...\n8824.000000\n8823.000000\n8824.000000\n8824.000000\n8824.000000\n8822.000000\n8824.000000\n8824.000000\n8822.000000\n8823.000000\n\n\nmean\n87.977559\n87.972575\n87.987534\n88.012466\n87.983791\n88.007480\n87.977561\n88.000000\n88.022441\n88.022441\n...\n87.972575\n87.977559\n87.972575\n87.987534\n88.012466\n87.983791\n88.007480\n87.977561\n88.000000\n88.022441\n\n\nstd\n47.535911\n47.535523\n47.521679\n47.521679\n47.535001\n47.519371\n47.529755\n47.536879\n47.535911\n47.535911\n...\n47.535523\n47.535911\n47.535523\n47.521679\n47.521679\n47.535001\n47.519371\n47.529755\n47.536879\n47.535911\n\n\nmin\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n...\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n\n\n25%\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n...\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n\n\n50%\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n...\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n\n\n75%\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n...\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n\n\nmax\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n...\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n\n\n\n\n8 rows × 26 columns"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#saving",
    "href": "posts/pandas/tools-pandas.html#saving",
    "title": "Tools - pandas",
    "section": "Saving",
    "text": "Saving\nLet’s save it to CSV, HTML and JSON:\n\n\nCode\nmy_df.to_csv(\"my_df.csv\")\nmy_df.to_html(\"my_df.html\")\nmy_df.to_json(\"my_df.json\")\n\n\nDone! Let’s take a peek at what was saved:\n\n\nCode\nfor filename in (\"my_df.csv\", \"my_df.html\", \"my_df.json\"):\n    print(\"#\", filename)\n    with open(filename, \"rt\") as f:\n        print(f.read())\n        print()\n\n\n# my_df.csv\n,hobby,weight,birthyear,children\nalice,Biking,68.5,1985,\nbob,Dancing,83.1,1984,3.0\n\n\n# my_df.html\n&lt;table border=\"1\" class=\"dataframe\"&gt;\n  &lt;thead&gt;\n    &lt;tr style=\"text-align: right;\"&gt;\n      &lt;th&gt;&lt;/th&gt;\n      &lt;th&gt;hobby&lt;/th&gt;\n      &lt;th&gt;weight&lt;/th&gt;\n      &lt;th&gt;birthyear&lt;/th&gt;\n      &lt;th&gt;children&lt;/th&gt;\n    &lt;/tr&gt;\n  &lt;/thead&gt;\n  &lt;tbody&gt;\n    &lt;tr&gt;\n      &lt;th&gt;alice&lt;/th&gt;\n      &lt;td&gt;Biking&lt;/td&gt;\n      &lt;td&gt;68.5&lt;/td&gt;\n      &lt;td&gt;1985&lt;/td&gt;\n      &lt;td&gt;NaN&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;th&gt;bob&lt;/th&gt;\n      &lt;td&gt;Dancing&lt;/td&gt;\n      &lt;td&gt;83.1&lt;/td&gt;\n      &lt;td&gt;1984&lt;/td&gt;\n      &lt;td&gt;3.0&lt;/td&gt;\n    &lt;/tr&gt;\n  &lt;/tbody&gt;\n&lt;/table&gt;\n\n# my_df.json\n{\"hobby\":{\"alice\":\"Biking\",\"bob\":\"Dancing\"},\"weight\":{\"alice\":68.5,\"bob\":83.1},\"birthyear\":{\"alice\":1985,\"bob\":1984},\"children\":{\"alice\":null,\"bob\":3.0}}\n\n\n\nNote that the index is saved as the first column (with no name) in a CSV file, as &lt;th&gt; tags in HTML and as keys in JSON.\nSaving to other formats works very similarly, but some formats require extra libraries to be installed. For example, saving to Excel requires the openpyxl library:\n\n\nCode\ntry:\n    my_df.to_excel(\"my_df.xlsx\", sheet_name='People')\nexcept ImportError as e:\n    print(e)\n\n\nNo module named 'openpyxl'"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#loading",
    "href": "posts/pandas/tools-pandas.html#loading",
    "title": "Tools - pandas",
    "section": "Loading",
    "text": "Loading\nNow let’s load our CSV file back into a DataFrame:\n\n\nCode\nmy_df_loaded = pd.read_csv(\"my_df.csv\", index_col=0)\nmy_df_loaded\n\n\n\n\n\n\n\n\n\nhobby\nweight\nbirthyear\nchildren\n\n\n\n\nalice\nBiking\n68.5\n1985\nNaN\n\n\nbob\nDancing\n83.1\n1984\n3.0\n\n\n\n\n\n\n\nAs you might guess, there are similar read_json, read_html, read_excel functions as well. We can also read data straight from the Internet. For example, let’s load the top 1,000 U.S. cities from github:\n\n\nCode\nus_cities = None\ntry:\n    csv_url = \"https://raw.githubusercontent.com/plotly/datasets/master/us-cities-top-1k.csv\"\n    us_cities = pd.read_csv(csv_url, index_col=0)\n    us_cities = us_cities.head()\nexcept IOError as e:\n    print(e)\nus_cities\n\n\n\n\n\n\n\n\n\nState\nPopulation\nlat\nlon\n\n\nCity\n\n\n\n\n\n\n\n\nMarysville\nWashington\n63269\n48.051764\n-122.177082\n\n\nPerris\nCalifornia\n72326\n33.782519\n-117.228648\n\n\nCleveland\nOhio\n390113\n41.499320\n-81.694361\n\n\nWorcester\nMassachusetts\n182544\n42.262593\n-71.802293\n\n\nColumbia\nSouth Carolina\n133358\n34.000710\n-81.034814\n\n\n\n\n\n\n\nThere are more options available, in particular regarding datetime format. Check out the documentation for more details."
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#sql-like-joins",
    "href": "posts/pandas/tools-pandas.html#sql-like-joins",
    "title": "Tools - pandas",
    "section": "SQL-like joins",
    "text": "SQL-like joins\nOne powerful feature of pandas is it’s ability to perform SQL-like joins on DataFrames. Various types of joins are supported: inner joins, left/right outer joins and full joins. To illustrate this, let’s start by creating a couple simple DataFrames:\n\n\nCode\ncity_loc = pd.DataFrame(\n    [\n        [\"CA\", \"San Francisco\", 37.781334, -122.416728],\n        [\"NY\", \"New York\", 40.705649, -74.008344],\n        [\"FL\", \"Miami\", 25.791100, -80.320733],\n        [\"OH\", \"Cleveland\", 41.473508, -81.739791],\n        [\"UT\", \"Salt Lake City\", 40.755851, -111.896657]\n    ], columns=[\"state\", \"city\", \"lat\", \"lng\"])\ncity_loc\n\n\n\n\n\n\n\n\n\nstate\ncity\nlat\nlng\n\n\n\n\n0\nCA\nSan Francisco\n37.781334\n-122.416728\n\n\n1\nNY\nNew York\n40.705649\n-74.008344\n\n\n2\nFL\nMiami\n25.791100\n-80.320733\n\n\n3\nOH\nCleveland\n41.473508\n-81.739791\n\n\n4\nUT\nSalt Lake City\n40.755851\n-111.896657\n\n\n\n\n\n\n\n\n\nCode\ncity_pop = pd.DataFrame(\n    [\n        [808976, \"San Francisco\", \"California\"],\n        [8363710, \"New York\", \"New-York\"],\n        [413201, \"Miami\", \"Florida\"],\n        [2242193, \"Houston\", \"Texas\"]\n    ], index=[3,4,5,6], columns=[\"population\", \"city\", \"state\"])\ncity_pop\n\n\n\n\n\n\n\n\n\npopulation\ncity\nstate\n\n\n\n\n3\n808976\nSan Francisco\nCalifornia\n\n\n4\n8363710\nNew York\nNew-York\n\n\n5\n413201\nMiami\nFlorida\n\n\n6\n2242193\nHouston\nTexas\n\n\n\n\n\n\n\nNow let’s join these DataFrames using the merge() function:\n\n\nCode\npd.merge(left=city_loc, right=city_pop, on=\"city\")\n\n\n\n\n\n\n\n\n\nstate_x\ncity\nlat\nlng\npopulation\nstate_y\n\n\n\n\n0\nCA\nSan Francisco\n37.781334\n-122.416728\n808976\nCalifornia\n\n\n1\nNY\nNew York\n40.705649\n-74.008344\n8363710\nNew-York\n\n\n2\nFL\nMiami\n25.791100\n-80.320733\n413201\nFlorida\n\n\n\n\n\n\n\nNote that both DataFrames have a column named state, so in the result they got renamed to state_x and state_y.\nAlso, note that Cleveland, Salt Lake City and Houston were dropped because they don’t exist in both DataFrames. This is the equivalent of a SQL INNER JOIN. If you want a FULL OUTER JOIN, where no city gets dropped and NaN values are added, you must specify how=\"outer\":\n\n\nCode\nall_cities = pd.merge(left=city_loc, right=city_pop, on=\"city\", how=\"outer\")\nall_cities\n\n\n\n\n\n\n\n\n\nstate_x\ncity\nlat\nlng\npopulation\nstate_y\n\n\n\n\n0\nCA\nSan Francisco\n37.781334\n-122.416728\n808976.0\nCalifornia\n\n\n1\nNY\nNew York\n40.705649\n-74.008344\n8363710.0\nNew-York\n\n\n2\nFL\nMiami\n25.791100\n-80.320733\n413201.0\nFlorida\n\n\n3\nOH\nCleveland\n41.473508\n-81.739791\nNaN\nNaN\n\n\n4\nUT\nSalt Lake City\n40.755851\n-111.896657\nNaN\nNaN\n\n\n5\nNaN\nHouston\nNaN\nNaN\n2242193.0\nTexas\n\n\n\n\n\n\n\nOf course LEFT OUTER JOIN is also available by setting how=\"left\": only the cities present in the left DataFrame end up in the result. Similarly, with how=\"right\" only cities in the right DataFrame appear in the result. For example:\n\n\nCode\npd.merge(left=city_loc, right=city_pop, on=\"city\", how=\"right\")\n\n\n\n\n\n\n\n\n\nstate_x\ncity\nlat\nlng\npopulation\nstate_y\n\n\n\n\n0\nCA\nSan Francisco\n37.781334\n-122.416728\n808976\nCalifornia\n\n\n1\nNY\nNew York\n40.705649\n-74.008344\n8363710\nNew-York\n\n\n2\nFL\nMiami\n25.791100\n-80.320733\n413201\nFlorida\n\n\n3\nNaN\nHouston\nNaN\nNaN\n2242193\nTexas\n\n\n\n\n\n\n\nIf the key to join on is actually in one (or both) DataFrame’s index, you must use left_index=True and/or right_index=True. If the key column names differ, you must use left_on and right_on. For example:\n\n\nCode\ncity_pop2 = city_pop.copy()\ncity_pop2.columns = [\"population\", \"name\", \"state\"]\npd.merge(left=city_loc, right=city_pop2, left_on=\"city\", right_on=\"name\")\n\n\n\n\n\n\n\n\n\nstate_x\ncity\nlat\nlng\npopulation\nname\nstate_y\n\n\n\n\n0\nCA\nSan Francisco\n37.781334\n-122.416728\n808976\nSan Francisco\nCalifornia\n\n\n1\nNY\nNew York\n40.705649\n-74.008344\n8363710\nNew York\nNew-York\n\n\n2\nFL\nMiami\n25.791100\n-80.320733\n413201\nMiami\nFlorida"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#concatenation",
    "href": "posts/pandas/tools-pandas.html#concatenation",
    "title": "Tools - pandas",
    "section": "Concatenation",
    "text": "Concatenation\nRather than joining DataFrames, we may just want to concatenate them. That’s what concat() is for:\n\n\nCode\nresult_concat = pd.concat([city_loc, city_pop])\nresult_concat\n\n\n\n\n\n\n\n\n\nstate\ncity\nlat\nlng\npopulation\n\n\n\n\n0\nCA\nSan Francisco\n37.781334\n-122.416728\nNaN\n\n\n1\nNY\nNew York\n40.705649\n-74.008344\nNaN\n\n\n2\nFL\nMiami\n25.791100\n-80.320733\nNaN\n\n\n3\nOH\nCleveland\n41.473508\n-81.739791\nNaN\n\n\n4\nUT\nSalt Lake City\n40.755851\n-111.896657\nNaN\n\n\n3\nCalifornia\nSan Francisco\nNaN\nNaN\n808976.0\n\n\n4\nNew-York\nNew York\nNaN\nNaN\n8363710.0\n\n\n5\nFlorida\nMiami\nNaN\nNaN\n413201.0\n\n\n6\nTexas\nHouston\nNaN\nNaN\n2242193.0\n\n\n\n\n\n\n\nNote that this operation aligned the data horizontally (by columns) but not vertically (by rows). In this example, we end up with multiple rows having the same index (eg. 3). Pandas handles this rather gracefully:\n\n\nCode\nresult_concat.loc[3]\n\n\n\n\n\n\n\n\n\nstate\ncity\nlat\nlng\npopulation\n\n\n\n\n3\nOH\nCleveland\n41.473508\n-81.739791\nNaN\n\n\n3\nCalifornia\nSan Francisco\nNaN\nNaN\n808976.0\n\n\n\n\n\n\n\nOr you can tell pandas to just ignore the index:\n\n\nCode\npd.concat([city_loc, city_pop], ignore_index=True)\n\n\n\n\n\n\n\n\n\nstate\ncity\nlat\nlng\npopulation\n\n\n\n\n0\nCA\nSan Francisco\n37.781334\n-122.416728\nNaN\n\n\n1\nNY\nNew York\n40.705649\n-74.008344\nNaN\n\n\n2\nFL\nMiami\n25.791100\n-80.320733\nNaN\n\n\n3\nOH\nCleveland\n41.473508\n-81.739791\nNaN\n\n\n4\nUT\nSalt Lake City\n40.755851\n-111.896657\nNaN\n\n\n5\nCalifornia\nSan Francisco\nNaN\nNaN\n808976.0\n\n\n6\nNew-York\nNew York\nNaN\nNaN\n8363710.0\n\n\n7\nFlorida\nMiami\nNaN\nNaN\n413201.0\n\n\n8\nTexas\nHouston\nNaN\nNaN\n2242193.0\n\n\n\n\n\n\n\nNotice that when a column does not exist in a DataFrame, it acts as if it was filled with NaN values. If we set join=\"inner\", then only columns that exist in both DataFrames are returned:\n\n\nCode\npd.concat([city_loc, city_pop], join=\"inner\")\n\n\n\n\n\n\n\n\n\nstate\ncity\n\n\n\n\n0\nCA\nSan Francisco\n\n\n1\nNY\nNew York\n\n\n2\nFL\nMiami\n\n\n3\nOH\nCleveland\n\n\n4\nUT\nSalt Lake City\n\n\n3\nCalifornia\nSan Francisco\n\n\n4\nNew-York\nNew York\n\n\n5\nFlorida\nMiami\n\n\n6\nTexas\nHouston\n\n\n\n\n\n\n\nYou can concatenate DataFrames horizontally instead of vertically by setting axis=1:\n\n\nCode\npd.concat([city_loc, city_pop], axis=1)\n\n\n\n\n\n\n\n\n\nstate\ncity\nlat\nlng\npopulation\ncity\nstate\n\n\n\n\n0\nCA\nSan Francisco\n37.781334\n-122.416728\nNaN\nNaN\nNaN\n\n\n1\nNY\nNew York\n40.705649\n-74.008344\nNaN\nNaN\nNaN\n\n\n2\nFL\nMiami\n25.791100\n-80.320733\nNaN\nNaN\nNaN\n\n\n3\nOH\nCleveland\n41.473508\n-81.739791\n808976.0\nSan Francisco\nCalifornia\n\n\n4\nUT\nSalt Lake City\n40.755851\n-111.896657\n8363710.0\nNew York\nNew-York\n\n\n5\nNaN\nNaN\nNaN\nNaN\n413201.0\nMiami\nFlorida\n\n\n6\nNaN\nNaN\nNaN\nNaN\n2242193.0\nHouston\nTexas\n\n\n\n\n\n\n\nIn this case it really does not make much sense because the indices do not align well (eg. Cleveland and San Francisco end up on the same row, because they shared the index label 3). So let’s reindex the DataFrames by city name before concatenating:\n\n\nCode\npd.concat([city_loc.set_index(\"city\"), city_pop.set_index(\"city\")], axis=1)\n\n\n\n\n\n\n\n\n\nstate\nlat\nlng\npopulation\nstate\n\n\ncity\n\n\n\n\n\n\n\n\n\nSan Francisco\nCA\n37.781334\n-122.416728\n808976.0\nCalifornia\n\n\nNew York\nNY\n40.705649\n-74.008344\n8363710.0\nNew-York\n\n\nMiami\nFL\n25.791100\n-80.320733\n413201.0\nFlorida\n\n\nCleveland\nOH\n41.473508\n-81.739791\nNaN\nNaN\n\n\nSalt Lake City\nUT\n40.755851\n-111.896657\nNaN\nNaN\n\n\nHouston\nNaN\nNaN\nNaN\n2242193.0\nTexas\n\n\n\n\n\n\n\nThis looks a lot like a FULL OUTER JOIN, except that the state columns were not renamed to state_x and state_y, and the city column is now the index.\nThe append() method is a useful shorthand for concatenating DataFrames vertically:\n\n\nCode\ncity_loc.append(city_pop)\n\n\n\n\n\n\n\n\n\nstate\ncity\nlat\nlng\npopulation\n\n\n\n\n0\nCA\nSan Francisco\n37.781334\n-122.416728\nNaN\n\n\n1\nNY\nNew York\n40.705649\n-74.008344\nNaN\n\n\n2\nFL\nMiami\n25.791100\n-80.320733\nNaN\n\n\n3\nOH\nCleveland\n41.473508\n-81.739791\nNaN\n\n\n4\nUT\nSalt Lake City\n40.755851\n-111.896657\nNaN\n\n\n3\nCalifornia\nSan Francisco\nNaN\nNaN\n808976.0\n\n\n4\nNew-York\nNew York\nNaN\nNaN\n8363710.0\n\n\n5\nFlorida\nMiami\nNaN\nNaN\n413201.0\n\n\n6\nTexas\nHouston\nNaN\nNaN\n2242193.0\n\n\n\n\n\n\n\nAs always in pandas, the append() method does not actually modify city_loc: it works on a copy and returns the modified copy."
  },
  {
    "objectID": "posts/python/Dataloading_fileformats.html",
    "href": "posts/python/Dataloading_fileformats.html",
    "title": "Data Structures",
    "section": "",
    "text": "Reading text files in pieces\nWriting data to text format\nWorking with other delimited formats\nJSON data\nXML and HTML: Web Scraping\nParsing XML and lmxl.objectify\n\n\n\n\n\nReading Microsoft Excel Files\nUsing HDF5 Format\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\n\n\n\n\ndf = pd.read_csv('username.csv')\n\n\ndf.head()\n\n\n\n\n\n\n\n\nUsername; Identifier;First name;Last name\n\n\n\n\n0\nbooker12;9012;Rachel;Booker\n\n\n1\ngrey07;2070;Laura;Grey\n\n\n2\njohnson81;4081;Craig;Johnson\n\n\n3\njenkins46;9346;Mary;Jenkins\n\n\n4\nsmith79;5079;Jamie;Smith\n\n\n\n\n\n\n\n\n# without header\ndf = pd.read_csv('username.csv', header = None)\ndf.head()\n\n\n\n\n\n\n\n\n0\n\n\n\n\n0\nUsername; Identifier;First name;Last name\n\n\n1\nbooker12;9012;Rachel;Booker\n\n\n2\ngrey07;2070;Laura;Grey\n\n\n3\njohnson81;4081;Craig;Johnson\n\n\n4\njenkins46;9346;Mary;Jenkins\n\n\n\n\n\n\n\n\ndf = pd.read_csv('E:\\pythonfordatanalysis\\username.csv', names= ['onboarding details'])\n\n\ndf\n\n\n\n\n\n\n\n\nonboarding details\n\n\n\n\n0\nUsername; Identifier;First name;Last name\n\n\n1\nbooker12;9012;Rachel;Booker\n\n\n2\ngrey07;2070;Laura;Grey\n\n\n3\njohnson81;4081;Craig;Johnson\n\n\n4\njenkins46;9346;Mary;Jenkins\n\n\n5\nsmith79;5079;Jamie;Smith\n\n\n\n\n\n\n\n\n\n\n\nimport sqlite3\n\nquery  = \"\"\"\nCreate table test\n(a tatata(29), n Blabla(20),\nc Real, d integer);\n\"\"\"\n\ncon = sqlite3.connect('mydata.sqlite')\n\ncon.execute(query)\n\n&lt;sqlite3.Cursor at 0x27ccb6f6420&gt;\n\n\n\ncon.commit()\n\n\n# insert a few rows of data\n\ndata= [('Atlanta', 'Georgia', 1.25, 6),\n      ('Tallahassee', 'Florida', 2.26, 3),\n      ('Sacramento', 'California', 1.5,3)]\n\nstmt = 'Insert into test values (?, ?, ?, ?)'\n\ncon.executemany(stmt, data)\n\n&lt;sqlite3.Cursor at 0x27ccb6f6340&gt;\n\n\n\n# most SQL drivers return a list of tuples when selecting data from table\n\ncursor = con.execute('SELECT * FROM test')\n\nrows = cursor.fetchall()\n\nrows\n\n[('Atlanta', 'Georgia', 1.25, 6),\n ('Tallahassee', 'Florida', 2.26, 3),\n ('Sacramento', 'California', 1.5, 3)]\n\n\n\ncursor.description\n\n(('a', None, None, None, None, None, None),\n ('n', None, None, None, None, None, None),\n ('c', None, None, None, None, None, None),\n ('d', None, None, None, None, None, None))\n\n\n\npd.DataFrame(rows, columns=[x[0] for x in cursor.description])\n\n\n\n\n\n\n\n\na\nn\nc\nd\n\n\n\n\n0\nAtlanta\nGeorgia\n1.25\n6\n\n\n1\nTallahassee\nFlorida\n2.26\n3\n\n\n2\nSacramento\nCalifornia\n1.50\n3\n\n\n\n\n\n\n\n\n!pip install sqlalchemy\n\n\n! pip install gradio typing_extensions\n\n\n! pip install jiwer\n\nRequirement already satisfied: jiwer in c:\\users\\khurana_kunal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (3.0.3)\nRequirement already satisfied: click&lt;9.0.0,&gt;=8.1.3 in c:\\users\\khurana_kunal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from jiwer) (8.1.7)\nRequirement already satisfied: rapidfuzz&lt;4,&gt;=3 in c:\\users\\khurana_kunal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from jiwer) (3.6.1)\nRequirement already satisfied: colorama in c:\\users\\khurana_kunal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from click&lt;9.0.0,&gt;=8.1.3-&gt;jiwer) (0.4.6)\n\n\n\n! pip install gradio typing-extensions\n\n\nimport sqlalchemy as sqla\nimport pandas as pd\n\n\ndb = sqla.create_engine('sqlite:///mydata.sqlite')\n\npd.read_sql('SELECT * FROM test', db)\n\n\n\n\n\n\n\n\na\nn\nc\nd"
  },
  {
    "objectID": "posts/python/Dataloading_fileformats.html#reading-and-writing-data-in-text-format",
    "href": "posts/python/Dataloading_fileformats.html#reading-and-writing-data-in-text-format",
    "title": "Data Structures",
    "section": "",
    "text": "Reading text files in pieces\nWriting data to text format\nWorking with other delimited formats\nJSON data\nXML and HTML: Web Scraping\nParsing XML and lmxl.objectify"
  },
  {
    "objectID": "posts/python/Dataloading_fileformats.html#binary-data-formats",
    "href": "posts/python/Dataloading_fileformats.html#binary-data-formats",
    "title": "Data Structures",
    "section": "",
    "text": "Reading Microsoft Excel Files\nUsing HDF5 Format"
  },
  {
    "objectID": "posts/python/Dataloading_fileformats.html#interacting-with-databases",
    "href": "posts/python/Dataloading_fileformats.html#interacting-with-databases",
    "title": "Data Structures",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "posts/python/Dataloading_fileformats.html#reading-and-writing-data-in-text-format-1",
    "href": "posts/python/Dataloading_fileformats.html#reading-and-writing-data-in-text-format-1",
    "title": "Data Structures",
    "section": "",
    "text": "df = pd.read_csv('username.csv')\n\n\ndf.head()\n\n\n\n\n\n\n\n\nUsername; Identifier;First name;Last name\n\n\n\n\n0\nbooker12;9012;Rachel;Booker\n\n\n1\ngrey07;2070;Laura;Grey\n\n\n2\njohnson81;4081;Craig;Johnson\n\n\n3\njenkins46;9346;Mary;Jenkins\n\n\n4\nsmith79;5079;Jamie;Smith\n\n\n\n\n\n\n\n\n# without header\ndf = pd.read_csv('username.csv', header = None)\ndf.head()\n\n\n\n\n\n\n\n\n0\n\n\n\n\n0\nUsername; Identifier;First name;Last name\n\n\n1\nbooker12;9012;Rachel;Booker\n\n\n2\ngrey07;2070;Laura;Grey\n\n\n3\njohnson81;4081;Craig;Johnson\n\n\n4\njenkins46;9346;Mary;Jenkins\n\n\n\n\n\n\n\n\ndf = pd.read_csv('E:\\pythonfordatanalysis\\username.csv', names= ['onboarding details'])\n\n\ndf\n\n\n\n\n\n\n\n\nonboarding details\n\n\n\n\n0\nUsername; Identifier;First name;Last name\n\n\n1\nbooker12;9012;Rachel;Booker\n\n\n2\ngrey07;2070;Laura;Grey\n\n\n3\njohnson81;4081;Craig;Johnson\n\n\n4\njenkins46;9346;Mary;Jenkins\n\n\n5\nsmith79;5079;Jamie;Smith"
  },
  {
    "objectID": "posts/python/Dataloading_fileformats.html#interacting-with-databases-1",
    "href": "posts/python/Dataloading_fileformats.html#interacting-with-databases-1",
    "title": "Data Structures",
    "section": "",
    "text": "import sqlite3\n\nquery  = \"\"\"\nCreate table test\n(a tatata(29), n Blabla(20),\nc Real, d integer);\n\"\"\"\n\ncon = sqlite3.connect('mydata.sqlite')\n\ncon.execute(query)\n\n&lt;sqlite3.Cursor at 0x27ccb6f6420&gt;\n\n\n\ncon.commit()\n\n\n# insert a few rows of data\n\ndata= [('Atlanta', 'Georgia', 1.25, 6),\n      ('Tallahassee', 'Florida', 2.26, 3),\n      ('Sacramento', 'California', 1.5,3)]\n\nstmt = 'Insert into test values (?, ?, ?, ?)'\n\ncon.executemany(stmt, data)\n\n&lt;sqlite3.Cursor at 0x27ccb6f6340&gt;\n\n\n\n# most SQL drivers return a list of tuples when selecting data from table\n\ncursor = con.execute('SELECT * FROM test')\n\nrows = cursor.fetchall()\n\nrows\n\n[('Atlanta', 'Georgia', 1.25, 6),\n ('Tallahassee', 'Florida', 2.26, 3),\n ('Sacramento', 'California', 1.5, 3)]\n\n\n\ncursor.description\n\n(('a', None, None, None, None, None, None),\n ('n', None, None, None, None, None, None),\n ('c', None, None, None, None, None, None),\n ('d', None, None, None, None, None, None))\n\n\n\npd.DataFrame(rows, columns=[x[0] for x in cursor.description])\n\n\n\n\n\n\n\n\na\nn\nc\nd\n\n\n\n\n0\nAtlanta\nGeorgia\n1.25\n6\n\n\n1\nTallahassee\nFlorida\n2.26\n3\n\n\n2\nSacramento\nCalifornia\n1.50\n3\n\n\n\n\n\n\n\n\n!pip install sqlalchemy\n\n\n! pip install gradio typing_extensions\n\n\n! pip install jiwer\n\nRequirement already satisfied: jiwer in c:\\users\\khurana_kunal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (3.0.3)\nRequirement already satisfied: click&lt;9.0.0,&gt;=8.1.3 in c:\\users\\khurana_kunal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from jiwer) (8.1.7)\nRequirement already satisfied: rapidfuzz&lt;4,&gt;=3 in c:\\users\\khurana_kunal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from jiwer) (3.6.1)\nRequirement already satisfied: colorama in c:\\users\\khurana_kunal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from click&lt;9.0.0,&gt;=8.1.3-&gt;jiwer) (0.4.6)\n\n\n\n! pip install gradio typing-extensions\n\n\nimport sqlalchemy as sqla\nimport pandas as pd\n\n\ndb = sqla.create_engine('sqlite:///mydata.sqlite')\n\npd.read_sql('SELECT * FROM test', db)\n\n\n\n\n\n\n\n\na\nn\nc\nd"
  },
  {
    "objectID": "posts/python/datawrangling_join_combine_reshape.html",
    "href": "posts/python/datawrangling_join_combine_reshape.html",
    "title": "Data Wrangling",
    "section": "",
    "text": "Reordering and Sorting levels\nSummary statitics by level\nIndexing with DataFrame’s Columns\n\n\n\n\n\nDatabase-Style DataFrame joins\nMerging on Index\nConcatenating Along an Axis\nCombining Data with Overlap\n\n\n\n\n\nReshaping with hierarchical Indexing\nPivoting ‘long’ to ‘wide’ format\npivoting ‘wide’ to ‘long’ format\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\n\ndata = pd.Series(np.random.uniform(size = 9),\n                index =  [['a', 'a', 'b', 'c', 'c', 'b', 'c', 'b','a'],\n                          [1, 2, 3, 1, 3, 4, 3, 2, 1]])\ndata\n\na  1    0.684862\n   2    0.701188\nb  3    0.870829\nc  1    0.958994\n   3    0.042434\nb  4    0.539591\nc  3    0.668997\nb  2    0.501304\na  1    0.260682\ndtype: float64\n\n\n\n# gaps for 'multi-index'\ndata.index\n\nMultiIndex([('a', 1),\n            ('a', 2),\n            ('b', 3),\n            ('c', 1),\n            ('c', 3),\n            ('b', 4),\n            ('c', 3),\n            ('b', 2),\n            ('a', 1)],\n           )\n\n\n\nmean = [0, 0]\ncov =  [[1,0], [0, 100]]\n\n\ndata\n\na  1    0.684862\n   2    0.701188\nb  3    0.870829\nc  1    0.958994\n   3    0.042434\nb  4    0.539591\nc  3    0.668997\nb  2    0.501304\na  1    0.260682\ndtype: float64\n\n\n\n# selecting subset\ndata['b']\n\n3    0.870829\n4    0.539591\n2    0.501304\ndtype: float64\n\n\n\n# selecting the data values with loc operator\ndata.loc[['a','b']]\n\na  1    0.684862\n   2    0.701188\n   1    0.260682\nb  3    0.870829\n   4    0.539591\n   2    0.501304\ndtype: float64\n\n\n\ndata.loc[:, 2]\n\na    0.701188\nb    0.501304\ndtype: float64\n\n\n\n\n\n\nframe = pd.DataFrame(np.arange(12).reshape((4,3)),\n                    index = [[\"a\", \"a\", \"b\", \"b\"], [1, 2, 1, 2]],\n                    columns = [['fdk', 'fzp', 'chd'],\n                               ['PB', 'PB', 'CHD']])\n\n\nframe\n\n\n\n\n\n\n\n\n\nfdk\nfzp\nchd\n\n\n\n\nPB\nPB\nCHD\n\n\n\n\na\n1\n0\n1\n2\n\n\n2\n3\n4\n5\n\n\nb\n1\n6\n7\n8\n\n\n2\n9\n10\n11\n\n\n\n\n\n\n\n\nframe.index.names = ['key1', 'key2']\n\n\nframe.columns.names = ['city', 'province']\n\n\nframe\n\n\n\n\n\n\n\n\ncity\nfdk\nfzp\nchd\n\n\n\nprovince\nPB\nPB\nCHD\n\n\nkey1\nkey2\n\n\n\n\n\n\n\na\n1\n0\n1\n2\n\n\n2\n3\n4\n5\n\n\nb\n1\n6\n7\n8\n\n\n2\n9\n10\n11\n\n\n\n\n\n\n\n\n# to check how many levels an index has\nframe.index.nlevels\n\n2\n\n\n\n# partial column indexing\nframe['fdk']\n\n\n\n\n\n\n\n\nprovince\nPB\n\n\nkey1\nkey2\n\n\n\n\n\na\n1\n0\n\n\n2\n3\n\n\nb\n1\n6\n\n\n2\n9\n\n\n\n\n\n\n\n\nframe['fzp']\n\n\n\n\n\n\n\n\nprovince\nPB\n\n\nkey1\nkey2\n\n\n\n\n\na\n1\n1\n\n\n2\n4\n\n\nb\n1\n7\n\n\n2\n10\n\n\n\n\n\n\n\n\nframe['chd']\n\n\n\n\n\n\n\n\nprovince\nCHD\n\n\nkey1\nkey2\n\n\n\n\n\na\n1\n2\n\n\n2\n5\n\n\nb\n1\n8\n\n\n2\n11\n\n\n\n\n\n\n\n\npd.MultiIndex.from_arrays([['fdk', 'fzp', 'chd'],\n                          ['PB', 'PB', 'CHD'],\n                           names=['city', 'capital'])\n                           \n                           \n\n\n\n\nframe.swaplevel('key1', 'key2')\n\n\n\n\n\n\n\n\ncity\nfdk\nfzp\nchd\n\n\n\nprovince\nPB\nPB\nCHD\n\n\nkey2\nkey1\n\n\n\n\n\n\n\n1\na\n0\n1\n2\n\n\n2\na\n3\n4\n5\n\n\n1\nb\n6\n7\n8\n\n\n2\nb\n9\n10\n11\n\n\n\n\n\n\n\n\nframe.sort_index(level=1)\n\n\n\n\n\n\n\n\ncity\nfdk\nfzp\nchd\n\n\n\nprovince\nPB\nPB\nCHD\n\n\nkey1\nkey2\n\n\n\n\n\n\n\na\n1\n0\n1\n2\n\n\nb\n1\n6\n7\n8\n\n\na\n2\n3\n4\n5\n\n\nb\n2\n9\n10\n11\n\n\n\n\n\n\n\n\nframe.swaplevel(0,1).sort_index(level=0)\n\n\n\n\n\n\n\n\ncity\nfdk\nfzp\nchd\n\n\n\nprovince\nPB\nPB\nCHD\n\n\nkey2\nkey1\n\n\n\n\n\n\n\n1\na\n0\n1\n2\n\n\nb\n6\n7\n8\n\n\n2\na\n3\n4\n5\n\n\nb\n9\n10\n11\n\n\n\n\n\n\n\n\n\n\n\nframe.groupby(level='key2').sum()\n\n\n\n\n\n\n\ncity\nfdk\nfzp\nchd\n\n\nprovince\nPB\nPB\nCHD\n\n\nkey2\n\n\n\n\n\n\n\n1\n6\n8\n10\n\n\n2\n12\n14\n16\n\n\n\n\n\n\n\n\nframe.groupby(level= 'province', axis = 'columns').sum()\n\n\n\n\n\n\n\n\nprovince\nCHD\nPB\n\n\nkey1\nkey2\n\n\n\n\n\n\na\n1\n2\n1\n\n\n2\n5\n7\n\n\nb\n1\n8\n13\n\n\n2\n11\n19\n\n\n\n\n\n\n\n\n\n\n\nframe2 = pd.DataFrame({'a': range(7), 'b': range(7,0,-1),\n                      'c': ['one', 'one', 'one', 'two', 'two',\n                           'two', 'two'],\n                      'd': [0, 1,2,0,1,3,2]})\n\n\nframe2\n\n\n\n\n\n\n\n\na\nb\nc\nd\n\n\n\n\n0\n0\n7\none\n0\n\n\n1\n1\n6\none\n1\n\n\n2\n2\n5\none\n2\n\n\n3\n3\n4\ntwo\n0\n\n\n4\n4\n3\ntwo\n1\n\n\n5\n5\n2\ntwo\n3\n\n\n6\n6\n1\ntwo\n2\n\n\n\n\n\n\n\n\n# set_index to create a new DataFrame\n\nframe3 = frame2.set_index(['c', 'd'])\n\nframe3\n\n\n\n\n\n\n\n\n\na\nb\n\n\nc\nd\n\n\n\n\n\n\none\n0\n0\n7\n\n\n1\n1\n6\n\n\n2\n2\n5\n\n\ntwo\n0\n3\n4\n\n\n1\n4\n3\n\n\n3\n5\n2\n\n\n2\n6\n1\n\n\n\n\n\n\n\n\n# we can set it to index by doing drop= False\n\nframe2.set_index([\"c\",'d'], drop= False)\n\n\n\n\n\n\n\n\n\na\nb\nc\nd\n\n\nc\nd\n\n\n\n\n\n\n\n\none\n0\n0\n7\none\n0\n\n\n1\n1\n6\none\n1\n\n\n2\n2\n5\none\n2\n\n\ntwo\n0\n3\n4\ntwo\n0\n\n\n1\n4\n3\ntwo\n1\n\n\n3\n5\n2\ntwo\n3\n\n\n2\n6\n1\ntwo\n2\n\n\n\n\n\n\n\n\n# reset_index brings it back to the orignal position\n\nframe2.reset_index()\n\n\n\n\n\n\n\n\nindex\na\nb\nc\nd\n\n\n\n\n0\n0\n0\n7\none\n0\n\n\n1\n1\n1\n6\none\n1\n\n\n2\n2\n2\n5\none\n2\n\n\n3\n3\n3\n4\ntwo\n0\n\n\n4\n4\n4\n3\ntwo\n1\n\n\n5\n5\n5\n2\ntwo\n3\n\n\n6\n6\n6\n1\ntwo\n2\n\n\n\n\n\n\n\n\n\n\n\n\npandas.merge (connects rows based on one/more keys) how\npandas.concat (stacks objects together on axis)\ncombine_first (slice together overlapping data to fill missing values)\nmerge function arguments\n\n\n# DataFrame joins\ndf1 = pd.DataFrame({\"key\": ['a', 'c', 'd', 'b', 'a', 'c'],\n                   'data1': pd.Series(range(6), dtype= 'Int64')})\n\ndf2 = pd.DataFrame({'key': ['a', 'b', 'c'],\n                   'data2': pd.Series(range(3), dtype='Int64')})\n\n\ndf1\n\n\n\n\n\n\n\n\nkey\ndata1\n\n\n\n\n0\na\n0\n\n\n1\nc\n1\n\n\n2\nd\n2\n\n\n3\nb\n3\n\n\n4\na\n4\n\n\n5\nc\n5\n\n\n\n\n\n\n\n\ndf2\n\n\n\n\n\n\n\n\nkey\ndata2\n\n\n\n\n0\na\n0\n\n\n1\nb\n1\n\n\n2\nc\n2\n\n\n\n\n\n\n\n\npd.merge(df1, df2)\n\n\n\n\n\n\n\n\nkey\ndata1\ndata2\n\n\n\n\n0\na\n0\n0\n\n\n1\na\n4\n0\n\n\n2\nc\n1\n2\n\n\n3\nc\n5\n2\n\n\n4\nb\n3\n1\n\n\n\n\n\n\n\n\n# specifying the column\npd.merge(df1, df2, on= 'key')\n\n\n\n\n\n\n\n\nkey\ndata1\ndata2\n\n\n\n\n0\na\n0\n0\n\n\n1\na\n4\n0\n\n\n2\nc\n1\n2\n\n\n3\nc\n5\n2\n\n\n4\nb\n3\n1\n\n\n\n\n\n\n\n\npd.merge(df1, df2, how= 'outer')\n\n\n\n\n\n\n\n\nkey\ndata1\ndata2\n\n\n\n\n0\na\n0\n0\n\n\n1\na\n4\n0\n\n\n2\nc\n1\n2\n\n\n3\nc\n5\n2\n\n\n4\nd\n2\n&lt;NA&gt;\n\n\n5\nb\n3\n1\n\n\n\n\n\n\n\n\n\n\npd.merge(df1, df2, on= 'key', suffixes = (\"_left\", \"_right\"))\n\n\n\n\n\n\n\n\nkey\ndata1\ndata2\n\n\n\n\n0\na\n0\n0\n\n\n1\na\n4\n0\n\n\n2\nc\n1\n2\n\n\n3\nc\n5\n2\n\n\n4\nb\n3\n1\n\n\n\n\n\n\n\n\n\n\n\ndf1.join(df1, on= 'key')\n\n\nanother = pd.DataFrame([[7., 8.], [9., 10.],\n                       [11., 12.], [16., 17.]],\n                      index = ['a', 'c', 'e', 'f'],\n                      columns= ['jandiala', 'faridkot'])\n\n\nanother\n\n\n\n\n\n\n\n\njandiala\nfaridkot\n\n\n\n\na\n7.0\n8.0\n\n\nc\n9.0\n10.0\n\n\ne\n11.0\n12.0\n\n\nf\n16.0\n17.0\n\n\n\n\n\n\n\n\ndf1.join(another, how= 'outer')\n\n\n\n\n\n\n\n\nkey\ndata1\njandiala\nfaridkot\n\n\n\n\n0\na\n0\nNaN\nNaN\n\n\n1\nc\n1\nNaN\nNaN\n\n\n2\nd\n2\nNaN\nNaN\n\n\n3\nb\n3\nNaN\nNaN\n\n\n4\na\n4\nNaN\nNaN\n\n\n5\nc\n5\nNaN\nNaN\n\n\na\nNaN\n&lt;NA&gt;\n7.0\n8.0\n\n\nc\nNaN\n&lt;NA&gt;\n9.0\n10.0\n\n\ne\nNaN\n&lt;NA&gt;\n11.0\n12.0\n\n\nf\nNaN\n&lt;NA&gt;\n16.0\n17.0\n\n\n\n\n\n\n\n\n\n\n\ndata combination\nfunction agruments pandas.concat\n\n\narr = np.arange(12).reshape((3,4))\n\n\narr\n\narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11]])\n\n\n\nnp.concatenate([arr, arr])\n\narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11],\n       [ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11]])\n\n\n\nnp.concatenate([arr, arr], axis = 1)\n\narray([[ 0,  1,  2,  3,  0,  1,  2,  3],\n       [ 4,  5,  6,  7,  4,  5,  6,  7],\n       [ 8,  9, 10, 11,  8,  9, 10, 11]])\n\n\n\n## series with no index overlap\n\ns1 = pd.Series([0, 1], index = ['a', 'b'], dtype = 'Int64')\ns2 = pd.Series([2,3,4], index = ['c', 'd', 'e'], dtype= 'Int64')\ns3 = pd.Series([5,6], index =['e', 'f'], dtype = 'Int64')\n\n\ns1\n\na    0\nb    1\ndtype: Int64\n\n\n\ns2\n\nc    2\nd    3\ne    4\ndtype: Int64\n\n\n\ns3\n\ne    5\nf    6\ndtype: Int64\n\n\n\npd.concat([s1, s2, s3])\n\na    0\nb    1\nc    2\nd    3\ne    4\ne    5\nf    6\ndtype: Int64\n\n\n\n# the result will be a DataFrame if we pass axis = 'columns'\n\npd.concat([s1, s2, s3], axis = 'columns')\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\na\n0\n&lt;NA&gt;\n&lt;NA&gt;\n\n\nb\n1\n&lt;NA&gt;\n&lt;NA&gt;\n\n\nc\n&lt;NA&gt;\n2\n&lt;NA&gt;\n\n\nd\n&lt;NA&gt;\n3\n&lt;NA&gt;\n\n\ne\n&lt;NA&gt;\n4\n5\n\n\nf\n&lt;NA&gt;\n&lt;NA&gt;\n6\n\n\n\n\n\n\n\n\n# trying inner join()\n\ns4 = pd.concat([s1, s3])\n\n\ns4\n\na    0\nb    1\ne    5\nf    6\ndtype: Int64\n\n\n\npd.concat([s1, s4], axis = 'columns')\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\na\n0\n0\n\n\nb\n1\n1\n\n\ne\n&lt;NA&gt;\n5\n\n\nf\n&lt;NA&gt;\n6\n\n\n\n\n\n\n\n\n# because of inner join, labels 'f' and 'g' disappeared\n\npd.concat([s1, s4], axis = 'columns', join = 'inner')\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\na\n0\n0\n\n\nb\n1\n1\n\n\n\n\n\n\n\n\nresult = pd.concat([s1, s1, s3], keys= ['one', 'two', 'three'])\n\n\nresult\n\none    a    0\n       b    1\ntwo    a    0\n       b    1\nthree  e    5\n       f    6\ndtype: Int64\n\n\n\nresult.unstack()\n\n\n\n\n\n\n\n\na\nb\ne\nf\n\n\n\n\none\n0\n1\n&lt;NA&gt;\n&lt;NA&gt;\n\n\ntwo\n0\n1\n&lt;NA&gt;\n&lt;NA&gt;\n\n\nthree\n&lt;NA&gt;\n&lt;NA&gt;\n5\n6\n\n\n\n\n\n\n\n\nin case of combining Series along axis= ‘columns’, &gt; keys become DataFrame column headers\n\n\npd.concat([s1, s2, s3], axis = 'columns', \n          keys = ['one', 'two', 'three'])\n\n\n\n\n\n\n\n\none\ntwo\nthree\n\n\n\n\na\n0\n&lt;NA&gt;\n&lt;NA&gt;\n\n\nb\n1\n&lt;NA&gt;\n&lt;NA&gt;\n\n\nc\n&lt;NA&gt;\n2\n&lt;NA&gt;\n\n\nd\n&lt;NA&gt;\n3\n&lt;NA&gt;\n\n\ne\n&lt;NA&gt;\n4\n5\n\n\nf\n&lt;NA&gt;\n&lt;NA&gt;\n6\n\n\n\n\n\n\n\n\n# same logic extends to DataFrame objects\npd.concat([df1, df2], axis = 'columns')\n\n\n\n\n\n\n\n\nkey\ndata1\nkey\ndata2\n\n\n\n\n0\na\n0\na\n0\n\n\n1\nc\n1\nb\n1\n\n\n2\nd\n2\nc\n2\n\n\n3\nb\n3\nNaN\n&lt;NA&gt;\n\n\n4\na\n4\nNaN\n&lt;NA&gt;\n\n\n5\nc\n5\nNaN\n&lt;NA&gt;\n\n\n\n\n\n\n\n\nIn dictionary objects, the keys will be used &gt; for key option\n\n\npd.concat({'level1': df1, 'level2': df2},\n         axis = 'columns')\n\n\n\n\n\n\n\n\nlevel1\nlevel2\n\n\n\nkey\ndata1\nkey\ndata2\n\n\n\n\n0\na\n0\na\n0\n\n\n1\nc\n1\nb\n1\n\n\n2\nd\n2\nc\n2\n\n\n3\nb\n3\nNaN\n&lt;NA&gt;\n\n\n4\na\n4\nNaN\n&lt;NA&gt;\n\n\n5\nc\n5\nNaN\n&lt;NA&gt;\n\n\n\n\n\n\n\n\n# additional arguments\n\npd.concat([df1, df2], axis = 'columns',\n         keys = ['level1', 'level2'],\n         names = ['upper', 'lower'])\n\n\n\n\n\n\n\nupper\nlevel1\nlevel2\n\n\nlower\nkey\ndata1\nkey\ndata2\n\n\n\n\n0\na\n0\na\n0\n\n\n1\nc\n1\nb\n1\n\n\n2\nd\n2\nc\n2\n\n\n3\nb\n3\nNaN\n&lt;NA&gt;\n\n\n4\na\n4\nNaN\n&lt;NA&gt;\n\n\n5\nc\n5\nNaN\n&lt;NA&gt;\n\n\n\n\n\n\n\n\n# merging by ignoring_index in DataFrame\n\ndf3 = pd.DataFrame(np.random.standard_normal((3, 4)),\n                  columns = ['a', 'b', 'c', 'd'])\n\n\ndf4 = pd.DataFrame(np.random.standard_normal((2,3)),\n                   columns = ['g', 'd', 'a'])\n                   \n\n\n\ndf3\n\n\n\n\n\n\n\n\na\nb\nc\nd\n\n\n\n\n0\n-0.692867\n-0.923164\n-1.055435\n0.938207\n\n\n1\n-0.060941\n1.029882\n-0.332099\n-1.697114\n\n\n2\n-0.274830\n1.991366\n-0.540897\n0.961377\n\n\n\n\n\n\n\n\ndf4\n\n\n\n\n\n\n\n\ng\nd\na\n\n\n\n\n0\n-1.397642\n1.511266\n-0.920547\n\n\n1\n0.518125\n-1.409185\n-1.092790\n\n\n\n\n\n\n\n\npd.concat([df3, df4], ignore_index = True)\n\n\n\n\n\n\n\n\na\nb\nc\nd\ng\n\n\n\n\n0\n1.711731\n-0.644975\n-0.093205\n0.074968\nNaN\n\n\n1\n-1.397718\n-1.585621\n0.808180\n-0.492032\nNaN\n\n\n2\n0.923910\n0.606571\n-1.045814\n1.247491\nNaN\n\n\n3\n-0.905022\nNaN\nNaN\n-1.122829\n-0.352158\n\n\n4\n0.091307\nNaN\nNaN\n-0.122968\n-0.349629\n\n\n\n\n\n\n\n\n\n\n\na = pd.Series([np.nan, 2.5, 0.0, 4.5, 3, np.nan],\n             index = ['a', 'b', 'c', 'g', 'k', 'o'])\n\nb = pd.Series([0., np.nan, 3., np.nan, 5., 2.],\n             index = ['a', 'b', 'c', 'd', 'e', 'f'])\n\n\na\n\na    NaN\nb    2.5\nc    0.0\nc    4.5\na    3.0\nb    NaN\ndtype: float64\n\n\n\nb\n\na    0.0\nb    NaN\nc    3.0\nd    NaN\ne    5.0\nf    2.0\ndtype: float64\n\n\n\nExplanation - &gt; selects non-null values from a or b\n\nnp.where doesnot check the index labels\n\n\nbetter to use combine_first method\n\n\ncombine_first method will have the union of all column names\n\n\n\nnp.where(pd.isna(a), b, a)\n\narray([0. , 2.5, 0. , 4.5, 3. , 2. ])\n\n\n\na.combine_first(b)\n\na    0.0\nb    2.5\nc    0.0\nd    NaN\ne    5.0\nf    2.0\ng    4.5\nk    3.0\no    NaN\ndtype: float64\n\n\n\n# using combine_first on DataFrame\n\ndf1.combine_first(df2)\n\n\n\n\n\n\n\n\ndata1\ndata2\nkey\n\n\n\n\n0\n0\n0\na\n\n\n1\n1\n1\nc\n\n\n2\n2\n2\nd\n\n\n3\n3\n&lt;NA&gt;\nb\n\n\n4\n4\n&lt;NA&gt;\na\n\n\n5\n5\n&lt;NA&gt;\nc\n\n\n\n\n\n\n\n\n\n\n\n\nstack method - rotates or pivots the columns\nunstack method - pivots the rows into columns\n\n\ndata = pd.DataFrame(np.arange(6).reshape((2,3)),\n                   index = pd.Index(['fdk', 'golewala'], \n                                    name = 'city'),\n                    columns = pd.Index(['one','two', 'three'], \n                                       name= 'number'))\n\n\ndata\n\n\n\n\n\n\n\nnumber\none\ntwo\nthree\n\n\ncity\n\n\n\n\n\n\n\nfdk\n0\n1\n2\n\n\ngolewala\n3\n4\n5\n\n\n\n\n\n\n\n\nresult_stack= data.stack()\n\nresult_stack\n\ncity      number\nfdk       one       0\n          two       1\n          three     2\ngolewala  one       3\n          two       4\n          three     5\ndtype: int32\n\n\n\nresult_stack.unstack()\n\n\n\n\n\n\n\nnumber\none\ntwo\nthree\n\n\ncity\n\n\n\n\n\n\n\nfdk\n0\n1\n2\n\n\ngolewala\n3\n4\n5\n\n\n\n\n\n\n\n\nresult_stack.unstack(level = 0)\n\n\n\n\n\n\n\ncity\nfdk\ngolewala\n\n\nnumber\n\n\n\n\n\n\none\n0\n3\n\n\ntwo\n1\n4\n\n\nthree\n2\n5\n\n\n\n\n\n\n\n\nresult_stack.unstack(level = 'city')\n\n\n\n\n\n\n\ncity\nfdk\ngolewala\n\n\nnumber\n\n\n\n\n\n\none\n0\n3\n\n\ntwo\n1\n4\n\n\nthree\n2\n5\n\n\n\n\n\n\n\n\n# unstacking a DataFrame\n\ndf5 = pd.DataFrame({'left': result_stack, 'right': result_stack+ 5},\n                  columns = pd.Index(['left', 'right']))\n\n\ndf5\n\n\n\n\n\n\n\n\n\nleft\nright\n\n\ncity\nnumber\n\n\n\n\n\n\nfdk\none\n0\n5\n\n\ntwo\n1\n6\n\n\nthree\n2\n7\n\n\ngolewala\none\n3\n8\n\n\ntwo\n4\n9\n\n\nthree\n5\n10\n\n\n\n\n\n\n\n\n\n\n\ndata = pd.read_csv(\"E:\\pythonfordatanalysis\\\\machine-readable-business-employment-data-sep-2023-quarter.csv\")\n\ndata.head()\n\n\n\n\n\n\n\n\nSeries_reference\nPeriod\nData_value\nSuppressed\nSTATUS\nUNITS\nMagnitude\nSubject\nGroup\nSeries_title_1\nSeries_title_2\nSeries_title_3\nSeries_title_4\nSeries_title_5\n\n\n\n\n0\nBDCQ.SEA1AA\n2011.06\n80078.0\nNaN\nF\nNumber\n0\nBusiness Data Collection - BDC\nIndustry by employment variable\nFilled jobs\nAgriculture, Forestry and Fishing\nActual\nNaN\nNaN\n\n\n1\nBDCQ.SEA1AA\n2011.09\n78324.0\nNaN\nF\nNumber\n0\nBusiness Data Collection - BDC\nIndustry by employment variable\nFilled jobs\nAgriculture, Forestry and Fishing\nActual\nNaN\nNaN\n\n\n2\nBDCQ.SEA1AA\n2011.12\n85850.0\nNaN\nF\nNumber\n0\nBusiness Data Collection - BDC\nIndustry by employment variable\nFilled jobs\nAgriculture, Forestry and Fishing\nActual\nNaN\nNaN\n\n\n3\nBDCQ.SEA1AA\n2012.03\n90743.0\nNaN\nF\nNumber\n0\nBusiness Data Collection - BDC\nIndustry by employment variable\nFilled jobs\nAgriculture, Forestry and Fishing\nActual\nNaN\nNaN\n\n\n4\nBDCQ.SEA1AA\n2012.06\n81780.0\nNaN\nF\nNumber\n0\nBusiness Data Collection - BDC\nIndustry by employment variable\nFilled jobs\nAgriculture, Forestry and Fishing\nActual\nNaN\nNaN\n\n\n\n\n\n\n\n\ndata2 = data.loc[:, ['Period', 'Group', 'Magnitude']]\n\n\ndata2.head()\n\n\n\n\n\n\n\n\nPeriod\nGroup\nMagnitude\n\n\n\n\n0\n2011.06\nIndustry by employment variable\n0\n\n\n1\n2011.09\nIndustry by employment variable\n0\n\n\n2\n2011.12\nIndustry by employment variable\n0\n\n\n3\n2012.03\nIndustry by employment variable\n0\n\n\n4\n2012.06\nIndustry by employment variable\n0\n\n\n\n\n\n\n\n\nhelp(pd.PeriodIndex)\n\n\ndivide = pd.PeriodIndex(year = [2000, 2002],\n                        quarter = [1,4])\n\n\ndivide\n\nPeriodIndex(['2000Q1', '2002Q4'], dtype='period[Q-DEC]')\n\n\n\ndata.columns\n\nIndex(['Series_reference', 'Period', 'Data_value', 'Suppressed', 'STATUS',\n       'UNITS', 'Magnitude', 'Subject', 'Group', 'Series_title_1',\n       'Series_title_2', 'Series_title_3', 'Series_title_4', 'Series_title_5'],\n      dtype='object')\n\n\n\ndata.columns.name = 'item'\n\n\ndata.head()\n\n\n\n\n\n\n\nitem\nSeries_reference\nData_value\nSuppressed\nSTATUS\nUNITS\nMagnitude\nSubject\nSeries_title_1\nSeries_title_2\nSeries_title_3\nSeries_title_4\nSeries_title_5\n\n\n\n\n0\nBDCQ.SEA1AA\n80078.0\nNaN\nF\nNumber\n0\nBusiness Data Collection - BDC\nFilled jobs\nAgriculture, Forestry and Fishing\nActual\nNaN\nNaN\n\n\n1\nBDCQ.SEA1AA\n78324.0\nNaN\nF\nNumber\n0\nBusiness Data Collection - BDC\nFilled jobs\nAgriculture, Forestry and Fishing\nActual\nNaN\nNaN\n\n\n2\nBDCQ.SEA1AA\n85850.0\nNaN\nF\nNumber\n0\nBusiness Data Collection - BDC\nFilled jobs\nAgriculture, Forestry and Fishing\nActual\nNaN\nNaN\n\n\n3\nBDCQ.SEA1AA\n90743.0\nNaN\nF\nNumber\n0\nBusiness Data Collection - BDC\nFilled jobs\nAgriculture, Forestry and Fishing\nActual\nNaN\nNaN\n\n\n4\nBDCQ.SEA1AA\n81780.0\nNaN\nF\nNumber\n0\nBusiness Data Collection - BDC\nFilled jobs\nAgriculture, Forestry and Fishing\nActual\nNaN\nNaN\n\n\n\n\n\n\n\n\nlong_data = (data.stack()\n            .reset_index()\n            .rename(columns = {0:'value'}))\n\n\nlong_data[:10]\n\n\n\n\n\n\n\n\nlevel_0\nitem\nvalue\n\n\n\n\n0\n0\nSeries_reference\nBDCQ.SEA1AA\n\n\n1\n0\nData_value\n80078.0\n\n\n2\n0\nSTATUS\nF\n\n\n3\n0\nUNITS\nNumber\n\n\n4\n0\nMagnitude\n0\n\n\n5\n0\nSubject\nBusiness Data Collection - BDC\n\n\n6\n0\nSeries_title_1\nFilled jobs\n\n\n7\n0\nSeries_title_2\nAgriculture, Forestry and Fishing\n\n\n8\n0\nSeries_title_3\nActual\n\n\n9\n1\nSeries_reference\nBDCQ.SEA1AA\n\n\n\n\n\n\n\n\n\n\n\npd.melt- using particular coloumn as a key indicator\npd.pivot- used to reset_index to move data back to column\n\n\ndf6 = pd.DataFrame({'key': ['foo', 'bar', 'xyz'],\n                  'A': [1, 3, 5],\n                   'C': [4, 6, 3],\n                   'D': [4, 64, 2]})\n\n\ndf6\n\n\n\n\n\n\n\n\nkey\nA\nC\nD\n\n\n\n\n0\nfoo\n1\n4\n4\n\n\n1\nbar\n3\n6\n64\n\n\n2\nxyz\n5\n3\n2\n\n\n\n\n\n\n\n\n# using pd.melt to use key as group indicator\nmelted = pd.melt(df6, id_vars = 'key')\n\n\nmelted\n\n\n\n\n\n\n\n\nkey\nvariable\nvalue\n\n\n\n\n0\nfoo\nA\n1\n\n\n1\nbar\nA\n3\n\n\n2\nxyz\nA\n5\n\n\n3\nfoo\nC\n4\n\n\n4\nbar\nC\n6\n\n\n5\nxyz\nC\n3\n\n\n6\nfoo\nD\n4\n\n\n7\nbar\nD\n64\n\n\n8\nxyz\nD\n2\n\n\n\n\n\n\n\n\n# back to orignal\nreshaped = melted.pivot(index = 'key', \n                       columns = 'variable',\n                       values = 'value')\n\n\nreshaped\n\n\n\n\n\n\n\nvariable\nA\nC\nD\n\n\nkey\n\n\n\n\n\n\n\nbar\n3\n6\n64\n\n\nfoo\n1\n4\n4\n\n\nxyz\n5\n3\n2\n\n\n\n\n\n\n\n\nreshaped.reset_index()\n\n\n\n\n\n\n\nvariable\nkey\nA\nC\nD\n\n\n\n\n0\nbar\n3\n6\n64\n\n\n1\nfoo\n1\n4\n4\n\n\n2\nxyz\n5\n3\n2\n\n\n\n\n\n\n\n\ndf6\n\n\n\n\n\n\n\n\nkey\nA\nC\nD\n\n\n\n\n0\nfoo\n1\n4\n4\n\n\n1\nbar\n3\n6\n64\n\n\n2\nxyz\n5\n3\n2\n\n\n\n\n\n\n\n\n# specify a subset of columns to use as a value columns\n\npd.melt(df6, id_vars = \"key\", value_vars = ['A', 'C'])\n\n\n\n\n\n\n\n\nkey\nvariable\nvalue\n\n\n\n\n0\nfoo\nA\n1\n\n\n1\nbar\nA\n3\n\n\n2\nxyz\nA\n5\n\n\n3\nfoo\nC\n4\n\n\n4\nbar\nC\n6\n\n\n5\nxyz\nC\n3"
  },
  {
    "objectID": "posts/python/datawrangling_join_combine_reshape.html#hierarchical-indexing",
    "href": "posts/python/datawrangling_join_combine_reshape.html#hierarchical-indexing",
    "title": "Data Wrangling",
    "section": "",
    "text": "Reordering and Sorting levels\nSummary statitics by level\nIndexing with DataFrame’s Columns"
  },
  {
    "objectID": "posts/python/datawrangling_join_combine_reshape.html#combining-and-merging-datasets",
    "href": "posts/python/datawrangling_join_combine_reshape.html#combining-and-merging-datasets",
    "title": "Data Wrangling",
    "section": "",
    "text": "Database-Style DataFrame joins\nMerging on Index\nConcatenating Along an Axis\nCombining Data with Overlap"
  },
  {
    "objectID": "posts/python/datawrangling_join_combine_reshape.html#reshaping-and-pivoting",
    "href": "posts/python/datawrangling_join_combine_reshape.html#reshaping-and-pivoting",
    "title": "Data Wrangling",
    "section": "",
    "text": "Reshaping with hierarchical Indexing\nPivoting ‘long’ to ‘wide’ format\npivoting ‘wide’ to ‘long’ format"
  },
  {
    "objectID": "posts/python/datawrangling_join_combine_reshape.html#hierarchical-indexing-series",
    "href": "posts/python/datawrangling_join_combine_reshape.html#hierarchical-indexing-series",
    "title": "Data Wrangling",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\n\ndata = pd.Series(np.random.uniform(size = 9),\n                index =  [['a', 'a', 'b', 'c', 'c', 'b', 'c', 'b','a'],\n                          [1, 2, 3, 1, 3, 4, 3, 2, 1]])\ndata\n\na  1    0.684862\n   2    0.701188\nb  3    0.870829\nc  1    0.958994\n   3    0.042434\nb  4    0.539591\nc  3    0.668997\nb  2    0.501304\na  1    0.260682\ndtype: float64\n\n\n\n# gaps for 'multi-index'\ndata.index\n\nMultiIndex([('a', 1),\n            ('a', 2),\n            ('b', 3),\n            ('c', 1),\n            ('c', 3),\n            ('b', 4),\n            ('c', 3),\n            ('b', 2),\n            ('a', 1)],\n           )\n\n\n\nmean = [0, 0]\ncov =  [[1,0], [0, 100]]\n\n\ndata\n\na  1    0.684862\n   2    0.701188\nb  3    0.870829\nc  1    0.958994\n   3    0.042434\nb  4    0.539591\nc  3    0.668997\nb  2    0.501304\na  1    0.260682\ndtype: float64\n\n\n\n# selecting subset\ndata['b']\n\n3    0.870829\n4    0.539591\n2    0.501304\ndtype: float64\n\n\n\n# selecting the data values with loc operator\ndata.loc[['a','b']]\n\na  1    0.684862\n   2    0.701188\n   1    0.260682\nb  3    0.870829\n   4    0.539591\n   2    0.501304\ndtype: float64\n\n\n\ndata.loc[:, 2]\n\na    0.701188\nb    0.501304\ndtype: float64"
  },
  {
    "objectID": "posts/python/datawrangling_join_combine_reshape.html#hierarchical-index-dataframe",
    "href": "posts/python/datawrangling_join_combine_reshape.html#hierarchical-index-dataframe",
    "title": "Data Wrangling",
    "section": "",
    "text": "frame = pd.DataFrame(np.arange(12).reshape((4,3)),\n                    index = [[\"a\", \"a\", \"b\", \"b\"], [1, 2, 1, 2]],\n                    columns = [['fdk', 'fzp', 'chd'],\n                               ['PB', 'PB', 'CHD']])\n\n\nframe\n\n\n\n\n\n\n\n\n\nfdk\nfzp\nchd\n\n\n\n\nPB\nPB\nCHD\n\n\n\n\na\n1\n0\n1\n2\n\n\n2\n3\n4\n5\n\n\nb\n1\n6\n7\n8\n\n\n2\n9\n10\n11\n\n\n\n\n\n\n\n\nframe.index.names = ['key1', 'key2']\n\n\nframe.columns.names = ['city', 'province']\n\n\nframe\n\n\n\n\n\n\n\n\ncity\nfdk\nfzp\nchd\n\n\n\nprovince\nPB\nPB\nCHD\n\n\nkey1\nkey2\n\n\n\n\n\n\n\na\n1\n0\n1\n2\n\n\n2\n3\n4\n5\n\n\nb\n1\n6\n7\n8\n\n\n2\n9\n10\n11\n\n\n\n\n\n\n\n\n# to check how many levels an index has\nframe.index.nlevels\n\n2\n\n\n\n# partial column indexing\nframe['fdk']\n\n\n\n\n\n\n\n\nprovince\nPB\n\n\nkey1\nkey2\n\n\n\n\n\na\n1\n0\n\n\n2\n3\n\n\nb\n1\n6\n\n\n2\n9\n\n\n\n\n\n\n\n\nframe['fzp']\n\n\n\n\n\n\n\n\nprovince\nPB\n\n\nkey1\nkey2\n\n\n\n\n\na\n1\n1\n\n\n2\n4\n\n\nb\n1\n7\n\n\n2\n10\n\n\n\n\n\n\n\n\nframe['chd']\n\n\n\n\n\n\n\n\nprovince\nCHD\n\n\nkey1\nkey2\n\n\n\n\n\na\n1\n2\n\n\n2\n5\n\n\nb\n1\n8\n\n\n2\n11\n\n\n\n\n\n\n\n\npd.MultiIndex.from_arrays([['fdk', 'fzp', 'chd'],\n                          ['PB', 'PB', 'CHD'],\n                           names=['city', 'capital'])\n                           \n                           \n\n\n\n\nframe.swaplevel('key1', 'key2')\n\n\n\n\n\n\n\n\ncity\nfdk\nfzp\nchd\n\n\n\nprovince\nPB\nPB\nCHD\n\n\nkey2\nkey1\n\n\n\n\n\n\n\n1\na\n0\n1\n2\n\n\n2\na\n3\n4\n5\n\n\n1\nb\n6\n7\n8\n\n\n2\nb\n9\n10\n11\n\n\n\n\n\n\n\n\nframe.sort_index(level=1)\n\n\n\n\n\n\n\n\ncity\nfdk\nfzp\nchd\n\n\n\nprovince\nPB\nPB\nCHD\n\n\nkey1\nkey2\n\n\n\n\n\n\n\na\n1\n0\n1\n2\n\n\nb\n1\n6\n7\n8\n\n\na\n2\n3\n4\n5\n\n\nb\n2\n9\n10\n11\n\n\n\n\n\n\n\n\nframe.swaplevel(0,1).sort_index(level=0)\n\n\n\n\n\n\n\n\ncity\nfdk\nfzp\nchd\n\n\n\nprovince\nPB\nPB\nCHD\n\n\nkey2\nkey1\n\n\n\n\n\n\n\n1\na\n0\n1\n2\n\n\nb\n6\n7\n8\n\n\n2\na\n3\n4\n5\n\n\nb\n9\n10\n11\n\n\n\n\n\n\n\n\n\n\n\nframe.groupby(level='key2').sum()\n\n\n\n\n\n\n\ncity\nfdk\nfzp\nchd\n\n\nprovince\nPB\nPB\nCHD\n\n\nkey2\n\n\n\n\n\n\n\n1\n6\n8\n10\n\n\n2\n12\n14\n16\n\n\n\n\n\n\n\n\nframe.groupby(level= 'province', axis = 'columns').sum()\n\n\n\n\n\n\n\n\nprovince\nCHD\nPB\n\n\nkey1\nkey2\n\n\n\n\n\n\na\n1\n2\n1\n\n\n2\n5\n7\n\n\nb\n1\n8\n13\n\n\n2\n11\n19\n\n\n\n\n\n\n\n\n\n\n\nframe2 = pd.DataFrame({'a': range(7), 'b': range(7,0,-1),\n                      'c': ['one', 'one', 'one', 'two', 'two',\n                           'two', 'two'],\n                      'd': [0, 1,2,0,1,3,2]})\n\n\nframe2\n\n\n\n\n\n\n\n\na\nb\nc\nd\n\n\n\n\n0\n0\n7\none\n0\n\n\n1\n1\n6\none\n1\n\n\n2\n2\n5\none\n2\n\n\n3\n3\n4\ntwo\n0\n\n\n4\n4\n3\ntwo\n1\n\n\n5\n5\n2\ntwo\n3\n\n\n6\n6\n1\ntwo\n2\n\n\n\n\n\n\n\n\n# set_index to create a new DataFrame\n\nframe3 = frame2.set_index(['c', 'd'])\n\nframe3\n\n\n\n\n\n\n\n\n\na\nb\n\n\nc\nd\n\n\n\n\n\n\none\n0\n0\n7\n\n\n1\n1\n6\n\n\n2\n2\n5\n\n\ntwo\n0\n3\n4\n\n\n1\n4\n3\n\n\n3\n5\n2\n\n\n2\n6\n1\n\n\n\n\n\n\n\n\n# we can set it to index by doing drop= False\n\nframe2.set_index([\"c\",'d'], drop= False)\n\n\n\n\n\n\n\n\n\na\nb\nc\nd\n\n\nc\nd\n\n\n\n\n\n\n\n\none\n0\n0\n7\none\n0\n\n\n1\n1\n6\none\n1\n\n\n2\n2\n5\none\n2\n\n\ntwo\n0\n3\n4\ntwo\n0\n\n\n1\n4\n3\ntwo\n1\n\n\n3\n5\n2\ntwo\n3\n\n\n2\n6\n1\ntwo\n2\n\n\n\n\n\n\n\n\n# reset_index brings it back to the orignal position\n\nframe2.reset_index()\n\n\n\n\n\n\n\n\nindex\na\nb\nc\nd\n\n\n\n\n0\n0\n0\n7\none\n0\n\n\n1\n1\n1\n6\none\n1\n\n\n2\n2\n2\n5\none\n2\n\n\n3\n3\n3\n4\ntwo\n0\n\n\n4\n4\n4\n3\ntwo\n1\n\n\n5\n5\n5\n2\ntwo\n3\n\n\n6\n6\n6\n1\ntwo\n2"
  },
  {
    "objectID": "posts/python/datawrangling_join_combine_reshape.html#combining-and-merging-datasets-1",
    "href": "posts/python/datawrangling_join_combine_reshape.html#combining-and-merging-datasets-1",
    "title": "Data Wrangling",
    "section": "",
    "text": "pandas.merge (connects rows based on one/more keys) how\npandas.concat (stacks objects together on axis)\ncombine_first (slice together overlapping data to fill missing values)\nmerge function arguments\n\n\n# DataFrame joins\ndf1 = pd.DataFrame({\"key\": ['a', 'c', 'd', 'b', 'a', 'c'],\n                   'data1': pd.Series(range(6), dtype= 'Int64')})\n\ndf2 = pd.DataFrame({'key': ['a', 'b', 'c'],\n                   'data2': pd.Series(range(3), dtype='Int64')})\n\n\ndf1\n\n\n\n\n\n\n\n\nkey\ndata1\n\n\n\n\n0\na\n0\n\n\n1\nc\n1\n\n\n2\nd\n2\n\n\n3\nb\n3\n\n\n4\na\n4\n\n\n5\nc\n5\n\n\n\n\n\n\n\n\ndf2\n\n\n\n\n\n\n\n\nkey\ndata2\n\n\n\n\n0\na\n0\n\n\n1\nb\n1\n\n\n2\nc\n2\n\n\n\n\n\n\n\n\npd.merge(df1, df2)\n\n\n\n\n\n\n\n\nkey\ndata1\ndata2\n\n\n\n\n0\na\n0\n0\n\n\n1\na\n4\n0\n\n\n2\nc\n1\n2\n\n\n3\nc\n5\n2\n\n\n4\nb\n3\n1\n\n\n\n\n\n\n\n\n# specifying the column\npd.merge(df1, df2, on= 'key')\n\n\n\n\n\n\n\n\nkey\ndata1\ndata2\n\n\n\n\n0\na\n0\n0\n\n\n1\na\n4\n0\n\n\n2\nc\n1\n2\n\n\n3\nc\n5\n2\n\n\n4\nb\n3\n1\n\n\n\n\n\n\n\n\npd.merge(df1, df2, how= 'outer')\n\n\n\n\n\n\n\n\nkey\ndata1\ndata2\n\n\n\n\n0\na\n0\n0\n\n\n1\na\n4\n0\n\n\n2\nc\n1\n2\n\n\n3\nc\n5\n2\n\n\n4\nd\n2\n&lt;NA&gt;\n\n\n5\nb\n3\n1\n\n\n\n\n\n\n\n\n\n\npd.merge(df1, df2, on= 'key', suffixes = (\"_left\", \"_right\"))\n\n\n\n\n\n\n\n\nkey\ndata1\ndata2\n\n\n\n\n0\na\n0\n0\n\n\n1\na\n4\n0\n\n\n2\nc\n1\n2\n\n\n3\nc\n5\n2\n\n\n4\nb\n3\n1\n\n\n\n\n\n\n\n\n\n\n\ndf1.join(df1, on= 'key')\n\n\nanother = pd.DataFrame([[7., 8.], [9., 10.],\n                       [11., 12.], [16., 17.]],\n                      index = ['a', 'c', 'e', 'f'],\n                      columns= ['jandiala', 'faridkot'])\n\n\nanother\n\n\n\n\n\n\n\n\njandiala\nfaridkot\n\n\n\n\na\n7.0\n8.0\n\n\nc\n9.0\n10.0\n\n\ne\n11.0\n12.0\n\n\nf\n16.0\n17.0\n\n\n\n\n\n\n\n\ndf1.join(another, how= 'outer')\n\n\n\n\n\n\n\n\nkey\ndata1\njandiala\nfaridkot\n\n\n\n\n0\na\n0\nNaN\nNaN\n\n\n1\nc\n1\nNaN\nNaN\n\n\n2\nd\n2\nNaN\nNaN\n\n\n3\nb\n3\nNaN\nNaN\n\n\n4\na\n4\nNaN\nNaN\n\n\n5\nc\n5\nNaN\nNaN\n\n\na\nNaN\n&lt;NA&gt;\n7.0\n8.0\n\n\nc\nNaN\n&lt;NA&gt;\n9.0\n10.0\n\n\ne\nNaN\n&lt;NA&gt;\n11.0\n12.0\n\n\nf\nNaN\n&lt;NA&gt;\n16.0\n17.0\n\n\n\n\n\n\n\n\n\n\n\ndata combination\nfunction agruments pandas.concat\n\n\narr = np.arange(12).reshape((3,4))\n\n\narr\n\narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11]])\n\n\n\nnp.concatenate([arr, arr])\n\narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11],\n       [ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11]])\n\n\n\nnp.concatenate([arr, arr], axis = 1)\n\narray([[ 0,  1,  2,  3,  0,  1,  2,  3],\n       [ 4,  5,  6,  7,  4,  5,  6,  7],\n       [ 8,  9, 10, 11,  8,  9, 10, 11]])\n\n\n\n## series with no index overlap\n\ns1 = pd.Series([0, 1], index = ['a', 'b'], dtype = 'Int64')\ns2 = pd.Series([2,3,4], index = ['c', 'd', 'e'], dtype= 'Int64')\ns3 = pd.Series([5,6], index =['e', 'f'], dtype = 'Int64')\n\n\ns1\n\na    0\nb    1\ndtype: Int64\n\n\n\ns2\n\nc    2\nd    3\ne    4\ndtype: Int64\n\n\n\ns3\n\ne    5\nf    6\ndtype: Int64\n\n\n\npd.concat([s1, s2, s3])\n\na    0\nb    1\nc    2\nd    3\ne    4\ne    5\nf    6\ndtype: Int64\n\n\n\n# the result will be a DataFrame if we pass axis = 'columns'\n\npd.concat([s1, s2, s3], axis = 'columns')\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\na\n0\n&lt;NA&gt;\n&lt;NA&gt;\n\n\nb\n1\n&lt;NA&gt;\n&lt;NA&gt;\n\n\nc\n&lt;NA&gt;\n2\n&lt;NA&gt;\n\n\nd\n&lt;NA&gt;\n3\n&lt;NA&gt;\n\n\ne\n&lt;NA&gt;\n4\n5\n\n\nf\n&lt;NA&gt;\n&lt;NA&gt;\n6\n\n\n\n\n\n\n\n\n# trying inner join()\n\ns4 = pd.concat([s1, s3])\n\n\ns4\n\na    0\nb    1\ne    5\nf    6\ndtype: Int64\n\n\n\npd.concat([s1, s4], axis = 'columns')\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\na\n0\n0\n\n\nb\n1\n1\n\n\ne\n&lt;NA&gt;\n5\n\n\nf\n&lt;NA&gt;\n6\n\n\n\n\n\n\n\n\n# because of inner join, labels 'f' and 'g' disappeared\n\npd.concat([s1, s4], axis = 'columns', join = 'inner')\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\na\n0\n0\n\n\nb\n1\n1\n\n\n\n\n\n\n\n\nresult = pd.concat([s1, s1, s3], keys= ['one', 'two', 'three'])\n\n\nresult\n\none    a    0\n       b    1\ntwo    a    0\n       b    1\nthree  e    5\n       f    6\ndtype: Int64\n\n\n\nresult.unstack()\n\n\n\n\n\n\n\n\na\nb\ne\nf\n\n\n\n\none\n0\n1\n&lt;NA&gt;\n&lt;NA&gt;\n\n\ntwo\n0\n1\n&lt;NA&gt;\n&lt;NA&gt;\n\n\nthree\n&lt;NA&gt;\n&lt;NA&gt;\n5\n6\n\n\n\n\n\n\n\n\nin case of combining Series along axis= ‘columns’, &gt; keys become DataFrame column headers\n\n\npd.concat([s1, s2, s3], axis = 'columns', \n          keys = ['one', 'two', 'three'])\n\n\n\n\n\n\n\n\none\ntwo\nthree\n\n\n\n\na\n0\n&lt;NA&gt;\n&lt;NA&gt;\n\n\nb\n1\n&lt;NA&gt;\n&lt;NA&gt;\n\n\nc\n&lt;NA&gt;\n2\n&lt;NA&gt;\n\n\nd\n&lt;NA&gt;\n3\n&lt;NA&gt;\n\n\ne\n&lt;NA&gt;\n4\n5\n\n\nf\n&lt;NA&gt;\n&lt;NA&gt;\n6\n\n\n\n\n\n\n\n\n# same logic extends to DataFrame objects\npd.concat([df1, df2], axis = 'columns')\n\n\n\n\n\n\n\n\nkey\ndata1\nkey\ndata2\n\n\n\n\n0\na\n0\na\n0\n\n\n1\nc\n1\nb\n1\n\n\n2\nd\n2\nc\n2\n\n\n3\nb\n3\nNaN\n&lt;NA&gt;\n\n\n4\na\n4\nNaN\n&lt;NA&gt;\n\n\n5\nc\n5\nNaN\n&lt;NA&gt;\n\n\n\n\n\n\n\n\nIn dictionary objects, the keys will be used &gt; for key option\n\n\npd.concat({'level1': df1, 'level2': df2},\n         axis = 'columns')\n\n\n\n\n\n\n\n\nlevel1\nlevel2\n\n\n\nkey\ndata1\nkey\ndata2\n\n\n\n\n0\na\n0\na\n0\n\n\n1\nc\n1\nb\n1\n\n\n2\nd\n2\nc\n2\n\n\n3\nb\n3\nNaN\n&lt;NA&gt;\n\n\n4\na\n4\nNaN\n&lt;NA&gt;\n\n\n5\nc\n5\nNaN\n&lt;NA&gt;\n\n\n\n\n\n\n\n\n# additional arguments\n\npd.concat([df1, df2], axis = 'columns',\n         keys = ['level1', 'level2'],\n         names = ['upper', 'lower'])\n\n\n\n\n\n\n\nupper\nlevel1\nlevel2\n\n\nlower\nkey\ndata1\nkey\ndata2\n\n\n\n\n0\na\n0\na\n0\n\n\n1\nc\n1\nb\n1\n\n\n2\nd\n2\nc\n2\n\n\n3\nb\n3\nNaN\n&lt;NA&gt;\n\n\n4\na\n4\nNaN\n&lt;NA&gt;\n\n\n5\nc\n5\nNaN\n&lt;NA&gt;\n\n\n\n\n\n\n\n\n# merging by ignoring_index in DataFrame\n\ndf3 = pd.DataFrame(np.random.standard_normal((3, 4)),\n                  columns = ['a', 'b', 'c', 'd'])\n\n\ndf4 = pd.DataFrame(np.random.standard_normal((2,3)),\n                   columns = ['g', 'd', 'a'])\n                   \n\n\n\ndf3\n\n\n\n\n\n\n\n\na\nb\nc\nd\n\n\n\n\n0\n-0.692867\n-0.923164\n-1.055435\n0.938207\n\n\n1\n-0.060941\n1.029882\n-0.332099\n-1.697114\n\n\n2\n-0.274830\n1.991366\n-0.540897\n0.961377\n\n\n\n\n\n\n\n\ndf4\n\n\n\n\n\n\n\n\ng\nd\na\n\n\n\n\n0\n-1.397642\n1.511266\n-0.920547\n\n\n1\n0.518125\n-1.409185\n-1.092790\n\n\n\n\n\n\n\n\npd.concat([df3, df4], ignore_index = True)\n\n\n\n\n\n\n\n\na\nb\nc\nd\ng\n\n\n\n\n0\n1.711731\n-0.644975\n-0.093205\n0.074968\nNaN\n\n\n1\n-1.397718\n-1.585621\n0.808180\n-0.492032\nNaN\n\n\n2\n0.923910\n0.606571\n-1.045814\n1.247491\nNaN\n\n\n3\n-0.905022\nNaN\nNaN\n-1.122829\n-0.352158\n\n\n4\n0.091307\nNaN\nNaN\n-0.122968\n-0.349629\n\n\n\n\n\n\n\n\n\n\n\na = pd.Series([np.nan, 2.5, 0.0, 4.5, 3, np.nan],\n             index = ['a', 'b', 'c', 'g', 'k', 'o'])\n\nb = pd.Series([0., np.nan, 3., np.nan, 5., 2.],\n             index = ['a', 'b', 'c', 'd', 'e', 'f'])\n\n\na\n\na    NaN\nb    2.5\nc    0.0\nc    4.5\na    3.0\nb    NaN\ndtype: float64\n\n\n\nb\n\na    0.0\nb    NaN\nc    3.0\nd    NaN\ne    5.0\nf    2.0\ndtype: float64\n\n\n\nExplanation - &gt; selects non-null values from a or b\n\nnp.where doesnot check the index labels\n\n\nbetter to use combine_first method\n\n\ncombine_first method will have the union of all column names\n\n\n\nnp.where(pd.isna(a), b, a)\n\narray([0. , 2.5, 0. , 4.5, 3. , 2. ])\n\n\n\na.combine_first(b)\n\na    0.0\nb    2.5\nc    0.0\nd    NaN\ne    5.0\nf    2.0\ng    4.5\nk    3.0\no    NaN\ndtype: float64\n\n\n\n# using combine_first on DataFrame\n\ndf1.combine_first(df2)\n\n\n\n\n\n\n\n\ndata1\ndata2\nkey\n\n\n\n\n0\n0\n0\na\n\n\n1\n1\n1\nc\n\n\n2\n2\n2\nd\n\n\n3\n3\n&lt;NA&gt;\nb\n\n\n4\n4\n&lt;NA&gt;\na\n\n\n5\n5\n&lt;NA&gt;\nc"
  },
  {
    "objectID": "posts/python/datawrangling_join_combine_reshape.html#reshaing-and-pivoting",
    "href": "posts/python/datawrangling_join_combine_reshape.html#reshaing-and-pivoting",
    "title": "Data Wrangling",
    "section": "",
    "text": "stack method - rotates or pivots the columns\nunstack method - pivots the rows into columns\n\n\ndata = pd.DataFrame(np.arange(6).reshape((2,3)),\n                   index = pd.Index(['fdk', 'golewala'], \n                                    name = 'city'),\n                    columns = pd.Index(['one','two', 'three'], \n                                       name= 'number'))\n\n\ndata\n\n\n\n\n\n\n\nnumber\none\ntwo\nthree\n\n\ncity\n\n\n\n\n\n\n\nfdk\n0\n1\n2\n\n\ngolewala\n3\n4\n5\n\n\n\n\n\n\n\n\nresult_stack= data.stack()\n\nresult_stack\n\ncity      number\nfdk       one       0\n          two       1\n          three     2\ngolewala  one       3\n          two       4\n          three     5\ndtype: int32\n\n\n\nresult_stack.unstack()\n\n\n\n\n\n\n\nnumber\none\ntwo\nthree\n\n\ncity\n\n\n\n\n\n\n\nfdk\n0\n1\n2\n\n\ngolewala\n3\n4\n5\n\n\n\n\n\n\n\n\nresult_stack.unstack(level = 0)\n\n\n\n\n\n\n\ncity\nfdk\ngolewala\n\n\nnumber\n\n\n\n\n\n\none\n0\n3\n\n\ntwo\n1\n4\n\n\nthree\n2\n5\n\n\n\n\n\n\n\n\nresult_stack.unstack(level = 'city')\n\n\n\n\n\n\n\ncity\nfdk\ngolewala\n\n\nnumber\n\n\n\n\n\n\none\n0\n3\n\n\ntwo\n1\n4\n\n\nthree\n2\n5\n\n\n\n\n\n\n\n\n# unstacking a DataFrame\n\ndf5 = pd.DataFrame({'left': result_stack, 'right': result_stack+ 5},\n                  columns = pd.Index(['left', 'right']))\n\n\ndf5\n\n\n\n\n\n\n\n\n\nleft\nright\n\n\ncity\nnumber\n\n\n\n\n\n\nfdk\none\n0\n5\n\n\ntwo\n1\n6\n\n\nthree\n2\n7\n\n\ngolewala\none\n3\n8\n\n\ntwo\n4\n9\n\n\nthree\n5\n10"
  },
  {
    "objectID": "posts/python/datawrangling_join_combine_reshape.html#pivoting-long-to-wide-format",
    "href": "posts/python/datawrangling_join_combine_reshape.html#pivoting-long-to-wide-format",
    "title": "Data Wrangling",
    "section": "",
    "text": "data = pd.read_csv(\"E:\\pythonfordatanalysis\\\\machine-readable-business-employment-data-sep-2023-quarter.csv\")\n\ndata.head()\n\n\n\n\n\n\n\n\nSeries_reference\nPeriod\nData_value\nSuppressed\nSTATUS\nUNITS\nMagnitude\nSubject\nGroup\nSeries_title_1\nSeries_title_2\nSeries_title_3\nSeries_title_4\nSeries_title_5\n\n\n\n\n0\nBDCQ.SEA1AA\n2011.06\n80078.0\nNaN\nF\nNumber\n0\nBusiness Data Collection - BDC\nIndustry by employment variable\nFilled jobs\nAgriculture, Forestry and Fishing\nActual\nNaN\nNaN\n\n\n1\nBDCQ.SEA1AA\n2011.09\n78324.0\nNaN\nF\nNumber\n0\nBusiness Data Collection - BDC\nIndustry by employment variable\nFilled jobs\nAgriculture, Forestry and Fishing\nActual\nNaN\nNaN\n\n\n2\nBDCQ.SEA1AA\n2011.12\n85850.0\nNaN\nF\nNumber\n0\nBusiness Data Collection - BDC\nIndustry by employment variable\nFilled jobs\nAgriculture, Forestry and Fishing\nActual\nNaN\nNaN\n\n\n3\nBDCQ.SEA1AA\n2012.03\n90743.0\nNaN\nF\nNumber\n0\nBusiness Data Collection - BDC\nIndustry by employment variable\nFilled jobs\nAgriculture, Forestry and Fishing\nActual\nNaN\nNaN\n\n\n4\nBDCQ.SEA1AA\n2012.06\n81780.0\nNaN\nF\nNumber\n0\nBusiness Data Collection - BDC\nIndustry by employment variable\nFilled jobs\nAgriculture, Forestry and Fishing\nActual\nNaN\nNaN\n\n\n\n\n\n\n\n\ndata2 = data.loc[:, ['Period', 'Group', 'Magnitude']]\n\n\ndata2.head()\n\n\n\n\n\n\n\n\nPeriod\nGroup\nMagnitude\n\n\n\n\n0\n2011.06\nIndustry by employment variable\n0\n\n\n1\n2011.09\nIndustry by employment variable\n0\n\n\n2\n2011.12\nIndustry by employment variable\n0\n\n\n3\n2012.03\nIndustry by employment variable\n0\n\n\n4\n2012.06\nIndustry by employment variable\n0\n\n\n\n\n\n\n\n\nhelp(pd.PeriodIndex)\n\n\ndivide = pd.PeriodIndex(year = [2000, 2002],\n                        quarter = [1,4])\n\n\ndivide\n\nPeriodIndex(['2000Q1', '2002Q4'], dtype='period[Q-DEC]')\n\n\n\ndata.columns\n\nIndex(['Series_reference', 'Period', 'Data_value', 'Suppressed', 'STATUS',\n       'UNITS', 'Magnitude', 'Subject', 'Group', 'Series_title_1',\n       'Series_title_2', 'Series_title_3', 'Series_title_4', 'Series_title_5'],\n      dtype='object')\n\n\n\ndata.columns.name = 'item'\n\n\ndata.head()\n\n\n\n\n\n\n\nitem\nSeries_reference\nData_value\nSuppressed\nSTATUS\nUNITS\nMagnitude\nSubject\nSeries_title_1\nSeries_title_2\nSeries_title_3\nSeries_title_4\nSeries_title_5\n\n\n\n\n0\nBDCQ.SEA1AA\n80078.0\nNaN\nF\nNumber\n0\nBusiness Data Collection - BDC\nFilled jobs\nAgriculture, Forestry and Fishing\nActual\nNaN\nNaN\n\n\n1\nBDCQ.SEA1AA\n78324.0\nNaN\nF\nNumber\n0\nBusiness Data Collection - BDC\nFilled jobs\nAgriculture, Forestry and Fishing\nActual\nNaN\nNaN\n\n\n2\nBDCQ.SEA1AA\n85850.0\nNaN\nF\nNumber\n0\nBusiness Data Collection - BDC\nFilled jobs\nAgriculture, Forestry and Fishing\nActual\nNaN\nNaN\n\n\n3\nBDCQ.SEA1AA\n90743.0\nNaN\nF\nNumber\n0\nBusiness Data Collection - BDC\nFilled jobs\nAgriculture, Forestry and Fishing\nActual\nNaN\nNaN\n\n\n4\nBDCQ.SEA1AA\n81780.0\nNaN\nF\nNumber\n0\nBusiness Data Collection - BDC\nFilled jobs\nAgriculture, Forestry and Fishing\nActual\nNaN\nNaN\n\n\n\n\n\n\n\n\nlong_data = (data.stack()\n            .reset_index()\n            .rename(columns = {0:'value'}))\n\n\nlong_data[:10]\n\n\n\n\n\n\n\n\nlevel_0\nitem\nvalue\n\n\n\n\n0\n0\nSeries_reference\nBDCQ.SEA1AA\n\n\n1\n0\nData_value\n80078.0\n\n\n2\n0\nSTATUS\nF\n\n\n3\n0\nUNITS\nNumber\n\n\n4\n0\nMagnitude\n0\n\n\n5\n0\nSubject\nBusiness Data Collection - BDC\n\n\n6\n0\nSeries_title_1\nFilled jobs\n\n\n7\n0\nSeries_title_2\nAgriculture, Forestry and Fishing\n\n\n8\n0\nSeries_title_3\nActual\n\n\n9\n1\nSeries_reference\nBDCQ.SEA1AA"
  },
  {
    "objectID": "posts/python/datawrangling_join_combine_reshape.html#pivoting-wide-to-long-format",
    "href": "posts/python/datawrangling_join_combine_reshape.html#pivoting-wide-to-long-format",
    "title": "Data Wrangling",
    "section": "",
    "text": "pd.melt- using particular coloumn as a key indicator\npd.pivot- used to reset_index to move data back to column\n\n\ndf6 = pd.DataFrame({'key': ['foo', 'bar', 'xyz'],\n                  'A': [1, 3, 5],\n                   'C': [4, 6, 3],\n                   'D': [4, 64, 2]})\n\n\ndf6\n\n\n\n\n\n\n\n\nkey\nA\nC\nD\n\n\n\n\n0\nfoo\n1\n4\n4\n\n\n1\nbar\n3\n6\n64\n\n\n2\nxyz\n5\n3\n2\n\n\n\n\n\n\n\n\n# using pd.melt to use key as group indicator\nmelted = pd.melt(df6, id_vars = 'key')\n\n\nmelted\n\n\n\n\n\n\n\n\nkey\nvariable\nvalue\n\n\n\n\n0\nfoo\nA\n1\n\n\n1\nbar\nA\n3\n\n\n2\nxyz\nA\n5\n\n\n3\nfoo\nC\n4\n\n\n4\nbar\nC\n6\n\n\n5\nxyz\nC\n3\n\n\n6\nfoo\nD\n4\n\n\n7\nbar\nD\n64\n\n\n8\nxyz\nD\n2\n\n\n\n\n\n\n\n\n# back to orignal\nreshaped = melted.pivot(index = 'key', \n                       columns = 'variable',\n                       values = 'value')\n\n\nreshaped\n\n\n\n\n\n\n\nvariable\nA\nC\nD\n\n\nkey\n\n\n\n\n\n\n\nbar\n3\n6\n64\n\n\nfoo\n1\n4\n4\n\n\nxyz\n5\n3\n2\n\n\n\n\n\n\n\n\nreshaped.reset_index()\n\n\n\n\n\n\n\nvariable\nkey\nA\nC\nD\n\n\n\n\n0\nbar\n3\n6\n64\n\n\n1\nfoo\n1\n4\n4\n\n\n2\nxyz\n5\n3\n2\n\n\n\n\n\n\n\n\ndf6\n\n\n\n\n\n\n\n\nkey\nA\nC\nD\n\n\n\n\n0\nfoo\n1\n4\n4\n\n\n1\nbar\n3\n6\n64\n\n\n2\nxyz\n5\n3\n2\n\n\n\n\n\n\n\n\n# specify a subset of columns to use as a value columns\n\npd.melt(df6, id_vars = \"key\", value_vars = ['A', 'C'])\n\n\n\n\n\n\n\n\nkey\nvariable\nvalue\n\n\n\n\n0\nfoo\nA\n1\n\n\n1\nbar\nA\n3\n\n\n2\nxyz\nA\n5\n\n\n3\nfoo\nC\n4\n\n\n4\nbar\nC\n6\n\n\n5\nxyz\nC\n3"
  },
  {
    "objectID": "posts/python/data_cleaning+prepration.html",
    "href": "posts/python/data_cleaning+prepration.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "Filtering\nFilling"
  },
  {
    "objectID": "posts/python/data_cleaning+prepration.html#handing-missing-data",
    "href": "posts/python/data_cleaning+prepration.html#handing-missing-data",
    "title": "Data Cleaning",
    "section": "",
    "text": "Filtering\nFilling"
  },
  {
    "objectID": "posts/python/data_cleaning+prepration.html#data-transformation",
    "href": "posts/python/data_cleaning+prepration.html#data-transformation",
    "title": "Data Cleaning",
    "section": "Data Transformation",
    "text": "Data Transformation\n\nRemoving duplicates\nTransforming data using function or Mapping\nReplacing values\nRenaming Axis Indexes\nDiscretization and Binning\nDetecting and Filtering Outliers\nPermutation and Random Sampling\nComputing Indicator/Dummy variables"
  },
  {
    "objectID": "posts/python/data_cleaning+prepration.html#extension-data-types",
    "href": "posts/python/data_cleaning+prepration.html#extension-data-types",
    "title": "Data Cleaning",
    "section": "Extension data types",
    "text": "Extension data types"
  },
  {
    "objectID": "posts/python/data_cleaning+prepration.html#string-manipulation",
    "href": "posts/python/data_cleaning+prepration.html#string-manipulation",
    "title": "Data Cleaning",
    "section": "String Manipulation",
    "text": "String Manipulation\n\nRegular expressions\nString functions in Pandas"
  },
  {
    "objectID": "posts/python/data_cleaning+prepration.html#categorical-data",
    "href": "posts/python/data_cleaning+prepration.html#categorical-data",
    "title": "Data Cleaning",
    "section": "Categorical Data",
    "text": "Categorical Data\n\nBackground\ntypes\ncomputations with categoricals\nBetter performance with categoricals\nCategorical methods\nCreating dummy variables for modeling\n\n\nimport pandas as pd\nimport numpy as np\n\n\nfloat_data = pd.Series([1.2, -3.5, np.nan, 0])\nfloat_data\n\n0    1.2\n1   -3.5\n2    NaN\n3    0.0\ndtype: float64\n\n\n\npandas.isna - other methds\n\n# checking for nan values with booleans\nfloat_data.isna()\n\n0    False\n1    False\n2     True\n3    False\ndtype: bool\n\n\n\n### filtering out missing data\nfloat_data.dropna()\n\n0    1.2\n1   -3.5\n3    0.0\ndtype: float64\n\n\n\n### or with notna()\nfloat_data[float_data.notna()]\n\n0    1.2\n1   -3.5\n3    0.0\ndtype: float64\n\n\n\ndata = pd.DataFrame([[1., 6.5, 3., 4], \n                     [1., np.nan, np.nan, 4], \n                     [3, 4, 22, np.nan],\n                     [np.nan, 434, 33, 1]])\n\ndata\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n1.0\n6.5\n3.0\n4.0\n\n\n1\n1.0\nNaN\nNaN\n4.0\n\n\n2\n3.0\n4.0\n22.0\nNaN\n\n\n3\nNaN\n434.0\n33.0\n1.0\n\n\n\n\n\n\n\n\ndata.dropna()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n1.0\n6.5\n3.0\n4.0\n\n\n\n\n\n\n\n\n# how = 'all' will drop all rows taht are all NA\ndata.dropna(how='all')\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n1.0\n6.5\n3.0\n4.0\n\n\n1\n1.0\nNaN\nNaN\n4.0\n\n\n2\n3.0\n4.0\n22.0\nNaN\n\n\n3\nNaN\n434.0\n33.0\n1.0\n\n\n\n\n\n\n\n\n# dropping columns by how= all\n\ndata[4] = np.nan\ndata\n \n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\n0\n1.0\n6.5\n3.0\n4.0\nNaN\n\n\n1\n1.0\nNaN\nNaN\n4.0\nNaN\n\n\n2\n3.0\n4.0\n22.0\nNaN\nNaN\n\n\n3\nNaN\n434.0\n33.0\n1.0\nNaN\n\n\n\n\n\n\n\n\ndata.dropna(axis = \"columns\", how=\"all\")\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n1.0\n6.5\n3.0\n4.0\n\n\n1\n1.0\nNaN\nNaN\n4.0\n\n\n2\n3.0\n4.0\n22.0\nNaN\n\n\n3\nNaN\n434.0\n33.0\n1.0\n\n\n\n\n\n\n\n\ndf = pd.DataFrame(np.random.standard_normal((7, 3)))\n\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n-1.716945\n1.430864\n0.198477\n\n\n1\n2.815565\n-0.425012\n-1.749359\n\n\n2\n-0.073905\n-0.618281\n0.826025\n\n\n3\n1.122968\n2.883936\n0.932057\n\n\n4\n-0.037637\n1.100561\n-0.328430\n\n\n5\n-0.077328\n-1.032715\n0.157982\n\n\n6\n0.363370\n1.845914\n-0.172841\n\n\n\n\n\n\n\n\n# make null- first four rows of second column\n\ndf.iloc[:4, 1] = np.nan\n\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n-1.716945\nNaN\n0.198477\n\n\n1\n2.815565\nNaN\n-1.749359\n\n\n2\n-0.073905\nNaN\n0.826025\n\n\n3\n1.122968\nNaN\n0.932057\n\n\n4\n-0.037637\n1.100561\n-0.328430\n\n\n5\n-0.077328\n-1.032715\n0.157982\n\n\n6\n0.363370\n1.845914\n-0.172841\n\n\n\n\n\n\n\n\ndf.dropna()\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n4\n-0.037637\n1.100561\n-0.328430\n\n\n5\n-0.077328\n-1.032715\n0.157982\n\n\n6\n0.363370\n1.845914\n-0.172841\n\n\n\n\n\n\n\n\n# to learn more about this method\nhelp(df.dropna())\n\n\n#ça marche pas\n\ndf.dropna(thresh=2)\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n-1.716945\nNaN\n0.198477\n\n\n1\n2.815565\nNaN\n-1.749359\n\n\n2\n-0.073905\nNaN\n0.826025\n\n\n3\n1.122968\nNaN\n0.932057\n\n\n4\n-0.037637\n1.100561\n-0.328430\n\n\n5\n-0.077328\n-1.032715\n0.157982\n\n\n6\n0.363370\n1.845914\n-0.172841"
  },
  {
    "objectID": "posts/python/data_cleaning+prepration.html#filling-in-missing-data",
    "href": "posts/python/data_cleaning+prepration.html#filling-in-missing-data",
    "title": "Data Cleaning",
    "section": "filling in missing data",
    "text": "filling in missing data\n\ndf.fillna(0)\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n-1.716945\n0.000000\n0.198477\n\n\n1\n2.815565\n0.000000\n-1.749359\n\n\n2\n-0.073905\n0.000000\n0.826025\n\n\n3\n1.122968\n0.000000\n0.932057\n\n\n4\n-0.037637\n1.100561\n-0.328430\n\n\n5\n-0.077328\n-1.032715\n0.157982\n\n\n6\n0.363370\n1.845914\n-0.172841\n\n\n\n\n\n\n\n\n# using different fillvalue for each column\n\ndf.fillna({1:0.5})\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n-1.716945\n0.500000\n0.198477\n\n\n1\n2.815565\n0.500000\n-1.749359\n\n\n2\n-0.073905\n0.500000\n0.826025\n\n\n3\n1.122968\n0.500000\n0.932057\n\n\n4\n-0.037637\n1.100561\n-0.328430\n\n\n5\n-0.077328\n-1.032715\n0.157982\n\n\n6\n0.363370\n1.845914\n-0.172841\n\n\n\n\n\n\n\n\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n-1.716945\nNaN\n0.198477\n\n\n1\n2.815565\nNaN\n-1.749359\n\n\n2\n-0.073905\nNaN\n0.826025\n\n\n3\n1.122968\nNaN\n0.932057\n\n\n4\n-0.037637\n1.100561\n-0.328430\n\n\n5\n-0.077328\n-1.032715\n0.157982\n\n\n6\n0.363370\n1.845914\n-0.172841\n\n\n\n\n\n\n\n\n# same interpolation using fillna()\n\ndf.fillna(method = \"ffill\").astype(float)\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n-1.716945\nNaN\n0.198477\n\n\n1\n2.815565\nNaN\n-1.749359\n\n\n2\n-0.073905\nNaN\n0.826025\n\n\n3\n1.122968\nNaN\n0.932057\n\n\n4\n-0.037637\n1.100561\n-0.328430\n\n\n5\n-0.077328\n-1.032715\n0.157982\n\n\n6\n0.363370\n1.845914\n-0.172841\n\n\n\n\n\n\n\n\n! python --version\n\nPython 3.9.13\n\n\n\nhelp(df)\n\n\nboxplot = df.boxplot()\n\nimport matplotlib as mlt\n\n%mtl.inline.boxplot.show()\n\n\nhelp(pd.Series)\n\nHelp on class Series in module pandas.core.series:\n\nclass Series(pandas.core.base.IndexOpsMixin, pandas.core.generic.NDFrame)\n |  Series(data=None, index=None, dtype: 'Dtype | None' = None, name=None, copy: 'bool | None' = None, fastpath: 'bool' = False) -&gt; 'None'\n |  \n |  One-dimensional ndarray with axis labels (including time series).\n |  \n |  Labels need not be unique but must be a hashable type. The object\n |  supports both integer- and label-based indexing and provides a host of\n |  methods for performing operations involving the index. Statistical\n |  methods from ndarray have been overridden to automatically exclude\n |  missing data (currently represented as NaN).\n |  \n |  Operations between Series (+, -, /, \\*, \\*\\*) align values based on their\n |  associated index values-- they need not be the same length. The result\n |  index will be the sorted union of the two indexes.\n |  \n |  Parameters\n |  ----------\n |  data : array-like, Iterable, dict, or scalar value\n |      Contains data stored in Series. If data is a dict, argument order is\n |      maintained.\n |  index : array-like or Index (1d)\n |      Values must be hashable and have the same length as `data`.\n |      Non-unique index values are allowed. Will default to\n |      RangeIndex (0, 1, 2, ..., n) if not provided. If data is dict-like\n |      and index is None, then the keys in the data are used as the index. If the\n |      index is not None, the resulting Series is reindexed with the index values.\n |  dtype : str, numpy.dtype, or ExtensionDtype, optional\n |      Data type for the output Series. If not specified, this will be\n |      inferred from `data`.\n |      See the :ref:`user guide &lt;basics.dtypes&gt;` for more usages.\n |  name : Hashable, default None\n |      The name to give to the Series.\n |  copy : bool, default False\n |      Copy input data. Only affects Series or 1d ndarray input. See examples.\n |  \n |  Notes\n |  -----\n |  Please reference the :ref:`User Guide &lt;basics.series&gt;` for more information.\n |  \n |  Examples\n |  --------\n |  Constructing Series from a dictionary with an Index specified\n |  \n |  &gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 3}\n |  &gt;&gt;&gt; ser = pd.Series(data=d, index=['a', 'b', 'c'])\n |  &gt;&gt;&gt; ser\n |  a   1\n |  b   2\n |  c   3\n |  dtype: int64\n |  \n |  The keys of the dictionary match with the Index values, hence the Index\n |  values have no effect.\n |  \n |  &gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 3}\n |  &gt;&gt;&gt; ser = pd.Series(data=d, index=['x', 'y', 'z'])\n |  &gt;&gt;&gt; ser\n |  x   NaN\n |  y   NaN\n |  z   NaN\n |  dtype: float64\n |  \n |  Note that the Index is first build with the keys from the dictionary.\n |  After this the Series is reindexed with the given Index values, hence we\n |  get all NaN as a result.\n |  \n |  Constructing Series from a list with `copy=False`.\n |  \n |  &gt;&gt;&gt; r = [1, 2]\n |  &gt;&gt;&gt; ser = pd.Series(r, copy=False)\n |  &gt;&gt;&gt; ser.iloc[0] = 999\n |  &gt;&gt;&gt; r\n |  [1, 2]\n |  &gt;&gt;&gt; ser\n |  0    999\n |  1      2\n |  dtype: int64\n |  \n |  Due to input data type the Series has a `copy` of\n |  the original data even though `copy=False`, so\n |  the data is unchanged.\n |  \n |  Constructing Series from a 1d ndarray with `copy=False`.\n |  \n |  &gt;&gt;&gt; r = np.array([1, 2])\n |  &gt;&gt;&gt; ser = pd.Series(r, copy=False)\n |  &gt;&gt;&gt; ser.iloc[0] = 999\n |  &gt;&gt;&gt; r\n |  array([999,   2])\n |  &gt;&gt;&gt; ser\n |  0    999\n |  1      2\n |  dtype: int64\n |  \n |  Due to input data type the Series has a `view` on\n |  the original data, so\n |  the data is changed as well.\n |  \n |  Method resolution order:\n |      Series\n |      pandas.core.base.IndexOpsMixin\n |      pandas.core.arraylike.OpsMixin\n |      pandas.core.generic.NDFrame\n |      pandas.core.base.PandasObject\n |      pandas.core.accessor.DirNamesMixin\n |      pandas.core.indexing.IndexingMixin\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __array__(self, dtype: 'npt.DTypeLike | None' = None) -&gt; 'np.ndarray'\n |      Return the values as a NumPy array.\n |      \n |      Users should not call this directly. Rather, it is invoked by\n |      :func:`numpy.array` and :func:`numpy.asarray`.\n |      \n |      Parameters\n |      ----------\n |      dtype : str or numpy.dtype, optional\n |          The dtype to use for the resulting NumPy array. By default,\n |          the dtype is inferred from the data.\n |      \n |      Returns\n |      -------\n |      numpy.ndarray\n |          The values in the series converted to a :class:`numpy.ndarray`\n |          with the specified `dtype`.\n |      \n |      See Also\n |      --------\n |      array : Create a new array from data.\n |      Series.array : Zero-copy view to the array backing the Series.\n |      Series.to_numpy : Series method for similar behavior.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; ser = pd.Series([1, 2, 3])\n |      &gt;&gt;&gt; np.asarray(ser)\n |      array([1, 2, 3])\n |      \n |      For timezone-aware data, the timezones may be retained with\n |      ``dtype='object'``\n |      \n |      &gt;&gt;&gt; tzser = pd.Series(pd.date_range('2000', periods=2, tz=\"CET\"))\n |      &gt;&gt;&gt; np.asarray(tzser, dtype=\"object\")\n |      array([Timestamp('2000-01-01 00:00:00+0100', tz='CET'),\n |             Timestamp('2000-01-02 00:00:00+0100', tz='CET')],\n |            dtype=object)\n |      \n |      Or the values may be localized to UTC and the tzinfo discarded with\n |      ``dtype='datetime64[ns]'``\n |      \n |      &gt;&gt;&gt; np.asarray(tzser, dtype=\"datetime64[ns]\")  # doctest: +ELLIPSIS\n |      array(['1999-12-31T23:00:00.000000000', ...],\n |            dtype='datetime64[ns]')\n |  \n |  __float__(self)\n |  \n |  __getitem__(self, key)\n |  \n |  __init__(self, data=None, index=None, dtype: 'Dtype | None' = None, name=None, copy: 'bool | None' = None, fastpath: 'bool' = False) -&gt; 'None'\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  __int__(self)\n |  \n |  __len__(self) -&gt; 'int'\n |      Return the length of the Series.\n |  \n |  __matmul__(self, other)\n |      Matrix multiplication using binary `@` operator in Python&gt;=3.5.\n |  \n |  __repr__(self) -&gt; 'str'\n |      Return a string representation for a particular Series.\n |  \n |  __rmatmul__(self, other)\n |      Matrix multiplication using binary `@` operator in Python&gt;=3.5.\n |  \n |  __setitem__(self, key, value) -&gt; 'None'\n |  \n |  add(self, other, level=None, fill_value=None, axis: 'Axis' = 0)\n |      Return Addition of series and other, element-wise (binary operator `add`).\n |      \n |      Equivalent to ``series + other``, but with support to substitute a fill_value for\n |      missing data in either one of the inputs.\n |      \n |      Parameters\n |      ----------\n |      other : Series or scalar value\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level.\n |      fill_value : None or float value, default None (NaN)\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful Series alignment, with this value before computation.\n |          If data in both corresponding Series locations is missing\n |          the result of filling (at that location) will be missing.\n |      axis : {0 or 'index'}\n |          Unused. Parameter needed for compatibility with DataFrame.\n |      \n |      Returns\n |      -------\n |      Series\n |          The result of the operation.\n |      \n |      See Also\n |      --------\n |      Series.radd : Reverse of the Addition operator, see\n |          `Python documentation\n |          &lt;https://docs.python.org/3/reference/datamodel.html#emulating-numeric-types&gt;`_\n |          for more details.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; a = pd.Series([1, 1, 1, np.nan], index=['a', 'b', 'c', 'd'])\n |      &gt;&gt;&gt; a\n |      a    1.0\n |      b    1.0\n |      c    1.0\n |      d    NaN\n |      dtype: float64\n |      &gt;&gt;&gt; b = pd.Series([1, np.nan, 1, np.nan], index=['a', 'b', 'd', 'e'])\n |      &gt;&gt;&gt; b\n |      a    1.0\n |      b    NaN\n |      d    1.0\n |      e    NaN\n |      dtype: float64\n |      &gt;&gt;&gt; a.add(b, fill_value=0)\n |      a    2.0\n |      b    1.0\n |      c    1.0\n |      d    1.0\n |      e    NaN\n |      dtype: float64\n |  \n |  agg = aggregate(self, func=None, axis: 'Axis' = 0, *args, **kwargs)\n |  \n |  aggregate(self, func=None, axis: 'Axis' = 0, *args, **kwargs)\n |      Aggregate using one or more operations over the specified axis.\n |      \n |      Parameters\n |      ----------\n |      func : function, str, list or dict\n |          Function to use for aggregating the data. If a function, must either\n |          work when passed a Series or when passed to Series.apply.\n |      \n |          Accepted combinations are:\n |      \n |          - function\n |          - string function name\n |          - list of functions and/or function names, e.g. ``[np.sum, 'mean']``\n |          - dict of axis labels -&gt; functions, function names or list of such.\n |      axis : {0 or 'index'}\n |              Unused. Parameter needed for compatibility with DataFrame.\n |      *args\n |          Positional arguments to pass to `func`.\n |      **kwargs\n |          Keyword arguments to pass to `func`.\n |      \n |      Returns\n |      -------\n |      scalar, Series or DataFrame\n |      \n |          The return can be:\n |      \n |          * scalar : when Series.agg is called with single function\n |          * Series : when DataFrame.agg is called with a single function\n |          * DataFrame : when DataFrame.agg is called with several functions\n |      \n |          Return scalar, Series or DataFrame.\n |      \n |      See Also\n |      --------\n |      Series.apply : Invoke function on a Series.\n |      Series.transform : Transform function producing a Series with like indexes.\n |      \n |      Notes\n |      -----\n |      `agg` is an alias for `aggregate`. Use the alias.\n |      \n |      Functions that mutate the passed object can produce unexpected\n |      behavior or errors and are not supported. See :ref:`gotchas.udf-mutation`\n |      for more details.\n |      \n |      A passed user-defined-function will be passed a Series for evaluation.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; s = pd.Series([1, 2, 3, 4])\n |      &gt;&gt;&gt; s\n |      0    1\n |      1    2\n |      2    3\n |      3    4\n |      dtype: int64\n |      \n |      &gt;&gt;&gt; s.agg('min')\n |      1\n |      \n |      &gt;&gt;&gt; s.agg(['min', 'max'])\n |      min   1\n |      max   4\n |      dtype: int64\n |  \n |  align(self, other: 'Series', join: 'AlignJoin' = 'outer', axis: 'Axis | None' = None, level: 'Level' = None, copy: 'bool | None' = None, fill_value: 'Hashable' = None, method: 'FillnaOptions | None' = None, limit: 'int | None' = None, fill_axis: 'Axis' = 0, broadcast_axis: 'Axis | None' = None) -&gt; 'Series'\n |      Align two objects on their axes with the specified join method.\n |      \n |      Join method is specified for each axis Index.\n |      \n |      Parameters\n |      ----------\n |      other : DataFrame or Series\n |      join : {'outer', 'inner', 'left', 'right'}, default 'outer'\n |      axis : allowed axis of the other object, default None\n |          Align on index (0), columns (1), or both (None).\n |      level : int or level name, default None\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level.\n |      copy : bool, default True\n |          Always returns new objects. If copy=False and no reindexing is\n |          required then original objects are returned.\n |      fill_value : scalar, default np.NaN\n |          Value to use for missing values. Defaults to NaN, but can be any\n |          \"compatible\" value.\n |      method : {'backfill', 'bfill', 'pad', 'ffill', None}, default None\n |          Method to use for filling holes in reindexed Series:\n |      \n |          - pad / ffill: propagate last valid observation forward to next valid.\n |          - backfill / bfill: use NEXT valid observation to fill gap.\n |      \n |      limit : int, default None\n |          If method is specified, this is the maximum number of consecutive\n |          NaN values to forward/backward fill. In other words, if there is\n |          a gap with more than this number of consecutive NaNs, it will only\n |          be partially filled. If method is not specified, this is the\n |          maximum number of entries along the entire axis where NaNs will be\n |          filled. Must be greater than 0 if not None.\n |      fill_axis : {0 or 'index'}, default 0\n |          Filling axis, method and limit.\n |      broadcast_axis : {0 or 'index'}, default None\n |          Broadcast values along this axis, if aligning two objects of\n |          different dimensions.\n |      \n |      Returns\n |      -------\n |      tuple of (Series, type of other)\n |          Aligned objects.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; df = pd.DataFrame(\n |      ...     [[1, 2, 3, 4], [6, 7, 8, 9]], columns=[\"D\", \"B\", \"E\", \"A\"], index=[1, 2]\n |      ... )\n |      &gt;&gt;&gt; other = pd.DataFrame(\n |      ...     [[10, 20, 30, 40], [60, 70, 80, 90], [600, 700, 800, 900]],\n |      ...     columns=[\"A\", \"B\", \"C\", \"D\"],\n |      ...     index=[2, 3, 4],\n |      ... )\n |      &gt;&gt;&gt; df\n |         D  B  E  A\n |      1  1  2  3  4\n |      2  6  7  8  9\n |      &gt;&gt;&gt; other\n |          A    B    C    D\n |      2   10   20   30   40\n |      3   60   70   80   90\n |      4  600  700  800  900\n |      \n |      Align on columns:\n |      \n |      &gt;&gt;&gt; left, right = df.align(other, join=\"outer\", axis=1)\n |      &gt;&gt;&gt; left\n |         A  B   C  D  E\n |      1  4  2 NaN  1  3\n |      2  9  7 NaN  6  8\n |      &gt;&gt;&gt; right\n |          A    B    C    D   E\n |      2   10   20   30   40 NaN\n |      3   60   70   80   90 NaN\n |      4  600  700  800  900 NaN\n |      \n |      We can also align on the index:\n |      \n |      &gt;&gt;&gt; left, right = df.align(other, join=\"outer\", axis=0)\n |      &gt;&gt;&gt; left\n |          D    B    E    A\n |      1  1.0  2.0  3.0  4.0\n |      2  6.0  7.0  8.0  9.0\n |      3  NaN  NaN  NaN  NaN\n |      4  NaN  NaN  NaN  NaN\n |      &gt;&gt;&gt; right\n |          A      B      C      D\n |      1    NaN    NaN    NaN    NaN\n |      2   10.0   20.0   30.0   40.0\n |      3   60.0   70.0   80.0   90.0\n |      4  600.0  700.0  800.0  900.0\n |      \n |      Finally, the default `axis=None` will align on both index and columns:\n |      \n |      &gt;&gt;&gt; left, right = df.align(other, join=\"outer\", axis=None)\n |      &gt;&gt;&gt; left\n |           A    B   C    D    E\n |      1  4.0  2.0 NaN  1.0  3.0\n |      2  9.0  7.0 NaN  6.0  8.0\n |      3  NaN  NaN NaN  NaN  NaN\n |      4  NaN  NaN NaN  NaN  NaN\n |      &gt;&gt;&gt; right\n |             A      B      C      D   E\n |      1    NaN    NaN    NaN    NaN NaN\n |      2   10.0   20.0   30.0   40.0 NaN\n |      3   60.0   70.0   80.0   90.0 NaN\n |      4  600.0  700.0  800.0  900.0 NaN\n |  \n |  all(self, axis: 'Axis' = 0, bool_only=None, skipna: 'bool_t' = True, **kwargs)\n |      Return whether all elements are True, potentially over an axis.\n |      \n |      Returns True unless there at least one element within a series or\n |      along a Dataframe axis that is False or equivalent (e.g. zero or\n |      empty).\n |      \n |      Parameters\n |      ----------\n |      axis : {0 or 'index', 1 or 'columns', None}, default 0\n |          Indicate which axis or axes should be reduced. For `Series` this parameter\n |          is unused and defaults to 0.\n |      \n |          * 0 / 'index' : reduce the index, return a Series whose index is the\n |            original column labels.\n |          * 1 / 'columns' : reduce the columns, return a Series whose index is the\n |            original index.\n |          * None : reduce all axes, return a scalar.\n |      \n |      bool_only : bool, default None\n |          Include only boolean columns. If None, will attempt to use everything,\n |          then use only boolean data. Not implemented for Series.\n |      skipna : bool, default True\n |          Exclude NA/null values. If the entire row/column is NA and skipna is\n |          True, then the result will be True, as for an empty row/column.\n |          If skipna is False, then NA are treated as True, because these are not\n |          equal to zero.\n |      **kwargs : any, default None\n |          Additional keywords have no effect but might be accepted for\n |          compatibility with NumPy.\n |      \n |      Returns\n |      -------\n |      scalar or Series\n |          If level is specified, then, Series is returned; otherwise, scalar\n |          is returned.\n |      \n |      See Also\n |      --------\n |      Series.all : Return True if all elements are True.\n |      DataFrame.any : Return True if one (or more) elements are True.\n |      \n |      Examples\n |      --------\n |      **Series**\n |      \n |      &gt;&gt;&gt; pd.Series([True, True]).all()\n |      True\n |      &gt;&gt;&gt; pd.Series([True, False]).all()\n |      False\n |      &gt;&gt;&gt; pd.Series([], dtype=\"float64\").all()\n |      True\n |      &gt;&gt;&gt; pd.Series([np.nan]).all()\n |      True\n |      &gt;&gt;&gt; pd.Series([np.nan]).all(skipna=False)\n |      True\n |      \n |      **DataFrames**\n |      \n |      Create a dataframe from a dictionary.\n |      \n |      &gt;&gt;&gt; df = pd.DataFrame({'col1': [True, True], 'col2': [True, False]})\n |      &gt;&gt;&gt; df\n |         col1   col2\n |      0  True   True\n |      1  True  False\n |      \n |      Default behaviour checks if values in each column all return True.\n |      \n |      &gt;&gt;&gt; df.all()\n |      col1     True\n |      col2    False\n |      dtype: bool\n |      \n |      Specify ``axis='columns'`` to check if values in each row all return True.\n |      \n |      &gt;&gt;&gt; df.all(axis='columns')\n |      0     True\n |      1    False\n |      dtype: bool\n |      \n |      Or ``axis=None`` for whether every value is True.\n |      \n |      &gt;&gt;&gt; df.all(axis=None)\n |      False\n |  \n |  any(self, *, axis: 'Axis' = 0, bool_only=None, skipna: 'bool_t' = True, **kwargs)\n |      Return whether any element is True, potentially over an axis.\n |      \n |      Returns False unless there is at least one element within a series or\n |      along a Dataframe axis that is True or equivalent (e.g. non-zero or\n |      non-empty).\n |      \n |      Parameters\n |      ----------\n |      axis : {0 or 'index', 1 or 'columns', None}, default 0\n |          Indicate which axis or axes should be reduced. For `Series` this parameter\n |          is unused and defaults to 0.\n |      \n |          * 0 / 'index' : reduce the index, return a Series whose index is the\n |            original column labels.\n |          * 1 / 'columns' : reduce the columns, return a Series whose index is the\n |            original index.\n |          * None : reduce all axes, return a scalar.\n |      \n |      bool_only : bool, default None\n |          Include only boolean columns. If None, will attempt to use everything,\n |          then use only boolean data. Not implemented for Series.\n |      skipna : bool, default True\n |          Exclude NA/null values. If the entire row/column is NA and skipna is\n |          True, then the result will be False, as for an empty row/column.\n |          If skipna is False, then NA are treated as True, because these are not\n |          equal to zero.\n |      **kwargs : any, default None\n |          Additional keywords have no effect but might be accepted for\n |          compatibility with NumPy.\n |      \n |      Returns\n |      -------\n |      scalar or Series\n |          If level is specified, then, Series is returned; otherwise, scalar\n |          is returned.\n |      \n |      See Also\n |      --------\n |      numpy.any : Numpy version of this method.\n |      Series.any : Return whether any element is True.\n |      Series.all : Return whether all elements are True.\n |      DataFrame.any : Return whether any element is True over requested axis.\n |      DataFrame.all : Return whether all elements are True over requested axis.\n |      \n |      Examples\n |      --------\n |      **Series**\n |      \n |      For Series input, the output is a scalar indicating whether any element\n |      is True.\n |      \n |      &gt;&gt;&gt; pd.Series([False, False]).any()\n |      False\n |      &gt;&gt;&gt; pd.Series([True, False]).any()\n |      True\n |      &gt;&gt;&gt; pd.Series([], dtype=\"float64\").any()\n |      False\n |      &gt;&gt;&gt; pd.Series([np.nan]).any()\n |      False\n |      &gt;&gt;&gt; pd.Series([np.nan]).any(skipna=False)\n |      True\n |      \n |      **DataFrame**\n |      \n |      Whether each column contains at least one True element (the default).\n |      \n |      &gt;&gt;&gt; df = pd.DataFrame({\"A\": [1, 2], \"B\": [0, 2], \"C\": [0, 0]})\n |      &gt;&gt;&gt; df\n |         A  B  C\n |      0  1  0  0\n |      1  2  2  0\n |      \n |      &gt;&gt;&gt; df.any()\n |      A     True\n |      B     True\n |      C    False\n |      dtype: bool\n |      \n |      Aggregating over the columns.\n |      \n |      &gt;&gt;&gt; df = pd.DataFrame({\"A\": [True, False], \"B\": [1, 2]})\n |      &gt;&gt;&gt; df\n |             A  B\n |      0   True  1\n |      1  False  2\n |      \n |      &gt;&gt;&gt; df.any(axis='columns')\n |      0    True\n |      1    True\n |      dtype: bool\n |      \n |      &gt;&gt;&gt; df = pd.DataFrame({\"A\": [True, False], \"B\": [1, 0]})\n |      &gt;&gt;&gt; df\n |             A  B\n |      0   True  1\n |      1  False  0\n |      \n |      &gt;&gt;&gt; df.any(axis='columns')\n |      0    True\n |      1    False\n |      dtype: bool\n |      \n |      Aggregating over the entire DataFrame with ``axis=None``.\n |      \n |      &gt;&gt;&gt; df.any(axis=None)\n |      True\n |      \n |      `any` for an empty DataFrame is an empty Series.\n |      \n |      &gt;&gt;&gt; pd.DataFrame([]).any()\n |      Series([], dtype: bool)\n |  \n |  apply(self, func: 'AggFuncType', convert_dtype: 'bool' = True, args: 'tuple[Any, ...]' = (), **kwargs) -&gt; 'DataFrame | Series'\n |      Invoke function on values of Series.\n |      \n |      Can be ufunc (a NumPy function that applies to the entire Series)\n |      or a Python function that only works on single values.\n |      \n |      Parameters\n |      ----------\n |      func : function\n |          Python function or NumPy ufunc to apply.\n |      convert_dtype : bool, default True\n |          Try to find better dtype for elementwise function results. If\n |          False, leave as dtype=object. Note that the dtype is always\n |          preserved for some extension array dtypes, such as Categorical.\n |      args : tuple\n |          Positional arguments passed to func after the series value.\n |      **kwargs\n |          Additional keyword arguments passed to func.\n |      \n |      Returns\n |      -------\n |      Series or DataFrame\n |          If func returns a Series object the result will be a DataFrame.\n |      \n |      See Also\n |      --------\n |      Series.map: For element-wise operations.\n |      Series.agg: Only perform aggregating type operations.\n |      Series.transform: Only perform transforming type operations.\n |      \n |      Notes\n |      -----\n |      Functions that mutate the passed object can produce unexpected\n |      behavior or errors and are not supported. See :ref:`gotchas.udf-mutation`\n |      for more details.\n |      \n |      Examples\n |      --------\n |      Create a series with typical summer temperatures for each city.\n |      \n |      &gt;&gt;&gt; s = pd.Series([20, 21, 12],\n |      ...               index=['London', 'New York', 'Helsinki'])\n |      &gt;&gt;&gt; s\n |      London      20\n |      New York    21\n |      Helsinki    12\n |      dtype: int64\n |      \n |      Square the values by defining a function and passing it as an\n |      argument to ``apply()``.\n |      \n |      &gt;&gt;&gt; def square(x):\n |      ...     return x ** 2\n |      &gt;&gt;&gt; s.apply(square)\n |      London      400\n |      New York    441\n |      Helsinki    144\n |      dtype: int64\n |      \n |      Square the values by passing an anonymous function as an\n |      argument to ``apply()``.\n |      \n |      &gt;&gt;&gt; s.apply(lambda x: x ** 2)\n |      London      400\n |      New York    441\n |      Helsinki    144\n |      dtype: int64\n |      \n |      Define a custom function that needs additional positional\n |      arguments and pass these additional arguments using the\n |      ``args`` keyword.\n |      \n |      &gt;&gt;&gt; def subtract_custom_value(x, custom_value):\n |      ...     return x - custom_value\n |      \n |      &gt;&gt;&gt; s.apply(subtract_custom_value, args=(5,))\n |      London      15\n |      New York    16\n |      Helsinki     7\n |      dtype: int64\n |      \n |      Define a custom function that takes keyword arguments\n |      and pass these arguments to ``apply``.\n |      \n |      &gt;&gt;&gt; def add_custom_values(x, **kwargs):\n |      ...     for month in kwargs:\n |      ...         x += kwargs[month]\n |      ...     return x\n |      \n |      &gt;&gt;&gt; s.apply(add_custom_values, june=30, july=20, august=25)\n |      London      95\n |      New York    96\n |      Helsinki    87\n |      dtype: int64\n |      \n |      Use a function from the Numpy library.\n |      \n |      &gt;&gt;&gt; s.apply(np.log)\n |      London      2.995732\n |      New York    3.044522\n |      Helsinki    2.484907\n |      dtype: float64\n |  \n |  argsort(self, axis: 'Axis' = 0, kind: 'SortKind' = 'quicksort', order: 'None' = None) -&gt; 'Series'\n |      Return the integer indices that would sort the Series values.\n |      \n |      Override ndarray.argsort. Argsorts the value, omitting NA/null values,\n |      and places the result in the same locations as the non-NA values.\n |      \n |      Parameters\n |      ----------\n |      axis : {0 or 'index'}\n |          Unused. Parameter needed for compatibility with DataFrame.\n |      kind : {'mergesort', 'quicksort', 'heapsort', 'stable'}, default 'quicksort'\n |          Choice of sorting algorithm. See :func:`numpy.sort` for more\n |          information. 'mergesort' and 'stable' are the only stable algorithms.\n |      order : None\n |          Has no effect but is accepted for compatibility with numpy.\n |      \n |      Returns\n |      -------\n |      Series[np.intp]\n |          Positions of values within the sort order with -1 indicating\n |          nan values.\n |      \n |      See Also\n |      --------\n |      numpy.ndarray.argsort : Returns the indices that would sort this array.\n |  \n |  asfreq(self, freq: 'Frequency', method: 'FillnaOptions | None' = None, how: 'str | None' = None, normalize: 'bool' = False, fill_value: 'Hashable' = None) -&gt; 'Series'\n |      Convert time series to specified frequency.\n |      \n |      Returns the original data conformed to a new index with the specified\n |      frequency.\n |      \n |      If the index of this Series is a :class:`~pandas.PeriodIndex`, the new index\n |      is the result of transforming the original index with\n |      :meth:`PeriodIndex.asfreq &lt;pandas.PeriodIndex.asfreq&gt;` (so the original index\n |      will map one-to-one to the new index).\n |      \n |      Otherwise, the new index will be equivalent to ``pd.date_range(start, end,\n |      freq=freq)`` where ``start`` and ``end`` are, respectively, the first and\n |      last entries in the original index (see :func:`pandas.date_range`). The\n |      values corresponding to any timesteps in the new index which were not present\n |      in the original index will be null (``NaN``), unless a method for filling\n |      such unknowns is provided (see the ``method`` parameter below).\n |      \n |      The :meth:`resample` method is more appropriate if an operation on each group of\n |      timesteps (such as an aggregate) is necessary to represent the data at the new\n |      frequency.\n |      \n |      Parameters\n |      ----------\n |      freq : DateOffset or str\n |          Frequency DateOffset or string.\n |      method : {'backfill'/'bfill', 'pad'/'ffill'}, default None\n |          Method to use for filling holes in reindexed Series (note this\n |          does not fill NaNs that already were present):\n |      \n |          * 'pad' / 'ffill': propagate last valid observation forward to next\n |            valid\n |          * 'backfill' / 'bfill': use NEXT valid observation to fill.\n |      how : {'start', 'end'}, default end\n |          For PeriodIndex only (see PeriodIndex.asfreq).\n |      normalize : bool, default False\n |          Whether to reset output index to midnight.\n |      fill_value : scalar, optional\n |          Value to use for missing values, applied during upsampling (note\n |          this does not fill NaNs that already were present).\n |      \n |      Returns\n |      -------\n |      Series\n |          Series object reindexed to the specified frequency.\n |      \n |      See Also\n |      --------\n |      reindex : Conform DataFrame to new index with optional filling logic.\n |      \n |      Notes\n |      -----\n |      To learn more about the frequency strings, please see `this link\n |      &lt;https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases&gt;`__.\n |      \n |      Examples\n |      --------\n |      Start by creating a series with 4 one minute timestamps.\n |      \n |      &gt;&gt;&gt; index = pd.date_range('1/1/2000', periods=4, freq='T')\n |      &gt;&gt;&gt; series = pd.Series([0.0, None, 2.0, 3.0], index=index)\n |      &gt;&gt;&gt; df = pd.DataFrame({'s': series})\n |      &gt;&gt;&gt; df\n |                             s\n |      2000-01-01 00:00:00    0.0\n |      2000-01-01 00:01:00    NaN\n |      2000-01-01 00:02:00    2.0\n |      2000-01-01 00:03:00    3.0\n |      \n |      Upsample the series into 30 second bins.\n |      \n |      &gt;&gt;&gt; df.asfreq(freq='30S')\n |                             s\n |      2000-01-01 00:00:00    0.0\n |      2000-01-01 00:00:30    NaN\n |      2000-01-01 00:01:00    NaN\n |      2000-01-01 00:01:30    NaN\n |      2000-01-01 00:02:00    2.0\n |      2000-01-01 00:02:30    NaN\n |      2000-01-01 00:03:00    3.0\n |      \n |      Upsample again, providing a ``fill value``.\n |      \n |      &gt;&gt;&gt; df.asfreq(freq='30S', fill_value=9.0)\n |                             s\n |      2000-01-01 00:00:00    0.0\n |      2000-01-01 00:00:30    9.0\n |      2000-01-01 00:01:00    NaN\n |      2000-01-01 00:01:30    9.0\n |      2000-01-01 00:02:00    2.0\n |      2000-01-01 00:02:30    9.0\n |      2000-01-01 00:03:00    3.0\n |      \n |      Upsample again, providing a ``method``.\n |      \n |      &gt;&gt;&gt; df.asfreq(freq='30S', method='bfill')\n |                             s\n |      2000-01-01 00:00:00    0.0\n |      2000-01-01 00:00:30    NaN\n |      2000-01-01 00:01:00    NaN\n |      2000-01-01 00:01:30    2.0\n |      2000-01-01 00:02:00    2.0\n |      2000-01-01 00:02:30    3.0\n |      2000-01-01 00:03:00    3.0\n |  \n |  autocorr(self, lag: 'int' = 1) -&gt; 'float'\n |      Compute the lag-N autocorrelation.\n |      \n |      This method computes the Pearson correlation between\n |      the Series and its shifted self.\n |      \n |      Parameters\n |      ----------\n |      lag : int, default 1\n |          Number of lags to apply before performing autocorrelation.\n |      \n |      Returns\n |      -------\n |      float\n |          The Pearson correlation between self and self.shift(lag).\n |      \n |      See Also\n |      --------\n |      Series.corr : Compute the correlation between two Series.\n |      Series.shift : Shift index by desired number of periods.\n |      DataFrame.corr : Compute pairwise correlation of columns.\n |      DataFrame.corrwith : Compute pairwise correlation between rows or\n |          columns of two DataFrame objects.\n |      \n |      Notes\n |      -----\n |      If the Pearson correlation is not well defined return 'NaN'.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; s = pd.Series([0.25, 0.5, 0.2, -0.05])\n |      &gt;&gt;&gt; s.autocorr()  # doctest: +ELLIPSIS\n |      0.10355...\n |      &gt;&gt;&gt; s.autocorr(lag=2)  # doctest: +ELLIPSIS\n |      -0.99999...\n |      \n |      If the Pearson correlation is not well defined, then 'NaN' is returned.\n |      \n |      &gt;&gt;&gt; s = pd.Series([1, 0, 0, 0])\n |      &gt;&gt;&gt; s.autocorr()\n |      nan\n |  \n |  between(self, left, right, inclusive: \"Literal['both', 'neither', 'left', 'right']\" = 'both') -&gt; 'Series'\n |      Return boolean Series equivalent to left &lt;= series &lt;= right.\n |      \n |      This function returns a boolean vector containing `True` wherever the\n |      corresponding Series element is between the boundary values `left` and\n |      `right`. NA values are treated as `False`.\n |      \n |      Parameters\n |      ----------\n |      left : scalar or list-like\n |          Left boundary.\n |      right : scalar or list-like\n |          Right boundary.\n |      inclusive : {\"both\", \"neither\", \"left\", \"right\"}\n |          Include boundaries. Whether to set each bound as closed or open.\n |      \n |          .. versionchanged:: 1.3.0\n |      \n |      Returns\n |      -------\n |      Series\n |          Series representing whether each element is between left and\n |          right (inclusive).\n |      \n |      See Also\n |      --------\n |      Series.gt : Greater than of series and other.\n |      Series.lt : Less than of series and other.\n |      \n |      Notes\n |      -----\n |      This function is equivalent to ``(left &lt;= ser) & (ser &lt;= right)``\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; s = pd.Series([2, 0, 4, 8, np.nan])\n |      \n |      Boundary values are included by default:\n |      \n |      &gt;&gt;&gt; s.between(1, 4)\n |      0     True\n |      1    False\n |      2     True\n |      3    False\n |      4    False\n |      dtype: bool\n |      \n |      With `inclusive` set to ``\"neither\"`` boundary values are excluded:\n |      \n |      &gt;&gt;&gt; s.between(1, 4, inclusive=\"neither\")\n |      0     True\n |      1    False\n |      2    False\n |      3    False\n |      4    False\n |      dtype: bool\n |      \n |      `left` and `right` can be any scalar value:\n |      \n |      &gt;&gt;&gt; s = pd.Series(['Alice', 'Bob', 'Carol', 'Eve'])\n |      &gt;&gt;&gt; s.between('Anna', 'Daniel')\n |      0    False\n |      1     True\n |      2     True\n |      3    False\n |      dtype: bool\n |  \n |  bfill(self, *, axis: 'None | Axis' = None, inplace: 'bool' = False, limit: 'None | int' = None, downcast: 'dict | None' = None) -&gt; 'Series | None'\n |      Synonym for :meth:`DataFrame.fillna` with ``method='bfill'``.\n |      \n |      Returns\n |      -------\n |      Series/DataFrame or None\n |          Object with missing values filled or None if ``inplace=True``.\n |  \n |  clip(self: 'Series', lower=None, upper=None, *, axis: 'Axis | None' = None, inplace: 'bool' = False, **kwargs) -&gt; 'Series | None'\n |      Trim values at input threshold(s).\n |      \n |      Assigns values outside boundary to boundary values. Thresholds\n |      can be singular values or array like, and in the latter case\n |      the clipping is performed element-wise in the specified axis.\n |      \n |      Parameters\n |      ----------\n |      lower : float or array-like, default None\n |          Minimum threshold value. All values below this\n |          threshold will be set to it. A missing\n |          threshold (e.g `NA`) will not clip the value.\n |      upper : float or array-like, default None\n |          Maximum threshold value. All values above this\n |          threshold will be set to it. A missing\n |          threshold (e.g `NA`) will not clip the value.\n |      axis : {{0 or 'index', 1 or 'columns', None}}, default None\n |          Align object with lower and upper along the given axis.\n |          For `Series` this parameter is unused and defaults to `None`.\n |      inplace : bool, default False\n |          Whether to perform the operation in place on the data.\n |      *args, **kwargs\n |          Additional keywords have no effect but might be accepted\n |          for compatibility with numpy.\n |      \n |      Returns\n |      -------\n |      Series or DataFrame or None\n |          Same type as calling object with the values outside the\n |          clip boundaries replaced or None if ``inplace=True``.\n |      \n |      See Also\n |      --------\n |      Series.clip : Trim values at input threshold in series.\n |      DataFrame.clip : Trim values at input threshold in dataframe.\n |      numpy.clip : Clip (limit) the values in an array.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; data = {'col_0': [9, -3, 0, -1, 5], 'col_1': [-2, -7, 6, 8, -5]}\n |      &gt;&gt;&gt; df = pd.DataFrame(data)\n |      &gt;&gt;&gt; df\n |         col_0  col_1\n |      0      9     -2\n |      1     -3     -7\n |      2      0      6\n |      3     -1      8\n |      4      5     -5\n |      \n |      Clips per column using lower and upper thresholds:\n |      \n |      &gt;&gt;&gt; df.clip(-4, 6)\n |         col_0  col_1\n |      0      6     -2\n |      1     -3     -4\n |      2      0      6\n |      3     -1      6\n |      4      5     -4\n |      \n |      Clips using specific lower and upper thresholds per column element:\n |      \n |      &gt;&gt;&gt; t = pd.Series([2, -4, -1, 6, 3])\n |      &gt;&gt;&gt; t\n |      0    2\n |      1   -4\n |      2   -1\n |      3    6\n |      4    3\n |      dtype: int64\n |      \n |      &gt;&gt;&gt; df.clip(t, t + 4, axis=0)\n |         col_0  col_1\n |      0      6      2\n |      1     -3     -4\n |      2      0      3\n |      3      6      8\n |      4      5      3\n |      \n |      Clips using specific lower threshold per column element, with missing values:\n |      \n |      &gt;&gt;&gt; t = pd.Series([2, -4, np.NaN, 6, 3])\n |      &gt;&gt;&gt; t\n |      0    2.0\n |      1   -4.0\n |      2    NaN\n |      3    6.0\n |      4    3.0\n |      dtype: float64\n |      \n |      &gt;&gt;&gt; df.clip(t, axis=0)\n |      col_0  col_1\n |      0      9      2\n |      1     -3     -4\n |      2      0      6\n |      3      6      8\n |      4      5      3\n |  \n |  combine(self, other: 'Series | Hashable', func: 'Callable[[Hashable, Hashable], Hashable]', fill_value: 'Hashable' = None) -&gt; 'Series'\n |      Combine the Series with a Series or scalar according to `func`.\n |      \n |      Combine the Series and `other` using `func` to perform elementwise\n |      selection for combined Series.\n |      `fill_value` is assumed when value is missing at some index\n |      from one of the two objects being combined.\n |      \n |      Parameters\n |      ----------\n |      other : Series or scalar\n |          The value(s) to be combined with the `Series`.\n |      func : function\n |          Function that takes two scalars as inputs and returns an element.\n |      fill_value : scalar, optional\n |          The value to assume when an index is missing from\n |          one Series or the other. The default specifies to use the\n |          appropriate NaN value for the underlying dtype of the Series.\n |      \n |      Returns\n |      -------\n |      Series\n |          The result of combining the Series with the other object.\n |      \n |      See Also\n |      --------\n |      Series.combine_first : Combine Series values, choosing the calling\n |          Series' values first.\n |      \n |      Examples\n |      --------\n |      Consider 2 Datasets ``s1`` and ``s2`` containing\n |      highest clocked speeds of different birds.\n |      \n |      &gt;&gt;&gt; s1 = pd.Series({'falcon': 330.0, 'eagle': 160.0})\n |      &gt;&gt;&gt; s1\n |      falcon    330.0\n |      eagle     160.0\n |      dtype: float64\n |      &gt;&gt;&gt; s2 = pd.Series({'falcon': 345.0, 'eagle': 200.0, 'duck': 30.0})\n |      &gt;&gt;&gt; s2\n |      falcon    345.0\n |      eagle     200.0\n |      duck       30.0\n |      dtype: float64\n |      \n |      Now, to combine the two datasets and view the highest speeds\n |      of the birds across the two datasets\n |      \n |      &gt;&gt;&gt; s1.combine(s2, max)\n |      duck        NaN\n |      eagle     200.0\n |      falcon    345.0\n |      dtype: float64\n |      \n |      In the previous example, the resulting value for duck is missing,\n |      because the maximum of a NaN and a float is a NaN.\n |      So, in the example, we set ``fill_value=0``,\n |      so the maximum value returned will be the value from some dataset.\n |      \n |      &gt;&gt;&gt; s1.combine(s2, max, fill_value=0)\n |      duck       30.0\n |      eagle     200.0\n |      falcon    345.0\n |      dtype: float64\n |  \n |  combine_first(self, other) -&gt; 'Series'\n |      Update null elements with value in the same location in 'other'.\n |      \n |      Combine two Series objects by filling null values in one Series with\n |      non-null values from the other Series. Result index will be the union\n |      of the two indexes.\n |      \n |      Parameters\n |      ----------\n |      other : Series\n |          The value(s) to be used for filling null values.\n |      \n |      Returns\n |      -------\n |      Series\n |          The result of combining the provided Series with the other object.\n |      \n |      See Also\n |      --------\n |      Series.combine : Perform element-wise operation on two Series\n |          using a given function.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; s1 = pd.Series([1, np.nan])\n |      &gt;&gt;&gt; s2 = pd.Series([3, 4, 5])\n |      &gt;&gt;&gt; s1.combine_first(s2)\n |      0    1.0\n |      1    4.0\n |      2    5.0\n |      dtype: float64\n |      \n |      Null values still persist if the location of that null value\n |      does not exist in `other`\n |      \n |      &gt;&gt;&gt; s1 = pd.Series({'falcon': np.nan, 'eagle': 160.0})\n |      &gt;&gt;&gt; s2 = pd.Series({'eagle': 200.0, 'duck': 30.0})\n |      &gt;&gt;&gt; s1.combine_first(s2)\n |      duck       30.0\n |      eagle     160.0\n |      falcon      NaN\n |      dtype: float64\n |  \n |  compare(self, other: 'Series', align_axis: 'Axis' = 1, keep_shape: 'bool' = False, keep_equal: 'bool' = False, result_names: 'Suffixes' = ('self', 'other')) -&gt; 'DataFrame | Series'\n |      Compare to another Series and show the differences.\n |      \n |      .. versionadded:: 1.1.0\n |      \n |      Parameters\n |      ----------\n |      other : Series\n |          Object to compare with.\n |      \n |      align_axis : {0 or 'index', 1 or 'columns'}, default 1\n |          Determine which axis to align the comparison on.\n |      \n |          * 0, or 'index' : Resulting differences are stacked vertically\n |              with rows drawn alternately from self and other.\n |          * 1, or 'columns' : Resulting differences are aligned horizontally\n |              with columns drawn alternately from self and other.\n |      \n |      keep_shape : bool, default False\n |          If true, all rows and columns are kept.\n |          Otherwise, only the ones with different values are kept.\n |      \n |      keep_equal : bool, default False\n |          If true, the result keeps values that are equal.\n |          Otherwise, equal values are shown as NaNs.\n |      \n |      result_names : tuple, default ('self', 'other')\n |          Set the dataframes names in the comparison.\n |      \n |          .. versionadded:: 1.5.0\n |      \n |      Returns\n |      -------\n |      Series or DataFrame\n |          If axis is 0 or 'index' the result will be a Series.\n |          The resulting index will be a MultiIndex with 'self' and 'other'\n |          stacked alternately at the inner level.\n |      \n |          If axis is 1 or 'columns' the result will be a DataFrame.\n |          It will have two columns namely 'self' and 'other'.\n |      \n |      See Also\n |      --------\n |      DataFrame.compare : Compare with another DataFrame and show differences.\n |      \n |      Notes\n |      -----\n |      Matching NaNs will not appear as a difference.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; s1 = pd.Series([\"a\", \"b\", \"c\", \"d\", \"e\"])\n |      &gt;&gt;&gt; s2 = pd.Series([\"a\", \"a\", \"c\", \"b\", \"e\"])\n |      \n |      Align the differences on columns\n |      \n |      &gt;&gt;&gt; s1.compare(s2)\n |        self other\n |      1    b     a\n |      3    d     b\n |      \n |      Stack the differences on indices\n |      \n |      &gt;&gt;&gt; s1.compare(s2, align_axis=0)\n |      1  self     b\n |         other    a\n |      3  self     d\n |         other    b\n |      dtype: object\n |      \n |      Keep all original rows\n |      \n |      &gt;&gt;&gt; s1.compare(s2, keep_shape=True)\n |        self other\n |      0  NaN   NaN\n |      1    b     a\n |      2  NaN   NaN\n |      3    d     b\n |      4  NaN   NaN\n |      \n |      Keep all original rows and also all original values\n |      \n |      &gt;&gt;&gt; s1.compare(s2, keep_shape=True, keep_equal=True)\n |        self other\n |      0    a     a\n |      1    b     a\n |      2    c     c\n |      3    d     b\n |      4    e     e\n |  \n |  corr(self, other: 'Series', method: 'CorrelationMethod' = 'pearson', min_periods: 'int | None' = None) -&gt; 'float'\n |      Compute correlation with `other` Series, excluding missing values.\n |      \n |      The two `Series` objects are not required to be the same length and will be\n |      aligned internally before the correlation function is applied.\n |      \n |      Parameters\n |      ----------\n |      other : Series\n |          Series with which to compute the correlation.\n |      method : {'pearson', 'kendall', 'spearman'} or callable\n |          Method used to compute correlation:\n |      \n |          - pearson : Standard correlation coefficient\n |          - kendall : Kendall Tau correlation coefficient\n |          - spearman : Spearman rank correlation\n |          - callable: Callable with input two 1d ndarrays and returning a float.\n |      \n |          .. warning::\n |              Note that the returned matrix from corr will have 1 along the\n |              diagonals and will be symmetric regardless of the callable's\n |              behavior.\n |      min_periods : int, optional\n |          Minimum number of observations needed to have a valid result.\n |      \n |      Returns\n |      -------\n |      float\n |          Correlation with other.\n |      \n |      See Also\n |      --------\n |      DataFrame.corr : Compute pairwise correlation between columns.\n |      DataFrame.corrwith : Compute pairwise correlation with another\n |          DataFrame or Series.\n |      \n |      Notes\n |      -----\n |      Pearson, Kendall and Spearman correlation are currently computed using pairwise complete observations.\n |      \n |      * `Pearson correlation coefficient &lt;https://en.wikipedia.org/wiki/Pearson_correlation_coefficient&gt;`_\n |      * `Kendall rank correlation coefficient &lt;https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient&gt;`_\n |      * `Spearman's rank correlation coefficient &lt;https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient&gt;`_\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; def histogram_intersection(a, b):\n |      ...     v = np.minimum(a, b).sum().round(decimals=1)\n |      ...     return v\n |      &gt;&gt;&gt; s1 = pd.Series([.2, .0, .6, .2])\n |      &gt;&gt;&gt; s2 = pd.Series([.3, .6, .0, .1])\n |      &gt;&gt;&gt; s1.corr(s2, method=histogram_intersection)\n |      0.3\n |  \n |  count(self)\n |      Return number of non-NA/null observations in the Series.\n |      \n |      Returns\n |      -------\n |      int or Series (if level specified)\n |          Number of non-null values in the Series.\n |      \n |      See Also\n |      --------\n |      DataFrame.count : Count non-NA cells for each column or row.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; s = pd.Series([0.0, 1.0, np.nan])\n |      &gt;&gt;&gt; s.count()\n |      2\n |  \n |  cov(self, other: 'Series', min_periods: 'int | None' = None, ddof: 'int | None' = 1) -&gt; 'float'\n |      Compute covariance with Series, excluding missing values.\n |      \n |      The two `Series` objects are not required to be the same length and\n |      will be aligned internally before the covariance is calculated.\n |      \n |      Parameters\n |      ----------\n |      other : Series\n |          Series with which to compute the covariance.\n |      min_periods : int, optional\n |          Minimum number of observations needed to have a valid result.\n |      ddof : int, default 1\n |          Delta degrees of freedom.  The divisor used in calculations\n |          is ``N - ddof``, where ``N`` represents the number of elements.\n |      \n |          .. versionadded:: 1.1.0\n |      \n |      Returns\n |      -------\n |      float\n |          Covariance between Series and other normalized by N-1\n |          (unbiased estimator).\n |      \n |      See Also\n |      --------\n |      DataFrame.cov : Compute pairwise covariance of columns.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; s1 = pd.Series([0.90010907, 0.13484424, 0.62036035])\n |      &gt;&gt;&gt; s2 = pd.Series([0.12528585, 0.26962463, 0.51111198])\n |      &gt;&gt;&gt; s1.cov(s2)\n |      -0.01685762652715874\n |  \n |  cummax(self, axis: 'Axis | None' = None, skipna: 'bool_t' = True, *args, **kwargs)\n |      Return cumulative maximum over a DataFrame or Series axis.\n |      \n |      Returns a DataFrame or Series of the same size containing the cumulative\n |      maximum.\n |      \n |      Parameters\n |      ----------\n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |          The index or the name of the axis. 0 is equivalent to None or 'index'.\n |          For `Series` this parameter is unused and defaults to 0.\n |      skipna : bool, default True\n |          Exclude NA/null values. If an entire row/column is NA, the result\n |          will be NA.\n |      *args, **kwargs\n |          Additional keywords have no effect but might be accepted for\n |          compatibility with NumPy.\n |      \n |      Returns\n |      -------\n |      scalar or Series\n |          Return cumulative maximum of scalar or Series.\n |      \n |      See Also\n |      --------\n |      core.window.expanding.Expanding.max : Similar functionality\n |          but ignores ``NaN`` values.\n |      Series.max : Return the maximum over\n |          Series axis.\n |      Series.cummax : Return cumulative maximum over Series axis.\n |      Series.cummin : Return cumulative minimum over Series axis.\n |      Series.cumsum : Return cumulative sum over Series axis.\n |      Series.cumprod : Return cumulative product over Series axis.\n |      \n |      Examples\n |      --------\n |      **Series**\n |      \n |      &gt;&gt;&gt; s = pd.Series([2, np.nan, 5, -1, 0])\n |      &gt;&gt;&gt; s\n |      0    2.0\n |      1    NaN\n |      2    5.0\n |      3   -1.0\n |      4    0.0\n |      dtype: float64\n |      \n |      By default, NA values are ignored.\n |      \n |      &gt;&gt;&gt; s.cummax()\n |      0    2.0\n |      1    NaN\n |      2    5.0\n |      3    5.0\n |      4    5.0\n |      dtype: float64\n |      \n |      To include NA values in the operation, use ``skipna=False``\n |      \n |      &gt;&gt;&gt; s.cummax(skipna=False)\n |      0    2.0\n |      1    NaN\n |      2    NaN\n |      3    NaN\n |      4    NaN\n |      dtype: float64\n |      \n |      **DataFrame**\n |      \n |      &gt;&gt;&gt; df = pd.DataFrame([[2.0, 1.0],\n |      ...                    [3.0, np.nan],\n |      ...                    [1.0, 0.0]],\n |      ...                   columns=list('AB'))\n |      &gt;&gt;&gt; df\n |           A    B\n |      0  2.0  1.0\n |      1  3.0  NaN\n |      2  1.0  0.0\n |      \n |      By default, iterates over rows and finds the maximum\n |      in each column. This is equivalent to ``axis=None`` or ``axis='index'``.\n |      \n |      &gt;&gt;&gt; df.cummax()\n |           A    B\n |      0  2.0  1.0\n |      1  3.0  NaN\n |      2  3.0  1.0\n |      \n |      To iterate over columns and find the maximum in each row,\n |      use ``axis=1``\n |      \n |      &gt;&gt;&gt; df.cummax(axis=1)\n |           A    B\n |      0  2.0  2.0\n |      1  3.0  NaN\n |      2  1.0  1.0\n |  \n |  cummin(self, axis: 'Axis | None' = None, skipna: 'bool_t' = True, *args, **kwargs)\n |      Return cumulative minimum over a DataFrame or Series axis.\n |      \n |      Returns a DataFrame or Series of the same size containing the cumulative\n |      minimum.\n |      \n |      Parameters\n |      ----------\n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |          The index or the name of the axis. 0 is equivalent to None or 'index'.\n |          For `Series` this parameter is unused and defaults to 0.\n |      skipna : bool, default True\n |          Exclude NA/null values. If an entire row/column is NA, the result\n |          will be NA.\n |      *args, **kwargs\n |          Additional keywords have no effect but might be accepted for\n |          compatibility with NumPy.\n |      \n |      Returns\n |      -------\n |      scalar or Series\n |          Return cumulative minimum of scalar or Series.\n |      \n |      See Also\n |      --------\n |      core.window.expanding.Expanding.min : Similar functionality\n |          but ignores ``NaN`` values.\n |      Series.min : Return the minimum over\n |          Series axis.\n |      Series.cummax : Return cumulative maximum over Series axis.\n |      Series.cummin : Return cumulative minimum over Series axis.\n |      Series.cumsum : Return cumulative sum over Series axis.\n |      Series.cumprod : Return cumulative product over Series axis.\n |      \n |      Examples\n |      --------\n |      **Series**\n |      \n |      &gt;&gt;&gt; s = pd.Series([2, np.nan, 5, -1, 0])\n |      &gt;&gt;&gt; s\n |      0    2.0\n |      1    NaN\n |      2    5.0\n |      3   -1.0\n |      4    0.0\n |      dtype: float64\n |      \n |      By default, NA values are ignored.\n |      \n |      &gt;&gt;&gt; s.cummin()\n |      0    2.0\n |      1    NaN\n |      2    2.0\n |      3   -1.0\n |      4   -1.0\n |      dtype: float64\n |      \n |      To include NA values in the operation, use ``skipna=False``\n |      \n |      &gt;&gt;&gt; s.cummin(skipna=False)\n |      0    2.0\n |      1    NaN\n |      2    NaN\n |      3    NaN\n |      4    NaN\n |      dtype: float64\n |      \n |      **DataFrame**\n |      \n |      &gt;&gt;&gt; df = pd.DataFrame([[2.0, 1.0],\n |      ...                    [3.0, np.nan],\n |      ...                    [1.0, 0.0]],\n |      ...                   columns=list('AB'))\n |      &gt;&gt;&gt; df\n |           A    B\n |      0  2.0  1.0\n |      1  3.0  NaN\n |      2  1.0  0.0\n |      \n |      By default, iterates over rows and finds the minimum\n |      in each column. This is equivalent to ``axis=None`` or ``axis='index'``.\n |      \n |      &gt;&gt;&gt; df.cummin()\n |           A    B\n |      0  2.0  1.0\n |      1  2.0  NaN\n |      2  1.0  0.0\n |      \n |      To iterate over columns and find the minimum in each row,\n |      use ``axis=1``\n |      \n |      &gt;&gt;&gt; df.cummin(axis=1)\n |           A    B\n |      0  2.0  1.0\n |      1  3.0  NaN\n |      2  1.0  0.0\n |  \n |  cumprod(self, axis: 'Axis | None' = None, skipna: 'bool_t' = True, *args, **kwargs)\n |      Return cumulative product over a DataFrame or Series axis.\n |      \n |      Returns a DataFrame or Series of the same size containing the cumulative\n |      product.\n |      \n |      Parameters\n |      ----------\n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |          The index or the name of the axis. 0 is equivalent to None or 'index'.\n |          For `Series` this parameter is unused and defaults to 0.\n |      skipna : bool, default True\n |          Exclude NA/null values. If an entire row/column is NA, the result\n |          will be NA.\n |      *args, **kwargs\n |          Additional keywords have no effect but might be accepted for\n |          compatibility with NumPy.\n |      \n |      Returns\n |      -------\n |      scalar or Series\n |          Return cumulative product of scalar or Series.\n |      \n |      See Also\n |      --------\n |      core.window.expanding.Expanding.prod : Similar functionality\n |          but ignores ``NaN`` values.\n |      Series.prod : Return the product over\n |          Series axis.\n |      Series.cummax : Return cumulative maximum over Series axis.\n |      Series.cummin : Return cumulative minimum over Series axis.\n |      Series.cumsum : Return cumulative sum over Series axis.\n |      Series.cumprod : Return cumulative product over Series axis.\n |      \n |      Examples\n |      --------\n |      **Series**\n |      \n |      &gt;&gt;&gt; s = pd.Series([2, np.nan, 5, -1, 0])\n |      &gt;&gt;&gt; s\n |      0    2.0\n |      1    NaN\n |      2    5.0\n |      3   -1.0\n |      4    0.0\n |      dtype: float64\n |      \n |      By default, NA values are ignored.\n |      \n |      &gt;&gt;&gt; s.cumprod()\n |      0     2.0\n |      1     NaN\n |      2    10.0\n |      3   -10.0\n |      4    -0.0\n |      dtype: float64\n |      \n |      To include NA values in the operation, use ``skipna=False``\n |      \n |      &gt;&gt;&gt; s.cumprod(skipna=False)\n |      0    2.0\n |      1    NaN\n |      2    NaN\n |      3    NaN\n |      4    NaN\n |      dtype: float64\n |      \n |      **DataFrame**\n |      \n |      &gt;&gt;&gt; df = pd.DataFrame([[2.0, 1.0],\n |      ...                    [3.0, np.nan],\n |      ...                    [1.0, 0.0]],\n |      ...                   columns=list('AB'))\n |      &gt;&gt;&gt; df\n |           A    B\n |      0  2.0  1.0\n |      1  3.0  NaN\n |      2  1.0  0.0\n |      \n |      By default, iterates over rows and finds the product\n |      in each column. This is equivalent to ``axis=None`` or ``axis='index'``.\n |      \n |      &gt;&gt;&gt; df.cumprod()\n |           A    B\n |      0  2.0  1.0\n |      1  6.0  NaN\n |      2  6.0  0.0\n |      \n |      To iterate over columns and find the product in each row,\n |      use ``axis=1``\n |      \n |      &gt;&gt;&gt; df.cumprod(axis=1)\n |           A    B\n |      0  2.0  2.0\n |      1  3.0  NaN\n |      2  1.0  0.0\n |  \n |  cumsum(self, axis: 'Axis | None' = None, skipna: 'bool_t' = True, *args, **kwargs)\n |      Return cumulative sum over a DataFrame or Series axis.\n |      \n |      Returns a DataFrame or Series of the same size containing the cumulative\n |      sum.\n |      \n |      Parameters\n |      ----------\n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |          The index or the name of the axis. 0 is equivalent to None or 'index'.\n |          For `Series` this parameter is unused and defaults to 0.\n |      skipna : bool, default True\n |          Exclude NA/null values. If an entire row/column is NA, the result\n |          will be NA.\n |      *args, **kwargs\n |          Additional keywords have no effect but might be accepted for\n |          compatibility with NumPy.\n |      \n |      Returns\n |      -------\n |      scalar or Series\n |          Return cumulative sum of scalar or Series.\n |      \n |      See Also\n |      --------\n |      core.window.expanding.Expanding.sum : Similar functionality\n |          but ignores ``NaN`` values.\n |      Series.sum : Return the sum over\n |          Series axis.\n |      Series.cummax : Return cumulative maximum over Series axis.\n |      Series.cummin : Return cumulative minimum over Series axis.\n |      Series.cumsum : Return cumulative sum over Series axis.\n |      Series.cumprod : Return cumulative product over Series axis.\n |      \n |      Examples\n |      --------\n |      **Series**\n |      \n |      &gt;&gt;&gt; s = pd.Series([2, np.nan, 5, -1, 0])\n |      &gt;&gt;&gt; s\n |      0    2.0\n |      1    NaN\n |      2    5.0\n |      3   -1.0\n |      4    0.0\n |      dtype: float64\n |      \n |      By default, NA values are ignored.\n |      \n |      &gt;&gt;&gt; s.cumsum()\n |      0    2.0\n |      1    NaN\n |      2    7.0\n |      3    6.0\n |      4    6.0\n |      dtype: float64\n |      \n |      To include NA values in the operation, use ``skipna=False``\n |      \n |      &gt;&gt;&gt; s.cumsum(skipna=False)\n |      0    2.0\n |      1    NaN\n |      2    NaN\n |      3    NaN\n |      4    NaN\n |      dtype: float64\n |      \n |      **DataFrame**\n |      \n |      &gt;&gt;&gt; df = pd.DataFrame([[2.0, 1.0],\n |      ...                    [3.0, np.nan],\n |      ...                    [1.0, 0.0]],\n |      ...                   columns=list('AB'))\n |      &gt;&gt;&gt; df\n |           A    B\n |      0  2.0  1.0\n |      1  3.0  NaN\n |      2  1.0  0.0\n |      \n |      By default, iterates over rows and finds the sum\n |      in each column. This is equivalent to ``axis=None`` or ``axis='index'``.\n |      \n |      &gt;&gt;&gt; df.cumsum()\n |           A    B\n |      0  2.0  1.0\n |      1  5.0  NaN\n |      2  6.0  1.0\n |      \n |      To iterate over columns and find the sum in each row,\n |      use ``axis=1``\n |      \n |      &gt;&gt;&gt; df.cumsum(axis=1)\n |           A    B\n |      0  2.0  3.0\n |      1  3.0  NaN\n |      2  1.0  1.0\n |  \n |  diff(self, periods: 'int' = 1) -&gt; 'Series'\n |      First discrete difference of element.\n |      \n |      Calculates the difference of a Series element compared with another\n |      element in the Series (default is element in previous row).\n |      \n |      Parameters\n |      ----------\n |      periods : int, default 1\n |          Periods to shift for calculating difference, accepts negative\n |          values.\n |      \n |      Returns\n |      -------\n |      Series\n |          First differences of the Series.\n |      \n |      See Also\n |      --------\n |      Series.pct_change: Percent change over given number of periods.\n |      Series.shift: Shift index by desired number of periods with an\n |          optional time freq.\n |      DataFrame.diff: First discrete difference of object.\n |      \n |      Notes\n |      -----\n |      For boolean dtypes, this uses :meth:`operator.xor` rather than\n |      :meth:`operator.sub`.\n |      The result is calculated according to current dtype in Series,\n |      however dtype of the result is always float64.\n |      \n |      Examples\n |      --------\n |      \n |      Difference with previous row\n |      \n |      &gt;&gt;&gt; s = pd.Series([1, 1, 2, 3, 5, 8])\n |      &gt;&gt;&gt; s.diff()\n |      0    NaN\n |      1    0.0\n |      2    1.0\n |      3    1.0\n |      4    2.0\n |      5    3.0\n |      dtype: float64\n |      \n |      Difference with 3rd previous row\n |      \n |      &gt;&gt;&gt; s.diff(periods=3)\n |      0    NaN\n |      1    NaN\n |      2    NaN\n |      3    2.0\n |      4    4.0\n |      5    6.0\n |      dtype: float64\n |      \n |      Difference with following row\n |      \n |      &gt;&gt;&gt; s.diff(periods=-1)\n |      0    0.0\n |      1   -1.0\n |      2   -1.0\n |      3   -2.0\n |      4   -3.0\n |      5    NaN\n |      dtype: float64\n |      \n |      Overflow in input dtype\n |      \n |      &gt;&gt;&gt; s = pd.Series([1, 0], dtype=np.uint8)\n |      &gt;&gt;&gt; s.diff()\n |      0      NaN\n |      1    255.0\n |      dtype: float64\n |  \n |  div = truediv(self, other, level=None, fill_value=None, axis: 'Axis' = 0)\n |  \n |  divide = truediv(self, other, level=None, fill_value=None, axis: 'Axis' = 0)\n |  \n |  divmod(self, other, level=None, fill_value=None, axis: 'Axis' = 0)\n |      Return Integer division and modulo of series and other, element-wise (binary operator `divmod`).\n |      \n |      Equivalent to ``divmod(series, other)``, but with support to substitute a fill_value for\n |      missing data in either one of the inputs.\n |      \n |      Parameters\n |      ----------\n |      other : Series or scalar value\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level.\n |      fill_value : None or float value, default None (NaN)\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful Series alignment, with this value before computation.\n |          If data in both corresponding Series locations is missing\n |          the result of filling (at that location) will be missing.\n |      axis : {0 or 'index'}\n |          Unused. Parameter needed for compatibility with DataFrame.\n |      \n |      Returns\n |      -------\n |      2-Tuple of Series\n |          The result of the operation.\n |      \n |      See Also\n |      --------\n |      Series.rdivmod : Reverse of the Integer division and modulo operator, see\n |          `Python documentation\n |          &lt;https://docs.python.org/3/reference/datamodel.html#emulating-numeric-types&gt;`_\n |          for more details.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; a = pd.Series([1, 1, 1, np.nan], index=['a', 'b', 'c', 'd'])\n |      &gt;&gt;&gt; a\n |      a    1.0\n |      b    1.0\n |      c    1.0\n |      d    NaN\n |      dtype: float64\n |      &gt;&gt;&gt; b = pd.Series([1, np.nan, 1, np.nan], index=['a', 'b', 'd', 'e'])\n |      &gt;&gt;&gt; b\n |      a    1.0\n |      b    NaN\n |      d    1.0\n |      e    NaN\n |      dtype: float64\n |      &gt;&gt;&gt; a.divmod(b, fill_value=0)\n |      (a    1.0\n |       b    NaN\n |       c    NaN\n |       d    0.0\n |       e    NaN\n |       dtype: float64,\n |       a    0.0\n |       b    NaN\n |       c    NaN\n |       d    0.0\n |       e    NaN\n |       dtype: float64)\n |  \n |  dot(self, other: 'AnyArrayLike') -&gt; 'Series | np.ndarray'\n |      Compute the dot product between the Series and the columns of other.\n |      \n |      This method computes the dot product between the Series and another\n |      one, or the Series and each columns of a DataFrame, or the Series and\n |      each columns of an array.\n |      \n |      It can also be called using `self @ other` in Python &gt;= 3.5.\n |      \n |      Parameters\n |      ----------\n |      other : Series, DataFrame or array-like\n |          The other object to compute the dot product with its columns.\n |      \n |      Returns\n |      -------\n |      scalar, Series or numpy.ndarray\n |          Return the dot product of the Series and other if other is a\n |          Series, the Series of the dot product of Series and each rows of\n |          other if other is a DataFrame or a numpy.ndarray between the Series\n |          and each columns of the numpy array.\n |      \n |      See Also\n |      --------\n |      DataFrame.dot: Compute the matrix product with the DataFrame.\n |      Series.mul: Multiplication of series and other, element-wise.\n |      \n |      Notes\n |      -----\n |      The Series and other has to share the same index if other is a Series\n |      or a DataFrame.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; s = pd.Series([0, 1, 2, 3])\n |      &gt;&gt;&gt; other = pd.Series([-1, 2, -3, 4])\n |      &gt;&gt;&gt; s.dot(other)\n |      8\n |      &gt;&gt;&gt; s @ other\n |      8\n |      &gt;&gt;&gt; df = pd.DataFrame([[0, 1], [-2, 3], [4, -5], [6, 7]])\n |      &gt;&gt;&gt; s.dot(df)\n |      0    24\n |      1    14\n |      dtype: int64\n |      &gt;&gt;&gt; arr = np.array([[0, 1], [-2, 3], [4, -5], [6, 7]])\n |      &gt;&gt;&gt; s.dot(arr)\n |      array([24, 14])\n |  \n |  drop(self, labels: 'IndexLabel' = None, *, axis: 'Axis' = 0, index: 'IndexLabel' = None, columns: 'IndexLabel' = None, level: 'Level | None' = None, inplace: 'bool' = False, errors: 'IgnoreRaise' = 'raise') -&gt; 'Series | None'\n |      Return Series with specified index labels removed.\n |      \n |      Remove elements of a Series based on specifying the index labels.\n |      When using a multi-index, labels on different levels can be removed\n |      by specifying the level.\n |      \n |      Parameters\n |      ----------\n |      labels : single label or list-like\n |          Index labels to drop.\n |      axis : {0 or 'index'}\n |          Unused. Parameter needed for compatibility with DataFrame.\n |      index : single label or list-like\n |          Redundant for application on Series, but 'index' can be used instead\n |          of 'labels'.\n |      columns : single label or list-like\n |          No change is made to the Series; use 'index' or 'labels' instead.\n |      level : int or level name, optional\n |          For MultiIndex, level for which the labels will be removed.\n |      inplace : bool, default False\n |          If True, do operation inplace and return None.\n |      errors : {'ignore', 'raise'}, default 'raise'\n |          If 'ignore', suppress error and only existing labels are dropped.\n |      \n |      Returns\n |      -------\n |      Series or None\n |          Series with specified index labels removed or None if ``inplace=True``.\n |      \n |      Raises\n |      ------\n |      KeyError\n |          If none of the labels are found in the index.\n |      \n |      See Also\n |      --------\n |      Series.reindex : Return only specified index labels of Series.\n |      Series.dropna : Return series without null values.\n |      Series.drop_duplicates : Return Series with duplicate values removed.\n |      DataFrame.drop : Drop specified labels from rows or columns.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; s = pd.Series(data=np.arange(3), index=['A', 'B', 'C'])\n |      &gt;&gt;&gt; s\n |      A  0\n |      B  1\n |      C  2\n |      dtype: int64\n |      \n |      Drop labels B en C\n |      \n |      &gt;&gt;&gt; s.drop(labels=['B', 'C'])\n |      A  0\n |      dtype: int64\n |      \n |      Drop 2nd level label in MultiIndex Series\n |      \n |      &gt;&gt;&gt; midx = pd.MultiIndex(levels=[['lama', 'cow', 'falcon'],\n |      ...                              ['speed', 'weight', 'length']],\n |      ...                      codes=[[0, 0, 0, 1, 1, 1, 2, 2, 2],\n |      ...                             [0, 1, 2, 0, 1, 2, 0, 1, 2]])\n |      &gt;&gt;&gt; s = pd.Series([45, 200, 1.2, 30, 250, 1.5, 320, 1, 0.3],\n |      ...               index=midx)\n |      &gt;&gt;&gt; s\n |      lama    speed      45.0\n |              weight    200.0\n |              length      1.2\n |      cow     speed      30.0\n |              weight    250.0\n |              length      1.5\n |      falcon  speed     320.0\n |              weight      1.0\n |              length      0.3\n |      dtype: float64\n |      \n |      &gt;&gt;&gt; s.drop(labels='weight', level=1)\n |      lama    speed      45.0\n |              length      1.2\n |      cow     speed      30.0\n |              length      1.5\n |      falcon  speed     320.0\n |              length      0.3\n |      dtype: float64\n |  \n |  drop_duplicates(self, *, keep: 'DropKeep' = 'first', inplace: 'bool' = False, ignore_index: 'bool' = False) -&gt; 'Series | None'\n |      Return Series with duplicate values removed.\n |      \n |      Parameters\n |      ----------\n |      keep : {'first', 'last', ``False``}, default 'first'\n |          Method to handle dropping duplicates:\n |      \n |          - 'first' : Drop duplicates except for the first occurrence.\n |          - 'last' : Drop duplicates except for the last occurrence.\n |          - ``False`` : Drop all duplicates.\n |      \n |      inplace : bool, default ``False``\n |          If ``True``, performs operation inplace and returns None.\n |      \n |      ignore_index : bool, default ``False``\n |          If ``True``, the resulting axis will be labeled 0, 1, …, n - 1.\n |      \n |          .. versionadded:: 2.0.0\n |      \n |      Returns\n |      -------\n |      Series or None\n |          Series with duplicates dropped or None if ``inplace=True``.\n |      \n |      See Also\n |      --------\n |      Index.drop_duplicates : Equivalent method on Index.\n |      DataFrame.drop_duplicates : Equivalent method on DataFrame.\n |      Series.duplicated : Related method on Series, indicating duplicate\n |          Series values.\n |      Series.unique : Return unique values as an array.\n |      \n |      Examples\n |      --------\n |      Generate a Series with duplicated entries.\n |      \n |      &gt;&gt;&gt; s = pd.Series(['lama', 'cow', 'lama', 'beetle', 'lama', 'hippo'],\n |      ...               name='animal')\n |      &gt;&gt;&gt; s\n |      0      lama\n |      1       cow\n |      2      lama\n |      3    beetle\n |      4      lama\n |      5     hippo\n |      Name: animal, dtype: object\n |      \n |      With the 'keep' parameter, the selection behaviour of duplicated values\n |      can be changed. The value 'first' keeps the first occurrence for each\n |      set of duplicated entries. The default value of keep is 'first'.\n |      \n |      &gt;&gt;&gt; s.drop_duplicates()\n |      0      lama\n |      1       cow\n |      3    beetle\n |      5     hippo\n |      Name: animal, dtype: object\n |      \n |      The value 'last' for parameter 'keep' keeps the last occurrence for\n |      each set of duplicated entries.\n |      \n |      &gt;&gt;&gt; s.drop_duplicates(keep='last')\n |      1       cow\n |      3    beetle\n |      4      lama\n |      5     hippo\n |      Name: animal, dtype: object\n |      \n |      The value ``False`` for parameter 'keep' discards all sets of\n |      duplicated entries.\n |      \n |      &gt;&gt;&gt; s.drop_duplicates(keep=False)\n |      1       cow\n |      3    beetle\n |      5     hippo\n |      Name: animal, dtype: object\n |  \n |  dropna(self, *, axis: 'Axis' = 0, inplace: 'bool' = False, how: 'AnyAll | None' = None, ignore_index: 'bool' = False) -&gt; 'Series | None'\n |      Return a new Series with missing values removed.\n |      \n |      See the :ref:`User Guide &lt;missing_data&gt;` for more on which values are\n |      considered missing, and how to work with missing data.\n |      \n |      Parameters\n |      ----------\n |      axis : {0 or 'index'}\n |          Unused. Parameter needed for compatibility with DataFrame.\n |      inplace : bool, default False\n |          If True, do operation inplace and return None.\n |      how : str, optional\n |          Not in use. Kept for compatibility.\n |      ignore_index : bool, default ``False``\n |          If ``True``, the resulting axis will be labeled 0, 1, …, n - 1.\n |      \n |          .. versionadded:: 2.0.0\n |      \n |      Returns\n |      -------\n |      Series or None\n |          Series with NA entries dropped from it or None if ``inplace=True``.\n |      \n |      See Also\n |      --------\n |      Series.isna: Indicate missing values.\n |      Series.notna : Indicate existing (non-missing) values.\n |      Series.fillna : Replace missing values.\n |      DataFrame.dropna : Drop rows or columns which contain NA values.\n |      Index.dropna : Drop missing indices.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; ser = pd.Series([1., 2., np.nan])\n |      &gt;&gt;&gt; ser\n |      0    1.0\n |      1    2.0\n |      2    NaN\n |      dtype: float64\n |      \n |      Drop NA values from a Series.\n |      \n |      &gt;&gt;&gt; ser.dropna()\n |      0    1.0\n |      1    2.0\n |      dtype: float64\n |      \n |      Empty strings are not considered NA values. ``None`` is considered an\n |      NA value.\n |      \n |      &gt;&gt;&gt; ser = pd.Series([np.NaN, 2, pd.NaT, '', None, 'I stay'])\n |      &gt;&gt;&gt; ser\n |      0       NaN\n |      1         2\n |      2       NaT\n |      3\n |      4      None\n |      5    I stay\n |      dtype: object\n |      &gt;&gt;&gt; ser.dropna()\n |      1         2\n |      3\n |      5    I stay\n |      dtype: object\n |  \n |  duplicated(self, keep: 'DropKeep' = 'first') -&gt; 'Series'\n |      Indicate duplicate Series values.\n |      \n |      Duplicated values are indicated as ``True`` values in the resulting\n |      Series. Either all duplicates, all except the first or all except the\n |      last occurrence of duplicates can be indicated.\n |      \n |      Parameters\n |      ----------\n |      keep : {'first', 'last', False}, default 'first'\n |          Method to handle dropping duplicates:\n |      \n |          - 'first' : Mark duplicates as ``True`` except for the first\n |            occurrence.\n |          - 'last' : Mark duplicates as ``True`` except for the last\n |            occurrence.\n |          - ``False`` : Mark all duplicates as ``True``.\n |      \n |      Returns\n |      -------\n |      Series[bool]\n |          Series indicating whether each value has occurred in the\n |          preceding values.\n |      \n |      See Also\n |      --------\n |      Index.duplicated : Equivalent method on pandas.Index.\n |      DataFrame.duplicated : Equivalent method on pandas.DataFrame.\n |      Series.drop_duplicates : Remove duplicate values from Series.\n |      \n |      Examples\n |      --------\n |      By default, for each set of duplicated values, the first occurrence is\n |      set on False and all others on True:\n |      \n |      &gt;&gt;&gt; animals = pd.Series(['lama', 'cow', 'lama', 'beetle', 'lama'])\n |      &gt;&gt;&gt; animals.duplicated()\n |      0    False\n |      1    False\n |      2     True\n |      3    False\n |      4     True\n |      dtype: bool\n |      \n |      which is equivalent to\n |      \n |      &gt;&gt;&gt; animals.duplicated(keep='first')\n |      0    False\n |      1    False\n |      2     True\n |      3    False\n |      4     True\n |      dtype: bool\n |      \n |      By using 'last', the last occurrence of each set of duplicated values\n |      is set on False and all others on True:\n |      \n |      &gt;&gt;&gt; animals.duplicated(keep='last')\n |      0     True\n |      1    False\n |      2     True\n |      3    False\n |      4    False\n |      dtype: bool\n |      \n |      By setting keep on ``False``, all duplicates are True:\n |      \n |      &gt;&gt;&gt; animals.duplicated(keep=False)\n |      0     True\n |      1    False\n |      2     True\n |      3    False\n |      4     True\n |      dtype: bool\n |  \n |  eq(self, other, level=None, fill_value=None, axis: 'Axis' = 0)\n |      Return Equal to of series and other, element-wise (binary operator `eq`).\n |      \n |      Equivalent to ``series == other``, but with support to substitute a fill_value for\n |      missing data in either one of the inputs.\n |      \n |      Parameters\n |      ----------\n |      other : Series or scalar value\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level.\n |      fill_value : None or float value, default None (NaN)\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful Series alignment, with this value before computation.\n |          If data in both corresponding Series locations is missing\n |          the result of filling (at that location) will be missing.\n |      axis : {0 or 'index'}\n |          Unused. Parameter needed for compatibility with DataFrame.\n |      \n |      Returns\n |      -------\n |      Series\n |          The result of the operation.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; a = pd.Series([1, 1, 1, np.nan], index=['a', 'b', 'c', 'd'])\n |      &gt;&gt;&gt; a\n |      a    1.0\n |      b    1.0\n |      c    1.0\n |      d    NaN\n |      dtype: float64\n |      &gt;&gt;&gt; b = pd.Series([1, np.nan, 1, np.nan], index=['a', 'b', 'd', 'e'])\n |      &gt;&gt;&gt; b\n |      a    1.0\n |      b    NaN\n |      d    1.0\n |      e    NaN\n |      dtype: float64\n |      &gt;&gt;&gt; a.eq(b, fill_value=0)\n |      a     True\n |      b    False\n |      c    False\n |      d    False\n |      e    False\n |      dtype: bool\n |  \n |  explode(self, ignore_index: 'bool' = False) -&gt; 'Series'\n |      Transform each element of a list-like to a row.\n |      \n |      Parameters\n |      ----------\n |      ignore_index : bool, default False\n |          If True, the resulting index will be labeled 0, 1, …, n - 1.\n |      \n |          .. versionadded:: 1.1.0\n |      \n |      Returns\n |      -------\n |      Series\n |          Exploded lists to rows; index will be duplicated for these rows.\n |      \n |      See Also\n |      --------\n |      Series.str.split : Split string values on specified separator.\n |      Series.unstack : Unstack, a.k.a. pivot, Series with MultiIndex\n |          to produce DataFrame.\n |      DataFrame.melt : Unpivot a DataFrame from wide format to long format.\n |      DataFrame.explode : Explode a DataFrame from list-like\n |          columns to long format.\n |      \n |      Notes\n |      -----\n |      This routine will explode list-likes including lists, tuples, sets,\n |      Series, and np.ndarray. The result dtype of the subset rows will\n |      be object. Scalars will be returned unchanged, and empty list-likes will\n |      result in a np.nan for that row. In addition, the ordering of elements in\n |      the output will be non-deterministic when exploding sets.\n |      \n |      Reference :ref:`the user guide &lt;reshaping.explode&gt;` for more examples.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; s = pd.Series([[1, 2, 3], 'foo', [], [3, 4]])\n |      &gt;&gt;&gt; s\n |      0    [1, 2, 3]\n |      1          foo\n |      2           []\n |      3       [3, 4]\n |      dtype: object\n |      \n |      &gt;&gt;&gt; s.explode()\n |      0      1\n |      0      2\n |      0      3\n |      1    foo\n |      2    NaN\n |      3      3\n |      3      4\n |      dtype: object\n |  \n |  ffill(self, *, axis: 'None | Axis' = None, inplace: 'bool' = False, limit: 'None | int' = None, downcast: 'dict | None' = None) -&gt; 'Series | None'\n |      Synonym for :meth:`DataFrame.fillna` with ``method='ffill'``.\n |      \n |      Returns\n |      -------\n |      Series/DataFrame or None\n |          Object with missing values filled or None if ``inplace=True``.\n |  \n |  fillna(self, value: 'Hashable | Mapping | Series | DataFrame' = None, *, method: 'FillnaOptions | None' = None, axis: 'Axis | None' = None, inplace: 'bool' = False, limit: 'int | None' = None, downcast: 'dict | None' = None) -&gt; 'Series | None'\n |      Fill NA/NaN values using the specified method.\n |      \n |      Parameters\n |      ----------\n |      value : scalar, dict, Series, or DataFrame\n |          Value to use to fill holes (e.g. 0), alternately a\n |          dict/Series/DataFrame of values specifying which value to use for\n |          each index (for a Series) or column (for a DataFrame).  Values not\n |          in the dict/Series/DataFrame will not be filled. This value cannot\n |          be a list.\n |      method : {'backfill', 'bfill', 'ffill', None}, default None\n |          Method to use for filling holes in reindexed Series:\n |      \n |          * ffill: propagate last valid observation forward to next valid.\n |          * backfill / bfill: use next valid observation to fill gap.\n |      \n |      axis : {0 or 'index'}\n |          Axis along which to fill missing values. For `Series`\n |          this parameter is unused and defaults to 0.\n |      inplace : bool, default False\n |          If True, fill in-place. Note: this will modify any\n |          other views on this object (e.g., a no-copy slice for a column in a\n |          DataFrame).\n |      limit : int, default None\n |          If method is specified, this is the maximum number of consecutive\n |          NaN values to forward/backward fill. In other words, if there is\n |          a gap with more than this number of consecutive NaNs, it will only\n |          be partially filled. If method is not specified, this is the\n |          maximum number of entries along the entire axis where NaNs will be\n |          filled. Must be greater than 0 if not None.\n |      downcast : dict, default is None\n |          A dict of item-&gt;dtype of what to downcast if possible,\n |          or the string 'infer' which will try to downcast to an appropriate\n |          equal type (e.g. float64 to int64 if possible).\n |      \n |      Returns\n |      -------\n |      Series or None\n |          Object with missing values filled or None if ``inplace=True``.\n |      \n |      See Also\n |      --------\n |      interpolate : Fill NaN values using interpolation.\n |      reindex : Conform object to new index.\n |      asfreq : Convert TimeSeries to specified frequency.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; df = pd.DataFrame([[np.nan, 2, np.nan, 0],\n |      ...                    [3, 4, np.nan, 1],\n |      ...                    [np.nan, np.nan, np.nan, np.nan],\n |      ...                    [np.nan, 3, np.nan, 4]],\n |      ...                   columns=list(\"ABCD\"))\n |      &gt;&gt;&gt; df\n |           A    B   C    D\n |      0  NaN  2.0 NaN  0.0\n |      1  3.0  4.0 NaN  1.0\n |      2  NaN  NaN NaN  NaN\n |      3  NaN  3.0 NaN  4.0\n |      \n |      Replace all NaN elements with 0s.\n |      \n |      &gt;&gt;&gt; df.fillna(0)\n |           A    B    C    D\n |      0  0.0  2.0  0.0  0.0\n |      1  3.0  4.0  0.0  1.0\n |      2  0.0  0.0  0.0  0.0\n |      3  0.0  3.0  0.0  4.0\n |      \n |      We can also propagate non-null values forward or backward.\n |      \n |      &gt;&gt;&gt; df.fillna(method=\"ffill\")\n |           A    B   C    D\n |      0  NaN  2.0 NaN  0.0\n |      1  3.0  4.0 NaN  1.0\n |      2  3.0  4.0 NaN  1.0\n |      3  3.0  3.0 NaN  4.0\n |      \n |      Replace all NaN elements in column 'A', 'B', 'C', and 'D', with 0, 1,\n |      2, and 3 respectively.\n |      \n |      &gt;&gt;&gt; values = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\n |      &gt;&gt;&gt; df.fillna(value=values)\n |           A    B    C    D\n |      0  0.0  2.0  2.0  0.0\n |      1  3.0  4.0  2.0  1.0\n |      2  0.0  1.0  2.0  3.0\n |      3  0.0  3.0  2.0  4.0\n |      \n |      Only replace the first NaN element.\n |      \n |      &gt;&gt;&gt; df.fillna(value=values, limit=1)\n |           A    B    C    D\n |      0  0.0  2.0  2.0  0.0\n |      1  3.0  4.0  NaN  1.0\n |      2  NaN  1.0  NaN  3.0\n |      3  NaN  3.0  NaN  4.0\n |      \n |      When filling using a DataFrame, replacement happens along\n |      the same column names and same indices\n |      \n |      &gt;&gt;&gt; df2 = pd.DataFrame(np.zeros((4, 4)), columns=list(\"ABCE\"))\n |      &gt;&gt;&gt; df.fillna(df2)\n |           A    B    C    D\n |      0  0.0  2.0  0.0  0.0\n |      1  3.0  4.0  0.0  1.0\n |      2  0.0  0.0  0.0  NaN\n |      3  0.0  3.0  0.0  4.0\n |      \n |      Note that column D is not affected since it is not present in df2.\n |  \n |  floordiv(self, other, level=None, fill_value=None, axis: 'Axis' = 0)\n |      Return Integer division of series and other, element-wise (binary operator `floordiv`).\n |      \n |      Equivalent to ``series // other``, but with support to substitute a fill_value for\n |      missing data in either one of the inputs.\n |      \n |      Parameters\n |      ----------\n |      other : Series or scalar value\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level.\n |      fill_value : None or float value, default None (NaN)\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful Series alignment, with this value before computation.\n |          If data in both corresponding Series locations is missing\n |          the result of filling (at that location) will be missing.\n |      axis : {0 or 'index'}\n |          Unused. Parameter needed for compatibility with DataFrame.\n |      \n |      Returns\n |      -------\n |      Series\n |          The result of the operation.\n |      \n |      See Also\n |      --------\n |      Series.rfloordiv : Reverse of the Integer division operator, see\n |          `Python documentation\n |          &lt;https://docs.python.org/3/reference/datamodel.html#emulating-numeric-types&gt;`_\n |          for more details.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; a = pd.Series([1, 1, 1, np.nan], index=['a', 'b', 'c', 'd'])\n |      &gt;&gt;&gt; a\n |      a    1.0\n |      b    1.0\n |      c    1.0\n |      d    NaN\n |      dtype: float64\n |      &gt;&gt;&gt; b = pd.Series([1, np.nan, 1, np.nan], index=['a', 'b', 'd', 'e'])\n |      &gt;&gt;&gt; b\n |      a    1.0\n |      b    NaN\n |      d    1.0\n |      e    NaN\n |      dtype: float64\n |      &gt;&gt;&gt; a.floordiv(b, fill_value=0)\n |      a    1.0\n |      b    inf\n |      c    inf\n |      d    0.0\n |      e    NaN\n |      dtype: float64\n |  \n |  ge(self, other, level=None, fill_value=None, axis: 'Axis' = 0)\n |      Return Greater than or equal to of series and other, element-wise (binary operator `ge`).\n |      \n |      Equivalent to ``series &gt;= other``, but with support to substitute a fill_value for\n |      missing data in either one of the inputs.\n |      \n |      Parameters\n |      ----------\n |      other : Series or scalar value\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level.\n |      fill_value : None or float value, default None (NaN)\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful Series alignment, with this value before computation.\n |          If data in both corresponding Series locations is missing\n |          the result of filling (at that location) will be missing.\n |      axis : {0 or 'index'}\n |          Unused. Parameter needed for compatibility with DataFrame.\n |      \n |      Returns\n |      -------\n |      Series\n |          The result of the operation.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; a = pd.Series([1, 1, 1, np.nan, 1], index=['a', 'b', 'c', 'd', 'e'])\n |      &gt;&gt;&gt; a\n |      a    1.0\n |      b    1.0\n |      c    1.0\n |      d    NaN\n |      e    1.0\n |      dtype: float64\n |      &gt;&gt;&gt; b = pd.Series([0, 1, 2, np.nan, 1], index=['a', 'b', 'c', 'd', 'f'])\n |      &gt;&gt;&gt; b\n |      a    0.0\n |      b    1.0\n |      c    2.0\n |      d    NaN\n |      f    1.0\n |      dtype: float64\n |      &gt;&gt;&gt; a.ge(b, fill_value=0)\n |      a     True\n |      b     True\n |      c    False\n |      d    False\n |      e     True\n |      f    False\n |      dtype: bool\n |  \n |  groupby(self, by=None, axis: 'Axis' = 0, level: 'IndexLabel' = None, as_index: 'bool' = True, sort: 'bool' = True, group_keys: 'bool' = True, observed: 'bool' = False, dropna: 'bool' = True) -&gt; 'SeriesGroupBy'\n |      Group Series using a mapper or by a Series of columns.\n |      \n |      A groupby operation involves some combination of splitting the\n |      object, applying a function, and combining the results. This can be\n |      used to group large amounts of data and compute operations on these\n |      groups.\n |      \n |      Parameters\n |      ----------\n |      by : mapping, function, label, pd.Grouper or list of such\n |          Used to determine the groups for the groupby.\n |          If ``by`` is a function, it's called on each value of the object's\n |          index. If a dict or Series is passed, the Series or dict VALUES\n |          will be used to determine the groups (the Series' values are first\n |          aligned; see ``.align()`` method). If a list or ndarray of length\n |          equal to the selected axis is passed (see the `groupby user guide\n |          &lt;https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#splitting-an-object-into-groups&gt;`_),\n |          the values are used as-is to determine the groups. A label or list\n |          of labels may be passed to group by the columns in ``self``.\n |          Notice that a tuple is interpreted as a (single) key.\n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |          Split along rows (0) or columns (1). For `Series` this parameter\n |          is unused and defaults to 0.\n |      level : int, level name, or sequence of such, default None\n |          If the axis is a MultiIndex (hierarchical), group by a particular\n |          level or levels. Do not specify both ``by`` and ``level``.\n |      as_index : bool, default True\n |          For aggregated output, return object with group labels as the\n |          index. Only relevant for DataFrame input. as_index=False is\n |          effectively \"SQL-style\" grouped output.\n |      sort : bool, default True\n |          Sort group keys. Get better performance by turning this off.\n |          Note this does not influence the order of observations within each\n |          group. Groupby preserves the order of rows within each group.\n |      \n |          .. versionchanged:: 2.0.0\n |      \n |              Specifying ``sort=False`` with an ordered categorical grouper will no\n |              longer sort the values.\n |      \n |      group_keys : bool, default True\n |          When calling apply and the ``by`` argument produces a like-indexed\n |          (i.e. :ref:`a transform &lt;groupby.transform&gt;`) result, add group keys to\n |          index to identify pieces. By default group keys are not included\n |          when the result's index (and column) labels match the inputs, and\n |          are included otherwise.\n |      \n |          .. versionchanged:: 1.5.0\n |      \n |             Warns that ``group_keys`` will no longer be ignored when the\n |             result from ``apply`` is a like-indexed Series or DataFrame.\n |             Specify ``group_keys`` explicitly to include the group keys or\n |             not.\n |      \n |          .. versionchanged:: 2.0.0\n |      \n |             ``group_keys`` now defaults to ``True``.\n |      \n |      observed : bool, default False\n |          This only applies if any of the groupers are Categoricals.\n |          If True: only show observed values for categorical groupers.\n |          If False: show all values for categorical groupers.\n |      dropna : bool, default True\n |          If True, and if group keys contain NA values, NA values together\n |          with row/column will be dropped.\n |          If False, NA values will also be treated as the key in groups.\n |      \n |          .. versionadded:: 1.1.0\n |      \n |      Returns\n |      -------\n |      SeriesGroupBy\n |          Returns a groupby object that contains information about the groups.\n |      \n |      See Also\n |      --------\n |      resample : Convenience method for frequency conversion and resampling\n |          of time series.\n |      \n |      Notes\n |      -----\n |      See the `user guide\n |      &lt;https://pandas.pydata.org/pandas-docs/stable/groupby.html&gt;`__ for more\n |      detailed usage and examples, including splitting an object into groups,\n |      iterating through groups, selecting a group, aggregation, and more.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; ser = pd.Series([390., 350., 30., 20.],\n |      ...                 index=['Falcon', 'Falcon', 'Parrot', 'Parrot'], name=\"Max Speed\")\n |      &gt;&gt;&gt; ser\n |      Falcon    390.0\n |      Falcon    350.0\n |      Parrot     30.0\n |      Parrot     20.0\n |      Name: Max Speed, dtype: float64\n |      &gt;&gt;&gt; ser.groupby([\"a\", \"b\", \"a\", \"b\"]).mean()\n |      a    210.0\n |      b    185.0\n |      Name: Max Speed, dtype: float64\n |      &gt;&gt;&gt; ser.groupby(level=0).mean()\n |      Falcon    370.0\n |      Parrot     25.0\n |      Name: Max Speed, dtype: float64\n |      &gt;&gt;&gt; ser.groupby(ser &gt; 100).mean()\n |      Max Speed\n |      False     25.0\n |      True     370.0\n |      Name: Max Speed, dtype: float64\n |      \n |      **Grouping by Indexes**\n |      \n |      We can groupby different levels of a hierarchical index\n |      using the `level` parameter:\n |      \n |      &gt;&gt;&gt; arrays = [['Falcon', 'Falcon', 'Parrot', 'Parrot'],\n |      ...           ['Captive', 'Wild', 'Captive', 'Wild']]\n |      &gt;&gt;&gt; index = pd.MultiIndex.from_arrays(arrays, names=('Animal', 'Type'))\n |      &gt;&gt;&gt; ser = pd.Series([390., 350., 30., 20.], index=index, name=\"Max Speed\")\n |      &gt;&gt;&gt; ser\n |      Animal  Type\n |      Falcon  Captive    390.0\n |              Wild       350.0\n |      Parrot  Captive     30.0\n |              Wild        20.0\n |      Name: Max Speed, dtype: float64\n |      &gt;&gt;&gt; ser.groupby(level=0).mean()\n |      Animal\n |      Falcon    370.0\n |      Parrot     25.0\n |      Name: Max Speed, dtype: float64\n |      &gt;&gt;&gt; ser.groupby(level=\"Type\").mean()\n |      Type\n |      Captive    210.0\n |      Wild       185.0\n |      Name: Max Speed, dtype: float64\n |      \n |      We can also choose to include `NA` in group keys or not by defining\n |      `dropna` parameter, the default setting is `True`.\n |      \n |      &gt;&gt;&gt; ser = pd.Series([1, 2, 3, 3], index=[\"a\", 'a', 'b', np.nan])\n |      &gt;&gt;&gt; ser.groupby(level=0).sum()\n |      a    3\n |      b    3\n |      dtype: int64\n |      \n |      &gt;&gt;&gt; ser.groupby(level=0, dropna=False).sum()\n |      a    3\n |      b    3\n |      NaN  3\n |      dtype: int64\n |      \n |      &gt;&gt;&gt; arrays = ['Falcon', 'Falcon', 'Parrot', 'Parrot']\n |      &gt;&gt;&gt; ser = pd.Series([390., 350., 30., 20.], index=arrays, name=\"Max Speed\")\n |      &gt;&gt;&gt; ser.groupby([\"a\", \"b\", \"a\", np.nan]).mean()\n |      a    210.0\n |      b    350.0\n |      Name: Max Speed, dtype: float64\n |      \n |      &gt;&gt;&gt; ser.groupby([\"a\", \"b\", \"a\", np.nan], dropna=False).mean()\n |      a    210.0\n |      b    350.0\n |      NaN   20.0\n |      Name: Max Speed, dtype: float64\n |  \n |  gt(self, other, level=None, fill_value=None, axis: 'Axis' = 0)\n |      Return Greater than of series and other, element-wise (binary operator `gt`).\n |      \n |      Equivalent to ``series &gt; other``, but with support to substitute a fill_value for\n |      missing data in either one of the inputs.\n |      \n |      Parameters\n |      ----------\n |      other : Series or scalar value\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level.\n |      fill_value : None or float value, default None (NaN)\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful Series alignment, with this value before computation.\n |          If data in both corresponding Series locations is missing\n |          the result of filling (at that location) will be missing.\n |      axis : {0 or 'index'}\n |          Unused. Parameter needed for compatibility with DataFrame.\n |      \n |      Returns\n |      -------\n |      Series\n |          The result of the operation.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; a = pd.Series([1, 1, 1, np.nan, 1], index=['a', 'b', 'c', 'd', 'e'])\n |      &gt;&gt;&gt; a\n |      a    1.0\n |      b    1.0\n |      c    1.0\n |      d    NaN\n |      e    1.0\n |      dtype: float64\n |      &gt;&gt;&gt; b = pd.Series([0, 1, 2, np.nan, 1], index=['a', 'b', 'c', 'd', 'f'])\n |      &gt;&gt;&gt; b\n |      a    0.0\n |      b    1.0\n |      c    2.0\n |      d    NaN\n |      f    1.0\n |      dtype: float64\n |      &gt;&gt;&gt; a.gt(b, fill_value=0)\n |      a     True\n |      b    False\n |      c    False\n |      d    False\n |      e     True\n |      f    False\n |      dtype: bool\n |  \n |  hist = hist_series(self, by=None, ax=None, grid: 'bool' = True, xlabelsize: 'int | None' = None, xrot: 'float | None' = None, ylabelsize: 'int | None' = None, yrot: 'float | None' = None, figsize: 'tuple[int, int] | None' = None, bins: 'int | Sequence[int]' = 10, backend: 'str | None' = None, legend: 'bool' = False, **kwargs)\n |      Draw histogram of the input series using matplotlib.\n |      \n |      Parameters\n |      ----------\n |      by : object, optional\n |          If passed, then used to form histograms for separate groups.\n |      ax : matplotlib axis object\n |          If not passed, uses gca().\n |      grid : bool, default True\n |          Whether to show axis grid lines.\n |      xlabelsize : int, default None\n |          If specified changes the x-axis label size.\n |      xrot : float, default None\n |          Rotation of x axis labels.\n |      ylabelsize : int, default None\n |          If specified changes the y-axis label size.\n |      yrot : float, default None\n |          Rotation of y axis labels.\n |      figsize : tuple, default None\n |          Figure size in inches by default.\n |      bins : int or sequence, default 10\n |          Number of histogram bins to be used. If an integer is given, bins + 1\n |          bin edges are calculated and returned. If bins is a sequence, gives\n |          bin edges, including left edge of first bin and right edge of last\n |          bin. In this case, bins is returned unmodified.\n |      backend : str, default None\n |          Backend to use instead of the backend specified in the option\n |          ``plotting.backend``. For instance, 'matplotlib'. Alternatively, to\n |          specify the ``plotting.backend`` for the whole session, set\n |          ``pd.options.plotting.backend``.\n |      legend : bool, default False\n |          Whether to show the legend.\n |      \n |          .. versionadded:: 1.1.0\n |      \n |      **kwargs\n |          To be passed to the actual plotting function.\n |      \n |      Returns\n |      -------\n |      matplotlib.AxesSubplot\n |          A histogram plot.\n |      \n |      See Also\n |      --------\n |      matplotlib.axes.Axes.hist : Plot a histogram using matplotlib.\n |  \n |  idxmax(self, axis: 'Axis' = 0, skipna: 'bool' = True, *args, **kwargs) -&gt; 'Hashable'\n |      Return the row label of the maximum value.\n |      \n |      If multiple values equal the maximum, the first row label with that\n |      value is returned.\n |      \n |      Parameters\n |      ----------\n |      axis : {0 or 'index'}\n |          Unused. Parameter needed for compatibility with DataFrame.\n |      skipna : bool, default True\n |          Exclude NA/null values. If the entire Series is NA, the result\n |          will be NA.\n |      *args, **kwargs\n |          Additional arguments and keywords have no effect but might be\n |          accepted for compatibility with NumPy.\n |      \n |      Returns\n |      -------\n |      Index\n |          Label of the maximum value.\n |      \n |      Raises\n |      ------\n |      ValueError\n |          If the Series is empty.\n |      \n |      See Also\n |      --------\n |      numpy.argmax : Return indices of the maximum values\n |          along the given axis.\n |      DataFrame.idxmax : Return index of first occurrence of maximum\n |          over requested axis.\n |      Series.idxmin : Return index *label* of the first occurrence\n |          of minimum of values.\n |      \n |      Notes\n |      -----\n |      This method is the Series version of ``ndarray.argmax``. This method\n |      returns the label of the maximum, while ``ndarray.argmax`` returns\n |      the position. To get the position, use ``series.values.argmax()``.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; s = pd.Series(data=[1, None, 4, 3, 4],\n |      ...               index=['A', 'B', 'C', 'D', 'E'])\n |      &gt;&gt;&gt; s\n |      A    1.0\n |      B    NaN\n |      C    4.0\n |      D    3.0\n |      E    4.0\n |      dtype: float64\n |      \n |      &gt;&gt;&gt; s.idxmax()\n |      'C'\n |      \n |      If `skipna` is False and there is an NA value in the data,\n |      the function returns ``nan``.\n |      \n |      &gt;&gt;&gt; s.idxmax(skipna=False)\n |      nan\n |  \n |  idxmin(self, axis: 'Axis' = 0, skipna: 'bool' = True, *args, **kwargs) -&gt; 'Hashable'\n |      Return the row label of the minimum value.\n |      \n |      If multiple values equal the minimum, the first row label with that\n |      value is returned.\n |      \n |      Parameters\n |      ----------\n |      axis : {0 or 'index'}\n |          Unused. Parameter needed for compatibility with DataFrame.\n |      skipna : bool, default True\n |          Exclude NA/null values. If the entire Series is NA, the result\n |          will be NA.\n |      *args, **kwargs\n |          Additional arguments and keywords have no effect but might be\n |          accepted for compatibility with NumPy.\n |      \n |      Returns\n |      -------\n |      Index\n |          Label of the minimum value.\n |      \n |      Raises\n |      ------\n |      ValueError\n |          If the Series is empty.\n |      \n |      See Also\n |      --------\n |      numpy.argmin : Return indices of the minimum values\n |          along the given axis.\n |      DataFrame.idxmin : Return index of first occurrence of minimum\n |          over requested axis.\n |      Series.idxmax : Return index *label* of the first occurrence\n |          of maximum of values.\n |      \n |      Notes\n |      -----\n |      This method is the Series version of ``ndarray.argmin``. This method\n |      returns the label of the minimum, while ``ndarray.argmin`` returns\n |      the position. To get the position, use ``series.values.argmin()``.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; s = pd.Series(data=[1, None, 4, 1],\n |      ...               index=['A', 'B', 'C', 'D'])\n |      &gt;&gt;&gt; s\n |      A    1.0\n |      B    NaN\n |      C    4.0\n |      D    1.0\n |      dtype: float64\n |      \n |      &gt;&gt;&gt; s.idxmin()\n |      'A'\n |      \n |      If `skipna` is False and there is an NA value in the data,\n |      the function returns ``nan``.\n |      \n |      &gt;&gt;&gt; s.idxmin(skipna=False)\n |      nan\n |  \n |  info(self, verbose: 'bool | None' = None, buf: 'IO[str] | None' = None, max_cols: 'int | None' = None, memory_usage: 'bool | str | None' = None, show_counts: 'bool' = True) -&gt; 'None'\n |      Print a concise summary of a Series.\n |      \n |      This method prints information about a Series including\n |      the index dtype, non-null values and memory usage.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      Parameters\n |      ----------\n |      verbose : bool, optional\n |          Whether to print the full summary. By default, the setting in\n |          ``pandas.options.display.max_info_columns`` is followed.\n |      buf : writable buffer, defaults to sys.stdout\n |          Where to send the output. By default, the output is printed to\n |          sys.stdout. Pass a writable buffer if you need to further process\n |          the output.\n |      \n |      memory_usage : bool, str, optional\n |          Specifies whether total memory usage of the Series\n |          elements (including the index) should be displayed. By default,\n |          this follows the ``pandas.options.display.memory_usage`` setting.\n |      \n |          True always show memory usage. False never shows memory usage.\n |          A value of 'deep' is equivalent to \"True with deep introspection\".\n |          Memory usage is shown in human-readable units (base-2\n |          representation). Without deep introspection a memory estimation is\n |          made based in column dtype and number of rows assuming values\n |          consume the same memory amount for corresponding dtypes. With deep\n |          memory introspection, a real memory usage calculation is performed\n |          at the cost of computational resources. See the\n |          :ref:`Frequently Asked Questions &lt;df-memory-usage&gt;` for more\n |          details.\n |      show_counts : bool, optional\n |          Whether to show the non-null counts. By default, this is shown\n |          only if the DataFrame is smaller than\n |          ``pandas.options.display.max_info_rows`` and\n |          ``pandas.options.display.max_info_columns``. A value of True always\n |          shows the counts, and False never shows the counts.\n |      \n |      Returns\n |      -------\n |      None\n |          This method prints a summary of a Series and returns None.\n |      \n |      See Also\n |      --------\n |      Series.describe: Generate descriptive statistics of Series.\n |      Series.memory_usage: Memory usage of Series.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; int_values = [1, 2, 3, 4, 5]\n |      &gt;&gt;&gt; text_values = ['alpha', 'beta', 'gamma', 'delta', 'epsilon']\n |      &gt;&gt;&gt; s = pd.Series(text_values, index=int_values)\n |      &gt;&gt;&gt; s.info()\n |      &lt;class 'pandas.core.series.Series'&gt;\n |      Index: 5 entries, 1 to 5\n |      Series name: None\n |      Non-Null Count  Dtype\n |      --------------  -----\n |      5 non-null      object\n |      dtypes: object(1)\n |      memory usage: 80.0+ bytes\n |      \n |      Prints a summary excluding information about its values:\n |      \n |      &gt;&gt;&gt; s.info(verbose=False)\n |      &lt;class 'pandas.core.series.Series'&gt;\n |      Index: 5 entries, 1 to 5\n |      dtypes: object(1)\n |      memory usage: 80.0+ bytes\n |      \n |      Pipe output of Series.info to buffer instead of sys.stdout, get\n |      buffer content and writes to a text file:\n |      \n |      &gt;&gt;&gt; import io\n |      &gt;&gt;&gt; buffer = io.StringIO()\n |      &gt;&gt;&gt; s.info(buf=buffer)\n |      &gt;&gt;&gt; s = buffer.getvalue()\n |      &gt;&gt;&gt; with open(\"df_info.txt\", \"w\",\n |      ...           encoding=\"utf-8\") as f:  # doctest: +SKIP\n |      ...     f.write(s)\n |      260\n |      \n |      The `memory_usage` parameter allows deep introspection mode, specially\n |      useful for big Series and fine-tune memory optimization:\n |      \n |      &gt;&gt;&gt; random_strings_array = np.random.choice(['a', 'b', 'c'], 10 ** 6)\n |      &gt;&gt;&gt; s = pd.Series(np.random.choice(['a', 'b', 'c'], 10 ** 6))\n |      &gt;&gt;&gt; s.info()\n |      &lt;class 'pandas.core.series.Series'&gt;\n |      RangeIndex: 1000000 entries, 0 to 999999\n |      Series name: None\n |      Non-Null Count    Dtype\n |      --------------    -----\n |      1000000 non-null  object\n |      dtypes: object(1)\n |      memory usage: 7.6+ MB\n |      \n |      &gt;&gt;&gt; s.info(memory_usage='deep')\n |      &lt;class 'pandas.core.series.Series'&gt;\n |      RangeIndex: 1000000 entries, 0 to 999999\n |      Series name: None\n |      Non-Null Count    Dtype\n |      --------------    -----\n |      1000000 non-null  object\n |      dtypes: object(1)\n |      memory usage: 55.3 MB\n |  \n |  interpolate(self: 'Series', method: 'str' = 'linear', *, axis: 'Axis' = 0, limit: 'int | None' = None, inplace: 'bool' = False, limit_direction: 'str | None' = None, limit_area: 'str | None' = None, downcast: 'str | None' = None, **kwargs) -&gt; 'Series | None'\n |      Fill NaN values using an interpolation method.\n |      \n |      Please note that only ``method='linear'`` is supported for\n |      DataFrame/Series with a MultiIndex.\n |      \n |      Parameters\n |      ----------\n |      method : str, default 'linear'\n |          Interpolation technique to use. One of:\n |      \n |          * 'linear': Ignore the index and treat the values as equally\n |            spaced. This is the only method supported on MultiIndexes.\n |          * 'time': Works on daily and higher resolution data to interpolate\n |            given length of interval.\n |          * 'index', 'values': use the actual numerical values of the index.\n |          * 'pad': Fill in NaNs using existing values.\n |          * 'nearest', 'zero', 'slinear', 'quadratic', 'cubic',\n |            'barycentric', 'polynomial': Passed to\n |            `scipy.interpolate.interp1d`, whereas 'spline' is passed to\n |            `scipy.interpolate.UnivariateSpline`. These methods use the numerical\n |            values of the index.  Both 'polynomial' and 'spline' require that\n |            you also specify an `order` (int), e.g.\n |            ``df.interpolate(method='polynomial', order=5)``. Note that,\n |            `slinear` method in Pandas refers to the Scipy first order `spline`\n |            instead of Pandas first order `spline`.\n |          * 'krogh', 'piecewise_polynomial', 'spline', 'pchip', 'akima',\n |            'cubicspline': Wrappers around the SciPy interpolation methods of\n |            similar names. See `Notes`.\n |          * 'from_derivatives': Refers to\n |            `scipy.interpolate.BPoly.from_derivatives` which\n |            replaces 'piecewise_polynomial' interpolation method in\n |            scipy 0.18.\n |      \n |      axis : {{0 or 'index', 1 or 'columns', None}}, default None\n |          Axis to interpolate along. For `Series` this parameter is unused\n |          and defaults to 0.\n |      limit : int, optional\n |          Maximum number of consecutive NaNs to fill. Must be greater than\n |          0.\n |      inplace : bool, default False\n |          Update the data in place if possible.\n |      limit_direction : {{'forward', 'backward', 'both'}}, Optional\n |          Consecutive NaNs will be filled in this direction.\n |      \n |          If limit is specified:\n |              * If 'method' is 'pad' or 'ffill', 'limit_direction' must be 'forward'.\n |              * If 'method' is 'backfill' or 'bfill', 'limit_direction' must be\n |                'backwards'.\n |      \n |          If 'limit' is not specified:\n |              * If 'method' is 'backfill' or 'bfill', the default is 'backward'\n |              * else the default is 'forward'\n |      \n |          .. versionchanged:: 1.1.0\n |              raises ValueError if `limit_direction` is 'forward' or 'both' and\n |                  method is 'backfill' or 'bfill'.\n |              raises ValueError if `limit_direction` is 'backward' or 'both' and\n |                  method is 'pad' or 'ffill'.\n |      \n |      limit_area : {{`None`, 'inside', 'outside'}}, default None\n |          If limit is specified, consecutive NaNs will be filled with this\n |          restriction.\n |      \n |          * ``None``: No fill restriction.\n |          * 'inside': Only fill NaNs surrounded by valid values\n |            (interpolate).\n |          * 'outside': Only fill NaNs outside valid values (extrapolate).\n |      \n |      downcast : optional, 'infer' or None, defaults to None\n |          Downcast dtypes if possible.\n |      ``**kwargs`` : optional\n |          Keyword arguments to pass on to the interpolating function.\n |      \n |      Returns\n |      -------\n |      Series or DataFrame or None\n |          Returns the same object type as the caller, interpolated at\n |          some or all ``NaN`` values or None if ``inplace=True``.\n |      \n |      See Also\n |      --------\n |      fillna : Fill missing values using different methods.\n |      scipy.interpolate.Akima1DInterpolator : Piecewise cubic polynomials\n |          (Akima interpolator).\n |      scipy.interpolate.BPoly.from_derivatives : Piecewise polynomial in the\n |          Bernstein basis.\n |      scipy.interpolate.interp1d : Interpolate a 1-D function.\n |      scipy.interpolate.KroghInterpolator : Interpolate polynomial (Krogh\n |          interpolator).\n |      scipy.interpolate.PchipInterpolator : PCHIP 1-d monotonic cubic\n |          interpolation.\n |      scipy.interpolate.CubicSpline : Cubic spline data interpolator.\n |      \n |      Notes\n |      -----\n |      The 'krogh', 'piecewise_polynomial', 'spline', 'pchip' and 'akima'\n |      methods are wrappers around the respective SciPy implementations of\n |      similar names. These use the actual numerical values of the index.\n |      For more information on their behavior, see the\n |      `SciPy documentation\n |      &lt;https://docs.scipy.org/doc/scipy/reference/interpolate.html#univariate-interpolation&gt;`__.\n |      \n |      Examples\n |      --------\n |      Filling in ``NaN`` in a :class:`~pandas.Series` via linear\n |      interpolation.\n |      \n |      &gt;&gt;&gt; s = pd.Series([0, 1, np.nan, 3])\n |      &gt;&gt;&gt; s\n |      0    0.0\n |      1    1.0\n |      2    NaN\n |      3    3.0\n |      dtype: float64\n |      &gt;&gt;&gt; s.interpolate()\n |      0    0.0\n |      1    1.0\n |      2    2.0\n |      3    3.0\n |      dtype: float64\n |      \n |      Filling in ``NaN`` in a Series by padding, but filling at most two\n |      consecutive ``NaN`` at a time.\n |      \n |      &gt;&gt;&gt; s = pd.Series([np.nan, \"single_one\", np.nan,\n |      ...                \"fill_two_more\", np.nan, np.nan, np.nan,\n |      ...                4.71, np.nan])\n |      &gt;&gt;&gt; s\n |      0              NaN\n |      1       single_one\n |      2              NaN\n |      3    fill_two_more\n |      4              NaN\n |      5              NaN\n |      6              NaN\n |      7             4.71\n |      8              NaN\n |      dtype: object\n |      &gt;&gt;&gt; s.interpolate(method='pad', limit=2)\n |      0              NaN\n |      1       single_one\n |      2       single_one\n |      3    fill_two_more\n |      4    fill_two_more\n |      5    fill_two_more\n |      6              NaN\n |      7             4.71\n |      8             4.71\n |      dtype: object\n |      \n |      Filling in ``NaN`` in a Series via polynomial interpolation or splines:\n |      Both 'polynomial' and 'spline' methods require that you also specify\n |      an ``order`` (int).\n |      \n |      &gt;&gt;&gt; s = pd.Series([0, 2, np.nan, 8])\n |      &gt;&gt;&gt; s.interpolate(method='polynomial', order=2)\n |      0    0.000000\n |      1    2.000000\n |      2    4.666667\n |      3    8.000000\n |      dtype: float64\n |      \n |      Fill the DataFrame forward (that is, going down) along each column\n |      using linear interpolation.\n |      \n |      Note how the last entry in column 'a' is interpolated differently,\n |      because there is no entry after it to use for interpolation.\n |      Note how the first entry in column 'b' remains ``NaN``, because there\n |      is no entry before it to use for interpolation.\n |      \n |      &gt;&gt;&gt; df = pd.DataFrame([(0.0, np.nan, -1.0, 1.0),\n |      ...                    (np.nan, 2.0, np.nan, np.nan),\n |      ...                    (2.0, 3.0, np.nan, 9.0),\n |      ...                    (np.nan, 4.0, -4.0, 16.0)],\n |      ...                   columns=list('abcd'))\n |      &gt;&gt;&gt; df\n |           a    b    c     d\n |      0  0.0  NaN -1.0   1.0\n |      1  NaN  2.0  NaN   NaN\n |      2  2.0  3.0  NaN   9.0\n |      3  NaN  4.0 -4.0  16.0\n |      &gt;&gt;&gt; df.interpolate(method='linear', limit_direction='forward', axis=0)\n |           a    b    c     d\n |      0  0.0  NaN -1.0   1.0\n |      1  1.0  2.0 -2.0   5.0\n |      2  2.0  3.0 -3.0   9.0\n |      3  2.0  4.0 -4.0  16.0\n |      \n |      Using polynomial interpolation.\n |      \n |      &gt;&gt;&gt; df['d'].interpolate(method='polynomial', order=2)\n |      0     1.0\n |      1     4.0\n |      2     9.0\n |      3    16.0\n |      Name: d, dtype: float64\n |  \n |  isin(self, values) -&gt; 'Series'\n |      Whether elements in Series are contained in `values`.\n |      \n |      Return a boolean Series showing whether each element in the Series\n |      matches an element in the passed sequence of `values` exactly.\n |      \n |      Parameters\n |      ----------\n |      values : set or list-like\n |          The sequence of values to test. Passing in a single string will\n |          raise a ``TypeError``. Instead, turn a single string into a\n |          list of one element.\n |      \n |      Returns\n |      -------\n |      Series\n |          Series of booleans indicating if each element is in values.\n |      \n |      Raises\n |      ------\n |      TypeError\n |        * If `values` is a string\n |      \n |      See Also\n |      --------\n |      DataFrame.isin : Equivalent method on DataFrame.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; s = pd.Series(['lama', 'cow', 'lama', 'beetle', 'lama',\n |      ...                'hippo'], name='animal')\n |      &gt;&gt;&gt; s.isin(['cow', 'lama'])\n |      0     True\n |      1     True\n |      2     True\n |      3    False\n |      4     True\n |      5    False\n |      Name: animal, dtype: bool\n |      \n |      To invert the boolean values, use the ``~`` operator:\n |      \n |      &gt;&gt;&gt; ~s.isin(['cow', 'lama'])\n |      0    False\n |      1    False\n |      2    False\n |      3     True\n |      4    False\n |      5     True\n |      Name: animal, dtype: bool\n |      \n |      Passing a single string as ``s.isin('lama')`` will raise an error. Use\n |      a list of one element instead:\n |      \n |      &gt;&gt;&gt; s.isin(['lama'])\n |      0     True\n |      1    False\n |      2     True\n |      3    False\n |      4     True\n |      5    False\n |      Name: animal, dtype: bool\n |      \n |      Strings and integers are distinct and are therefore not comparable:\n |      \n |      &gt;&gt;&gt; pd.Series([1]).isin(['1'])\n |      0    False\n |      dtype: bool\n |      &gt;&gt;&gt; pd.Series([1.1]).isin(['1.1'])\n |      0    False\n |      dtype: bool\n |  \n |  isna(self) -&gt; 'Series'\n |      Detect missing values.\n |      \n |      Return a boolean same-sized object indicating if the values are NA.\n |      NA values, such as None or :attr:`numpy.NaN`, gets mapped to True\n |      values.\n |      Everything else gets mapped to False values. Characters such as empty\n |      strings ``''`` or :attr:`numpy.inf` are not considered NA values\n |      (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n |      \n |      Returns\n |      -------\n |      Series\n |          Mask of bool values for each element in Series that\n |          indicates whether an element is an NA value.\n |      \n |      See Also\n |      --------\n |      Series.isnull : Alias of isna.\n |      Series.notna : Boolean inverse of isna.\n |      Series.dropna : Omit axes labels with missing values.\n |      isna : Top-level isna.\n |      \n |      Examples\n |      --------\n |      Show which entries in a DataFrame are NA.\n |      \n |      &gt;&gt;&gt; df = pd.DataFrame(dict(age=[5, 6, np.NaN],\n |      ...                        born=[pd.NaT, pd.Timestamp('1939-05-27'),\n |      ...                              pd.Timestamp('1940-04-25')],\n |      ...                        name=['Alfred', 'Batman', ''],\n |      ...                        toy=[None, 'Batmobile', 'Joker']))\n |      &gt;&gt;&gt; df\n |         age       born    name        toy\n |      0  5.0        NaT  Alfred       None\n |      1  6.0 1939-05-27  Batman  Batmobile\n |      2  NaN 1940-04-25              Joker\n |      \n |      &gt;&gt;&gt; df.isna()\n |           age   born   name    toy\n |      0  False   True  False   True\n |      1  False  False  False  False\n |      2   True  False  False  False\n |      \n |      Show which entries in a Series are NA.\n |      \n |      &gt;&gt;&gt; ser = pd.Series([5, 6, np.NaN])\n |      &gt;&gt;&gt; ser\n |      0    5.0\n |      1    6.0\n |      2    NaN\n |      dtype: float64\n |      \n |      &gt;&gt;&gt; ser.isna()\n |      0    False\n |      1    False\n |      2     True\n |      dtype: bool\n |  \n |  isnull(self) -&gt; 'Series'\n |      Series.isnull is an alias for Series.isna.\n |      \n |      Detect missing values.\n |      \n |      Return a boolean same-sized object indicating if the values are NA.\n |      NA values, such as None or :attr:`numpy.NaN`, gets mapped to True\n |      values.\n |      Everything else gets mapped to False values. Characters such as empty\n |      strings ``''`` or :attr:`numpy.inf` are not considered NA values\n |      (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n |      \n |      Returns\n |      -------\n |      Series\n |          Mask of bool values for each element in Series that\n |          indicates whether an element is an NA value.\n |      \n |      See Also\n |      --------\n |      Series.isnull : Alias of isna.\n |      Series.notna : Boolean inverse of isna.\n |      Series.dropna : Omit axes labels with missing values.\n |      isna : Top-level isna.\n |      \n |      Examples\n |      --------\n |      Show which entries in a DataFrame are NA.\n |      \n |      &gt;&gt;&gt; df = pd.DataFrame(dict(age=[5, 6, np.NaN],\n |      ...                        born=[pd.NaT, pd.Timestamp('1939-05-27'),\n |      ...                              pd.Timestamp('1940-04-25')],\n |      ...                        name=['Alfred', 'Batman', ''],\n |      ...                        toy=[None, 'Batmobile', 'Joker']))\n |      &gt;&gt;&gt; df\n |         age       born    name        toy\n |      0  5.0        NaT  Alfred       None\n |      1  6.0 1939-05-27  Batman  Batmobile\n |      2  NaN 1940-04-25              Joker\n |      \n |      &gt;&gt;&gt; df.isna()\n |           age   born   name    toy\n |      0  False   True  False   True\n |      1  False  False  False  False\n |      2   True  False  False  False\n |      \n |      Show which entries in a Series are NA.\n |      \n |      &gt;&gt;&gt; ser = pd.Series([5, 6, np.NaN])\n |      &gt;&gt;&gt; ser\n |      0    5.0\n |      1    6.0\n |      2    NaN\n |      dtype: float64\n |      \n |      &gt;&gt;&gt; ser.isna()\n |      0    False\n |      1    False\n |      2     True\n |      dtype: bool\n |  \n |  items(self) -&gt; 'Iterable[tuple[Hashable, Any]]'\n |      Lazily iterate over (index, value) tuples.\n |      \n |      This method returns an iterable tuple (index, value). This is\n |      convenient if you want to create a lazy iterator.\n |      \n |      Returns\n |      -------\n |      iterable\n |          Iterable of tuples containing the (index, value) pairs from a\n |          Series.\n |      \n |      See Also\n |      --------\n |      DataFrame.items : Iterate over (column name, Series) pairs.\n |      DataFrame.iterrows : Iterate over DataFrame rows as (index, Series) pairs.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; s = pd.Series(['A', 'B', 'C'])\n |      &gt;&gt;&gt; for index, value in s.items():\n |      ...     print(f\"Index : {index}, Value : {value}\")\n |      Index : 0, Value : A\n |      Index : 1, Value : B\n |      Index : 2, Value : C\n |  \n |  keys(self) -&gt; 'Index'\n |      Return alias for index.\n |      \n |      Returns\n |      -------\n |      Index\n |          Index of the Series.\n |  \n |  kurt(self, axis: 'Axis | None' = 0, skipna: 'bool_t' = True, numeric_only: 'bool_t' = False, **kwargs)\n |      Return unbiased kurtosis over requested axis.\n |      \n |      Kurtosis obtained using Fisher's definition of\n |      kurtosis (kurtosis of normal == 0.0). Normalized by N-1.\n |      \n |      Parameters\n |      ----------\n |      axis : {index (0)}\n |          Axis for the function to be applied on.\n |          For `Series` this parameter is unused and defaults to 0.\n |      \n |          For DataFrames, specifying ``axis=None`` will apply the aggregation\n |          across both axes.\n |      \n |          .. versionadded:: 2.0.0\n |      \n |      skipna : bool, default True\n |          Exclude NA/null values when computing the result.\n |      numeric_only : bool, default False\n |          Include only float, int, boolean columns. Not implemented for Series.\n |      \n |      **kwargs\n |          Additional keyword arguments to be passed to the function.\n |      \n |      Returns\n |      -------\n |      scalar or scalar\n |  \n |  kurtosis = kurt(self, axis: 'Axis | None' = 0, skipna: 'bool_t' = True, numeric_only: 'bool_t' = False, **kwargs)\n |  \n |  le(self, other, level=None, fill_value=None, axis: 'Axis' = 0)\n |      Return Less than or equal to of series and other, element-wise (binary operator `le`).\n |      \n |      Equivalent to ``series &lt;= other``, but with support to substitute a fill_value for\n |      missing data in either one of the inputs.\n |      \n |      Parameters\n |      ----------\n |      other : Series or scalar value\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level.\n |      fill_value : None or float value, default None (NaN)\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful Series alignment, with this value before computation.\n |          If data in both corresponding Series locations is missing\n |          the result of filling (at that location) will be missing.\n |      axis : {0 or 'index'}\n |          Unused. Parameter needed for compatibility with DataFrame.\n |      \n |      Returns\n |      -------\n |      Series\n |          The result of the operation.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; a = pd.Series([1, 1, 1, np.nan, 1], index=['a', 'b', 'c', 'd', 'e'])\n |      &gt;&gt;&gt; a\n |      a    1.0\n |      b    1.0\n |      c    1.0\n |      d    NaN\n |      e    1.0\n |      dtype: float64\n |      &gt;&gt;&gt; b = pd.Series([0, 1, 2, np.nan, 1], index=['a', 'b', 'c', 'd', 'f'])\n |      &gt;&gt;&gt; b\n |      a    0.0\n |      b    1.0\n |      c    2.0\n |      d    NaN\n |      f    1.0\n |      dtype: float64\n |      &gt;&gt;&gt; a.le(b, fill_value=0)\n |      a    False\n |      b     True\n |      c     True\n |      d    False\n |      e    False\n |      f     True\n |      dtype: bool\n |  \n |  lt(self, other, level=None, fill_value=None, axis: 'Axis' = 0)\n |      Return Less than of series and other, element-wise (binary operator `lt`).\n |      \n |      Equivalent to ``series &lt; other``, but with support to substitute a fill_value for\n |      missing data in either one of the inputs.\n |      \n |      Parameters\n |      ----------\n |      other : Series or scalar value\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level.\n |      fill_value : None or float value, default None (NaN)\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful Series alignment, with this value before computation.\n |          If data in both corresponding Series locations is missing\n |          the result of filling (at that location) will be missing.\n |      axis : {0 or 'index'}\n |          Unused. Parameter needed for compatibility with DataFrame.\n |      \n |      Returns\n |      -------\n |      Series\n |          The result of the operation.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; a = pd.Series([1, 1, 1, np.nan, 1], index=['a', 'b', 'c', 'd', 'e'])\n |      &gt;&gt;&gt; a\n |      a    1.0\n |      b    1.0\n |      c    1.0\n |      d    NaN\n |      e    1.0\n |      dtype: float64\n |      &gt;&gt;&gt; b = pd.Series([0, 1, 2, np.nan, 1], index=['a', 'b', 'c', 'd', 'f'])\n |      &gt;&gt;&gt; b\n |      a    0.0\n |      b    1.0\n |      c    2.0\n |      d    NaN\n |      f    1.0\n |      dtype: float64\n |      &gt;&gt;&gt; a.lt(b, fill_value=0)\n |      a    False\n |      b    False\n |      c     True\n |      d    False\n |      e    False\n |      f     True\n |      dtype: bool\n |  \n |  map(self, arg: 'Callable | Mapping | Series', na_action: \"Literal['ignore'] | None\" = None) -&gt; 'Series'\n |      Map values of Series according to an input mapping or function.\n |      \n |      Used for substituting each value in a Series with another value,\n |      that may be derived from a function, a ``dict`` or\n |      a :class:`Series`.\n |      \n |      Parameters\n |      ----------\n |      arg : function, collections.abc.Mapping subclass or Series\n |          Mapping correspondence.\n |      na_action : {None, 'ignore'}, default None\n |          If 'ignore', propagate NaN values, without passing them to the\n |          mapping correspondence.\n |      \n |      Returns\n |      -------\n |      Series\n |          Same index as caller.\n |      \n |      See Also\n |      --------\n |      Series.apply : For applying more complex functions on a Series.\n |      DataFrame.apply : Apply a function row-/column-wise.\n |      DataFrame.applymap : Apply a function elementwise on a whole DataFrame.\n |      \n |      Notes\n |      -----\n |      When ``arg`` is a dictionary, values in Series that are not in the\n |      dictionary (as keys) are converted to ``NaN``. However, if the\n |      dictionary is a ``dict`` subclass that defines ``__missing__`` (i.e.\n |      provides a method for default values), then this default is used\n |      rather than ``NaN``.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; s = pd.Series(['cat', 'dog', np.nan, 'rabbit'])\n |      &gt;&gt;&gt; s\n |      0      cat\n |      1      dog\n |      2      NaN\n |      3   rabbit\n |      dtype: object\n |      \n |      ``map`` accepts a ``dict`` or a ``Series``. Values that are not found\n |      in the ``dict`` are converted to ``NaN``, unless the dict has a default\n |      value (e.g. ``defaultdict``):\n |      \n |      &gt;&gt;&gt; s.map({'cat': 'kitten', 'dog': 'puppy'})\n |      0   kitten\n |      1    puppy\n |      2      NaN\n |      3      NaN\n |      dtype: object\n |      \n |      It also accepts a function:\n |      \n |      &gt;&gt;&gt; s.map('I am a {}'.format)\n |      0       I am a cat\n |      1       I am a dog\n |      2       I am a nan\n |      3    I am a rabbit\n |      dtype: object\n |      \n |      To avoid applying the function to missing values (and keep them as\n |      ``NaN``) ``na_action='ignore'`` can be used:\n |      \n |      &gt;&gt;&gt; s.map('I am a {}'.format, na_action='ignore')\n |      0     I am a cat\n |      1     I am a dog\n |      2            NaN\n |      3  I am a rabbit\n |      dtype: object\n |  \n |  mask(self, cond, other=&lt;no_default&gt;, *, inplace: 'bool' = False, axis: 'Axis | None' = None, level: 'Level' = None) -&gt; 'Series | None'\n |      Replace values where the condition is True.\n |      \n |      Parameters\n |      ----------\n |      cond : bool Series/DataFrame, array-like, or callable\n |          Where `cond` is False, keep the original value. Where\n |          True, replace with corresponding value from `other`.\n |          If `cond` is callable, it is computed on the Series/DataFrame and\n |          should return boolean Series/DataFrame or array. The callable must\n |          not change input Series/DataFrame (though pandas doesn't check it).\n |      other : scalar, Series/DataFrame, or callable\n |          Entries where `cond` is True are replaced with\n |          corresponding value from `other`.\n |          If other is callable, it is computed on the Series/DataFrame and\n |          should return scalar or Series/DataFrame. The callable must not\n |          change input Series/DataFrame (though pandas doesn't check it).\n |          If not specified, entries will be filled with the corresponding\n |          NULL value (``np.nan`` for numpy dtypes, ``pd.NA`` for extension\n |          dtypes).\n |      inplace : bool, default False\n |          Whether to perform the operation in place on the data.\n |      axis : int, default None\n |          Alignment axis if needed. For `Series` this parameter is\n |          unused and defaults to 0.\n |      level : int, default None\n |          Alignment level if needed.\n |      \n |      Returns\n |      -------\n |      Same type as caller or None if ``inplace=True``.\n |      \n |      See Also\n |      --------\n |      :func:`DataFrame.where` : Return an object of same shape as\n |          self.\n |      \n |      Notes\n |      -----\n |      The mask method is an application of the if-then idiom. For each\n |      element in the calling DataFrame, if ``cond`` is ``False`` the\n |      element is used; otherwise the corresponding element from the DataFrame\n |      ``other`` is used. If the axis of ``other`` does not align with axis of\n |      ``cond`` Series/DataFrame, the misaligned index positions will be filled with\n |      True.\n |      \n |      The signature for :func:`DataFrame.where` differs from\n |      :func:`numpy.where`. Roughly ``df1.where(m, df2)`` is equivalent to\n |      ``np.where(m, df1, df2)``.\n |      \n |      For further details and examples see the ``mask`` documentation in\n |      :ref:`indexing &lt;indexing.where_mask&gt;`.\n |      \n |      The dtype of the object takes precedence. The fill value is casted to\n |      the object's dtype, if this can be done losslessly.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; s = pd.Series(range(5))\n |      &gt;&gt;&gt; s.where(s &gt; 0)\n |      0    NaN\n |      1    1.0\n |      2    2.0\n |      3    3.0\n |      4    4.0\n |      dtype: float64\n |      &gt;&gt;&gt; s.mask(s &gt; 0)\n |      0    0.0\n |      1    NaN\n |      2    NaN\n |      3    NaN\n |      4    NaN\n |      dtype: float64\n |      \n |      &gt;&gt;&gt; s = pd.Series(range(5))\n |      &gt;&gt;&gt; t = pd.Series([True, False])\n |      &gt;&gt;&gt; s.where(t, 99)\n |      0     0\n |      1    99\n |      2    99\n |      3    99\n |      4    99\n |      dtype: int64\n |      &gt;&gt;&gt; s.mask(t, 99)\n |      0    99\n |      1     1\n |      2    99\n |      3    99\n |      4    99\n |      dtype: int64\n |      \n |      &gt;&gt;&gt; s.where(s &gt; 1, 10)\n |      0    10\n |      1    10\n |      2    2\n |      3    3\n |      4    4\n |      dtype: int64\n |      &gt;&gt;&gt; s.mask(s &gt; 1, 10)\n |      0     0\n |      1     1\n |      2    10\n |      3    10\n |      4    10\n |      dtype: int64\n |      \n |      &gt;&gt;&gt; df = pd.DataFrame(np.arange(10).reshape(-1, 2), columns=['A', 'B'])\n |      &gt;&gt;&gt; df\n |         A  B\n |      0  0  1\n |      1  2  3\n |      2  4  5\n |      3  6  7\n |      4  8  9\n |      &gt;&gt;&gt; m = df % 3 == 0\n |      &gt;&gt;&gt; df.where(m, -df)\n |         A  B\n |      0  0 -1\n |      1 -2  3\n |      2 -4 -5\n |      3  6 -7\n |      4 -8  9\n |      &gt;&gt;&gt; df.where(m, -df) == np.where(m, df, -df)\n |            A     B\n |      0  True  True\n |      1  True  True\n |      2  True  True\n |      3  True  True\n |      4  True  True\n |      &gt;&gt;&gt; df.where(m, -df) == df.mask(~m, -df)\n |            A     B\n |      0  True  True\n |      1  True  True\n |      2  True  True\n |      3  True  True\n |      4  True  True\n |  \n |  max(self, axis: 'AxisInt | None' = 0, skipna: 'bool_t' = True, numeric_only: 'bool_t' = False, **kwargs)\n |      Return the maximum of the values over the requested axis.\n |      \n |      If you want the *index* of the maximum, use ``idxmax``. This is the equivalent of the ``numpy.ndarray`` method ``argmax``.\n |      \n |      Parameters\n |      ----------\n |      axis : {index (0)}\n |          Axis for the function to be applied on.\n |          For `Series` this parameter is unused and defaults to 0.\n |      \n |          For DataFrames, specifying ``axis=None`` will apply the aggregation\n |          across both axes.\n |      \n |          .. versionadded:: 2.0.0\n |      \n |      skipna : bool, default True\n |          Exclude NA/null values when computing the result.\n |      numeric_only : bool, default False\n |          Include only float, int, boolean columns. Not implemented for Series.\n |      \n |      **kwargs\n |          Additional keyword arguments to be passed to the function.\n |      \n |      Returns\n |      -------\n |      scalar or scalar\n |      \n |      See Also\n |      --------\n |      Series.sum : Return the sum.\n |      Series.min : Return the minimum.\n |      Series.max : Return the maximum.\n |      Series.idxmin : Return the index of the minimum.\n |      Series.idxmax : Return the index of the maximum.\n |      DataFrame.sum : Return the sum over the requested axis.\n |      DataFrame.min : Return the minimum over the requested axis.\n |      DataFrame.max : Return the maximum over the requested axis.\n |      DataFrame.idxmin : Return the index of the minimum over the requested axis.\n |      DataFrame.idxmax : Return the index of the maximum over the requested axis.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; idx = pd.MultiIndex.from_arrays([\n |      ...     ['warm', 'warm', 'cold', 'cold'],\n |      ...     ['dog', 'falcon', 'fish', 'spider']],\n |      ...     names=['blooded', 'animal'])\n |      &gt;&gt;&gt; s = pd.Series([4, 2, 0, 8], name='legs', index=idx)\n |      &gt;&gt;&gt; s\n |      blooded  animal\n |      warm     dog       4\n |               falcon    2\n |      cold     fish      0\n |               spider    8\n |      Name: legs, dtype: int64\n |      \n |      &gt;&gt;&gt; s.max()\n |      8\n |  \n |  mean(self, axis: 'AxisInt | None' = 0, skipna: 'bool_t' = True, numeric_only: 'bool_t' = False, **kwargs)\n |      Return the mean of the values over the requested axis.\n |      \n |      Parameters\n |      ----------\n |      axis : {index (0)}\n |          Axis for the function to be applied on.\n |          For `Series` this parameter is unused and defaults to 0.\n |      \n |          For DataFrames, specifying ``axis=None`` will apply the aggregation\n |          across both axes.\n |      \n |          .. versionadded:: 2.0.0\n |      \n |      skipna : bool, default True\n |          Exclude NA/null values when computing the result.\n |      numeric_only : bool, default False\n |          Include only float, int, boolean columns. Not implemented for Series.\n |      \n |      **kwargs\n |          Additional keyword arguments to be passed to the function.\n |      \n |      Returns\n |      -------\n |      scalar or scalar\n |  \n |  median(self, axis: 'AxisInt | None' = 0, skipna: 'bool_t' = True, numeric_only: 'bool_t' = False, **kwargs)\n |      Return the median of the values over the requested axis.\n |      \n |      Parameters\n |      ----------\n |      axis : {index (0)}\n |          Axis for the function to be applied on.\n |          For `Series` this parameter is unused and defaults to 0.\n |      \n |          For DataFrames, specifying ``axis=None`` will apply the aggregation\n |          across both axes.\n |      \n |          .. versionadded:: 2.0.0\n |      \n |      skipna : bool, default True\n |          Exclude NA/null values when computing the result.\n |      numeric_only : bool, default False\n |          Include only float, int, boolean columns. Not implemented for Series.\n |      \n |      **kwargs\n |          Additional keyword arguments to be passed to the function.\n |      \n |      Returns\n |      -------\n |      scalar or scalar\n |  \n |  memory_usage(self, index: 'bool' = True, deep: 'bool' = False) -&gt; 'int'\n |      Return the memory usage of the Series.\n |      \n |      The memory usage can optionally include the contribution of\n |      the index and of elements of `object` dtype.\n |      \n |      Parameters\n |      ----------\n |      index : bool, default True\n |          Specifies whether to include the memory usage of the Series index.\n |      deep : bool, default False\n |          If True, introspect the data deeply by interrogating\n |          `object` dtypes for system-level memory consumption, and include\n |          it in the returned value.\n |      \n |      Returns\n |      -------\n |      int\n |          Bytes of memory consumed.\n |      \n |      See Also\n |      --------\n |      numpy.ndarray.nbytes : Total bytes consumed by the elements of the\n |          array.\n |      DataFrame.memory_usage : Bytes consumed by a DataFrame.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; s = pd.Series(range(3))\n |      &gt;&gt;&gt; s.memory_usage()\n |      152\n |      \n |      Not including the index gives the size of the rest of the data, which\n |      is necessarily smaller:\n |      \n |      &gt;&gt;&gt; s.memory_usage(index=False)\n |      24\n |      \n |      The memory footprint of `object` values is ignored by default:\n |      \n |      &gt;&gt;&gt; s = pd.Series([\"a\", \"b\"])\n |      &gt;&gt;&gt; s.values\n |      array(['a', 'b'], dtype=object)\n |      &gt;&gt;&gt; s.memory_usage()\n |      144\n |      &gt;&gt;&gt; s.memory_usage(deep=True)\n |      244\n |  \n |  min(self, axis: 'AxisInt | None' = 0, skipna: 'bool_t' = True, numeric_only: 'bool_t' = False, **kwargs)\n |      Return the minimum of the values over the requested axis.\n |      \n |      If you want the *index* of the minimum, use ``idxmin``. This is the equivalent of the ``numpy.ndarray`` method ``argmin``.\n |      \n |      Parameters\n |      ----------\n |      axis : {index (0)}\n |          Axis for the function to be applied on.\n |          For `Series` this parameter is unused and defaults to 0.\n |      \n |          For DataFrames, specifying ``axis=None`` will apply the aggregation\n |          across both axes.\n |      \n |          .. versionadded:: 2.0.0\n |      \n |      skipna : bool, default True\n |          Exclude NA/null values when computing the result.\n |      numeric_only : bool, default False\n |          Include only float, int, boolean columns. Not implemented for Series.\n |      \n |      **kwargs\n |          Additional keyword arguments to be passed to the function.\n |      \n |      Returns\n |      -------\n |      scalar or scalar\n |      \n |      See Also\n |      --------\n |      Series.sum : Return the sum.\n |      Series.min : Return the minimum.\n |      Series.max : Return the maximum.\n |      Series.idxmin : Return the index of the minimum.\n |      Series.idxmax : Return the index of the maximum.\n |      DataFrame.sum : Return the sum over the requested axis.\n |      DataFrame.min : Return the minimum over the requested axis.\n |      DataFrame.max : Return the maximum over the requested axis.\n |      DataFrame.idxmin : Return the index of the minimum over the requested axis.\n |      DataFrame.idxmax : Return the index of the maximum over the requested axis.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; idx = pd.MultiIndex.from_arrays([\n |      ...     ['warm', 'warm', 'cold', 'cold'],\n |      ...     ['dog', 'falcon', 'fish', 'spider']],\n |      ...     names=['blooded', 'animal'])\n |      &gt;&gt;&gt; s = pd.Series([4, 2, 0, 8], name='legs', index=idx)\n |      &gt;&gt;&gt; s\n |      blooded  animal\n |      warm     dog       4\n |               falcon    2\n |      cold     fish      0\n |               spider    8\n |      Name: legs, dtype: int64\n |      \n |      &gt;&gt;&gt; s.min()\n |      0\n |  \n |  mod(self, other, level=None, fill_value=None, axis: 'Axis' = 0)\n |      Return Modulo of series and other, element-wise (binary operator `mod`).\n |      \n |      Equivalent to ``series % other``, but with support to substitute a fill_value for\n |      missing data in either one of the inputs.\n |      \n |      Parameters\n |      ----------\n |      other : Series or scalar value\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level.\n |      fill_value : None or float value, default None (NaN)\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful Series alignment, with this value before computation.\n |          If data in both corresponding Series locations is missing\n |          the result of filling (at that location) will be missing.\n |      axis : {0 or 'index'}\n |          Unused. Parameter needed for compatibility with DataFrame.\n |      \n |      Returns\n |      -------\n |      Series\n |          The result of the operation.\n |      \n |      See Also\n |      --------\n |      Series.rmod : Reverse of the Modulo operator, see\n |          `Python documentation\n |          &lt;https://docs.python.org/3/reference/datamodel.html#emulating-numeric-types&gt;`_\n |          for more details.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; a = pd.Series([1, 1, 1, np.nan], index=['a', 'b', 'c', 'd'])\n |      &gt;&gt;&gt; a\n |      a    1.0\n |      b    1.0\n |      c    1.0\n |      d    NaN\n |      dtype: float64\n |      &gt;&gt;&gt; b = pd.Series([1, np.nan, 1, np.nan], index=['a', 'b', 'd', 'e'])\n |      &gt;&gt;&gt; b\n |      a    1.0\n |      b    NaN\n |      d    1.0\n |      e    NaN\n |      dtype: float64\n |      &gt;&gt;&gt; a.mod(b, fill_value=0)\n |      a    0.0\n |      b    NaN\n |      c    NaN\n |      d    0.0\n |      e    NaN\n |      dtype: float64\n |  \n |  mode(self, dropna: 'bool' = True) -&gt; 'Series'\n |      Return the mode(s) of the Series.\n |      \n |      The mode is the value that appears most often. There can be multiple modes.\n |      \n |      Always returns Series even if only one value is returned.\n |      \n |      Parameters\n |      ----------\n |      dropna : bool, default True\n |          Don't consider counts of NaN/NaT.\n |      \n |      Returns\n |      -------\n |      Series\n |          Modes of the Series in sorted order.\n |  \n |  mul(self, other, level=None, fill_value=None, axis: 'Axis' = 0)\n |      Return Multiplication of series and other, element-wise (binary operator `mul`).\n |      \n |      Equivalent to ``series * other``, but with support to substitute a fill_value for\n |      missing data in either one of the inputs.\n |      \n |      Parameters\n |      ----------\n |      other : Series or scalar value\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level.\n |      fill_value : None or float value, default None (NaN)\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful Series alignment, with this value before computation.\n |          If data in both corresponding Series locations is missing\n |          the result of filling (at that location) will be missing.\n |      axis : {0 or 'index'}\n |          Unused. Parameter needed for compatibility with DataFrame.\n |      \n |      Returns\n |      -------\n |      Series\n |          The result of the operation.\n |      \n |      See Also\n |      --------\n |      Series.rmul : Reverse of the Multiplication operator, see\n |          `Python documentation\n |          &lt;https://docs.python.org/3/reference/datamodel.html#emulating-numeric-types&gt;`_\n |          for more details.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; a = pd.Series([1, 1, 1, np.nan], index=['a', 'b', 'c', 'd'])\n |      &gt;&gt;&gt; a\n |      a    1.0\n |      b    1.0\n |      c    1.0\n |      d    NaN\n |      dtype: float64\n |      &gt;&gt;&gt; b = pd.Series([1, np.nan, 1, np.nan], index=['a', 'b', 'd', 'e'])\n |      &gt;&gt;&gt; b\n |      a    1.0\n |      b    NaN\n |      d    1.0\n |      e    NaN\n |      dtype: float64\n |      &gt;&gt;&gt; a.multiply(b, fill_value=0)\n |      a    1.0\n |      b    0.0\n |      c    0.0\n |      d    0.0\n |      e    NaN\n |      dtype: float64\n |  \n |  multiply = mul(self, other, level=None, fill_value=None, axis: 'Axis' = 0)\n |  \n |  ne(self, other, level=None, fill_value=None, axis: 'Axis' = 0)\n |      Return Not equal to of series and other, element-wise (binary operator `ne`).\n |      \n |      Equivalent to ``series != other``, but with support to substitute a fill_value for\n |      missing data in either one of the inputs.\n |      \n |      Parameters\n |      ----------\n |      other : Series or scalar value\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level.\n |      fill_value : None or float value, default None (NaN)\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful Series alignment, with this value before computation.\n |          If data in both corresponding Series locations is missing\n |          the result of filling (at that location) will be missing.\n |      axis : {0 or 'index'}\n |          Unused. Parameter needed for compatibility with DataFrame.\n |      \n |      Returns\n |      -------\n |      Series\n |          The result of the operation.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; a = pd.Series([1, 1, 1, np.nan], index=['a', 'b', 'c', 'd'])\n |      &gt;&gt;&gt; a\n |      a    1.0\n |      b    1.0\n |      c    1.0\n |      d    NaN\n |      dtype: float64\n |      &gt;&gt;&gt; b = pd.Series([1, np.nan, 1, np.nan], index=['a', 'b', 'd', 'e'])\n |      &gt;&gt;&gt; b\n |      a    1.0\n |      b    NaN\n |      d    1.0\n |      e    NaN\n |      dtype: float64\n |      &gt;&gt;&gt; a.ne(b, fill_value=0)\n |      a    False\n |      b     True\n |      c     True\n |      d     True\n |      e     True\n |      dtype: bool\n |  \n |  nlargest(self, n: 'int' = 5, keep: \"Literal['first', 'last', 'all']\" = 'first') -&gt; 'Series'\n |      Return the largest `n` elements.\n |      \n |      Parameters\n |      ----------\n |      n : int, default 5\n |          Return this many descending sorted values.\n |      keep : {'first', 'last', 'all'}, default 'first'\n |          When there are duplicate values that cannot all fit in a\n |          Series of `n` elements:\n |      \n |          - ``first`` : return the first `n` occurrences in order\n |            of appearance.\n |          - ``last`` : return the last `n` occurrences in reverse\n |            order of appearance.\n |          - ``all`` : keep all occurrences. This can result in a Series of\n |            size larger than `n`.\n |      \n |      Returns\n |      -------\n |      Series\n |          The `n` largest values in the Series, sorted in decreasing order.\n |      \n |      See Also\n |      --------\n |      Series.nsmallest: Get the `n` smallest elements.\n |      Series.sort_values: Sort Series by values.\n |      Series.head: Return the first `n` rows.\n |      \n |      Notes\n |      -----\n |      Faster than ``.sort_values(ascending=False).head(n)`` for small `n`\n |      relative to the size of the ``Series`` object.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; countries_population = {\"Italy\": 59000000, \"France\": 65000000,\n |      ...                         \"Malta\": 434000, \"Maldives\": 434000,\n |      ...                         \"Brunei\": 434000, \"Iceland\": 337000,\n |      ...                         \"Nauru\": 11300, \"Tuvalu\": 11300,\n |      ...                         \"Anguilla\": 11300, \"Montserrat\": 5200}\n |      &gt;&gt;&gt; s = pd.Series(countries_population)\n |      &gt;&gt;&gt; s\n |      Italy       59000000\n |      France      65000000\n |      Malta         434000\n |      Maldives      434000\n |      Brunei        434000\n |      Iceland       337000\n |      Nauru          11300\n |      Tuvalu         11300\n |      Anguilla       11300\n |      Montserrat      5200\n |      dtype: int64\n |      \n |      The `n` largest elements where ``n=5`` by default.\n |      \n |      &gt;&gt;&gt; s.nlargest()\n |      France      65000000\n |      Italy       59000000\n |      Malta         434000\n |      Maldives      434000\n |      Brunei        434000\n |      dtype: int64\n |      \n |      The `n` largest elements where ``n=3``. Default `keep` value is 'first'\n |      so Malta will be kept.\n |      \n |      &gt;&gt;&gt; s.nlargest(3)\n |      France    65000000\n |      Italy     59000000\n |      Malta       434000\n |      dtype: int64\n |      \n |      The `n` largest elements where ``n=3`` and keeping the last duplicates.\n |      Brunei will be kept since it is the last with value 434000 based on\n |      the index order.\n |      \n |      &gt;&gt;&gt; s.nlargest(3, keep='last')\n |      France      65000000\n |      Italy       59000000\n |      Brunei        434000\n |      dtype: int64\n |      \n |      The `n` largest elements where ``n=3`` with all duplicates kept. Note\n |      that the returned Series has five elements due to the three duplicates.\n |      \n |      &gt;&gt;&gt; s.nlargest(3, keep='all')\n |      France      65000000\n |      Italy       59000000\n |      Malta         434000\n |      Maldives      434000\n |      Brunei        434000\n |      dtype: int64\n |  \n |  notna(self) -&gt; 'Series'\n |      Detect existing (non-missing) values.\n |      \n |      Return a boolean same-sized object indicating if the values are not NA.\n |      Non-missing values get mapped to True. Characters such as empty\n |      strings ``''`` or :attr:`numpy.inf` are not considered NA values\n |      (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n |      NA values, such as None or :attr:`numpy.NaN`, get mapped to False\n |      values.\n |      \n |      Returns\n |      -------\n |      Series\n |          Mask of bool values for each element in Series that\n |          indicates whether an element is not an NA value.\n |      \n |      See Also\n |      --------\n |      Series.notnull : Alias of notna.\n |      Series.isna : Boolean inverse of notna.\n |      Series.dropna : Omit axes labels with missing values.\n |      notna : Top-level notna.\n |      \n |      Examples\n |      --------\n |      Show which entries in a DataFrame are not NA.\n |      \n |      &gt;&gt;&gt; df = pd.DataFrame(dict(age=[5, 6, np.NaN],\n |      ...                        born=[pd.NaT, pd.Timestamp('1939-05-27'),\n |      ...                              pd.Timestamp('1940-04-25')],\n |      ...                        name=['Alfred', 'Batman', ''],\n |      ...                        toy=[None, 'Batmobile', 'Joker']))\n |      &gt;&gt;&gt; df\n |         age       born    name        toy\n |      0  5.0        NaT  Alfred       None\n |      1  6.0 1939-05-27  Batman  Batmobile\n |      2  NaN 1940-04-25              Joker\n |      \n |      &gt;&gt;&gt; df.notna()\n |           age   born  name    toy\n |      0   True  False  True  False\n |      1   True   True  True   True\n |      2  False   True  True   True\n |      \n |      Show which entries in a Series are not NA.\n |      \n |      &gt;&gt;&gt; ser = pd.Series([5, 6, np.NaN])\n |      &gt;&gt;&gt; ser\n |      0    5.0\n |      1    6.0\n |      2    NaN\n |      dtype: float64\n |      \n |      &gt;&gt;&gt; ser.notna()\n |      0     True\n |      1     True\n |      2    False\n |      dtype: bool\n |  \n |  notnull(self) -&gt; 'Series'\n |      Series.notnull is an alias for Series.notna.\n |      \n |      Detect existing (non-missing) values.\n |      \n |      Return a boolean same-sized object indicating if the values are not NA.\n |      Non-missing values get mapped to True. Characters such as empty\n |      strings ``''`` or :attr:`numpy.inf` are not considered NA values\n |      (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n |      NA values, such as None or :attr:`numpy.NaN`, get mapped to False\n |      values.\n |      \n |      Returns\n |      -------\n |      Series\n |          Mask of bool values for each element in Series that\n |          indicates whether an element is not an NA value.\n |      \n |      See Also\n |      --------\n |      Series.notnull : Alias of notna.\n |      Series.isna : Boolean inverse of notna.\n |      Series.dropna : Omit axes labels with missing values.\n |      notna : Top-level notna.\n |      \n |      Examples\n |      --------\n |      Show which entries in a DataFrame are not NA.\n |      \n |      &gt;&gt;&gt; df = pd.DataFrame(dict(age=[5, 6, np.NaN],\n |      ...                        born=[pd.NaT, pd.Timestamp('1939-05-27'),\n |      ...                              pd.Timestamp('1940-04-25')],\n |      ...                        name=['Alfred', 'Batman', ''],\n |      ...                        toy=[None, 'Batmobile', 'Joker']))\n |      &gt;&gt;&gt; df\n |         age       born    name        toy\n |      0  5.0        NaT  Alfred       None\n |      1  6.0 1939-05-27  Batman  Batmobile\n |      2  NaN 1940-04-25              Joker\n |      \n |      &gt;&gt;&gt; df.notna()\n |           age   born  name    toy\n |      0   True  False  True  False\n |      1   True   True  True   True\n |      2  False   True  True   True\n |      \n |      Show which entries in a Series are not NA.\n |      \n |      &gt;&gt;&gt; ser = pd.Series([5, 6, np.NaN])\n |      &gt;&gt;&gt; ser\n |      0    5.0\n |      1    6.0\n |      2    NaN\n |      dtype: float64\n |      \n |      &gt;&gt;&gt; ser.notna()\n |      0     True\n |      1     True\n |      2    False\n |      dtype: bool\n |  \n |  nsmallest(self, n: 'int' = 5, keep: 'str' = 'first') -&gt; 'Series'\n |      Return the smallest `n` elements.\n |      \n |      Parameters\n |      ----------\n |      n : int, default 5\n |          Return this many ascending sorted values.\n |      keep : {'first', 'last', 'all'}, default 'first'\n |          When there are duplicate values that cannot all fit in a\n |          Series of `n` elements:\n |      \n |          - ``first`` : return the first `n` occurrences in order\n |            of appearance.\n |          - ``last`` : return the last `n` occurrences in reverse\n |            order of appearance.\n |          - ``all`` : keep all occurrences. This can result in a Series of\n |            size larger than `n`.\n |      \n |      Returns\n |      -------\n |      Series\n |          The `n` smallest values in the Series, sorted in increasing order.\n |      \n |      See Also\n |      --------\n |      Series.nlargest: Get the `n` largest elements.\n |      Series.sort_values: Sort Series by values.\n |      Series.head: Return the first `n` rows.\n |      \n |      Notes\n |      -----\n |      Faster than ``.sort_values().head(n)`` for small `n` relative to\n |      the size of the ``Series`` object.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; countries_population = {\"Italy\": 59000000, \"France\": 65000000,\n |      ...                         \"Brunei\": 434000, \"Malta\": 434000,\n |      ...                         \"Maldives\": 434000, \"Iceland\": 337000,\n |      ...                         \"Nauru\": 11300, \"Tuvalu\": 11300,\n |      ...                         \"Anguilla\": 11300, \"Montserrat\": 5200}\n |      &gt;&gt;&gt; s = pd.Series(countries_population)\n |      &gt;&gt;&gt; s\n |      Italy       59000000\n |      France      65000000\n |      Brunei        434000\n |      Malta         434000\n |      Maldives      434000\n |      Iceland       337000\n |      Nauru          11300\n |      Tuvalu         11300\n |      Anguilla       11300\n |      Montserrat      5200\n |      dtype: int64\n |      \n |      The `n` smallest elements where ``n=5`` by default.\n |      \n |      &gt;&gt;&gt; s.nsmallest()\n |      Montserrat    5200\n |      Nauru        11300\n |      Tuvalu       11300\n |      Anguilla     11300\n |      Iceland     337000\n |      dtype: int64\n |      \n |      The `n` smallest elements where ``n=3``. Default `keep` value is\n |      'first' so Nauru and Tuvalu will be kept.\n |      \n |      &gt;&gt;&gt; s.nsmallest(3)\n |      Montserrat   5200\n |      Nauru       11300\n |      Tuvalu      11300\n |      dtype: int64\n |      \n |      The `n` smallest elements where ``n=3`` and keeping the last\n |      duplicates. Anguilla and Tuvalu will be kept since they are the last\n |      with value 11300 based on the index order.\n |      \n |      &gt;&gt;&gt; s.nsmallest(3, keep='last')\n |      Montserrat   5200\n |      Anguilla    11300\n |      Tuvalu      11300\n |      dtype: int64\n |      \n |      The `n` smallest elements where ``n=3`` with all duplicates kept. Note\n |      that the returned Series has four elements due to the three duplicates.\n |      \n |      &gt;&gt;&gt; s.nsmallest(3, keep='all')\n |      Montserrat   5200\n |      Nauru       11300\n |      Tuvalu      11300\n |      Anguilla    11300\n |      dtype: int64\n |  \n |  pop(self, item: 'Hashable') -&gt; 'Any'\n |      Return item and drops from series. Raise KeyError if not found.\n |      \n |      Parameters\n |      ----------\n |      item : label\n |          Index of the element that needs to be removed.\n |      \n |      Returns\n |      -------\n |      Value that is popped from series.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; ser = pd.Series([1,2,3])\n |      \n |      &gt;&gt;&gt; ser.pop(0)\n |      1\n |      \n |      &gt;&gt;&gt; ser\n |      1    2\n |      2    3\n |      dtype: int64\n |  \n |  pow(self, other, level=None, fill_value=None, axis: 'Axis' = 0)\n |      Return Exponential power of series and other, element-wise (binary operator `pow`).\n |      \n |      Equivalent to ``series ** other``, but with support to substitute a fill_value for\n |      missing data in either one of the inputs.\n |      \n |      Parameters\n |      ----------\n |      other : Series or scalar value\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level.\n |      fill_value : None or float value, default None (NaN)\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful Series alignment, with this value before computation.\n |          If data in both corresponding Series locations is missing\n |          the result of filling (at that location) will be missing.\n |      axis : {0 or 'index'}\n |          Unused. Parameter needed for compatibility with DataFrame.\n |      \n |      Returns\n |      -------\n |      Series\n |          The result of the operation.\n |      \n |      See Also\n |      --------\n |      Series.rpow : Reverse of the Exponential power operator, see\n |          `Python documentation\n |          &lt;https://docs.python.org/3/reference/datamodel.html#emulating-numeric-types&gt;`_\n |          for more details.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; a = pd.Series([1, 1, 1, np.nan], index=['a', 'b', 'c', 'd'])\n |      &gt;&gt;&gt; a\n |      a    1.0\n |      b    1.0\n |      c    1.0\n |      d    NaN\n |      dtype: float64\n |      &gt;&gt;&gt; b = pd.Series([1, np.nan, 1, np.nan], index=['a', 'b', 'd', 'e'])\n |      &gt;&gt;&gt; b\n |      a    1.0\n |      b    NaN\n |      d    1.0\n |      e    NaN\n |      dtype: float64\n |      &gt;&gt;&gt; a.pow(b, fill_value=0)\n |      a    1.0\n |      b    1.0\n |      c    1.0\n |      d    0.0\n |      e    NaN\n |      dtype: float64\n |  \n |  prod(self, axis: 'Axis | None' = None, skipna: 'bool_t' = True, numeric_only: 'bool_t' = False, min_count: 'int' = 0, **kwargs)\n |      Return the product of the values over the requested axis.\n |      \n |      Parameters\n |      ----------\n |      axis : {index (0)}\n |          Axis for the function to be applied on.\n |          For `Series` this parameter is unused and defaults to 0.\n |      \n |          For DataFrames, specifying ``axis=None`` will apply the aggregation\n |          across both axes.\n |      \n |          .. versionadded:: 2.0.0\n |      \n |      skipna : bool, default True\n |          Exclude NA/null values when computing the result.\n |      numeric_only : bool, default False\n |          Include only float, int, boolean columns. Not implemented for Series.\n |      \n |      min_count : int, default 0\n |          The required number of valid values to perform the operation. If fewer than\n |          ``min_count`` non-NA values are present the result will be NA.\n |      **kwargs\n |          Additional keyword arguments to be passed to the function.\n |      \n |      Returns\n |      -------\n |      scalar or scalar\n |      \n |      See Also\n |      --------\n |      Series.sum : Return the sum.\n |      Series.min : Return the minimum.\n |      Series.max : Return the maximum.\n |      Series.idxmin : Return the index of the minimum.\n |      Series.idxmax : Return the index of the maximum.\n |      DataFrame.sum : Return the sum over the requested axis.\n |      DataFrame.min : Return the minimum over the requested axis.\n |      DataFrame.max : Return the maximum over the requested axis.\n |      DataFrame.idxmin : Return the index of the minimum over the requested axis.\n |      DataFrame.idxmax : Return the index of the maximum over the requested axis.\n |      \n |      Examples\n |      --------\n |      By default, the product of an empty or all-NA Series is ``1``\n |      \n |      &gt;&gt;&gt; pd.Series([], dtype=\"float64\").prod()\n |      1.0\n |      \n |      This can be controlled with the ``min_count`` parameter\n |      \n |      &gt;&gt;&gt; pd.Series([], dtype=\"float64\").prod(min_count=1)\n |      nan\n |      \n |      Thanks to the ``skipna`` parameter, ``min_count`` handles all-NA and\n |      empty series identically.\n |      \n |      &gt;&gt;&gt; pd.Series([np.nan]).prod()\n |      1.0\n |      \n |      &gt;&gt;&gt; pd.Series([np.nan]).prod(min_count=1)\n |      nan\n |  \n |  product = prod(self, axis: 'Axis | None' = None, skipna: 'bool_t' = True, numeric_only: 'bool_t' = False, min_count: 'int' = 0, **kwargs)\n |  \n |  quantile(self, q: 'float | Sequence[float] | AnyArrayLike' = 0.5, interpolation: 'QuantileInterpolation' = 'linear') -&gt; 'float | Series'\n |      Return value at the given quantile.\n |      \n |      Parameters\n |      ----------\n |      q : float or array-like, default 0.5 (50% quantile)\n |          The quantile(s) to compute, which can lie in range: 0 &lt;= q &lt;= 1.\n |      interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n |          This optional parameter specifies the interpolation method to use,\n |          when the desired quantile lies between two data points `i` and `j`:\n |      \n |              * linear: `i + (j - i) * fraction`, where `fraction` is the\n |                fractional part of the index surrounded by `i` and `j`.\n |              * lower: `i`.\n |              * higher: `j`.\n |              * nearest: `i` or `j` whichever is nearest.\n |              * midpoint: (`i` + `j`) / 2.\n |      \n |      Returns\n |      -------\n |      float or Series\n |          If ``q`` is an array, a Series will be returned where the\n |          index is ``q`` and the values are the quantiles, otherwise\n |          a float will be returned.\n |      \n |      See Also\n |      --------\n |      core.window.Rolling.quantile : Calculate the rolling quantile.\n |      numpy.percentile : Returns the q-th percentile(s) of the array elements.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; s = pd.Series([1, 2, 3, 4])\n |      &gt;&gt;&gt; s.quantile(.5)\n |      2.5\n |      &gt;&gt;&gt; s.quantile([.25, .5, .75])\n |      0.25    1.75\n |      0.50    2.50\n |      0.75    3.25\n |      dtype: float64\n |  \n |  radd(self, other, level=None, fill_value=None, axis: 'Axis' = 0)\n |      Return Addition of series and other, element-wise (binary operator `radd`).\n |      \n |      Equivalent to ``other + series``, but with support to substitute a fill_value for\n |      missing data in either one of the inputs.\n |      \n |      Parameters\n |      ----------\n |      other : Series or scalar value\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level.\n |      fill_value : None or float value, default None (NaN)\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful Series alignment, with this value before computation.\n |          If data in both corresponding Series locations is missing\n |          the result of filling (at that location) will be missing.\n |      axis : {0 or 'index'}\n |          Unused. Parameter needed for compatibility with DataFrame.\n |      \n |      Returns\n |      -------\n |      Series\n |          The result of the operation.\n |      \n |      See Also\n |      --------\n |      Series.add : Element-wise Addition, see\n |          `Python documentation\n |          &lt;https://docs.python.org/3/reference/datamodel.html#emulating-numeric-types&gt;`_\n |          for more details.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; a = pd.Series([1, 1, 1, np.nan], index=['a', 'b', 'c', 'd'])\n |      &gt;&gt;&gt; a\n |      a    1.0\n |      b    1.0\n |      c    1.0\n |      d    NaN\n |      dtype: float64\n |      &gt;&gt;&gt; b = pd.Series([1, np.nan, 1, np.nan], index=['a', 'b', 'd', 'e'])\n |      &gt;&gt;&gt; b\n |      a    1.0\n |      b    NaN\n |      d    1.0\n |      e    NaN\n |      dtype: float64\n |      &gt;&gt;&gt; a.add(b, fill_value=0)\n |      a    2.0\n |      b    1.0\n |      c    1.0\n |      d    1.0\n |      e    NaN\n |      dtype: float64\n |  \n |  ravel(self, order: 'str' = 'C') -&gt; 'ArrayLike'\n |      Return the flattened underlying data as an ndarray or ExtensionArray.\n |      \n |      Returns\n |      -------\n |      numpy.ndarray or ExtensionArray\n |          Flattened data of the Series.\n |      \n |      See Also\n |      --------\n |      numpy.ndarray.ravel : Return a flattened array.\n |  \n |  rdiv = rtruediv(self, other, level=None, fill_value=None, axis: 'Axis' = 0)\n |  \n |  rdivmod(self, other, level=None, fill_value=None, axis: 'Axis' = 0)\n |      Return Integer division and modulo of series and other, element-wise (binary operator `rdivmod`).\n |      \n |      Equivalent to ``other divmod series``, but with support to substitute a fill_value for\n |      missing data in either one of the inputs.\n |      \n |      Parameters\n |      ----------\n |      other : Series or scalar value\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level.\n |      fill_value : None or float value, default None (NaN)\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful Series alignment, with this value before computation.\n |          If data in both corresponding Series locations is missing\n |          the result of filling (at that location) will be missing.\n |      axis : {0 or 'index'}\n |          Unused. Parameter needed for compatibility with DataFrame.\n |      \n |      Returns\n |      -------\n |      2-Tuple of Series\n |          The result of the operation.\n |      \n |      See Also\n |      --------\n |      Series.divmod : Element-wise Integer division and modulo, see\n |          `Python documentation\n |          &lt;https://docs.python.org/3/reference/datamodel.html#emulating-numeric-types&gt;`_\n |          for more details.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; a = pd.Series([1, 1, 1, np.nan], index=['a', 'b', 'c', 'd'])\n |      &gt;&gt;&gt; a\n |      a    1.0\n |      b    1.0\n |      c    1.0\n |      d    NaN\n |      dtype: float64\n |      &gt;&gt;&gt; b = pd.Series([1, np.nan, 1, np.nan], index=['a', 'b', 'd', 'e'])\n |      &gt;&gt;&gt; b\n |      a    1.0\n |      b    NaN\n |      d    1.0\n |      e    NaN\n |      dtype: float64\n |      &gt;&gt;&gt; a.divmod(b, fill_value=0)\n |      (a    1.0\n |       b    NaN\n |       c    NaN\n |       d    0.0\n |       e    NaN\n |       dtype: float64,\n |       a    0.0\n |       b    NaN\n |       c    NaN\n |       d    0.0\n |       e    NaN\n |       dtype: float64)\n |  \n |  reindex(self, index=None, *, axis: 'Axis | None' = None, method: 'str | None' = None, copy: 'bool | None' = None, level: 'Level | None' = None, fill_value: 'Scalar | None' = None, limit: 'int | None' = None, tolerance=None) -&gt; 'Series'\n |      Conform Series to new index with optional filling logic.\n |      \n |      Places NA/NaN in locations having no value in the previous index. A new object\n |      is produced unless the new index is equivalent to the current one and\n |      ``copy=False``.\n |      \n |      Parameters\n |      ----------\n |      \n |      index : array-like, optional\n |          New labels for the index. Preferably an Index object to avoid\n |          duplicating data.\n |      axis : int or str, optional\n |          Unused.\n |      method : {None, 'backfill'/'bfill', 'pad'/'ffill', 'nearest'}\n |          Method to use for filling holes in reindexed DataFrame.\n |          Please note: this is only applicable to DataFrames/Series with a\n |          monotonically increasing/decreasing index.\n |      \n |          * None (default): don't fill gaps\n |          * pad / ffill: Propagate last valid observation forward to next\n |            valid.\n |          * backfill / bfill: Use next valid observation to fill gap.\n |          * nearest: Use nearest valid observations to fill gap.\n |      \n |      copy : bool, default True\n |          Return a new object, even if the passed indexes are the same.\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level.\n |      fill_value : scalar, default np.NaN\n |          Value to use for missing values. Defaults to NaN, but can be any\n |          \"compatible\" value.\n |      limit : int, default None\n |          Maximum number of consecutive elements to forward or backward fill.\n |      tolerance : optional\n |          Maximum distance between original and new labels for inexact\n |          matches. The values of the index at the matching locations most\n |          satisfy the equation ``abs(index[indexer] - target) &lt;= tolerance``.\n |      \n |          Tolerance may be a scalar value, which applies the same tolerance\n |          to all values, or list-like, which applies variable tolerance per\n |          element. List-like includes list, tuple, array, Series, and must be\n |          the same size as the index and its dtype must exactly match the\n |          index's type.\n |      \n |      Returns\n |      -------\n |      Series with changed index.\n |      \n |      See Also\n |      --------\n |      DataFrame.set_index : Set row labels.\n |      DataFrame.reset_index : Remove row labels or move them to new columns.\n |      DataFrame.reindex_like : Change to same indices as other DataFrame.\n |      \n |      Examples\n |      --------\n |      ``DataFrame.reindex`` supports two calling conventions\n |      \n |      * ``(index=index_labels, columns=column_labels, ...)``\n |      * ``(labels, axis={'index', 'columns'}, ...)``\n |      \n |      We *highly* recommend using keyword arguments to clarify your\n |      intent.\n |      \n |      Create a dataframe with some fictional data.\n |      \n |      &gt;&gt;&gt; index = ['Firefox', 'Chrome', 'Safari', 'IE10', 'Konqueror']\n |      &gt;&gt;&gt; df = pd.DataFrame({'http_status': [200, 200, 404, 404, 301],\n |      ...                   'response_time': [0.04, 0.02, 0.07, 0.08, 1.0]},\n |      ...                   index=index)\n |      &gt;&gt;&gt; df\n |                 http_status  response_time\n |      Firefox            200           0.04\n |      Chrome             200           0.02\n |      Safari             404           0.07\n |      IE10               404           0.08\n |      Konqueror          301           1.00\n |      \n |      Create a new index and reindex the dataframe. By default\n |      values in the new index that do not have corresponding\n |      records in the dataframe are assigned ``NaN``.\n |      \n |      &gt;&gt;&gt; new_index = ['Safari', 'Iceweasel', 'Comodo Dragon', 'IE10',\n |      ...              'Chrome']\n |      &gt;&gt;&gt; df.reindex(new_index)\n |                     http_status  response_time\n |      Safari               404.0           0.07\n |      Iceweasel              NaN            NaN\n |      Comodo Dragon          NaN            NaN\n |      IE10                 404.0           0.08\n |      Chrome               200.0           0.02\n |      \n |      We can fill in the missing values by passing a value to\n |      the keyword ``fill_value``. Because the index is not monotonically\n |      increasing or decreasing, we cannot use arguments to the keyword\n |      ``method`` to fill the ``NaN`` values.\n |      \n |      &gt;&gt;&gt; df.reindex(new_index, fill_value=0)\n |                     http_status  response_time\n |      Safari                 404           0.07\n |      Iceweasel                0           0.00\n |      Comodo Dragon            0           0.00\n |      IE10                   404           0.08\n |      Chrome                 200           0.02\n |      \n |      &gt;&gt;&gt; df.reindex(new_index, fill_value='missing')\n |                    http_status response_time\n |      Safari                404          0.07\n |      Iceweasel         missing       missing\n |      Comodo Dragon     missing       missing\n |      IE10                  404          0.08\n |      Chrome                200          0.02\n |      \n |      We can also reindex the columns.\n |      \n |      &gt;&gt;&gt; df.reindex(columns=['http_status', 'user_agent'])\n |                 http_status  user_agent\n |      Firefox            200         NaN\n |      Chrome             200         NaN\n |      Safari             404         NaN\n |      IE10               404         NaN\n |      Konqueror          301         NaN\n |      \n |      Or we can use \"axis-style\" keyword arguments\n |      \n |      &gt;&gt;&gt; df.reindex(['http_status', 'user_agent'], axis=\"columns\")\n |                 http_status  user_agent\n |      Firefox            200         NaN\n |      Chrome             200         NaN\n |      Safari             404         NaN\n |      IE10               404         NaN\n |      Konqueror          301         NaN\n |      \n |      To further illustrate the filling functionality in\n |      ``reindex``, we will create a dataframe with a\n |      monotonically increasing index (for example, a sequence\n |      of dates).\n |      \n |      &gt;&gt;&gt; date_index = pd.date_range('1/1/2010', periods=6, freq='D')\n |      &gt;&gt;&gt; df2 = pd.DataFrame({\"prices\": [100, 101, np.nan, 100, 89, 88]},\n |      ...                    index=date_index)\n |      &gt;&gt;&gt; df2\n |                  prices\n |      2010-01-01   100.0\n |      2010-01-02   101.0\n |      2010-01-03     NaN\n |      2010-01-04   100.0\n |      2010-01-05    89.0\n |      2010-01-06    88.0\n |      \n |      Suppose we decide to expand the dataframe to cover a wider\n |      date range.\n |      \n |      &gt;&gt;&gt; date_index2 = pd.date_range('12/29/2009', periods=10, freq='D')\n |      &gt;&gt;&gt; df2.reindex(date_index2)\n |                  prices\n |      2009-12-29     NaN\n |      2009-12-30     NaN\n |      2009-12-31     NaN\n |      2010-01-01   100.0\n |      2010-01-02   101.0\n |      2010-01-03     NaN\n |      2010-01-04   100.0\n |      2010-01-05    89.0\n |      2010-01-06    88.0\n |      2010-01-07     NaN\n |      \n |      The index entries that did not have a value in the original data frame\n |      (for example, '2009-12-29') are by default filled with ``NaN``.\n |      If desired, we can fill in the missing values using one of several\n |      options.\n |      \n |      For example, to back-propagate the last valid value to fill the ``NaN``\n |      values, pass ``bfill`` as an argument to the ``method`` keyword.\n |      \n |      &gt;&gt;&gt; df2.reindex(date_index2, method='bfill')\n |                  prices\n |      2009-12-29   100.0\n |      2009-12-30   100.0\n |      2009-12-31   100.0\n |      2010-01-01   100.0\n |      2010-01-02   101.0\n |      2010-01-03     NaN\n |      2010-01-04   100.0\n |      2010-01-05    89.0\n |      2010-01-06    88.0\n |      2010-01-07     NaN\n |      \n |      Please note that the ``NaN`` value present in the original dataframe\n |      (at index value 2010-01-03) will not be filled by any of the\n |      value propagation schemes. This is because filling while reindexing\n |      does not look at dataframe values, but only compares the original and\n |      desired indexes. If you do want to fill in the ``NaN`` values present\n |      in the original dataframe, use the ``fillna()`` method.\n |      \n |      See the :ref:`user guide &lt;basics.reindexing&gt;` for more.\n |  \n |  rename(self, index: 'Renamer | Hashable | None' = None, *, axis: 'Axis | None' = None, copy: 'bool' = True, inplace: 'bool' = False, level: 'Level | None' = None, errors: 'IgnoreRaise' = 'ignore') -&gt; 'Series | None'\n |      Alter Series index labels or name.\n |      \n |      Function / dict values must be unique (1-to-1). Labels not contained in\n |      a dict / Series will be left as-is. Extra labels listed don't throw an\n |      error.\n |      \n |      Alternatively, change ``Series.name`` with a scalar value.\n |      \n |      See the :ref:`user guide &lt;basics.rename&gt;` for more.\n |      \n |      Parameters\n |      ----------\n |      index : scalar, hashable sequence, dict-like or function optional\n |          Functions or dict-like are transformations to apply to\n |          the index.\n |          Scalar or hashable sequence-like will alter the ``Series.name``\n |          attribute.\n |      axis : {0 or 'index'}\n |          Unused. Parameter needed for compatibility with DataFrame.\n |      copy : bool, default True\n |          Also copy underlying data.\n |      inplace : bool, default False\n |          Whether to return a new Series. If True the value of copy is ignored.\n |      level : int or level name, default None\n |          In case of MultiIndex, only rename labels in the specified level.\n |      errors : {'ignore', 'raise'}, default 'ignore'\n |          If 'raise', raise `KeyError` when a `dict-like mapper` or\n |          `index` contains labels that are not present in the index being transformed.\n |          If 'ignore', existing keys will be renamed and extra keys will be ignored.\n |      \n |      Returns\n |      -------\n |      Series or None\n |          Series with index labels or name altered or None if ``inplace=True``.\n |      \n |      See Also\n |      --------\n |      DataFrame.rename : Corresponding DataFrame method.\n |      Series.rename_axis : Set the name of the axis.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; s = pd.Series([1, 2, 3])\n |      &gt;&gt;&gt; s\n |      0    1\n |      1    2\n |      2    3\n |      dtype: int64\n |      &gt;&gt;&gt; s.rename(\"my_name\")  # scalar, changes Series.name\n |      0    1\n |      1    2\n |      2    3\n |      Name: my_name, dtype: int64\n |      &gt;&gt;&gt; s.rename(lambda x: x ** 2)  # function, changes labels\n |      0    1\n |      1    2\n |      4    3\n |      dtype: int64\n |      &gt;&gt;&gt; s.rename({1: 3, 2: 5})  # mapping, changes labels\n |      0    1\n |      3    2\n |      5    3\n |      dtype: int64\n |  \n |  rename_axis(self: 'Series', mapper: 'IndexLabel | lib.NoDefault' = &lt;no_default&gt;, *, index=&lt;no_default&gt;, axis: 'Axis' = 0, copy: 'bool' = True, inplace: 'bool' = False) -&gt; 'Series | None'\n |      Set the name of the axis for the index or columns.\n |      \n |      Parameters\n |      ----------\n |      mapper : scalar, list-like, optional\n |          Value to set the axis name attribute.\n |      index, columns : scalar, list-like, dict-like or function, optional\n |          A scalar, list-like, dict-like or functions transformations to\n |          apply to that axis' values.\n |          Note that the ``columns`` parameter is not allowed if the\n |          object is a Series. This parameter only apply for DataFrame\n |          type objects.\n |      \n |          Use either ``mapper`` and ``axis`` to\n |          specify the axis to target with ``mapper``, or ``index``\n |          and/or ``columns``.\n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |          The axis to rename. For `Series` this parameter is unused and defaults to 0.\n |      copy : bool, default None\n |          Also copy underlying data.\n |      inplace : bool, default False\n |          Modifies the object directly, instead of creating a new Series\n |          or DataFrame.\n |      \n |      Returns\n |      -------\n |      Series, DataFrame, or None\n |          The same type as the caller or None if ``inplace=True``.\n |      \n |      See Also\n |      --------\n |      Series.rename : Alter Series index labels or name.\n |      DataFrame.rename : Alter DataFrame index labels or name.\n |      Index.rename : Set new names on index.\n |      \n |      Notes\n |      -----\n |      ``DataFrame.rename_axis`` supports two calling conventions\n |      \n |      * ``(index=index_mapper, columns=columns_mapper, ...)``\n |      * ``(mapper, axis={'index', 'columns'}, ...)``\n |      \n |      The first calling convention will only modify the names of\n |      the index and/or the names of the Index object that is the columns.\n |      In this case, the parameter ``copy`` is ignored.\n |      \n |      The second calling convention will modify the names of the\n |      corresponding index if mapper is a list or a scalar.\n |      However, if mapper is dict-like or a function, it will use the\n |      deprecated behavior of modifying the axis *labels*.\n |      \n |      We *highly* recommend using keyword arguments to clarify your\n |      intent.\n |      \n |      Examples\n |      --------\n |      **Series**\n |      \n |      &gt;&gt;&gt; s = pd.Series([\"dog\", \"cat\", \"monkey\"])\n |      &gt;&gt;&gt; s\n |      0       dog\n |      1       cat\n |      2    monkey\n |      dtype: object\n |      &gt;&gt;&gt; s.rename_axis(\"animal\")\n |      animal\n |      0    dog\n |      1    cat\n |      2    monkey\n |      dtype: object\n |      \n |      **DataFrame**\n |      \n |      &gt;&gt;&gt; df = pd.DataFrame({\"num_legs\": [4, 4, 2],\n |      ...                    \"num_arms\": [0, 0, 2]},\n |      ...                   [\"dog\", \"cat\", \"monkey\"])\n |      &gt;&gt;&gt; df\n |              num_legs  num_arms\n |      dog            4         0\n |      cat            4         0\n |      monkey         2         2\n |      &gt;&gt;&gt; df = df.rename_axis(\"animal\")\n |      &gt;&gt;&gt; df\n |              num_legs  num_arms\n |      animal\n |      dog            4         0\n |      cat            4         0\n |      monkey         2         2\n |      &gt;&gt;&gt; df = df.rename_axis(\"limbs\", axis=\"columns\")\n |      &gt;&gt;&gt; df\n |      limbs   num_legs  num_arms\n |      animal\n |      dog            4         0\n |      cat            4         0\n |      monkey         2         2\n |      \n |      **MultiIndex**\n |      \n |      &gt;&gt;&gt; df.index = pd.MultiIndex.from_product([['mammal'],\n |      ...                                        ['dog', 'cat', 'monkey']],\n |      ...                                       names=['type', 'name'])\n |      &gt;&gt;&gt; df\n |      limbs          num_legs  num_arms\n |      type   name\n |      mammal dog            4         0\n |             cat            4         0\n |             monkey         2         2\n |      \n |      &gt;&gt;&gt; df.rename_axis(index={'type': 'class'})\n |      limbs          num_legs  num_arms\n |      class  name\n |      mammal dog            4         0\n |             cat            4         0\n |             monkey         2         2\n |      \n |      &gt;&gt;&gt; df.rename_axis(columns=str.upper)\n |      LIMBS          num_legs  num_arms\n |      type   name\n |      mammal dog            4         0\n |             cat            4         0\n |             monkey         2         2\n |  \n |  reorder_levels(self, order: 'Sequence[Level]') -&gt; 'Series'\n |      Rearrange index levels using input order.\n |      \n |      May not drop or duplicate levels.\n |      \n |      Parameters\n |      ----------\n |      order : list of int representing new level order\n |          Reference level by number or key.\n |      \n |      Returns\n |      -------\n |      type of caller (new object)\n |  \n |  repeat(self, repeats: 'int | Sequence[int]', axis: 'None' = None) -&gt; 'Series'\n |      Repeat elements of a Series.\n |      \n |      Returns a new Series where each element of the current Series\n |      is repeated consecutively a given number of times.\n |      \n |      Parameters\n |      ----------\n |      repeats : int or array of ints\n |          The number of repetitions for each element. This should be a\n |          non-negative integer. Repeating 0 times will return an empty\n |          Series.\n |      axis : None\n |          Unused. Parameter needed for compatibility with DataFrame.\n |      \n |      Returns\n |      -------\n |      Series\n |          Newly created Series with repeated elements.\n |      \n |      See Also\n |      --------\n |      Index.repeat : Equivalent function for Index.\n |      numpy.repeat : Similar method for :class:`numpy.ndarray`.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; s = pd.Series(['a', 'b', 'c'])\n |      &gt;&gt;&gt; s\n |      0    a\n |      1    b\n |      2    c\n |      dtype: object\n |      &gt;&gt;&gt; s.repeat(2)\n |      0    a\n |      0    a\n |      1    b\n |      1    b\n |      2    c\n |      2    c\n |      dtype: object\n |      &gt;&gt;&gt; s.repeat([1, 2, 3])\n |      0    a\n |      1    b\n |      1    b\n |      2    c\n |      2    c\n |      2    c\n |      dtype: object\n |  \n |  replace(self, to_replace=None, value=&lt;no_default&gt;, *, inplace: 'bool' = False, limit: 'int | None' = None, regex: 'bool' = False, method: \"Literal['pad', 'ffill', 'bfill'] | lib.NoDefault\" = &lt;no_default&gt;) -&gt; 'Series | None'\n |      Replace values given in `to_replace` with `value`.\n |      \n |      Values of the Series are replaced with other values dynamically.\n |      \n |      This differs from updating with ``.loc`` or ``.iloc``, which require\n |      you to specify a location to update with some value.\n |      \n |      Parameters\n |      ----------\n |      to_replace : str, regex, list, dict, Series, int, float, or None\n |          How to find the values that will be replaced.\n |      \n |          * numeric, str or regex:\n |      \n |              - numeric: numeric values equal to `to_replace` will be\n |                replaced with `value`\n |              - str: string exactly matching `to_replace` will be replaced\n |                with `value`\n |              - regex: regexs matching `to_replace` will be replaced with\n |                `value`\n |      \n |          * list of str, regex, or numeric:\n |      \n |              - First, if `to_replace` and `value` are both lists, they\n |                **must** be the same length.\n |              - Second, if ``regex=True`` then all of the strings in **both**\n |                lists will be interpreted as regexs otherwise they will match\n |                directly. This doesn't matter much for `value` since there\n |                are only a few possible substitution regexes you can use.\n |              - str, regex and numeric rules apply as above.\n |      \n |          * dict:\n |      \n |              - Dicts can be used to specify different replacement values\n |                for different existing values. For example,\n |                ``{'a': 'b', 'y': 'z'}`` replaces the value 'a' with 'b' and\n |                'y' with 'z'. To use a dict in this way, the optional `value`\n |                parameter should not be given.\n |              - For a DataFrame a dict can specify that different values\n |                should be replaced in different columns. For example,\n |                ``{'a': 1, 'b': 'z'}`` looks for the value 1 in column 'a'\n |                and the value 'z' in column 'b' and replaces these values\n |                with whatever is specified in `value`. The `value` parameter\n |                should not be ``None`` in this case. You can treat this as a\n |                special case of passing two lists except that you are\n |                specifying the column to search in.\n |              - For a DataFrame nested dictionaries, e.g.,\n |                ``{'a': {'b': np.nan}}``, are read as follows: look in column\n |                'a' for the value 'b' and replace it with NaN. The optional `value`\n |                parameter should not be specified to use a nested dict in this\n |                way. You can nest regular expressions as well. Note that\n |                column names (the top-level dictionary keys in a nested\n |                dictionary) **cannot** be regular expressions.\n |      \n |          * None:\n |      \n |              - This means that the `regex` argument must be a string,\n |                compiled regular expression, or list, dict, ndarray or\n |                Series of such elements. If `value` is also ``None`` then\n |                this **must** be a nested dictionary or Series.\n |      \n |          See the examples section for examples of each of these.\n |      value : scalar, dict, list, str, regex, default None\n |          Value to replace any values matching `to_replace` with.\n |          For a DataFrame a dict of values can be used to specify which\n |          value to use for each column (columns not in the dict will not be\n |          filled). Regular expressions, strings and lists or dicts of such\n |          objects are also allowed.\n |      inplace : bool, default False\n |          If True, performs operation inplace and returns None.\n |      limit : int, default None\n |          Maximum size gap to forward or backward fill.\n |      regex : bool or same types as `to_replace`, default False\n |          Whether to interpret `to_replace` and/or `value` as regular\n |          expressions. If this is ``True`` then `to_replace` *must* be a\n |          string. Alternatively, this could be a regular expression or a\n |          list, dict, or array of regular expressions in which case\n |          `to_replace` must be ``None``.\n |      method : {'pad', 'ffill', 'bfill'}\n |          The method to use when for replacement, when `to_replace` is a\n |          scalar, list or tuple and `value` is ``None``.\n |      \n |      Returns\n |      -------\n |      Series\n |          Object after replacement.\n |      \n |      Raises\n |      ------\n |      AssertionError\n |          * If `regex` is not a ``bool`` and `to_replace` is not\n |            ``None``.\n |      \n |      TypeError\n |          * If `to_replace` is not a scalar, array-like, ``dict``, or ``None``\n |          * If `to_replace` is a ``dict`` and `value` is not a ``list``,\n |            ``dict``, ``ndarray``, or ``Series``\n |          * If `to_replace` is ``None`` and `regex` is not compilable\n |            into a regular expression or is a list, dict, ndarray, or\n |            Series.\n |          * When replacing multiple ``bool`` or ``datetime64`` objects and\n |            the arguments to `to_replace` does not match the type of the\n |            value being replaced\n |      \n |      ValueError\n |          * If a ``list`` or an ``ndarray`` is passed to `to_replace` and\n |            `value` but they are not the same length.\n |      \n |      See Also\n |      --------\n |      Series.fillna : Fill NA values.\n |      Series.where : Replace values based on boolean condition.\n |      Series.str.replace : Simple string replacement.\n |      \n |      Notes\n |      -----\n |      * Regex substitution is performed under the hood with ``re.sub``. The\n |        rules for substitution for ``re.sub`` are the same.\n |      * Regular expressions will only substitute on strings, meaning you\n |        cannot provide, for example, a regular expression matching floating\n |        point numbers and expect the columns in your frame that have a\n |        numeric dtype to be matched. However, if those floating point\n |        numbers *are* strings, then you can do this.\n |      * This method has *a lot* of options. You are encouraged to experiment\n |        and play with this method to gain intuition about how it works.\n |      * When dict is used as the `to_replace` value, it is like\n |        key(s) in the dict are the to_replace part and\n |        value(s) in the dict are the value parameter.\n |      \n |      Examples\n |      --------\n |      \n |      **Scalar `to_replace` and `value`**\n |      \n |      &gt;&gt;&gt; s = pd.Series([1, 2, 3, 4, 5])\n |      &gt;&gt;&gt; s.replace(1, 5)\n |      0    5\n |      1    2\n |      2    3\n |      3    4\n |      4    5\n |      dtype: int64\n |      \n |      &gt;&gt;&gt; df = pd.DataFrame({'A': [0, 1, 2, 3, 4],\n |      ...                    'B': [5, 6, 7, 8, 9],\n |      ...                    'C': ['a', 'b', 'c', 'd', 'e']})\n |      &gt;&gt;&gt; df.replace(0, 5)\n |          A  B  C\n |      0  5  5  a\n |      1  1  6  b\n |      2  2  7  c\n |      3  3  8  d\n |      4  4  9  e\n |      \n |      **List-like `to_replace`**\n |      \n |      &gt;&gt;&gt; df.replace([0, 1, 2, 3], 4)\n |          A  B  C\n |      0  4  5  a\n |      1  4  6  b\n |      2  4  7  c\n |      3  4  8  d\n |      4  4  9  e\n |      \n |      &gt;&gt;&gt; df.replace([0, 1, 2, 3], [4, 3, 2, 1])\n |          A  B  C\n |      0  4  5  a\n |      1  3  6  b\n |      2  2  7  c\n |      3  1  8  d\n |      4  4  9  e\n |      \n |      &gt;&gt;&gt; s.replace([1, 2], method='bfill')\n |      0    3\n |      1    3\n |      2    3\n |      3    4\n |      4    5\n |      dtype: int64\n |      \n |      **dict-like `to_replace`**\n |      \n |      &gt;&gt;&gt; df.replace({0: 10, 1: 100})\n |              A  B  C\n |      0   10  5  a\n |      1  100  6  b\n |      2    2  7  c\n |      3    3  8  d\n |      4    4  9  e\n |      \n |      &gt;&gt;&gt; df.replace({'A': 0, 'B': 5}, 100)\n |              A    B  C\n |      0  100  100  a\n |      1    1    6  b\n |      2    2    7  c\n |      3    3    8  d\n |      4    4    9  e\n |      \n |      &gt;&gt;&gt; df.replace({'A': {0: 100, 4: 400}})\n |              A  B  C\n |      0  100  5  a\n |      1    1  6  b\n |      2    2  7  c\n |      3    3  8  d\n |      4  400  9  e\n |      \n |      **Regular expression `to_replace`**\n |      \n |      &gt;&gt;&gt; df = pd.DataFrame({'A': ['bat', 'foo', 'bait'],\n |      ...                    'B': ['abc', 'bar', 'xyz']})\n |      &gt;&gt;&gt; df.replace(to_replace=r'^ba.$', value='new', regex=True)\n |              A    B\n |      0   new  abc\n |      1   foo  new\n |      2  bait  xyz\n |      \n |      &gt;&gt;&gt; df.replace({'A': r'^ba.$'}, {'A': 'new'}, regex=True)\n |              A    B\n |      0   new  abc\n |      1   foo  bar\n |      2  bait  xyz\n |      \n |      &gt;&gt;&gt; df.replace(regex=r'^ba.$', value='new')\n |              A    B\n |      0   new  abc\n |      1   foo  new\n |      2  bait  xyz\n |      \n |      &gt;&gt;&gt; df.replace(regex={r'^ba.$': 'new', 'foo': 'xyz'})\n |              A    B\n |      0   new  abc\n |      1   xyz  new\n |      2  bait  xyz\n |      \n |      &gt;&gt;&gt; df.replace(regex=[r'^ba.$', 'foo'], value='new')\n |              A    B\n |      0   new  abc\n |      1   new  new\n |      2  bait  xyz\n |      \n |      Compare the behavior of ``s.replace({'a': None})`` and\n |      ``s.replace('a', None)`` to understand the peculiarities\n |      of the `to_replace` parameter:\n |      \n |      &gt;&gt;&gt; s = pd.Series([10, 'a', 'a', 'b', 'a'])\n |      \n |      When one uses a dict as the `to_replace` value, it is like the\n |      value(s) in the dict are equal to the `value` parameter.\n |      ``s.replace({'a': None})`` is equivalent to\n |      ``s.replace(to_replace={'a': None}, value=None, method=None)``:\n |      \n |      &gt;&gt;&gt; s.replace({'a': None})\n |      0      10\n |      1    None\n |      2    None\n |      3       b\n |      4    None\n |      dtype: object\n |      \n |      When ``value`` is not explicitly passed and `to_replace` is a scalar, list\n |      or tuple, `replace` uses the method parameter (default 'pad') to do the\n |      replacement. So this is why the 'a' values are being replaced by 10\n |      in rows 1 and 2 and 'b' in row 4 in this case.\n |      \n |      &gt;&gt;&gt; s.replace('a')\n |      0    10\n |      1    10\n |      2    10\n |      3     b\n |      4     b\n |      dtype: object\n |      \n |      On the other hand, if ``None`` is explicitly passed for ``value``, it will\n |      be respected:\n |      \n |      &gt;&gt;&gt; s.replace('a', None)\n |      0      10\n |      1    None\n |      2    None\n |      3       b\n |      4    None\n |      dtype: object\n |      \n |          .. versionchanged:: 1.4.0\n |              Previously the explicit ``None`` was silently ignored.\n |  \n |  resample(self, rule, axis: 'Axis' = 0, closed: 'str | None' = None, label: 'str | None' = None, convention: 'str' = 'start', kind: 'str | None' = None, on: 'Level' = None, level: 'Level' = None, origin: 'str | TimestampConvertibleTypes' = 'start_day', offset: 'TimedeltaConvertibleTypes | None' = None, group_keys: 'bool' = False) -&gt; 'Resampler'\n |      Resample time-series data.\n |      \n |      Convenience method for frequency conversion and resampling of time series.\n |      The object must have a datetime-like index (`DatetimeIndex`, `PeriodIndex`,\n |      or `TimedeltaIndex`), or the caller must pass the label of a datetime-like\n |      series/index to the ``on``/``level`` keyword parameter.\n |      \n |      Parameters\n |      ----------\n |      rule : DateOffset, Timedelta or str\n |          The offset string or object representing target conversion.\n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |          Which axis to use for up- or down-sampling. For `Series` this parameter\n |          is unused and defaults to 0. Must be\n |          `DatetimeIndex`, `TimedeltaIndex` or `PeriodIndex`.\n |      closed : {'right', 'left'}, default None\n |          Which side of bin interval is closed. The default is 'left'\n |          for all frequency offsets except for 'M', 'A', 'Q', 'BM',\n |          'BA', 'BQ', and 'W' which all have a default of 'right'.\n |      label : {'right', 'left'}, default None\n |          Which bin edge label to label bucket with. The default is 'left'\n |          for all frequency offsets except for 'M', 'A', 'Q', 'BM',\n |          'BA', 'BQ', and 'W' which all have a default of 'right'.\n |      convention : {'start', 'end', 's', 'e'}, default 'start'\n |          For `PeriodIndex` only, controls whether to use the start or\n |          end of `rule`.\n |      kind : {'timestamp', 'period'}, optional, default None\n |          Pass 'timestamp' to convert the resulting index to a\n |          `DateTimeIndex` or 'period' to convert it to a `PeriodIndex`.\n |          By default the input representation is retained.\n |      \n |      on : str, optional\n |          For a DataFrame, column to use instead of index for resampling.\n |          Column must be datetime-like.\n |      level : str or int, optional\n |          For a MultiIndex, level (name or number) to use for\n |          resampling. `level` must be datetime-like.\n |      origin : Timestamp or str, default 'start_day'\n |          The timestamp on which to adjust the grouping. The timezone of origin\n |          must match the timezone of the index.\n |          If string, must be one of the following:\n |      \n |          - 'epoch': `origin` is 1970-01-01\n |          - 'start': `origin` is the first value of the timeseries\n |          - 'start_day': `origin` is the first day at midnight of the timeseries\n |      \n |          .. versionadded:: 1.1.0\n |      \n |          - 'end': `origin` is the last value of the timeseries\n |          - 'end_day': `origin` is the ceiling midnight of the last day\n |      \n |          .. versionadded:: 1.3.0\n |      \n |      offset : Timedelta or str, default is None\n |          An offset timedelta added to the origin.\n |      \n |          .. versionadded:: 1.1.0\n |      \n |      group_keys : bool, default False\n |          Whether to include the group keys in the result index when using\n |          ``.apply()`` on the resampled object.\n |      \n |          .. versionadded:: 1.5.0\n |      \n |              Not specifying ``group_keys`` will retain values-dependent behavior\n |              from pandas 1.4 and earlier (see :ref:`pandas 1.5.0 Release notes\n |              &lt;whatsnew_150.enhancements.resample_group_keys&gt;` for examples).\n |      \n |          .. versionchanged:: 2.0.0\n |      \n |              ``group_keys`` now defaults to ``False``.\n |      \n |      Returns\n |      -------\n |      pandas.core.Resampler\n |          :class:`~pandas.core.Resampler` object.\n |      \n |      See Also\n |      --------\n |      Series.resample : Resample a Series.\n |      DataFrame.resample : Resample a DataFrame.\n |      groupby : Group Series by mapping, function, label, or list of labels.\n |      asfreq : Reindex a Series with the given frequency without grouping.\n |      \n |      Notes\n |      -----\n |      See the `user guide\n |      &lt;https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#resampling&gt;`__\n |      for more.\n |      \n |      To learn more about the offset strings, please see `this link\n |      &lt;https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects&gt;`__.\n |      \n |      Examples\n |      --------\n |      Start by creating a series with 9 one minute timestamps.\n |      \n |      &gt;&gt;&gt; index = pd.date_range('1/1/2000', periods=9, freq='T')\n |      &gt;&gt;&gt; series = pd.Series(range(9), index=index)\n |      &gt;&gt;&gt; series\n |      2000-01-01 00:00:00    0\n |      2000-01-01 00:01:00    1\n |      2000-01-01 00:02:00    2\n |      2000-01-01 00:03:00    3\n |      2000-01-01 00:04:00    4\n |      2000-01-01 00:05:00    5\n |      2000-01-01 00:06:00    6\n |      2000-01-01 00:07:00    7\n |      2000-01-01 00:08:00    8\n |      Freq: T, dtype: int64\n |      \n |      Downsample the series into 3 minute bins and sum the values\n |      of the timestamps falling into a bin.\n |      \n |      &gt;&gt;&gt; series.resample('3T').sum()\n |      2000-01-01 00:00:00     3\n |      2000-01-01 00:03:00    12\n |      2000-01-01 00:06:00    21\n |      Freq: 3T, dtype: int64\n |      \n |      Downsample the series into 3 minute bins as above, but label each\n |      bin using the right edge instead of the left. Please note that the\n |      value in the bucket used as the label is not included in the bucket,\n |      which it labels. For example, in the original series the\n |      bucket ``2000-01-01 00:03:00`` contains the value 3, but the summed\n |      value in the resampled bucket with the label ``2000-01-01 00:03:00``\n |      does not include 3 (if it did, the summed value would be 6, not 3).\n |      To include this value close the right side of the bin interval as\n |      illustrated in the example below this one.\n |      \n |      &gt;&gt;&gt; series.resample('3T', label='right').sum()\n |      2000-01-01 00:03:00     3\n |      2000-01-01 00:06:00    12\n |      2000-01-01 00:09:00    21\n |      Freq: 3T, dtype: int64\n |      \n |      Downsample the series into 3 minute bins as above, but close the right\n |      side of the bin interval.\n |      \n |      &gt;&gt;&gt; series.resample('3T', label='right', closed='right').sum()\n |      2000-01-01 00:00:00     0\n |      2000-01-01 00:03:00     6\n |      2000-01-01 00:06:00    15\n |      2000-01-01 00:09:00    15\n |      Freq: 3T, dtype: int64\n |      \n |      Upsample the series into 30 second bins.\n |      \n |      &gt;&gt;&gt; series.resample('30S').asfreq()[0:5]   # Select first 5 rows\n |      2000-01-01 00:00:00   0.0\n |      2000-01-01 00:00:30   NaN\n |      2000-01-01 00:01:00   1.0\n |      2000-01-01 00:01:30   NaN\n |      2000-01-01 00:02:00   2.0\n |      Freq: 30S, dtype: float64\n |      \n |      Upsample the series into 30 second bins and fill the ``NaN``\n |      values using the ``ffill`` method.\n |      \n |      &gt;&gt;&gt; series.resample('30S').ffill()[0:5]\n |      2000-01-01 00:00:00    0\n |      2000-01-01 00:00:30    0\n |      2000-01-01 00:01:00    1\n |      2000-01-01 00:01:30    1\n |      2000-01-01 00:02:00    2\n |      Freq: 30S, dtype: int64\n |      \n |      Upsample the series into 30 second bins and fill the\n |      ``NaN`` values using the ``bfill`` method.\n |      \n |      &gt;&gt;&gt; series.resample('30S').bfill()[0:5]\n |      2000-01-01 00:00:00    0\n |      2000-01-01 00:00:30    1\n |      2000-01-01 00:01:00    1\n |      2000-01-01 00:01:30    2\n |      2000-01-01 00:02:00    2\n |      Freq: 30S, dtype: int64\n |      \n |      Pass a custom function via ``apply``\n |      \n |      &gt;&gt;&gt; def custom_resampler(arraylike):\n |      ...     return np.sum(arraylike) + 5\n |      ...\n |      &gt;&gt;&gt; series.resample('3T').apply(custom_resampler)\n |      2000-01-01 00:00:00     8\n |      2000-01-01 00:03:00    17\n |      2000-01-01 00:06:00    26\n |      Freq: 3T, dtype: int64\n |      \n |      For a Series with a PeriodIndex, the keyword `convention` can be\n |      used to control whether to use the start or end of `rule`.\n |      \n |      Resample a year by quarter using 'start' `convention`. Values are\n |      assigned to the first quarter of the period.\n |      \n |      &gt;&gt;&gt; s = pd.Series([1, 2], index=pd.period_range('2012-01-01',\n |      ...                                             freq='A',\n |      ...                                             periods=2))\n |      &gt;&gt;&gt; s\n |      2012    1\n |      2013    2\n |      Freq: A-DEC, dtype: int64\n |      &gt;&gt;&gt; s.resample('Q', convention='start').asfreq()\n |      2012Q1    1.0\n |      2012Q2    NaN\n |      2012Q3    NaN\n |      2012Q4    NaN\n |      2013Q1    2.0\n |      2013Q2    NaN\n |      2013Q3    NaN\n |      2013Q4    NaN\n |      Freq: Q-DEC, dtype: float64\n |      \n |      Resample quarters by month using 'end' `convention`. Values are\n |      assigned to the last month of the period.\n |      \n |      &gt;&gt;&gt; q = pd.Series([1, 2, 3, 4], index=pd.period_range('2018-01-01',\n |      ...                                                   freq='Q',\n |      ...                                                   periods=4))\n |      &gt;&gt;&gt; q\n |      2018Q1    1\n |      2018Q2    2\n |      2018Q3    3\n |      2018Q4    4\n |      Freq: Q-DEC, dtype: int64\n |      &gt;&gt;&gt; q.resample('M', convention='end').asfreq()\n |      2018-03    1.0\n |      2018-04    NaN\n |      2018-05    NaN\n |      2018-06    2.0\n |      2018-07    NaN\n |      2018-08    NaN\n |      2018-09    3.0\n |      2018-10    NaN\n |      2018-11    NaN\n |      2018-12    4.0\n |      Freq: M, dtype: float64\n |      \n |      For DataFrame objects, the keyword `on` can be used to specify the\n |      column instead of the index for resampling.\n |      \n |      &gt;&gt;&gt; d = {'price': [10, 11, 9, 13, 14, 18, 17, 19],\n |      ...      'volume': [50, 60, 40, 100, 50, 100, 40, 50]}\n |      &gt;&gt;&gt; df = pd.DataFrame(d)\n |      &gt;&gt;&gt; df['week_starting'] = pd.date_range('01/01/2018',\n |      ...                                     periods=8,\n |      ...                                     freq='W')\n |      &gt;&gt;&gt; df\n |         price  volume week_starting\n |      0     10      50    2018-01-07\n |      1     11      60    2018-01-14\n |      2      9      40    2018-01-21\n |      3     13     100    2018-01-28\n |      4     14      50    2018-02-04\n |      5     18     100    2018-02-11\n |      6     17      40    2018-02-18\n |      7     19      50    2018-02-25\n |      &gt;&gt;&gt; df.resample('M', on='week_starting').mean()\n |                     price  volume\n |      week_starting\n |      2018-01-31     10.75    62.5\n |      2018-02-28     17.00    60.0\n |      \n |      For a DataFrame with MultiIndex, the keyword `level` can be used to\n |      specify on which level the resampling needs to take place.\n |      \n |      &gt;&gt;&gt; days = pd.date_range('1/1/2000', periods=4, freq='D')\n |      &gt;&gt;&gt; d2 = {'price': [10, 11, 9, 13, 14, 18, 17, 19],\n |      ...       'volume': [50, 60, 40, 100, 50, 100, 40, 50]}\n |      &gt;&gt;&gt; df2 = pd.DataFrame(\n |      ...     d2,\n |      ...     index=pd.MultiIndex.from_product(\n |      ...         [days, ['morning', 'afternoon']]\n |      ...     )\n |      ... )\n |      &gt;&gt;&gt; df2\n |                            price  volume\n |      2000-01-01 morning       10      50\n |                 afternoon     11      60\n |      2000-01-02 morning        9      40\n |                 afternoon     13     100\n |      2000-01-03 morning       14      50\n |                 afternoon     18     100\n |      2000-01-04 morning       17      40\n |                 afternoon     19      50\n |      &gt;&gt;&gt; df2.resample('D', level=0).sum()\n |                  price  volume\n |      2000-01-01     21     110\n |      2000-01-02     22     140\n |      2000-01-03     32     150\n |      2000-01-04     36      90\n |      \n |      If you want to adjust the start of the bins based on a fixed timestamp:\n |      \n |      &gt;&gt;&gt; start, end = '2000-10-01 23:30:00', '2000-10-02 00:30:00'\n |      &gt;&gt;&gt; rng = pd.date_range(start, end, freq='7min')\n |      &gt;&gt;&gt; ts = pd.Series(np.arange(len(rng)) * 3, index=rng)\n |      &gt;&gt;&gt; ts\n |      2000-10-01 23:30:00     0\n |      2000-10-01 23:37:00     3\n |      2000-10-01 23:44:00     6\n |      2000-10-01 23:51:00     9\n |      2000-10-01 23:58:00    12\n |      2000-10-02 00:05:00    15\n |      2000-10-02 00:12:00    18\n |      2000-10-02 00:19:00    21\n |      2000-10-02 00:26:00    24\n |      Freq: 7T, dtype: int64\n |      \n |      &gt;&gt;&gt; ts.resample('17min').sum()\n |      2000-10-01 23:14:00     0\n |      2000-10-01 23:31:00     9\n |      2000-10-01 23:48:00    21\n |      2000-10-02 00:05:00    54\n |      2000-10-02 00:22:00    24\n |      Freq: 17T, dtype: int64\n |      \n |      &gt;&gt;&gt; ts.resample('17min', origin='epoch').sum()\n |      2000-10-01 23:18:00     0\n |      2000-10-01 23:35:00    18\n |      2000-10-01 23:52:00    27\n |      2000-10-02 00:09:00    39\n |      2000-10-02 00:26:00    24\n |      Freq: 17T, dtype: int64\n |      \n |      &gt;&gt;&gt; ts.resample('17min', origin='2000-01-01').sum()\n |      2000-10-01 23:24:00     3\n |      2000-10-01 23:41:00    15\n |      2000-10-01 23:58:00    45\n |      2000-10-02 00:15:00    45\n |      Freq: 17T, dtype: int64\n |      \n |      If you want to adjust the start of the bins with an `offset` Timedelta, the two\n |      following lines are equivalent:\n |      \n |      &gt;&gt;&gt; ts.resample('17min', origin='start').sum()\n |      2000-10-01 23:30:00     9\n |      2000-10-01 23:47:00    21\n |      2000-10-02 00:04:00    54\n |      2000-10-02 00:21:00    24\n |      Freq: 17T, dtype: int64\n |      \n |      &gt;&gt;&gt; ts.resample('17min', offset='23h30min').sum()\n |      2000-10-01 23:30:00     9\n |      2000-10-01 23:47:00    21\n |      2000-10-02 00:04:00    54\n |      2000-10-02 00:21:00    24\n |      Freq: 17T, dtype: int64\n |      \n |      If you want to take the largest Timestamp as the end of the bins:\n |      \n |      &gt;&gt;&gt; ts.resample('17min', origin='end').sum()\n |      2000-10-01 23:35:00     0\n |      2000-10-01 23:52:00    18\n |      2000-10-02 00:09:00    27\n |      2000-10-02 00:26:00    63\n |      Freq: 17T, dtype: int64\n |      \n |      In contrast with the `start_day`, you can use `end_day` to take the ceiling\n |      midnight of the largest Timestamp as the end of the bins and drop the bins\n |      not containing data:\n |      \n |      &gt;&gt;&gt; ts.resample('17min', origin='end_day').sum()\n |      2000-10-01 23:38:00     3\n |      2000-10-01 23:55:00    15\n |      2000-10-02 00:12:00    45\n |      2000-10-02 00:29:00    45\n |      Freq: 17T, dtype: int64\n |  \n |  reset_index(self, level: 'IndexLabel' = None, *, drop: 'bool' = False, name: 'Level' = &lt;no_default&gt;, inplace: 'bool' = False, allow_duplicates: 'bool' = False) -&gt; 'DataFrame | Series | None'\n |      Generate a new DataFrame or Series with the index reset.\n |      \n |      This is useful when the index needs to be treated as a column, or\n |      when the index is meaningless and needs to be reset to the default\n |      before another operation.\n |      \n |      Parameters\n |      ----------\n |      level : int, str, tuple, or list, default optional\n |          For a Series with a MultiIndex, only remove the specified levels\n |          from the index. Removes all levels by default.\n |      drop : bool, default False\n |          Just reset the index, without inserting it as a column in\n |          the new DataFrame.\n |      name : object, optional\n |          The name to use for the column containing the original Series\n |          values. Uses ``self.name`` by default. This argument is ignored\n |          when `drop` is True.\n |      inplace : bool, default False\n |          Modify the Series in place (do not create a new object).\n |      allow_duplicates : bool, default False\n |          Allow duplicate column labels to be created.\n |      \n |          .. versionadded:: 1.5.0\n |      \n |      Returns\n |      -------\n |      Series or DataFrame or None\n |          When `drop` is False (the default), a DataFrame is returned.\n |          The newly created columns will come first in the DataFrame,\n |          followed by the original Series values.\n |          When `drop` is True, a `Series` is returned.\n |          In either case, if ``inplace=True``, no value is returned.\n |      \n |      See Also\n |      --------\n |      DataFrame.reset_index: Analogous function for DataFrame.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; s = pd.Series([1, 2, 3, 4], name='foo',\n |      ...               index=pd.Index(['a', 'b', 'c', 'd'], name='idx'))\n |      \n |      Generate a DataFrame with default index.\n |      \n |      &gt;&gt;&gt; s.reset_index()\n |        idx  foo\n |      0   a    1\n |      1   b    2\n |      2   c    3\n |      3   d    4\n |      \n |      To specify the name of the new column use `name`.\n |      \n |      &gt;&gt;&gt; s.reset_index(name='values')\n |        idx  values\n |      0   a       1\n |      1   b       2\n |      2   c       3\n |      3   d       4\n |      \n |      To generate a new Series with the default set `drop` to True.\n |      \n |      &gt;&gt;&gt; s.reset_index(drop=True)\n |      0    1\n |      1    2\n |      2    3\n |      3    4\n |      Name: foo, dtype: int64\n |      \n |      The `level` parameter is interesting for Series with a multi-level\n |      index.\n |      \n |      &gt;&gt;&gt; arrays = [np.array(['bar', 'bar', 'baz', 'baz']),\n |      ...           np.array(['one', 'two', 'one', 'two'])]\n |      &gt;&gt;&gt; s2 = pd.Series(\n |      ...     range(4), name='foo',\n |      ...     index=pd.MultiIndex.from_arrays(arrays,\n |      ...                                     names=['a', 'b']))\n |      \n |      To remove a specific level from the Index, use `level`.\n |      \n |      &gt;&gt;&gt; s2.reset_index(level='a')\n |             a  foo\n |      b\n |      one  bar    0\n |      two  bar    1\n |      one  baz    2\n |      two  baz    3\n |      \n |      If `level` is not set, all levels are removed from the Index.\n |      \n |      &gt;&gt;&gt; s2.reset_index()\n |           a    b  foo\n |      0  bar  one    0\n |      1  bar  two    1\n |      2  baz  one    2\n |      3  baz  two    3\n |  \n |  rfloordiv(self, other, level=None, fill_value=None, axis: 'Axis' = 0)\n |      Return Integer division of series and other, element-wise (binary operator `rfloordiv`).\n |      \n |      Equivalent to ``other // series``, but with support to substitute a fill_value for\n |      missing data in either one of the inputs.\n |      \n |      Parameters\n |      ----------\n |      other : Series or scalar value\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level.\n |      fill_value : None or float value, default None (NaN)\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful Series alignment, with this value before computation.\n |          If data in both corresponding Series locations is missing\n |          the result of filling (at that location) will be missing.\n |      axis : {0 or 'index'}\n |          Unused. Parameter needed for compatibility with DataFrame.\n |      \n |      Returns\n |      -------\n |      Series\n |          The result of the operation.\n |      \n |      See Also\n |      --------\n |      Series.floordiv : Element-wise Integer division, see\n |          `Python documentation\n |          &lt;https://docs.python.org/3/reference/datamodel.html#emulating-numeric-types&gt;`_\n |          for more details.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; a = pd.Series([1, 1, 1, np.nan], index=['a', 'b', 'c', 'd'])\n |      &gt;&gt;&gt; a\n |      a    1.0\n |      b    1.0\n |      c    1.0\n |      d    NaN\n |      dtype: float64\n |      &gt;&gt;&gt; b = pd.Series([1, np.nan, 1, np.nan], index=['a', 'b', 'd', 'e'])\n |      &gt;&gt;&gt; b\n |      a    1.0\n |      b    NaN\n |      d    1.0\n |      e    NaN\n |      dtype: float64\n |      &gt;&gt;&gt; a.floordiv(b, fill_value=0)\n |      a    1.0\n |      b    inf\n |      c    inf\n |      d    0.0\n |      e    NaN\n |      dtype: float64\n |  \n |  rmod(self, other, level=None, fill_value=None, axis: 'Axis' = 0)\n |      Return Modulo of series and other, element-wise (binary operator `rmod`).\n |      \n |      Equivalent to ``other % series``, but with support to substitute a fill_value for\n |      missing data in either one of the inputs.\n |      \n |      Parameters\n |      ----------\n |      other : Series or scalar value\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level.\n |      fill_value : None or float value, default None (NaN)\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful Series alignment, with this value before computation.\n |          If data in both corresponding Series locations is missing\n |          the result of filling (at that location) will be missing.\n |      axis : {0 or 'index'}\n |          Unused. Parameter needed for compatibility with DataFrame.\n |      \n |      Returns\n |      -------\n |      Series\n |          The result of the operation.\n |      \n |      See Also\n |      --------\n |      Series.mod : Element-wise Modulo, see\n |          `Python documentation\n |          &lt;https://docs.python.org/3/reference/datamodel.html#emulating-numeric-types&gt;`_\n |          for more details.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; a = pd.Series([1, 1, 1, np.nan], index=['a', 'b', 'c', 'd'])\n |      &gt;&gt;&gt; a\n |      a    1.0\n |      b    1.0\n |      c    1.0\n |      d    NaN\n |      dtype: float64\n |      &gt;&gt;&gt; b = pd.Series([1, np.nan, 1, np.nan], index=['a', 'b', 'd', 'e'])\n |      &gt;&gt;&gt; b\n |      a    1.0\n |      b    NaN\n |      d    1.0\n |      e    NaN\n |      dtype: float64\n |      &gt;&gt;&gt; a.mod(b, fill_value=0)\n |      a    0.0\n |      b    NaN\n |      c    NaN\n |      d    0.0\n |      e    NaN\n |      dtype: float64\n |  \n |  rmul(self, other, level=None, fill_value=None, axis: 'Axis' = 0)\n |      Return Multiplication of series and other, element-wise (binary operator `rmul`).\n |      \n |      Equivalent to ``other * series``, but with support to substitute a fill_value for\n |      missing data in either one of the inputs.\n |      \n |      Parameters\n |      ----------\n |      other : Series or scalar value\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level.\n |      fill_value : None or float value, default None (NaN)\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful Series alignment, with this value before computation.\n |          If data in both corresponding Series locations is missing\n |          the result of filling (at that location) will be missing.\n |      axis : {0 or 'index'}\n |          Unused. Parameter needed for compatibility with DataFrame.\n |      \n |      Returns\n |      -------\n |      Series\n |          The result of the operation.\n |      \n |      See Also\n |      --------\n |      Series.mul : Element-wise Multiplication, see\n |          `Python documentation\n |          &lt;https://docs.python.org/3/reference/datamodel.html#emulating-numeric-types&gt;`_\n |          for more details.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; a = pd.Series([1, 1, 1, np.nan], index=['a', 'b', 'c', 'd'])\n |      &gt;&gt;&gt; a\n |      a    1.0\n |      b    1.0\n |      c    1.0\n |      d    NaN\n |      dtype: float64\n |      &gt;&gt;&gt; b = pd.Series([1, np.nan, 1, np.nan], index=['a', 'b', 'd', 'e'])\n |      &gt;&gt;&gt; b\n |      a    1.0\n |      b    NaN\n |      d    1.0\n |      e    NaN\n |      dtype: float64\n |      &gt;&gt;&gt; a.multiply(b, fill_value=0)\n |      a    1.0\n |      b    0.0\n |      c    0.0\n |      d    0.0\n |      e    NaN\n |      dtype: float64\n |  \n |  round(self, decimals: 'int' = 0, *args, **kwargs) -&gt; 'Series'\n |      Round each value in a Series to the given number of decimals.\n |      \n |      Parameters\n |      ----------\n |      decimals : int, default 0\n |          Number of decimal places to round to. If decimals is negative,\n |          it specifies the number of positions to the left of the decimal point.\n |      *args, **kwargs\n |          Additional arguments and keywords have no effect but might be\n |          accepted for compatibility with NumPy.\n |      \n |      Returns\n |      -------\n |      Series\n |          Rounded values of the Series.\n |      \n |      See Also\n |      --------\n |      numpy.around : Round values of an np.array.\n |      DataFrame.round : Round values of a DataFrame.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; s = pd.Series([0.1, 1.3, 2.7])\n |      &gt;&gt;&gt; s.round()\n |      0    0.0\n |      1    1.0\n |      2    3.0\n |      dtype: float64\n |  \n |  rpow(self, other, level=None, fill_value=None, axis: 'Axis' = 0)\n |      Return Exponential power of series and other, element-wise (binary operator `rpow`).\n |      \n |      Equivalent to ``other ** series``, but with support to substitute a fill_value for\n |      missing data in either one of the inputs.\n |      \n |      Parameters\n |      ----------\n |      other : Series or scalar value\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level.\n |      fill_value : None or float value, default None (NaN)\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful Series alignment, with this value before computation.\n |          If data in both corresponding Series locations is missing\n |          the result of filling (at that location) will be missing.\n |      axis : {0 or 'index'}\n |          Unused. Parameter needed for compatibility with DataFrame.\n |      \n |      Returns\n |      -------\n |      Series\n |          The result of the operation.\n |      \n |      See Also\n |      --------\n |      Series.pow : Element-wise Exponential power, see\n |          `Python documentation\n |          &lt;https://docs.python.org/3/reference/datamodel.html#emulating-numeric-types&gt;`_\n |          for more details.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; a = pd.Series([1, 1, 1, np.nan], index=['a', 'b', 'c', 'd'])\n |      &gt;&gt;&gt; a\n |      a    1.0\n |      b    1.0\n |      c    1.0\n |      d    NaN\n |      dtype: float64\n |      &gt;&gt;&gt; b = pd.Series([1, np.nan, 1, np.nan], index=['a', 'b', 'd', 'e'])\n |      &gt;&gt;&gt; b\n |      a    1.0\n |      b    NaN\n |      d    1.0\n |      e    NaN\n |      dtype: float64\n |      &gt;&gt;&gt; a.pow(b, fill_value=0)\n |      a    1.0\n |      b    1.0\n |      c    1.0\n |      d    0.0\n |      e    NaN\n |      dtype: float64\n |  \n |  rsub(self, other, level=None, fill_value=None, axis: 'Axis' = 0)\n |      Return Subtraction of series and other, element-wise (binary operator `rsub`).\n |      \n |      Equivalent to ``other - series``, but with support to substitute a fill_value for\n |      missing data in either one of the inputs.\n |      \n |      Parameters\n |      ----------\n |      other : Series or scalar value\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level.\n |      fill_value : None or float value, default None (NaN)\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful Series alignment, with this value before computation.\n |          If data in both corresponding Series locations is missing\n |          the result of filling (at that location) will be missing.\n |      axis : {0 or 'index'}\n |          Unused. Parameter needed for compatibility with DataFrame.\n |      \n |      Returns\n |      -------\n |      Series\n |          The result of the operation.\n |      \n |      See Also\n |      --------\n |      Series.sub : Element-wise Subtraction, see\n |          `Python documentation\n |          &lt;https://docs.python.org/3/reference/datamodel.html#emulating-numeric-types&gt;`_\n |          for more details.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; a = pd.Series([1, 1, 1, np.nan], index=['a', 'b', 'c', 'd'])\n |      &gt;&gt;&gt; a\n |      a    1.0\n |      b    1.0\n |      c    1.0\n |      d    NaN\n |      dtype: float64\n |      &gt;&gt;&gt; b = pd.Series([1, np.nan, 1, np.nan], index=['a', 'b', 'd', 'e'])\n |      &gt;&gt;&gt; b\n |      a    1.0\n |      b    NaN\n |      d    1.0\n |      e    NaN\n |      dtype: float64\n |      &gt;&gt;&gt; a.subtract(b, fill_value=0)\n |      a    0.0\n |      b    1.0\n |      c    1.0\n |      d   -1.0\n |      e    NaN\n |      dtype: float64\n |  \n |  rtruediv(self, other, level=None, fill_value=None, axis: 'Axis' = 0)\n |      Return Floating division of series and other, element-wise (binary operator `rtruediv`).\n |      \n |      Equivalent to ``other / series``, but with support to substitute a fill_value for\n |      missing data in either one of the inputs.\n |      \n |      Parameters\n |      ----------\n |      other : Series or scalar value\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level.\n |      fill_value : None or float value, default None (NaN)\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful Series alignment, with this value before computation.\n |          If data in both corresponding Series locations is missing\n |          the result of filling (at that location) will be missing.\n |      axis : {0 or 'index'}\n |          Unused. Parameter needed for compatibility with DataFrame.\n |      \n |      Returns\n |      -------\n |      Series\n |          The result of the operation.\n |      \n |      See Also\n |      --------\n |      Series.truediv : Element-wise Floating division, see\n |          `Python documentation\n |          &lt;https://docs.python.org/3/reference/datamodel.html#emulating-numeric-types&gt;`_\n |          for more details.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; a = pd.Series([1, 1, 1, np.nan], index=['a', 'b', 'c', 'd'])\n |      &gt;&gt;&gt; a\n |      a    1.0\n |      b    1.0\n |      c    1.0\n |      d    NaN\n |      dtype: float64\n |      &gt;&gt;&gt; b = pd.Series([1, np.nan, 1, np.nan], index=['a', 'b', 'd', 'e'])\n |      &gt;&gt;&gt; b\n |      a    1.0\n |      b    NaN\n |      d    1.0\n |      e    NaN\n |      dtype: float64\n |      &gt;&gt;&gt; a.divide(b, fill_value=0)\n |      a    1.0\n |      b    inf\n |      c    inf\n |      d    0.0\n |      e    NaN\n |      dtype: float64\n |  \n |  searchsorted(self, value: 'NumpyValueArrayLike | ExtensionArray', side: \"Literal['left', 'right']\" = 'left', sorter: 'NumpySorter' = None) -&gt; 'npt.NDArray[np.intp] | np.intp'\n |      Find indices where elements should be inserted to maintain order.\n |      \n |      Find the indices into a sorted Series `self` such that, if the\n |      corresponding elements in `value` were inserted before the indices,\n |      the order of `self` would be preserved.\n |      \n |      .. note::\n |      \n |          The Series *must* be monotonically sorted, otherwise\n |          wrong locations will likely be returned. Pandas does *not*\n |          check this for you.\n |      \n |      Parameters\n |      ----------\n |      value : array-like or scalar\n |          Values to insert into `self`.\n |      side : {'left', 'right'}, optional\n |          If 'left', the index of the first suitable location found is given.\n |          If 'right', return the last such index.  If there is no suitable\n |          index, return either 0 or N (where N is the length of `self`).\n |      sorter : 1-D array-like, optional\n |          Optional array of integer indices that sort `self` into ascending\n |          order. They are typically the result of ``np.argsort``.\n |      \n |      Returns\n |      -------\n |      int or array of int\n |          A scalar or array of insertion points with the\n |          same shape as `value`.\n |      \n |      See Also\n |      --------\n |      sort_values : Sort by the values along either axis.\n |      numpy.searchsorted : Similar method from NumPy.\n |      \n |      Notes\n |      -----\n |      Binary search is used to find the required insertion points.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; ser = pd.Series([1, 2, 3])\n |      &gt;&gt;&gt; ser\n |      0    1\n |      1    2\n |      2    3\n |      dtype: int64\n |      \n |      &gt;&gt;&gt; ser.searchsorted(4)\n |      3\n |      \n |      &gt;&gt;&gt; ser.searchsorted([0, 4])\n |      array([0, 3])\n |      \n |      &gt;&gt;&gt; ser.searchsorted([1, 3], side='left')\n |      array([0, 2])\n |      \n |      &gt;&gt;&gt; ser.searchsorted([1, 3], side='right')\n |      array([1, 3])\n |      \n |      &gt;&gt;&gt; ser = pd.Series(pd.to_datetime(['3/11/2000', '3/12/2000', '3/13/2000']))\n |      &gt;&gt;&gt; ser\n |      0   2000-03-11\n |      1   2000-03-12\n |      2   2000-03-13\n |      dtype: datetime64[ns]\n |      \n |      &gt;&gt;&gt; ser.searchsorted('3/14/2000')\n |      3\n |      \n |      &gt;&gt;&gt; ser = pd.Categorical(\n |      ...     ['apple', 'bread', 'bread', 'cheese', 'milk'], ordered=True\n |      ... )\n |      &gt;&gt;&gt; ser\n |      ['apple', 'bread', 'bread', 'cheese', 'milk']\n |      Categories (4, object): ['apple' &lt; 'bread' &lt; 'cheese' &lt; 'milk']\n |      \n |      &gt;&gt;&gt; ser.searchsorted('bread')\n |      1\n |      \n |      &gt;&gt;&gt; ser.searchsorted(['bread'], side='right')\n |      array([3])\n |      \n |      If the values are not monotonically sorted, wrong locations\n |      may be returned:\n |      \n |      &gt;&gt;&gt; ser = pd.Series([2, 1, 3])\n |      &gt;&gt;&gt; ser\n |      0    2\n |      1    1\n |      2    3\n |      dtype: int64\n |      \n |      &gt;&gt;&gt; ser.searchsorted(1)  # doctest: +SKIP\n |      0  # wrong result, correct would be 1\n |  \n |  sem(self, axis: 'Axis | None' = None, skipna: 'bool_t' = True, ddof: 'int' = 1, numeric_only: 'bool_t' = False, **kwargs)\n |      Return unbiased standard error of the mean over requested axis.\n |      \n |      Normalized by N-1 by default. This can be changed using the ddof argument\n |      \n |      Parameters\n |      ----------\n |      axis : {index (0)}\n |          For `Series` this parameter is unused and defaults to 0.\n |      skipna : bool, default True\n |          Exclude NA/null values. If an entire row/column is NA, the result\n |          will be NA.\n |      ddof : int, default 1\n |          Delta Degrees of Freedom. The divisor used in calculations is N - ddof,\n |          where N represents the number of elements.\n |      numeric_only : bool, default False\n |          Include only float, int, boolean columns. Not implemented for Series.\n |      \n |      Returns\n |      -------\n |      scalar or Series (if level specified)\n |  \n |  set_axis(self, labels, *, axis: 'Axis' = 0, copy: 'bool | None' = None) -&gt; 'Series'\n |      Assign desired index to given axis.\n |      \n |      Indexes for row labels can be changed by assigning\n |      a list-like or Index.\n |      \n |      Parameters\n |      ----------\n |      labels : list-like, Index\n |          The values for the new index.\n |      \n |      axis : {0 or 'index'}, default 0\n |          The axis to update. The value 0 identifies the rows. For `Series`\n |          this parameter is unused and defaults to 0.\n |      \n |      copy : bool, default True\n |          Whether to make a copy of the underlying data.\n |      \n |          .. versionadded:: 1.5.0\n |      \n |      Returns\n |      -------\n |      Series\n |          An object of type Series.\n |      \n |      See Also\n |      --------\n |      Series.rename_axis : Alter the name of the index.\n |      \n |              Examples\n |              --------\n |              &gt;&gt;&gt; s = pd.Series([1, 2, 3])\n |              &gt;&gt;&gt; s\n |              0    1\n |              1    2\n |              2    3\n |              dtype: int64\n |      \n |              &gt;&gt;&gt; s.set_axis(['a', 'b', 'c'], axis=0)\n |              a    1\n |              b    2\n |              c    3\n |              dtype: int64\n |  \n |  shift(self, periods: 'int' = 1, freq=None, axis: 'Axis' = 0, fill_value: 'Hashable' = None) -&gt; 'Series'\n |      Shift index by desired number of periods with an optional time `freq`.\n |      \n |      When `freq` is not passed, shift the index without realigning the data.\n |      If `freq` is passed (in this case, the index must be date or datetime,\n |      or it will raise a `NotImplementedError`), the index will be\n |      increased using the periods and the `freq`. `freq` can be inferred\n |      when specified as \"infer\" as long as either freq or inferred_freq\n |      attribute is set in the index.\n |      \n |      Parameters\n |      ----------\n |      periods : int\n |          Number of periods to shift. Can be positive or negative.\n |      freq : DateOffset, tseries.offsets, timedelta, or str, optional\n |          Offset to use from the tseries module or time rule (e.g. 'EOM').\n |          If `freq` is specified then the index values are shifted but the\n |          data is not realigned. That is, use `freq` if you would like to\n |          extend the index when shifting and preserve the original data.\n |          If `freq` is specified as \"infer\" then it will be inferred from\n |          the freq or inferred_freq attributes of the index. If neither of\n |          those attributes exist, a ValueError is thrown.\n |      axis : {0 or 'index', 1 or 'columns', None}, default None\n |          Shift direction. For `Series` this parameter is unused and defaults to 0.\n |      fill_value : object, optional\n |          The scalar value to use for newly introduced missing values.\n |          the default depends on the dtype of `self`.\n |          For numeric data, ``np.nan`` is used.\n |          For datetime, timedelta, or period data, etc. :attr:`NaT` is used.\n |          For extension dtypes, ``self.dtype.na_value`` is used.\n |      \n |          .. versionchanged:: 1.1.0\n |      \n |      Returns\n |      -------\n |      Series\n |          Copy of input object, shifted.\n |      \n |      See Also\n |      --------\n |      Index.shift : Shift values of Index.\n |      DatetimeIndex.shift : Shift values of DatetimeIndex.\n |      PeriodIndex.shift : Shift values of PeriodIndex.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; df = pd.DataFrame({\"Col1\": [10, 20, 15, 30, 45],\n |      ...                    \"Col2\": [13, 23, 18, 33, 48],\n |      ...                    \"Col3\": [17, 27, 22, 37, 52]},\n |      ...                   index=pd.date_range(\"2020-01-01\", \"2020-01-05\"))\n |      &gt;&gt;&gt; df\n |                  Col1  Col2  Col3\n |      2020-01-01    10    13    17\n |      2020-01-02    20    23    27\n |      2020-01-03    15    18    22\n |      2020-01-04    30    33    37\n |      2020-01-05    45    48    52\n |      \n |      &gt;&gt;&gt; df.shift(periods=3)\n |                  Col1  Col2  Col3\n |      2020-01-01   NaN   NaN   NaN\n |      2020-01-02   NaN   NaN   NaN\n |      2020-01-03   NaN   NaN   NaN\n |      2020-01-04  10.0  13.0  17.0\n |      2020-01-05  20.0  23.0  27.0\n |      \n |      &gt;&gt;&gt; df.shift(periods=1, axis=\"columns\")\n |                  Col1  Col2  Col3\n |      2020-01-01   NaN    10    13\n |      2020-01-02   NaN    20    23\n |      2020-01-03   NaN    15    18\n |      2020-01-04   NaN    30    33\n |      2020-01-05   NaN    45    48\n |      \n |      &gt;&gt;&gt; df.shift(periods=3, fill_value=0)\n |                  Col1  Col2  Col3\n |      2020-01-01     0     0     0\n |      2020-01-02     0     0     0\n |      2020-01-03     0     0     0\n |      2020-01-04    10    13    17\n |      2020-01-05    20    23    27\n |      \n |      &gt;&gt;&gt; df.shift(periods=3, freq=\"D\")\n |                  Col1  Col2  Col3\n |      2020-01-04    10    13    17\n |      2020-01-05    20    23    27\n |      2020-01-06    15    18    22\n |      2020-01-07    30    33    37\n |      2020-01-08    45    48    52\n |      \n |      &gt;&gt;&gt; df.shift(periods=3, freq=\"infer\")\n |                  Col1  Col2  Col3\n |      2020-01-04    10    13    17\n |      2020-01-05    20    23    27\n |      2020-01-06    15    18    22\n |      2020-01-07    30    33    37\n |      2020-01-08    45    48    52\n |  \n |  skew(self, axis: 'AxisInt | None' = 0, skipna: 'bool_t' = True, numeric_only: 'bool_t' = False, **kwargs)\n |      Return unbiased skew over requested axis.\n |      \n |      Normalized by N-1.\n |      \n |      Parameters\n |      ----------\n |      axis : {index (0)}\n |          Axis for the function to be applied on.\n |          For `Series` this parameter is unused and defaults to 0.\n |      \n |          For DataFrames, specifying ``axis=None`` will apply the aggregation\n |          across both axes.\n |      \n |          .. versionadded:: 2.0.0\n |      \n |      skipna : bool, default True\n |          Exclude NA/null values when computing the result.\n |      numeric_only : bool, default False\n |          Include only float, int, boolean columns. Not implemented for Series.\n |      \n |      **kwargs\n |          Additional keyword arguments to be passed to the function.\n |      \n |      Returns\n |      -------\n |      scalar or scalar\n |  \n |  sort_index(self, *, axis: 'Axis' = 0, level: 'IndexLabel' = None, ascending: 'bool | Sequence[bool]' = True, inplace: 'bool' = False, kind: 'SortKind' = 'quicksort', na_position: 'NaPosition' = 'last', sort_remaining: 'bool' = True, ignore_index: 'bool' = False, key: 'IndexKeyFunc' = None) -&gt; 'Series | None'\n |      Sort Series by index labels.\n |      \n |      Returns a new Series sorted by label if `inplace` argument is\n |      ``False``, otherwise updates the original series and returns None.\n |      \n |      Parameters\n |      ----------\n |      axis : {0 or 'index'}\n |          Unused. Parameter needed for compatibility with DataFrame.\n |      level : int, optional\n |          If not None, sort on values in specified index level(s).\n |      ascending : bool or list-like of bools, default True\n |          Sort ascending vs. descending. When the index is a MultiIndex the\n |          sort direction can be controlled for each level individually.\n |      inplace : bool, default False\n |          If True, perform operation in-place.\n |      kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, default 'quicksort'\n |          Choice of sorting algorithm. See also :func:`numpy.sort` for more\n |          information. 'mergesort' and 'stable' are the only stable algorithms. For\n |          DataFrames, this option is only applied when sorting on a single\n |          column or label.\n |      na_position : {'first', 'last'}, default 'last'\n |          If 'first' puts NaNs at the beginning, 'last' puts NaNs at the end.\n |          Not implemented for MultiIndex.\n |      sort_remaining : bool, default True\n |          If True and sorting by level and index is multilevel, sort by other\n |          levels too (in order) after sorting by specified level.\n |      ignore_index : bool, default False\n |          If True, the resulting axis will be labeled 0, 1, …, n - 1.\n |      key : callable, optional\n |          If not None, apply the key function to the index values\n |          before sorting. This is similar to the `key` argument in the\n |          builtin :meth:`sorted` function, with the notable difference that\n |          this `key` function should be *vectorized*. It should expect an\n |          ``Index`` and return an ``Index`` of the same shape.\n |      \n |          .. versionadded:: 1.1.0\n |      \n |      Returns\n |      -------\n |      Series or None\n |          The original Series sorted by the labels or None if ``inplace=True``.\n |      \n |      See Also\n |      --------\n |      DataFrame.sort_index: Sort DataFrame by the index.\n |      DataFrame.sort_values: Sort DataFrame by the value.\n |      Series.sort_values : Sort Series by the value.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; s = pd.Series(['a', 'b', 'c', 'd'], index=[3, 2, 1, 4])\n |      &gt;&gt;&gt; s.sort_index()\n |      1    c\n |      2    b\n |      3    a\n |      4    d\n |      dtype: object\n |      \n |      Sort Descending\n |      \n |      &gt;&gt;&gt; s.sort_index(ascending=False)\n |      4    d\n |      3    a\n |      2    b\n |      1    c\n |      dtype: object\n |      \n |      By default NaNs are put at the end, but use `na_position` to place\n |      them at the beginning\n |      \n |      &gt;&gt;&gt; s = pd.Series(['a', 'b', 'c', 'd'], index=[3, 2, 1, np.nan])\n |      &gt;&gt;&gt; s.sort_index(na_position='first')\n |      NaN     d\n |       1.0    c\n |       2.0    b\n |       3.0    a\n |      dtype: object\n |      \n |      Specify index level to sort\n |      \n |      &gt;&gt;&gt; arrays = [np.array(['qux', 'qux', 'foo', 'foo',\n |      ...                     'baz', 'baz', 'bar', 'bar']),\n |      ...           np.array(['two', 'one', 'two', 'one',\n |      ...                     'two', 'one', 'two', 'one'])]\n |      &gt;&gt;&gt; s = pd.Series([1, 2, 3, 4, 5, 6, 7, 8], index=arrays)\n |      &gt;&gt;&gt; s.sort_index(level=1)\n |      bar  one    8\n |      baz  one    6\n |      foo  one    4\n |      qux  one    2\n |      bar  two    7\n |      baz  two    5\n |      foo  two    3\n |      qux  two    1\n |      dtype: int64\n |      \n |      Does not sort by remaining levels when sorting by levels\n |      \n |      &gt;&gt;&gt; s.sort_index(level=1, sort_remaining=False)\n |      qux  one    2\n |      foo  one    4\n |      baz  one    6\n |      bar  one    8\n |      qux  two    1\n |      foo  two    3\n |      baz  two    5\n |      bar  two    7\n |      dtype: int64\n |      \n |      Apply a key function before sorting\n |      \n |      &gt;&gt;&gt; s = pd.Series([1, 2, 3, 4], index=['A', 'b', 'C', 'd'])\n |      &gt;&gt;&gt; s.sort_index(key=lambda x : x.str.lower())\n |      A    1\n |      b    2\n |      C    3\n |      d    4\n |      dtype: int64\n |  \n |  sort_values(self, *, axis: 'Axis' = 0, ascending: 'bool | int | Sequence[bool] | Sequence[int]' = True, inplace: 'bool' = False, kind: 'str' = 'quicksort', na_position: 'str' = 'last', ignore_index: 'bool' = False, key: 'ValueKeyFunc' = None) -&gt; 'Series | None'\n |      Sort by the values.\n |      \n |      Sort a Series in ascending or descending order by some\n |      criterion.\n |      \n |      Parameters\n |      ----------\n |      axis : {0 or 'index'}\n |          Unused. Parameter needed for compatibility with DataFrame.\n |      ascending : bool or list of bools, default True\n |          If True, sort values in ascending order, otherwise descending.\n |      inplace : bool, default False\n |          If True, perform operation in-place.\n |      kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, default 'quicksort'\n |          Choice of sorting algorithm. See also :func:`numpy.sort` for more\n |          information. 'mergesort' and 'stable' are the only stable  algorithms.\n |      na_position : {'first' or 'last'}, default 'last'\n |          Argument 'first' puts NaNs at the beginning, 'last' puts NaNs at\n |          the end.\n |      ignore_index : bool, default False\n |          If True, the resulting axis will be labeled 0, 1, …, n - 1.\n |      key : callable, optional\n |          If not None, apply the key function to the series values\n |          before sorting. This is similar to the `key` argument in the\n |          builtin :meth:`sorted` function, with the notable difference that\n |          this `key` function should be *vectorized*. It should expect a\n |          ``Series`` and return an array-like.\n |      \n |          .. versionadded:: 1.1.0\n |      \n |      Returns\n |      -------\n |      Series or None\n |          Series ordered by values or None if ``inplace=True``.\n |      \n |      See Also\n |      --------\n |      Series.sort_index : Sort by the Series indices.\n |      DataFrame.sort_values : Sort DataFrame by the values along either axis.\n |      DataFrame.sort_index : Sort DataFrame by indices.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; s = pd.Series([np.nan, 1, 3, 10, 5])\n |      &gt;&gt;&gt; s\n |      0     NaN\n |      1     1.0\n |      2     3.0\n |      3     10.0\n |      4     5.0\n |      dtype: float64\n |      \n |      Sort values ascending order (default behaviour)\n |      \n |      &gt;&gt;&gt; s.sort_values(ascending=True)\n |      1     1.0\n |      2     3.0\n |      4     5.0\n |      3    10.0\n |      0     NaN\n |      dtype: float64\n |      \n |      Sort values descending order\n |      \n |      &gt;&gt;&gt; s.sort_values(ascending=False)\n |      3    10.0\n |      4     5.0\n |      2     3.0\n |      1     1.0\n |      0     NaN\n |      dtype: float64\n |      \n |      Sort values putting NAs first\n |      \n |      &gt;&gt;&gt; s.sort_values(na_position='first')\n |      0     NaN\n |      1     1.0\n |      2     3.0\n |      4     5.0\n |      3    10.0\n |      dtype: float64\n |      \n |      Sort a series of strings\n |      \n |      &gt;&gt;&gt; s = pd.Series(['z', 'b', 'd', 'a', 'c'])\n |      &gt;&gt;&gt; s\n |      0    z\n |      1    b\n |      2    d\n |      3    a\n |      4    c\n |      dtype: object\n |      \n |      &gt;&gt;&gt; s.sort_values()\n |      3    a\n |      1    b\n |      4    c\n |      2    d\n |      0    z\n |      dtype: object\n |      \n |      Sort using a key function. Your `key` function will be\n |      given the ``Series`` of values and should return an array-like.\n |      \n |      &gt;&gt;&gt; s = pd.Series(['a', 'B', 'c', 'D', 'e'])\n |      &gt;&gt;&gt; s.sort_values()\n |      1    B\n |      3    D\n |      0    a\n |      2    c\n |      4    e\n |      dtype: object\n |      &gt;&gt;&gt; s.sort_values(key=lambda x: x.str.lower())\n |      0    a\n |      1    B\n |      2    c\n |      3    D\n |      4    e\n |      dtype: object\n |      \n |      NumPy ufuncs work well here. For example, we can\n |      sort by the ``sin`` of the value\n |      \n |      &gt;&gt;&gt; s = pd.Series([-4, -2, 0, 2, 4])\n |      &gt;&gt;&gt; s.sort_values(key=np.sin)\n |      1   -2\n |      4    4\n |      2    0\n |      0   -4\n |      3    2\n |      dtype: int64\n |      \n |      More complicated user-defined functions can be used,\n |      as long as they expect a Series and return an array-like\n |      \n |      &gt;&gt;&gt; s.sort_values(key=lambda x: (np.tan(x.cumsum())))\n |      0   -4\n |      3    2\n |      4    4\n |      1   -2\n |      2    0\n |      dtype: int64\n |  \n |  std(self, axis: 'Axis | None' = None, skipna: 'bool_t' = True, ddof: 'int' = 1, numeric_only: 'bool_t' = False, **kwargs)\n |      Return sample standard deviation over requested axis.\n |      \n |      Normalized by N-1 by default. This can be changed using the ddof argument.\n |      \n |      Parameters\n |      ----------\n |      axis : {index (0)}\n |          For `Series` this parameter is unused and defaults to 0.\n |      skipna : bool, default True\n |          Exclude NA/null values. If an entire row/column is NA, the result\n |          will be NA.\n |      ddof : int, default 1\n |          Delta Degrees of Freedom. The divisor used in calculations is N - ddof,\n |          where N represents the number of elements.\n |      numeric_only : bool, default False\n |          Include only float, int, boolean columns. Not implemented for Series.\n |      \n |      Returns\n |      -------\n |      scalar or Series (if level specified) \n |      \n |      Notes\n |      -----\n |      To have the same behaviour as `numpy.std`, use `ddof=0` (instead of the\n |      default `ddof=1`)\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; df = pd.DataFrame({'person_id': [0, 1, 2, 3],\n |      ...                    'age': [21, 25, 62, 43],\n |      ...                    'height': [1.61, 1.87, 1.49, 2.01]}\n |      ...                   ).set_index('person_id')\n |      &gt;&gt;&gt; df\n |                 age  height\n |      person_id\n |      0           21    1.61\n |      1           25    1.87\n |      2           62    1.49\n |      3           43    2.01\n |      \n |      The standard deviation of the columns can be found as follows:\n |      \n |      &gt;&gt;&gt; df.std()\n |      age       18.786076\n |      height     0.237417\n |      dtype: float64\n |      \n |      Alternatively, `ddof=0` can be set to normalize by N instead of N-1:\n |      \n |      &gt;&gt;&gt; df.std(ddof=0)\n |      age       16.269219\n |      height     0.205609\n |      dtype: float64\n |  \n |  sub(self, other, level=None, fill_value=None, axis: 'Axis' = 0)\n |      Return Subtraction of series and other, element-wise (binary operator `sub`).\n |      \n |      Equivalent to ``series - other``, but with support to substitute a fill_value for\n |      missing data in either one of the inputs.\n |      \n |      Parameters\n |      ----------\n |      other : Series or scalar value\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level.\n |      fill_value : None or float value, default None (NaN)\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful Series alignment, with this value before computation.\n |          If data in both corresponding Series locations is missing\n |          the result of filling (at that location) will be missing.\n |      axis : {0 or 'index'}\n |          Unused. Parameter needed for compatibility with DataFrame.\n |      \n |      Returns\n |      -------\n |      Series\n |          The result of the operation.\n |      \n |      See Also\n |      --------\n |      Series.rsub : Reverse of the Subtraction operator, see\n |          `Python documentation\n |          &lt;https://docs.python.org/3/reference/datamodel.html#emulating-numeric-types&gt;`_\n |          for more details.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; a = pd.Series([1, 1, 1, np.nan], index=['a', 'b', 'c', 'd'])\n |      &gt;&gt;&gt; a\n |      a    1.0\n |      b    1.0\n |      c    1.0\n |      d    NaN\n |      dtype: float64\n |      &gt;&gt;&gt; b = pd.Series([1, np.nan, 1, np.nan], index=['a', 'b', 'd', 'e'])\n |      &gt;&gt;&gt; b\n |      a    1.0\n |      b    NaN\n |      d    1.0\n |      e    NaN\n |      dtype: float64\n |      &gt;&gt;&gt; a.subtract(b, fill_value=0)\n |      a    0.0\n |      b    1.0\n |      c    1.0\n |      d   -1.0\n |      e    NaN\n |      dtype: float64\n |  \n |  subtract = sub(self, other, level=None, fill_value=None, axis: 'Axis' = 0)\n |  \n |  sum(self, axis: 'Axis | None' = None, skipna: 'bool_t' = True, numeric_only: 'bool_t' = False, min_count: 'int' = 0, **kwargs)\n |      Return the sum of the values over the requested axis.\n |      \n |      This is equivalent to the method ``numpy.sum``.\n |      \n |      Parameters\n |      ----------\n |      axis : {index (0)}\n |          Axis for the function to be applied on.\n |          For `Series` this parameter is unused and defaults to 0.\n |      \n |          For DataFrames, specifying ``axis=None`` will apply the aggregation\n |          across both axes.\n |      \n |          .. versionadded:: 2.0.0\n |      \n |      skipna : bool, default True\n |          Exclude NA/null values when computing the result.\n |      numeric_only : bool, default False\n |          Include only float, int, boolean columns. Not implemented for Series.\n |      \n |      min_count : int, default 0\n |          The required number of valid values to perform the operation. If fewer than\n |          ``min_count`` non-NA values are present the result will be NA.\n |      **kwargs\n |          Additional keyword arguments to be passed to the function.\n |      \n |      Returns\n |      -------\n |      scalar or scalar\n |      \n |      See Also\n |      --------\n |      Series.sum : Return the sum.\n |      Series.min : Return the minimum.\n |      Series.max : Return the maximum.\n |      Series.idxmin : Return the index of the minimum.\n |      Series.idxmax : Return the index of the maximum.\n |      DataFrame.sum : Return the sum over the requested axis.\n |      DataFrame.min : Return the minimum over the requested axis.\n |      DataFrame.max : Return the maximum over the requested axis.\n |      DataFrame.idxmin : Return the index of the minimum over the requested axis.\n |      DataFrame.idxmax : Return the index of the maximum over the requested axis.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; idx = pd.MultiIndex.from_arrays([\n |      ...     ['warm', 'warm', 'cold', 'cold'],\n |      ...     ['dog', 'falcon', 'fish', 'spider']],\n |      ...     names=['blooded', 'animal'])\n |      &gt;&gt;&gt; s = pd.Series([4, 2, 0, 8], name='legs', index=idx)\n |      &gt;&gt;&gt; s\n |      blooded  animal\n |      warm     dog       4\n |               falcon    2\n |      cold     fish      0\n |               spider    8\n |      Name: legs, dtype: int64\n |      \n |      &gt;&gt;&gt; s.sum()\n |      14\n |      \n |      By default, the sum of an empty or all-NA Series is ``0``.\n |      \n |      &gt;&gt;&gt; pd.Series([], dtype=\"float64\").sum()  # min_count=0 is the default\n |      0.0\n |      \n |      This can be controlled with the ``min_count`` parameter. For example, if\n |      you'd like the sum of an empty series to be NaN, pass ``min_count=1``.\n |      \n |      &gt;&gt;&gt; pd.Series([], dtype=\"float64\").sum(min_count=1)\n |      nan\n |      \n |      Thanks to the ``skipna`` parameter, ``min_count`` handles all-NA and\n |      empty series identically.\n |      \n |      &gt;&gt;&gt; pd.Series([np.nan]).sum()\n |      0.0\n |      \n |      &gt;&gt;&gt; pd.Series([np.nan]).sum(min_count=1)\n |      nan\n |  \n |  swaplevel(self, i: 'Level' = -2, j: 'Level' = -1, copy: 'bool | None' = None) -&gt; 'Series'\n |      Swap levels i and j in a :class:`MultiIndex`.\n |      \n |      Default is to swap the two innermost levels of the index.\n |      \n |      Parameters\n |      ----------\n |      i, j : int or str\n |          Levels of the indices to be swapped. Can pass level name as string.\n |      copy : bool, default True\n |                  Whether to copy underlying data.\n |      \n |      Returns\n |      -------\n |      Series\n |          Series with levels swapped in MultiIndex.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; s = pd.Series(\n |      ...     [\"A\", \"B\", \"A\", \"C\"],\n |      ...     index=[\n |      ...         [\"Final exam\", \"Final exam\", \"Coursework\", \"Coursework\"],\n |      ...         [\"History\", \"Geography\", \"History\", \"Geography\"],\n |      ...         [\"January\", \"February\", \"March\", \"April\"],\n |      ...     ],\n |      ... )\n |      &gt;&gt;&gt; s\n |      Final exam  History     January      A\n |                  Geography   February     B\n |      Coursework  History     March        A\n |                  Geography   April        C\n |      dtype: object\n |      \n |      In the following example, we will swap the levels of the indices.\n |      Here, we will swap the levels column-wise, but levels can be swapped row-wise\n |      in a similar manner. Note that column-wise is the default behaviour.\n |      By not supplying any arguments for i and j, we swap the last and second to\n |      last indices.\n |      \n |      &gt;&gt;&gt; s.swaplevel()\n |      Final exam  January     History         A\n |                  February    Geography       B\n |      Coursework  March       History         A\n |                  April       Geography       C\n |      dtype: object\n |      \n |      By supplying one argument, we can choose which index to swap the last\n |      index with. We can for example swap the first index with the last one as\n |      follows.\n |      \n |      &gt;&gt;&gt; s.swaplevel(0)\n |      January     History     Final exam      A\n |      February    Geography   Final exam      B\n |      March       History     Coursework      A\n |      April       Geography   Coursework      C\n |      dtype: object\n |      \n |      We can also define explicitly which indices we want to swap by supplying values\n |      for both i and j. Here, we for example swap the first and second indices.\n |      \n |      &gt;&gt;&gt; s.swaplevel(0, 1)\n |      History     Final exam  January         A\n |      Geography   Final exam  February        B\n |      History     Coursework  March           A\n |      Geography   Coursework  April           C\n |      dtype: object\n |  \n |  take(self, indices, axis: 'Axis' = 0, **kwargs) -&gt; 'Series'\n |      Return the elements in the given *positional* indices along an axis.\n |      \n |      This means that we are not indexing according to actual values in\n |      the index attribute of the object. We are indexing according to the\n |      actual position of the element in the object.\n |      \n |      Parameters\n |      ----------\n |      indices : array-like\n |          An array of ints indicating which positions to take.\n |      axis : {0 or 'index', 1 or 'columns', None}, default 0\n |          The axis on which to select elements. ``0`` means that we are\n |          selecting rows, ``1`` means that we are selecting columns.\n |          For `Series` this parameter is unused and defaults to 0.\n |      **kwargs\n |          For compatibility with :meth:`numpy.take`. Has no effect on the\n |          output.\n |      \n |      Returns\n |      -------\n |      same type as caller\n |          An array-like containing the elements taken from the object.\n |      \n |      See Also\n |      --------\n |      DataFrame.loc : Select a subset of a DataFrame by labels.\n |      DataFrame.iloc : Select a subset of a DataFrame by positions.\n |      numpy.take : Take elements from an array along an axis.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; df = pd.DataFrame([('falcon', 'bird', 389.0),\n |      ...                    ('parrot', 'bird', 24.0),\n |      ...                    ('lion', 'mammal', 80.5),\n |      ...                    ('monkey', 'mammal', np.nan)],\n |      ...                   columns=['name', 'class', 'max_speed'],\n |      ...                   index=[0, 2, 3, 1])\n |      &gt;&gt;&gt; df\n |           name   class  max_speed\n |      0  falcon    bird      389.0\n |      2  parrot    bird       24.0\n |      3    lion  mammal       80.5\n |      1  monkey  mammal        NaN\n |      \n |      Take elements at positions 0 and 3 along the axis 0 (default).\n |      \n |      Note how the actual indices selected (0 and 1) do not correspond to\n |      our selected indices 0 and 3. That's because we are selecting the 0th\n |      and 3rd rows, not rows whose indices equal 0 and 3.\n |      \n |      &gt;&gt;&gt; df.take([0, 3])\n |           name   class  max_speed\n |      0  falcon    bird      389.0\n |      1  monkey  mammal        NaN\n |      \n |      Take elements at indices 1 and 2 along the axis 1 (column selection).\n |      \n |      &gt;&gt;&gt; df.take([1, 2], axis=1)\n |          class  max_speed\n |      0    bird      389.0\n |      2    bird       24.0\n |      3  mammal       80.5\n |      1  mammal        NaN\n |      \n |      We may take elements using negative integers for positive indices,\n |      starting from the end of the object, just like with Python lists.\n |      \n |      &gt;&gt;&gt; df.take([-1, -2])\n |           name   class  max_speed\n |      1  monkey  mammal        NaN\n |      3    lion  mammal       80.5\n |  \n |  to_dict(self, into: 'type[dict]' = &lt;class 'dict'&gt;) -&gt; 'dict'\n |      Convert Series to {label -&gt; value} dict or dict-like object.\n |      \n |      Parameters\n |      ----------\n |      into : class, default dict\n |          The collections.abc.Mapping subclass to use as the return\n |          object. Can be the actual class or an empty\n |          instance of the mapping type you want.  If you want a\n |          collections.defaultdict, you must pass it initialized.\n |      \n |      Returns\n |      -------\n |      collections.abc.Mapping\n |          Key-value representation of Series.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; s = pd.Series([1, 2, 3, 4])\n |      &gt;&gt;&gt; s.to_dict()\n |      {0: 1, 1: 2, 2: 3, 3: 4}\n |      &gt;&gt;&gt; from collections import OrderedDict, defaultdict\n |      &gt;&gt;&gt; s.to_dict(OrderedDict)\n |      OrderedDict([(0, 1), (1, 2), (2, 3), (3, 4)])\n |      &gt;&gt;&gt; dd = defaultdict(list)\n |      &gt;&gt;&gt; s.to_dict(dd)\n |      defaultdict(&lt;class 'list'&gt;, {0: 1, 1: 2, 2: 3, 3: 4})\n |  \n |  to_frame(self, name: 'Hashable' = &lt;no_default&gt;) -&gt; 'DataFrame'\n |      Convert Series to DataFrame.\n |      \n |      Parameters\n |      ----------\n |      name : object, optional\n |          The passed name should substitute for the series name (if it has\n |          one).\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          DataFrame representation of Series.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; s = pd.Series([\"a\", \"b\", \"c\"],\n |      ...               name=\"vals\")\n |      &gt;&gt;&gt; s.to_frame()\n |        vals\n |      0    a\n |      1    b\n |      2    c\n |  \n |  to_markdown(self, buf: 'IO[str] | None' = None, mode: 'str' = 'wt', index: 'bool' = True, storage_options: 'StorageOptions' = None, **kwargs) -&gt; 'str | None'\n |      Print Series in Markdown-friendly format.\n |      \n |      Parameters\n |      ----------\n |      buf : str, Path or StringIO-like, optional, default None\n |          Buffer to write to. If None, the output is returned as a string.\n |      mode : str, optional\n |          Mode in which file is opened, \"wt\" by default.\n |      index : bool, optional, default True\n |          Add index (row) labels.\n |      \n |          .. versionadded:: 1.1.0\n |      storage_options : dict, optional\n |          Extra options that make sense for a particular storage connection, e.g.\n |          host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n |          are forwarded to ``urllib.request.Request`` as header options. For other\n |          URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n |          forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n |          details, and for more examples on storage options refer `here\n |          &lt;https://pandas.pydata.org/docs/user_guide/io.html?\n |          highlight=storage_options#reading-writing-remote-files&gt;`_.\n |      \n |          .. versionadded:: 1.2.0\n |      \n |      **kwargs\n |          These parameters will be passed to `tabulate                 &lt;https://pypi.org/project/tabulate&gt;`_.\n |      \n |      Returns\n |      -------\n |      str\n |          Series in Markdown-friendly format.\n |      \n |      Notes\n |      -----\n |      Requires the `tabulate &lt;https://pypi.org/project/tabulate&gt;`_ package.\n |      \n |      Examples\n |                  --------\n |                  &gt;&gt;&gt; s = pd.Series([\"elk\", \"pig\", \"dog\", \"quetzal\"], name=\"animal\")\n |                  &gt;&gt;&gt; print(s.to_markdown())\n |                  |    | animal   |\n |                  |---:|:---------|\n |                  |  0 | elk      |\n |                  |  1 | pig      |\n |                  |  2 | dog      |\n |                  |  3 | quetzal  |\n |      \n |                  Output markdown with a tabulate option.\n |      \n |                  &gt;&gt;&gt; print(s.to_markdown(tablefmt=\"grid\"))\n |                  +----+----------+\n |                  |    | animal   |\n |                  +====+==========+\n |                  |  0 | elk      |\n |                  +----+----------+\n |                  |  1 | pig      |\n |                  +----+----------+\n |                  |  2 | dog      |\n |                  +----+----------+\n |                  |  3 | quetzal  |\n |                  +----+----------+\n |  \n |  to_period(self, freq: 'str | None' = None, copy: 'bool | None' = None) -&gt; 'Series'\n |      Convert Series from DatetimeIndex to PeriodIndex.\n |      \n |      Parameters\n |      ----------\n |      freq : str, default None\n |          Frequency associated with the PeriodIndex.\n |      copy : bool, default True\n |          Whether or not to return a copy.\n |      \n |      Returns\n |      -------\n |      Series\n |          Series with index converted to PeriodIndex.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; idx = pd.DatetimeIndex(['2023', '2024', '2025'])\n |      &gt;&gt;&gt; s = pd.Series([1, 2, 3], index=idx)\n |      &gt;&gt;&gt; s = s.to_period()\n |      &gt;&gt;&gt; s\n |      2023    1\n |      2024    2\n |      2025    3\n |      Freq: A-DEC, dtype: int64\n |      \n |      Viewing the index\n |      \n |      &gt;&gt;&gt; s.index\n |      PeriodIndex(['2023', '2024', '2025'], dtype='period[A-DEC]')\n |  \n |  to_string(self, buf: 'FilePath | WriteBuffer[str] | None' = None, na_rep: 'str' = 'NaN', float_format: 'str | None' = None, header: 'bool' = True, index: 'bool' = True, length: 'bool' = False, dtype: 'bool' = False, name: 'bool' = False, max_rows: 'int | None' = None, min_rows: 'int | None' = None) -&gt; 'str | None'\n |      Render a string representation of the Series.\n |      \n |      Parameters\n |      ----------\n |      buf : StringIO-like, optional\n |          Buffer to write to.\n |      na_rep : str, optional\n |          String representation of NaN to use, default 'NaN'.\n |      float_format : one-parameter function, optional\n |          Formatter function to apply to columns' elements if they are\n |          floats, default None.\n |      header : bool, default True\n |          Add the Series header (index name).\n |      index : bool, optional\n |          Add index (row) labels, default True.\n |      length : bool, default False\n |          Add the Series length.\n |      dtype : bool, default False\n |          Add the Series dtype.\n |      name : bool, default False\n |          Add the Series name if not None.\n |      max_rows : int, optional\n |          Maximum number of rows to show before truncating. If None, show\n |          all.\n |      min_rows : int, optional\n |          The number of rows to display in a truncated repr (when number\n |          of rows is above `max_rows`).\n |      \n |      Returns\n |      -------\n |      str or None\n |          String representation of Series if ``buf=None``, otherwise None.\n |  \n |  to_timestamp(self, freq=None, how: \"Literal['s', 'e', 'start', 'end']\" = 'start', copy: 'bool | None' = None) -&gt; 'Series'\n |      Cast to DatetimeIndex of Timestamps, at *beginning* of period.\n |      \n |      Parameters\n |      ----------\n |      freq : str, default frequency of PeriodIndex\n |          Desired frequency.\n |      how : {'s', 'e', 'start', 'end'}\n |          Convention for converting period to timestamp; start of period\n |          vs. end.\n |      copy : bool, default True\n |          Whether or not to return a copy.\n |      \n |      Returns\n |      -------\n |      Series with DatetimeIndex\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; idx = pd.PeriodIndex(['2023', '2024', '2025'], freq='Y')\n |      &gt;&gt;&gt; s1 = pd.Series([1, 2, 3], index=idx)\n |      &gt;&gt;&gt; s1\n |      2023    1\n |      2024    2\n |      2025    3\n |      Freq: A-DEC, dtype: int64\n |      \n |      The resulting frequency of the Timestamps is `YearBegin`\n |      \n |      &gt;&gt;&gt; s1 = s1.to_timestamp()\n |      &gt;&gt;&gt; s1\n |      2023-01-01    1\n |      2024-01-01    2\n |      2025-01-01    3\n |      Freq: AS-JAN, dtype: int64\n |      \n |      Using `freq` which is the offset that the Timestamps will have\n |      \n |      &gt;&gt;&gt; s2 = pd.Series([1, 2, 3], index=idx)\n |      &gt;&gt;&gt; s2 = s2.to_timestamp(freq='M')\n |      &gt;&gt;&gt; s2\n |      2023-01-31    1\n |      2024-01-31    2\n |      2025-01-31    3\n |      Freq: A-JAN, dtype: int64\n |  \n |  transform(self, func: 'AggFuncType', axis: 'Axis' = 0, *args, **kwargs) -&gt; 'DataFrame | Series'\n |      Call ``func`` on self producing a Series with the same axis shape as self.\n |      \n |      Parameters\n |      ----------\n |      func : function, str, list-like or dict-like\n |          Function to use for transforming the data. If a function, must either\n |          work when passed a Series or when passed to Series.apply. If func\n |          is both list-like and dict-like, dict-like behavior takes precedence.\n |      \n |          Accepted combinations are:\n |      \n |          - function\n |          - string function name\n |          - list-like of functions and/or function names, e.g. ``[np.exp, 'sqrt']``\n |          - dict-like of axis labels -&gt; functions, function names or list-like of such.\n |      axis : {0 or 'index'}\n |              Unused. Parameter needed for compatibility with DataFrame.\n |      *args\n |          Positional arguments to pass to `func`.\n |      **kwargs\n |          Keyword arguments to pass to `func`.\n |      \n |      Returns\n |      -------\n |      Series\n |          A Series that must have the same length as self.\n |      \n |      Raises\n |      ------\n |      ValueError : If the returned Series has a different length than self.\n |      \n |      See Also\n |      --------\n |      Series.agg : Only perform aggregating type operations.\n |      Series.apply : Invoke function on a Series.\n |      \n |      Notes\n |      -----\n |      Functions that mutate the passed object can produce unexpected\n |      behavior or errors and are not supported. See :ref:`gotchas.udf-mutation`\n |      for more details.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; df = pd.DataFrame({'A': range(3), 'B': range(1, 4)})\n |      &gt;&gt;&gt; df\n |         A  B\n |      0  0  1\n |      1  1  2\n |      2  2  3\n |      &gt;&gt;&gt; df.transform(lambda x: x + 1)\n |         A  B\n |      0  1  2\n |      1  2  3\n |      2  3  4\n |      \n |      Even though the resulting Series must have the same length as the\n |      input Series, it is possible to provide several input functions:\n |      \n |      &gt;&gt;&gt; s = pd.Series(range(3))\n |      &gt;&gt;&gt; s\n |      0    0\n |      1    1\n |      2    2\n |      dtype: int64\n |      &gt;&gt;&gt; s.transform([np.sqrt, np.exp])\n |             sqrt        exp\n |      0  0.000000   1.000000\n |      1  1.000000   2.718282\n |      2  1.414214   7.389056\n |      \n |      You can call transform on a GroupBy object:\n |      \n |      &gt;&gt;&gt; df = pd.DataFrame({\n |      ...     \"Date\": [\n |      ...         \"2015-05-08\", \"2015-05-07\", \"2015-05-06\", \"2015-05-05\",\n |      ...         \"2015-05-08\", \"2015-05-07\", \"2015-05-06\", \"2015-05-05\"],\n |      ...     \"Data\": [5, 8, 6, 1, 50, 100, 60, 120],\n |      ... })\n |      &gt;&gt;&gt; df\n |               Date  Data\n |      0  2015-05-08     5\n |      1  2015-05-07     8\n |      2  2015-05-06     6\n |      3  2015-05-05     1\n |      4  2015-05-08    50\n |      5  2015-05-07   100\n |      6  2015-05-06    60\n |      7  2015-05-05   120\n |      &gt;&gt;&gt; df.groupby('Date')['Data'].transform('sum')\n |      0     55\n |      1    108\n |      2     66\n |      3    121\n |      4     55\n |      5    108\n |      6     66\n |      7    121\n |      Name: Data, dtype: int64\n |      \n |      &gt;&gt;&gt; df = pd.DataFrame({\n |      ...     \"c\": [1, 1, 1, 2, 2, 2, 2],\n |      ...     \"type\": [\"m\", \"n\", \"o\", \"m\", \"m\", \"n\", \"n\"]\n |      ... })\n |      &gt;&gt;&gt; df\n |         c type\n |      0  1    m\n |      1  1    n\n |      2  1    o\n |      3  2    m\n |      4  2    m\n |      5  2    n\n |      6  2    n\n |      &gt;&gt;&gt; df['size'] = df.groupby('c')['type'].transform(len)\n |      &gt;&gt;&gt; df\n |         c type size\n |      0  1    m    3\n |      1  1    n    3\n |      2  1    o    3\n |      3  2    m    4\n |      4  2    m    4\n |      5  2    n    4\n |      6  2    n    4\n |  \n |  truediv(self, other, level=None, fill_value=None, axis: 'Axis' = 0)\n |      Return Floating division of series and other, element-wise (binary operator `truediv`).\n |      \n |      Equivalent to ``series / other``, but with support to substitute a fill_value for\n |      missing data in either one of the inputs.\n |      \n |      Parameters\n |      ----------\n |      other : Series or scalar value\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level.\n |      fill_value : None or float value, default None (NaN)\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful Series alignment, with this value before computation.\n |          If data in both corresponding Series locations is missing\n |          the result of filling (at that location) will be missing.\n |      axis : {0 or 'index'}\n |          Unused. Parameter needed for compatibility with DataFrame.\n |      \n |      Returns\n |      -------\n |      Series\n |          The result of the operation.\n |      \n |      See Also\n |      --------\n |      Series.rtruediv : Reverse of the Floating division operator, see\n |          `Python documentation\n |          &lt;https://docs.python.org/3/reference/datamodel.html#emulating-numeric-types&gt;`_\n |          for more details.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; a = pd.Series([1, 1, 1, np.nan], index=['a', 'b', 'c', 'd'])\n |      &gt;&gt;&gt; a\n |      a    1.0\n |      b    1.0\n |      c    1.0\n |      d    NaN\n |      dtype: float64\n |      &gt;&gt;&gt; b = pd.Series([1, np.nan, 1, np.nan], index=['a', 'b', 'd', 'e'])\n |      &gt;&gt;&gt; b\n |      a    1.0\n |      b    NaN\n |      d    1.0\n |      e    NaN\n |      dtype: float64\n |      &gt;&gt;&gt; a.divide(b, fill_value=0)\n |      a    1.0\n |      b    inf\n |      c    inf\n |      d    0.0\n |      e    NaN\n |      dtype: float64\n |  \n |  unique(self) -&gt; 'ArrayLike'\n |      Return unique values of Series object.\n |      \n |      Uniques are returned in order of appearance. Hash table-based unique,\n |      therefore does NOT sort.\n |      \n |      Returns\n |      -------\n |      ndarray or ExtensionArray\n |          The unique values returned as a NumPy array. See Notes.\n |      \n |      See Also\n |      --------\n |      Series.drop_duplicates : Return Series with duplicate values removed.\n |      unique : Top-level unique method for any 1-d array-like object.\n |      Index.unique : Return Index with unique values from an Index object.\n |      \n |      Notes\n |      -----\n |      Returns the unique values as a NumPy array. In case of an\n |      extension-array backed Series, a new\n |      :class:`~api.extensions.ExtensionArray` of that type with just\n |      the unique values is returned. This includes\n |      \n |          * Categorical\n |          * Period\n |          * Datetime with Timezone\n |          * Datetime without Timezone\n |          * Timedelta\n |          * Interval\n |          * Sparse\n |          * IntegerNA\n |      \n |      See Examples section.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; pd.Series([2, 1, 3, 3], name='A').unique()\n |      array([2, 1, 3])\n |      \n |      &gt;&gt;&gt; pd.Series([pd.Timestamp('2016-01-01') for _ in range(3)]).unique()\n |      &lt;DatetimeArray&gt;\n |      ['2016-01-01 00:00:00']\n |      Length: 1, dtype: datetime64[ns]\n |      \n |      &gt;&gt;&gt; pd.Series([pd.Timestamp('2016-01-01', tz='US/Eastern')\n |      ...            for _ in range(3)]).unique()\n |      &lt;DatetimeArray&gt;\n |      ['2016-01-01 00:00:00-05:00']\n |      Length: 1, dtype: datetime64[ns, US/Eastern]\n |      \n |      An Categorical will return categories in the order of\n |      appearance and with the same dtype.\n |      \n |      &gt;&gt;&gt; pd.Series(pd.Categorical(list('baabc'))).unique()\n |      ['b', 'a', 'c']\n |      Categories (3, object): ['a', 'b', 'c']\n |      &gt;&gt;&gt; pd.Series(pd.Categorical(list('baabc'), categories=list('abc'),\n |      ...                          ordered=True)).unique()\n |      ['b', 'a', 'c']\n |      Categories (3, object): ['a' &lt; 'b' &lt; 'c']\n |  \n |  unstack(self, level: 'IndexLabel' = -1, fill_value: 'Hashable' = None) -&gt; 'DataFrame'\n |      Unstack, also known as pivot, Series with MultiIndex to produce DataFrame.\n |      \n |      Parameters\n |      ----------\n |      level : int, str, or list of these, default last level\n |          Level(s) to unstack, can pass level name.\n |      fill_value : scalar value, default None\n |          Value to use when replacing NaN values.\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          Unstacked Series.\n |      \n |      Notes\n |      -----\n |      Reference :ref:`the user guide &lt;reshaping.stacking&gt;` for more examples.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; s = pd.Series([1, 2, 3, 4],\n |      ...               index=pd.MultiIndex.from_product([['one', 'two'],\n |      ...                                                 ['a', 'b']]))\n |      &gt;&gt;&gt; s\n |      one  a    1\n |           b    2\n |      two  a    3\n |           b    4\n |      dtype: int64\n |      \n |      &gt;&gt;&gt; s.unstack(level=-1)\n |           a  b\n |      one  1  2\n |      two  3  4\n |      \n |      &gt;&gt;&gt; s.unstack(level=0)\n |         one  two\n |      a    1    3\n |      b    2    4\n |  \n |  update(self, other: 'Series | Sequence | Mapping') -&gt; 'None'\n |      Modify Series in place using values from passed Series.\n |      \n |      Uses non-NA values from passed Series to make updates. Aligns\n |      on index.\n |      \n |      Parameters\n |      ----------\n |      other : Series, or object coercible into Series\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; s = pd.Series([1, 2, 3])\n |      &gt;&gt;&gt; s.update(pd.Series([4, 5, 6]))\n |      &gt;&gt;&gt; s\n |      0    4\n |      1    5\n |      2    6\n |      dtype: int64\n |      \n |      &gt;&gt;&gt; s = pd.Series(['a', 'b', 'c'])\n |      &gt;&gt;&gt; s.update(pd.Series(['d', 'e'], index=[0, 2]))\n |      &gt;&gt;&gt; s\n |      0    d\n |      1    b\n |      2    e\n |      dtype: object\n |      \n |      &gt;&gt;&gt; s = pd.Series([1, 2, 3])\n |      &gt;&gt;&gt; s.update(pd.Series([4, 5, 6, 7, 8]))\n |      &gt;&gt;&gt; s\n |      0    4\n |      1    5\n |      2    6\n |      dtype: int64\n |      \n |      If ``other`` contains NaNs the corresponding values are not updated\n |      in the original Series.\n |      \n |      &gt;&gt;&gt; s = pd.Series([1, 2, 3])\n |      &gt;&gt;&gt; s.update(pd.Series([4, np.nan, 6]))\n |      &gt;&gt;&gt; s\n |      0    4\n |      1    2\n |      2    6\n |      dtype: int64\n |      \n |      ``other`` can also be a non-Series object type\n |      that is coercible into a Series\n |      \n |      &gt;&gt;&gt; s = pd.Series([1, 2, 3])\n |      &gt;&gt;&gt; s.update([4, np.nan, 6])\n |      &gt;&gt;&gt; s\n |      0    4\n |      1    2\n |      2    6\n |      dtype: int64\n |      \n |      &gt;&gt;&gt; s = pd.Series([1, 2, 3])\n |      &gt;&gt;&gt; s.update({1: 9})\n |      &gt;&gt;&gt; s\n |      0    1\n |      1    9\n |      2    3\n |      dtype: int64\n |  \n |  var(self, axis: 'Axis | None' = None, skipna: 'bool_t' = True, ddof: 'int' = 1, numeric_only: 'bool_t' = False, **kwargs)\n |      Return unbiased variance over requested axis.\n |      \n |      Normalized by N-1 by default. This can be changed using the ddof argument.\n |      \n |      Parameters\n |      ----------\n |      axis : {index (0)}\n |          For `Series` this parameter is unused and defaults to 0.\n |      skipna : bool, default True\n |          Exclude NA/null values. If an entire row/column is NA, the result\n |          will be NA.\n |      ddof : int, default 1\n |          Delta Degrees of Freedom. The divisor used in calculations is N - ddof,\n |          where N represents the number of elements.\n |      numeric_only : bool, default False\n |          Include only float, int, boolean columns. Not implemented for Series.\n |      \n |      Returns\n |      -------\n |      scalar or Series (if level specified) \n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; df = pd.DataFrame({'person_id': [0, 1, 2, 3],\n |      ...                   'age': [21, 25, 62, 43],\n |      ...                   'height': [1.61, 1.87, 1.49, 2.01]}\n |      ...                  ).set_index('person_id')\n |      &gt;&gt;&gt; df\n |                 age  height\n |      person_id\n |      0           21    1.61\n |      1           25    1.87\n |      2           62    1.49\n |      3           43    2.01\n |      \n |      &gt;&gt;&gt; df.var()\n |      age       352.916667\n |      height      0.056367\n |      dtype: float64\n |      \n |      Alternatively, ``ddof=0`` can be set to normalize by N instead of N-1:\n |      \n |      &gt;&gt;&gt; df.var(ddof=0)\n |      age       264.687500\n |      height      0.042275\n |      dtype: float64\n |  \n |  view(self, dtype: 'Dtype | None' = None) -&gt; 'Series'\n |      Create a new view of the Series.\n |      \n |      This function will return a new Series with a view of the same\n |      underlying values in memory, optionally reinterpreted with a new data\n |      type. The new data type must preserve the same size in bytes as to not\n |      cause index misalignment.\n |      \n |      Parameters\n |      ----------\n |      dtype : data type\n |          Data type object or one of their string representations.\n |      \n |      Returns\n |      -------\n |      Series\n |          A new Series object as a view of the same data in memory.\n |      \n |      See Also\n |      --------\n |      numpy.ndarray.view : Equivalent numpy function to create a new view of\n |          the same data in memory.\n |      \n |      Notes\n |      -----\n |      Series are instantiated with ``dtype=float64`` by default. While\n |      ``numpy.ndarray.view()`` will return a view with the same data type as\n |      the original array, ``Series.view()`` (without specified dtype)\n |      will try using ``float64`` and may fail if the original data type size\n |      in bytes is not the same.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; s = pd.Series([-2, -1, 0, 1, 2], dtype='int8')\n |      &gt;&gt;&gt; s\n |      0   -2\n |      1   -1\n |      2    0\n |      3    1\n |      4    2\n |      dtype: int8\n |      \n |      The 8 bit signed integer representation of `-1` is `0b11111111`, but\n |      the same bytes represent 255 if read as an 8 bit unsigned integer:\n |      \n |      &gt;&gt;&gt; us = s.view('uint8')\n |      &gt;&gt;&gt; us\n |      0    254\n |      1    255\n |      2      0\n |      3      1\n |      4      2\n |      dtype: uint8\n |      \n |      The views share the same underlying values:\n |      \n |      &gt;&gt;&gt; us[0] = 128\n |      &gt;&gt;&gt; s\n |      0   -128\n |      1     -1\n |      2      0\n |      3      1\n |      4      2\n |      dtype: int8\n |  \n |  where(self, cond, other=&lt;no_default&gt;, *, inplace: 'bool' = False, axis: 'Axis | None' = None, level: 'Level' = None) -&gt; 'Series | None'\n |      Replace values where the condition is False.\n |      \n |      Parameters\n |      ----------\n |      cond : bool Series/DataFrame, array-like, or callable\n |          Where `cond` is True, keep the original value. Where\n |          False, replace with corresponding value from `other`.\n |          If `cond` is callable, it is computed on the Series/DataFrame and\n |          should return boolean Series/DataFrame or array. The callable must\n |          not change input Series/DataFrame (though pandas doesn't check it).\n |      other : scalar, Series/DataFrame, or callable\n |          Entries where `cond` is False are replaced with\n |          corresponding value from `other`.\n |          If other is callable, it is computed on the Series/DataFrame and\n |          should return scalar or Series/DataFrame. The callable must not\n |          change input Series/DataFrame (though pandas doesn't check it).\n |          If not specified, entries will be filled with the corresponding\n |          NULL value (``np.nan`` for numpy dtypes, ``pd.NA`` for extension\n |          dtypes).\n |      inplace : bool, default False\n |          Whether to perform the operation in place on the data.\n |      axis : int, default None\n |          Alignment axis if needed. For `Series` this parameter is\n |          unused and defaults to 0.\n |      level : int, default None\n |          Alignment level if needed.\n |      \n |      Returns\n |      -------\n |      Same type as caller or None if ``inplace=True``.\n |      \n |      See Also\n |      --------\n |      :func:`DataFrame.mask` : Return an object of same shape as\n |          self.\n |      \n |      Notes\n |      -----\n |      The where method is an application of the if-then idiom. For each\n |      element in the calling DataFrame, if ``cond`` is ``True`` the\n |      element is used; otherwise the corresponding element from the DataFrame\n |      ``other`` is used. If the axis of ``other`` does not align with axis of\n |      ``cond`` Series/DataFrame, the misaligned index positions will be filled with\n |      False.\n |      \n |      The signature for :func:`DataFrame.where` differs from\n |      :func:`numpy.where`. Roughly ``df1.where(m, df2)`` is equivalent to\n |      ``np.where(m, df1, df2)``.\n |      \n |      For further details and examples see the ``where`` documentation in\n |      :ref:`indexing &lt;indexing.where_mask&gt;`.\n |      \n |      The dtype of the object takes precedence. The fill value is casted to\n |      the object's dtype, if this can be done losslessly.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; s = pd.Series(range(5))\n |      &gt;&gt;&gt; s.where(s &gt; 0)\n |      0    NaN\n |      1    1.0\n |      2    2.0\n |      3    3.0\n |      4    4.0\n |      dtype: float64\n |      &gt;&gt;&gt; s.mask(s &gt; 0)\n |      0    0.0\n |      1    NaN\n |      2    NaN\n |      3    NaN\n |      4    NaN\n |      dtype: float64\n |      \n |      &gt;&gt;&gt; s = pd.Series(range(5))\n |      &gt;&gt;&gt; t = pd.Series([True, False])\n |      &gt;&gt;&gt; s.where(t, 99)\n |      0     0\n |      1    99\n |      2    99\n |      3    99\n |      4    99\n |      dtype: int64\n |      &gt;&gt;&gt; s.mask(t, 99)\n |      0    99\n |      1     1\n |      2    99\n |      3    99\n |      4    99\n |      dtype: int64\n |      \n |      &gt;&gt;&gt; s.where(s &gt; 1, 10)\n |      0    10\n |      1    10\n |      2    2\n |      3    3\n |      4    4\n |      dtype: int64\n |      &gt;&gt;&gt; s.mask(s &gt; 1, 10)\n |      0     0\n |      1     1\n |      2    10\n |      3    10\n |      4    10\n |      dtype: int64\n |      \n |      &gt;&gt;&gt; df = pd.DataFrame(np.arange(10).reshape(-1, 2), columns=['A', 'B'])\n |      &gt;&gt;&gt; df\n |         A  B\n |      0  0  1\n |      1  2  3\n |      2  4  5\n |      3  6  7\n |      4  8  9\n |      &gt;&gt;&gt; m = df % 3 == 0\n |      &gt;&gt;&gt; df.where(m, -df)\n |         A  B\n |      0  0 -1\n |      1 -2  3\n |      2 -4 -5\n |      3  6 -7\n |      4 -8  9\n |      &gt;&gt;&gt; df.where(m, -df) == np.where(m, df, -df)\n |            A     B\n |      0  True  True\n |      1  True  True\n |      2  True  True\n |      3  True  True\n |      4  True  True\n |      &gt;&gt;&gt; df.where(m, -df) == df.mask(~m, -df)\n |            A     B\n |      0  True  True\n |      1  True  True\n |      2  True  True\n |      3  True  True\n |      4  True  True\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties defined here:\n |  \n |  array\n |      The ExtensionArray of the data backing this Series or Index.\n |      \n |      Returns\n |      -------\n |      ExtensionArray\n |          An ExtensionArray of the values stored within. For extension\n |          types, this is the actual array. For NumPy native types, this\n |          is a thin (no copy) wrapper around :class:`numpy.ndarray`.\n |      \n |          ``.array`` differs ``.values`` which may require converting the\n |          data to a different form.\n |      \n |      See Also\n |      --------\n |      Index.to_numpy : Similar method that always returns a NumPy array.\n |      Series.to_numpy : Similar method that always returns a NumPy array.\n |      \n |      Notes\n |      -----\n |      This table lays out the different array types for each extension\n |      dtype within pandas.\n |      \n |      ================== =============================\n |      dtype              array type\n |      ================== =============================\n |      category           Categorical\n |      period             PeriodArray\n |      interval           IntervalArray\n |      IntegerNA          IntegerArray\n |      string             StringArray\n |      boolean            BooleanArray\n |      datetime64[ns, tz] DatetimeArray\n |      ================== =============================\n |      \n |      For any 3rd-party extension types, the array type will be an\n |      ExtensionArray.\n |      \n |      For all remaining dtypes ``.array`` will be a\n |      :class:`arrays.NumpyExtensionArray` wrapping the actual ndarray\n |      stored within. If you absolutely need a NumPy array (possibly with\n |      copying / coercing data), then use :meth:`Series.to_numpy` instead.\n |      \n |      Examples\n |      --------\n |      For regular NumPy types like int, and float, a PandasArray\n |      is returned.\n |      \n |      &gt;&gt;&gt; pd.Series([1, 2, 3]).array\n |      &lt;PandasArray&gt;\n |      [1, 2, 3]\n |      Length: 3, dtype: int64\n |      \n |      For extension types, like Categorical, the actual ExtensionArray\n |      is returned\n |      \n |      &gt;&gt;&gt; ser = pd.Series(pd.Categorical(['a', 'b', 'a']))\n |      &gt;&gt;&gt; ser.array\n |      ['a', 'b', 'a']\n |      Categories (2, object): ['a', 'b']\n |  \n |  axes\n |      Return a list of the row axis labels.\n |  \n |  dtype\n |      Return the dtype object of the underlying data.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; s = pd.Series([1, 2, 3])\n |      &gt;&gt;&gt; s.dtype\n |      dtype('int64')\n |  \n |  dtypes\n |      Return the dtype object of the underlying data.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; s = pd.Series([1, 2, 3])\n |      &gt;&gt;&gt; s.dtypes\n |      dtype('int64')\n |  \n |  hasnans\n |      Return True if there are any NaNs.\n |      \n |      Enables various performance speedups.\n |      \n |      Returns\n |      -------\n |      bool\n |  \n |  values\n |      Return Series as ndarray or ndarray-like depending on the dtype.\n |      \n |      .. warning::\n |      \n |         We recommend using :attr:`Series.array` or\n |         :meth:`Series.to_numpy`, depending on whether you need\n |         a reference to the underlying data or a NumPy array.\n |      \n |      Returns\n |      -------\n |      numpy.ndarray or ndarray-like\n |      \n |      See Also\n |      --------\n |      Series.array : Reference to the underlying data.\n |      Series.to_numpy : A NumPy array representing the underlying data.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; pd.Series([1, 2, 3]).values\n |      array([1, 2, 3])\n |      \n |      &gt;&gt;&gt; pd.Series(list('aabc')).values\n |      array(['a', 'a', 'b', 'c'], dtype=object)\n |      \n |      &gt;&gt;&gt; pd.Series(list('aabc')).astype('category').values\n |      ['a', 'a', 'b', 'c']\n |      Categories (3, object): ['a', 'b', 'c']\n |      \n |      Timezone aware datetime data is converted to UTC:\n |      \n |      &gt;&gt;&gt; pd.Series(pd.date_range('20130101', periods=3,\n |      ...                         tz='US/Eastern')).values\n |      array(['2013-01-01T05:00:00.000000000',\n |             '2013-01-02T05:00:00.000000000',\n |             '2013-01-03T05:00:00.000000000'], dtype='datetime64[ns]')\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  index\n |      The index (axis labels) of the Series.\n |  \n |  name\n |      Return the name of the Series.\n |      \n |      The name of a Series becomes its index or column name if it is used\n |      to form a DataFrame. It is also used whenever displaying the Series\n |      using the interpreter.\n |      \n |      Returns\n |      -------\n |      label (hashable object)\n |          The name of the Series, also the column name if part of a DataFrame.\n |      \n |      See Also\n |      --------\n |      Series.rename : Sets the Series name when given a scalar input.\n |      Index.name : Corresponding Index property.\n |      \n |      Examples\n |      --------\n |      The Series name can be set initially when calling the constructor.\n |      \n |      &gt;&gt;&gt; s = pd.Series([1, 2, 3], dtype=np.int64, name='Numbers')\n |      &gt;&gt;&gt; s\n |      0    1\n |      1    2\n |      2    3\n |      Name: Numbers, dtype: int64\n |      &gt;&gt;&gt; s.name = \"Integers\"\n |      &gt;&gt;&gt; s\n |      0    1\n |      1    2\n |      2    3\n |      Name: Integers, dtype: int64\n |      \n |      The name of a Series within a DataFrame is its column name.\n |      \n |      &gt;&gt;&gt; df = pd.DataFrame([[1, 2], [3, 4], [5, 6]],\n |      ...                   columns=[\"Odd Numbers\", \"Even Numbers\"])\n |      &gt;&gt;&gt; df\n |         Odd Numbers  Even Numbers\n |      0            1             2\n |      1            3             4\n |      2            5             6\n |      &gt;&gt;&gt; df[\"Even Numbers\"].name\n |      'Even Numbers'\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  __annotations__ = {'_AXIS_ORDERS': \"list[Literal['index', 'columns']]\"...\n |  \n |  cat = &lt;class 'pandas.core.arrays.categorical.CategoricalAccessor'&gt;\n |      Accessor object for categorical properties of the Series values.\n |      \n |      Parameters\n |      ----------\n |      data : Series or CategoricalIndex\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; s = pd.Series(list(\"abbccc\")).astype(\"category\")\n |      &gt;&gt;&gt; s\n |      0    a\n |      1    b\n |      2    b\n |      3    c\n |      4    c\n |      5    c\n |      dtype: category\n |      Categories (3, object): ['a', 'b', 'c']\n |      \n |      &gt;&gt;&gt; s.cat.categories\n |      Index(['a', 'b', 'c'], dtype='object')\n |      \n |      &gt;&gt;&gt; s.cat.rename_categories(list(\"cba\"))\n |      0    c\n |      1    b\n |      2    b\n |      3    a\n |      4    a\n |      5    a\n |      dtype: category\n |      Categories (3, object): ['c', 'b', 'a']\n |      \n |      &gt;&gt;&gt; s.cat.reorder_categories(list(\"cba\"))\n |      0    a\n |      1    b\n |      2    b\n |      3    c\n |      4    c\n |      5    c\n |      dtype: category\n |      Categories (3, object): ['c', 'b', 'a']\n |      \n |      &gt;&gt;&gt; s.cat.add_categories([\"d\", \"e\"])\n |      0    a\n |      1    b\n |      2    b\n |      3    c\n |      4    c\n |      5    c\n |      dtype: category\n |      Categories (5, object): ['a', 'b', 'c', 'd', 'e']\n |      \n |      &gt;&gt;&gt; s.cat.remove_categories([\"a\", \"c\"])\n |      0    NaN\n |      1      b\n |      2      b\n |      3    NaN\n |      4    NaN\n |      5    NaN\n |      dtype: category\n |      Categories (1, object): ['b']\n |      \n |      &gt;&gt;&gt; s1 = s.cat.add_categories([\"d\", \"e\"])\n |      &gt;&gt;&gt; s1.cat.remove_unused_categories()\n |      0    a\n |      1    b\n |      2    b\n |      3    c\n |      4    c\n |      5    c\n |      dtype: category\n |      Categories (3, object): ['a', 'b', 'c']\n |      \n |      &gt;&gt;&gt; s.cat.set_categories(list(\"abcde\"))\n |      0    a\n |      1    b\n |      2    b\n |      3    c\n |      4    c\n |      5    c\n |      dtype: category\n |      Categories (5, object): ['a', 'b', 'c', 'd', 'e']\n |      \n |      &gt;&gt;&gt; s.cat.as_ordered()\n |      0    a\n |      1    b\n |      2    b\n |      3    c\n |      4    c\n |      5    c\n |      dtype: category\n |      Categories (3, object): ['a' &lt; 'b' &lt; 'c']\n |      \n |      &gt;&gt;&gt; s.cat.as_unordered()\n |      0    a\n |      1    b\n |      2    b\n |      3    c\n |      4    c\n |      5    c\n |      dtype: category\n |      Categories (3, object): ['a', 'b', 'c']\n |  \n |  \n |  dt = &lt;class 'pandas.core.indexes.accessors.CombinedDatetimelikePropert...\n |  \n |  plot = &lt;class 'pandas.plotting._core.PlotAccessor'&gt;\n |      Make plots of Series or DataFrame.\n |      \n |      Uses the backend specified by the\n |      option ``plotting.backend``. By default, matplotlib is used.\n |      \n |      Parameters\n |      ----------\n |      data : Series or DataFrame\n |          The object for which the method is called.\n |      x : label or position, default None\n |          Only used if data is a DataFrame.\n |      y : label, position or list of label, positions, default None\n |          Allows plotting of one column versus another. Only used if data is a\n |          DataFrame.\n |      kind : str\n |          The kind of plot to produce:\n |      \n |          - 'line' : line plot (default)\n |          - 'bar' : vertical bar plot\n |          - 'barh' : horizontal bar plot\n |          - 'hist' : histogram\n |          - 'box' : boxplot\n |          - 'kde' : Kernel Density Estimation plot\n |          - 'density' : same as 'kde'\n |          - 'area' : area plot\n |          - 'pie' : pie plot\n |          - 'scatter' : scatter plot (DataFrame only)\n |          - 'hexbin' : hexbin plot (DataFrame only)\n |      ax : matplotlib axes object, default None\n |          An axes of the current figure.\n |      subplots : bool or sequence of iterables, default False\n |          Whether to group columns into subplots:\n |      \n |          - ``False`` : No subplots will be used\n |          - ``True`` : Make separate subplots for each column.\n |          - sequence of iterables of column labels: Create a subplot for each\n |            group of columns. For example `[('a', 'c'), ('b', 'd')]` will\n |            create 2 subplots: one with columns 'a' and 'c', and one\n |            with columns 'b' and 'd'. Remaining columns that aren't specified\n |            will be plotted in additional subplots (one per column).\n |      \n |            .. versionadded:: 1.5.0\n |      \n |      sharex : bool, default True if ax is None else False\n |          In case ``subplots=True``, share x axis and set some x axis labels\n |          to invisible; defaults to True if ax is None otherwise False if\n |          an ax is passed in; Be aware, that passing in both an ax and\n |          ``sharex=True`` will alter all x axis labels for all axis in a figure.\n |      sharey : bool, default False\n |          In case ``subplots=True``, share y axis and set some y axis labels to invisible.\n |      layout : tuple, optional\n |          (rows, columns) for the layout of subplots.\n |      figsize : a tuple (width, height) in inches\n |          Size of a figure object.\n |      use_index : bool, default True\n |          Use index as ticks for x axis.\n |      title : str or list\n |          Title to use for the plot. If a string is passed, print the string\n |          at the top of the figure. If a list is passed and `subplots` is\n |          True, print each item in the list above the corresponding subplot.\n |      grid : bool, default None (matlab style default)\n |          Axis grid lines.\n |      legend : bool or {'reverse'}\n |          Place legend on axis subplots.\n |      style : list or dict\n |          The matplotlib line style per column.\n |      logx : bool or 'sym', default False\n |          Use log scaling or symlog scaling on x axis.\n |      \n |      logy : bool or 'sym' default False\n |          Use log scaling or symlog scaling on y axis.\n |      \n |      loglog : bool or 'sym', default False\n |          Use log scaling or symlog scaling on both x and y axes.\n |      \n |      xticks : sequence\n |          Values to use for the xticks.\n |      yticks : sequence\n |          Values to use for the yticks.\n |      xlim : 2-tuple/list\n |          Set the x limits of the current axes.\n |      ylim : 2-tuple/list\n |          Set the y limits of the current axes.\n |      xlabel : label, optional\n |          Name to use for the xlabel on x-axis. Default uses index name as xlabel, or the\n |          x-column name for planar plots.\n |      \n |          .. versionadded:: 1.1.0\n |      \n |          .. versionchanged:: 1.2.0\n |      \n |             Now applicable to planar plots (`scatter`, `hexbin`).\n |      \n |          .. versionchanged:: 2.0.0\n |      \n |              Now applicable to histograms.\n |      \n |      ylabel : label, optional\n |          Name to use for the ylabel on y-axis. Default will show no ylabel, or the\n |          y-column name for planar plots.\n |      \n |          .. versionadded:: 1.1.0\n |      \n |          .. versionchanged:: 1.2.0\n |      \n |             Now applicable to planar plots (`scatter`, `hexbin`).\n |      \n |          .. versionchanged:: 2.0.0\n |      \n |              Now applicable to histograms.\n |      \n |      rot : float, default None\n |          Rotation for ticks (xticks for vertical, yticks for horizontal\n |          plots).\n |      fontsize : float, default None\n |          Font size for xticks and yticks.\n |      colormap : str or matplotlib colormap object, default None\n |          Colormap to select colors from. If string, load colormap with that\n |          name from matplotlib.\n |      colorbar : bool, optional\n |          If True, plot colorbar (only relevant for 'scatter' and 'hexbin'\n |          plots).\n |      position : float\n |          Specify relative alignments for bar plot layout.\n |          From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5\n |          (center).\n |      table : bool, Series or DataFrame, default False\n |          If True, draw a table using the data in the DataFrame and the data\n |          will be transposed to meet matplotlib's default layout.\n |          If a Series or DataFrame is passed, use passed data to draw a\n |          table.\n |      yerr : DataFrame, Series, array-like, dict and str\n |          See :ref:`Plotting with Error Bars &lt;visualization.errorbars&gt;` for\n |          detail.\n |      xerr : DataFrame, Series, array-like, dict and str\n |          Equivalent to yerr.\n |      stacked : bool, default False in line and bar plots, and True in area plot\n |          If True, create stacked plot.\n |      secondary_y : bool or sequence, default False\n |          Whether to plot on the secondary y-axis if a list/tuple, which\n |          columns to plot on secondary y-axis.\n |      mark_right : bool, default True\n |          When using a secondary_y axis, automatically mark the column\n |          labels with \"(right)\" in the legend.\n |      include_bool : bool, default is False\n |          If True, boolean values can be plotted.\n |      backend : str, default None\n |          Backend to use instead of the backend specified in the option\n |          ``plotting.backend``. For instance, 'matplotlib'. Alternatively, to\n |          specify the ``plotting.backend`` for the whole session, set\n |          ``pd.options.plotting.backend``.\n |      **kwargs\n |          Options to pass to matplotlib plotting method.\n |      \n |      Returns\n |      -------\n |      :class:`matplotlib.axes.Axes` or numpy.ndarray of them\n |          If the backend is not the default matplotlib one, the return value\n |          will be the object returned by the backend.\n |      \n |      Notes\n |      -----\n |      - See matplotlib documentation online for more on this subject\n |      - If `kind` = 'bar' or 'barh', you can specify relative alignments\n |        for bar plot layout by `position` keyword.\n |        From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5\n |        (center)\n |  \n |  \n |  sparse = &lt;class 'pandas.core.arrays.sparse.accessor.SparseAccessor'&gt;\n |      Accessor for SparseSparse from other sparse matrix data types.\n |  \n |  \n |  str = &lt;class 'pandas.core.strings.accessor.StringMethods'&gt;\n |      Vectorized string functions for Series and Index.\n |      \n |      NAs stay NA unless handled otherwise by a particular method.\n |      Patterned after Python's string methods, with some inspiration from\n |      R's stringr package.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; s = pd.Series([\"A_Str_Series\"])\n |      &gt;&gt;&gt; s\n |      0    A_Str_Series\n |      dtype: object\n |      \n |      &gt;&gt;&gt; s.str.split(\"_\")\n |      0    [A, Str, Series]\n |      dtype: object\n |      \n |      &gt;&gt;&gt; s.str.replace(\"_\", \"\")\n |      0    AStrSeries\n |      dtype: object\n |  \n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pandas.core.base.IndexOpsMixin:\n |  \n |  __iter__(self) -&gt; 'Iterator'\n |      Return an iterator of the values.\n |      \n |      These are each a scalar type, which is a Python scalar\n |      (for str, int, float) or a pandas scalar\n |      (for Timestamp/Timedelta/Interval/Period)\n |      \n |      Returns\n |      -------\n |      iterator\n |  \n |  argmax(self, axis: 'AxisInt | None' = None, skipna: 'bool' = True, *args, **kwargs) -&gt; 'int'\n |      Return int position of the largest value in the Series.\n |      \n |      If the maximum is achieved in multiple locations,\n |      the first row position is returned.\n |      \n |      Parameters\n |      ----------\n |      axis : {None}\n |          Unused. Parameter needed for compatibility with DataFrame.\n |      skipna : bool, default True\n |          Exclude NA/null values when showing the result.\n |      *args, **kwargs\n |          Additional arguments and keywords for compatibility with NumPy.\n |      \n |      Returns\n |      -------\n |      int\n |          Row position of the maximum value.\n |      \n |      See Also\n |      --------\n |      Series.argmax : Return position of the maximum value.\n |      Series.argmin : Return position of the minimum value.\n |      numpy.ndarray.argmax : Equivalent method for numpy arrays.\n |      Series.idxmax : Return index label of the maximum values.\n |      Series.idxmin : Return index label of the minimum values.\n |      \n |      Examples\n |      --------\n |      Consider dataset containing cereal calories\n |      \n |      &gt;&gt;&gt; s = pd.Series({'Corn Flakes': 100.0, 'Almond Delight': 110.0,\n |      ...                'Cinnamon Toast Crunch': 120.0, 'Cocoa Puff': 110.0})\n |      &gt;&gt;&gt; s\n |      Corn Flakes              100.0\n |      Almond Delight           110.0\n |      Cinnamon Toast Crunch    120.0\n |      Cocoa Puff               110.0\n |      dtype: float64\n |      \n |      &gt;&gt;&gt; s.argmax()\n |      2\n |      &gt;&gt;&gt; s.argmin()\n |      0\n |      \n |      The maximum cereal calories is the third element and\n |      the minimum cereal calories is the first element,\n |      since series is zero-indexed.\n |  \n |  argmin(self, axis: 'AxisInt | None' = None, skipna: 'bool' = True, *args, **kwargs) -&gt; 'int'\n |      Return int position of the smallest value in the Series.\n |      \n |      If the minimum is achieved in multiple locations,\n |      the first row position is returned.\n |      \n |      Parameters\n |      ----------\n |      axis : {None}\n |          Unused. Parameter needed for compatibility with DataFrame.\n |      skipna : bool, default True\n |          Exclude NA/null values when showing the result.\n |      *args, **kwargs\n |          Additional arguments and keywords for compatibility with NumPy.\n |      \n |      Returns\n |      -------\n |      int\n |          Row position of the minimum value.\n |      \n |      See Also\n |      --------\n |      Series.argmin : Return position of the minimum value.\n |      Series.argmax : Return position of the maximum value.\n |      numpy.ndarray.argmin : Equivalent method for numpy arrays.\n |      Series.idxmax : Return index label of the maximum values.\n |      Series.idxmin : Return index label of the minimum values.\n |      \n |      Examples\n |      --------\n |      Consider dataset containing cereal calories\n |      \n |      &gt;&gt;&gt; s = pd.Series({'Corn Flakes': 100.0, 'Almond Delight': 110.0,\n |      ...                'Cinnamon Toast Crunch': 120.0, 'Cocoa Puff': 110.0})\n |      &gt;&gt;&gt; s\n |      Corn Flakes              100.0\n |      Almond Delight           110.0\n |      Cinnamon Toast Crunch    120.0\n |      Cocoa Puff               110.0\n |      dtype: float64\n |      \n |      &gt;&gt;&gt; s.argmax()\n |      2\n |      &gt;&gt;&gt; s.argmin()\n |      0\n |      \n |      The maximum cereal calories is the third element and\n |      the minimum cereal calories is the first element,\n |      since series is zero-indexed.\n |  \n |  factorize(self, sort: 'bool' = False, use_na_sentinel: 'bool' = True) -&gt; 'tuple[npt.NDArray[np.intp], Index]'\n |      Encode the object as an enumerated type or categorical variable.\n |      \n |      This method is useful for obtaining a numeric representation of an\n |      array when all that matters is identifying distinct values. `factorize`\n |      is available as both a top-level function :func:`pandas.factorize`,\n |      and as a method :meth:`Series.factorize` and :meth:`Index.factorize`.\n |      \n |      Parameters\n |      ----------\n |      sort : bool, default False\n |          Sort `uniques` and shuffle `codes` to maintain the\n |          relationship.\n |      \n |      use_na_sentinel : bool, default True\n |          If True, the sentinel -1 will be used for NaN values. If False,\n |          NaN values will be encoded as non-negative integers and will not drop the\n |          NaN from the uniques of the values.\n |      \n |          .. versionadded:: 1.5.0\n |      \n |      Returns\n |      -------\n |      codes : ndarray\n |          An integer ndarray that's an indexer into `uniques`.\n |          ``uniques.take(codes)`` will have the same values as `values`.\n |      uniques : ndarray, Index, or Categorical\n |          The unique valid values. When `values` is Categorical, `uniques`\n |          is a Categorical. When `values` is some other pandas object, an\n |          `Index` is returned. Otherwise, a 1-D ndarray is returned.\n |      \n |          .. note::\n |      \n |             Even if there's a missing value in `values`, `uniques` will\n |             *not* contain an entry for it.\n |      \n |      See Also\n |      --------\n |      cut : Discretize continuous-valued array.\n |      unique : Find the unique value in an array.\n |      \n |      Notes\n |      -----\n |      Reference :ref:`the user guide &lt;reshaping.factorize&gt;` for more examples.\n |      \n |      Examples\n |      --------\n |      These examples all show factorize as a top-level method like\n |      ``pd.factorize(values)``. The results are identical for methods like\n |      :meth:`Series.factorize`.\n |      \n |      &gt;&gt;&gt; codes, uniques = pd.factorize(['b', 'b', 'a', 'c', 'b'])\n |      &gt;&gt;&gt; codes\n |      array([0, 0, 1, 2, 0])\n |      &gt;&gt;&gt; uniques\n |      array(['b', 'a', 'c'], dtype=object)\n |      \n |      With ``sort=True``, the `uniques` will be sorted, and `codes` will be\n |      shuffled so that the relationship is the maintained.\n |      \n |      &gt;&gt;&gt; codes, uniques = pd.factorize(['b', 'b', 'a', 'c', 'b'], sort=True)\n |      &gt;&gt;&gt; codes\n |      array([1, 1, 0, 2, 1])\n |      &gt;&gt;&gt; uniques\n |      array(['a', 'b', 'c'], dtype=object)\n |      \n |      When ``use_na_sentinel=True`` (the default), missing values are indicated in\n |      the `codes` with the sentinel value ``-1`` and missing values are not\n |      included in `uniques`.\n |      \n |      &gt;&gt;&gt; codes, uniques = pd.factorize(['b', None, 'a', 'c', 'b'])\n |      &gt;&gt;&gt; codes\n |      array([ 0, -1,  1,  2,  0])\n |      &gt;&gt;&gt; uniques\n |      array(['b', 'a', 'c'], dtype=object)\n |      \n |      Thus far, we've only factorized lists (which are internally coerced to\n |      NumPy arrays). When factorizing pandas objects, the type of `uniques`\n |      will differ. For Categoricals, a `Categorical` is returned.\n |      \n |      &gt;&gt;&gt; cat = pd.Categorical(['a', 'a', 'c'], categories=['a', 'b', 'c'])\n |      &gt;&gt;&gt; codes, uniques = pd.factorize(cat)\n |      &gt;&gt;&gt; codes\n |      array([0, 0, 1])\n |      &gt;&gt;&gt; uniques\n |      ['a', 'c']\n |      Categories (3, object): ['a', 'b', 'c']\n |      \n |      Notice that ``'b'`` is in ``uniques.categories``, despite not being\n |      present in ``cat.values``.\n |      \n |      For all other pandas objects, an Index of the appropriate type is\n |      returned.\n |      \n |      &gt;&gt;&gt; cat = pd.Series(['a', 'a', 'c'])\n |      &gt;&gt;&gt; codes, uniques = pd.factorize(cat)\n |      &gt;&gt;&gt; codes\n |      array([0, 0, 1])\n |      &gt;&gt;&gt; uniques\n |      Index(['a', 'c'], dtype='object')\n |      \n |      If NaN is in the values, and we want to include NaN in the uniques of the\n |      values, it can be achieved by setting ``use_na_sentinel=False``.\n |      \n |      &gt;&gt;&gt; values = np.array([1, 2, 1, np.nan])\n |      &gt;&gt;&gt; codes, uniques = pd.factorize(values)  # default: use_na_sentinel=True\n |      &gt;&gt;&gt; codes\n |      array([ 0,  1,  0, -1])\n |      &gt;&gt;&gt; uniques\n |      array([1., 2.])\n |      \n |      &gt;&gt;&gt; codes, uniques = pd.factorize(values, use_na_sentinel=False)\n |      &gt;&gt;&gt; codes\n |      array([0, 1, 0, 2])\n |      &gt;&gt;&gt; uniques\n |      array([ 1.,  2., nan])\n |  \n |  item(self)\n |      Return the first element of the underlying data as a Python scalar.\n |      \n |      Returns\n |      -------\n |      scalar\n |          The first element of %(klass)s.\n |      \n |      Raises\n |      ------\n |      ValueError\n |          If the data is not length-1.\n |  \n |  nunique(self, dropna: 'bool' = True) -&gt; 'int'\n |      Return number of unique elements in the object.\n |      \n |      Excludes NA values by default.\n |      \n |      Parameters\n |      ----------\n |      dropna : bool, default True\n |          Don't include NaN in the count.\n |      \n |      Returns\n |      -------\n |      int\n |      \n |      See Also\n |      --------\n |      DataFrame.nunique: Method nunique for DataFrame.\n |      Series.count: Count non-NA/null observations in the Series.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; s = pd.Series([1, 3, 5, 7, 7])\n |      &gt;&gt;&gt; s\n |      0    1\n |      1    3\n |      2    5\n |      3    7\n |      4    7\n |      dtype: int64\n |      \n |      &gt;&gt;&gt; s.nunique()\n |      4\n |  \n |  to_list = tolist(self)\n |  \n |  to_numpy(self, dtype: 'npt.DTypeLike | None' = None, copy: 'bool' = False, na_value: 'object' = &lt;no_default&gt;, **kwargs) -&gt; 'np.ndarray'\n |      A NumPy ndarray representing the values in this Series or Index.\n |      \n |      Parameters\n |      ----------\n |      dtype : str or numpy.dtype, optional\n |          The dtype to pass to :meth:`numpy.asarray`.\n |      copy : bool, default False\n |          Whether to ensure that the returned value is not a view on\n |          another array. Note that ``copy=False`` does not *ensure* that\n |          ``to_numpy()`` is no-copy. Rather, ``copy=True`` ensure that\n |          a copy is made, even if not strictly necessary.\n |      na_value : Any, optional\n |          The value to use for missing values. The default value depends\n |          on `dtype` and the type of the array.\n |      **kwargs\n |          Additional keywords passed through to the ``to_numpy`` method\n |          of the underlying array (for extension arrays).\n |      \n |      Returns\n |      -------\n |      numpy.ndarray\n |      \n |      See Also\n |      --------\n |      Series.array : Get the actual data stored within.\n |      Index.array : Get the actual data stored within.\n |      DataFrame.to_numpy : Similar method for DataFrame.\n |      \n |      Notes\n |      -----\n |      The returned array will be the same up to equality (values equal\n |      in `self` will be equal in the returned array; likewise for values\n |      that are not equal). When `self` contains an ExtensionArray, the\n |      dtype may be different. For example, for a category-dtype Series,\n |      ``to_numpy()`` will return a NumPy array and the categorical dtype\n |      will be lost.\n |      \n |      For NumPy dtypes, this will be a reference to the actual data stored\n |      in this Series or Index (assuming ``copy=False``). Modifying the result\n |      in place will modify the data stored in the Series or Index (not that\n |      we recommend doing that).\n |      \n |      For extension types, ``to_numpy()`` *may* require copying data and\n |      coercing the result to a NumPy type (possibly object), which may be\n |      expensive. When you need a no-copy reference to the underlying data,\n |      :attr:`Series.array` should be used instead.\n |      \n |      This table lays out the different dtypes and default return types of\n |      ``to_numpy()`` for various dtypes within pandas.\n |      \n |      ================== ================================\n |      dtype              array type\n |      ================== ================================\n |      category[T]        ndarray[T] (same dtype as input)\n |      period             ndarray[object] (Periods)\n |      interval           ndarray[object] (Intervals)\n |      IntegerNA          ndarray[object]\n |      datetime64[ns]     datetime64[ns]\n |      datetime64[ns, tz] ndarray[object] (Timestamps)\n |      ================== ================================\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; ser = pd.Series(pd.Categorical(['a', 'b', 'a']))\n |      &gt;&gt;&gt; ser.to_numpy()\n |      array(['a', 'b', 'a'], dtype=object)\n |      \n |      Specify the `dtype` to control how datetime-aware data is represented.\n |      Use ``dtype=object`` to return an ndarray of pandas :class:`Timestamp`\n |      objects, each with the correct ``tz``.\n |      \n |      &gt;&gt;&gt; ser = pd.Series(pd.date_range('2000', periods=2, tz=\"CET\"))\n |      &gt;&gt;&gt; ser.to_numpy(dtype=object)\n |      array([Timestamp('2000-01-01 00:00:00+0100', tz='CET'),\n |             Timestamp('2000-01-02 00:00:00+0100', tz='CET')],\n |            dtype=object)\n |      \n |      Or ``dtype='datetime64[ns]'`` to return an ndarray of native\n |      datetime64 values. The values are converted to UTC and the timezone\n |      info is dropped.\n |      \n |      &gt;&gt;&gt; ser.to_numpy(dtype=\"datetime64[ns]\")\n |      ... # doctest: +ELLIPSIS\n |      array(['1999-12-31T23:00:00.000000000', '2000-01-01T23:00:00...'],\n |            dtype='datetime64[ns]')\n |  \n |  tolist(self)\n |      Return a list of the values.\n |      \n |      These are each a scalar type, which is a Python scalar\n |      (for str, int, float) or a pandas scalar\n |      (for Timestamp/Timedelta/Interval/Period)\n |      \n |      Returns\n |      -------\n |      list\n |      \n |      See Also\n |      --------\n |      numpy.ndarray.tolist : Return the array as an a.ndim-levels deep\n |          nested list of Python scalars.\n |  \n |  transpose(self: '_T', *args, **kwargs) -&gt; '_T'\n |      Return the transpose, which is by definition self.\n |      \n |      Returns\n |      -------\n |      %(klass)s\n |  \n |  value_counts(self, normalize: 'bool' = False, sort: 'bool' = True, ascending: 'bool' = False, bins=None, dropna: 'bool' = True) -&gt; 'Series'\n |      Return a Series containing counts of unique values.\n |      \n |      The resulting object will be in descending order so that the\n |      first element is the most frequently-occurring element.\n |      Excludes NA values by default.\n |      \n |      Parameters\n |      ----------\n |      normalize : bool, default False\n |          If True then the object returned will contain the relative\n |          frequencies of the unique values.\n |      sort : bool, default True\n |          Sort by frequencies.\n |      ascending : bool, default False\n |          Sort in ascending order.\n |      bins : int, optional\n |          Rather than count values, group them into half-open bins,\n |          a convenience for ``pd.cut``, only works with numeric data.\n |      dropna : bool, default True\n |          Don't include counts of NaN.\n |      \n |      Returns\n |      -------\n |      Series\n |      \n |      See Also\n |      --------\n |      Series.count: Number of non-NA elements in a Series.\n |      DataFrame.count: Number of non-NA elements in a DataFrame.\n |      DataFrame.value_counts: Equivalent method on DataFrames.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; index = pd.Index([3, 1, 2, 3, 4, np.nan])\n |      &gt;&gt;&gt; index.value_counts()\n |      3.0    2\n |      1.0    1\n |      2.0    1\n |      4.0    1\n |      Name: count, dtype: int64\n |      \n |      With `normalize` set to `True`, returns the relative frequency by\n |      dividing all values by the sum of values.\n |      \n |      &gt;&gt;&gt; s = pd.Series([3, 1, 2, 3, 4, np.nan])\n |      &gt;&gt;&gt; s.value_counts(normalize=True)\n |      3.0    0.4\n |      1.0    0.2\n |      2.0    0.2\n |      4.0    0.2\n |      Name: proportion, dtype: float64\n |      \n |      **bins**\n |      \n |      Bins can be useful for going from a continuous variable to a\n |      categorical variable; instead of counting unique\n |      apparitions of values, divide the index in the specified\n |      number of half-open bins.\n |      \n |      &gt;&gt;&gt; s.value_counts(bins=3)\n |      (0.996, 2.0]    2\n |      (2.0, 3.0]      2\n |      (3.0, 4.0]      1\n |      Name: count, dtype: int64\n |      \n |      **dropna**\n |      \n |      With `dropna` set to `False` we can also see NaN index values.\n |      \n |      &gt;&gt;&gt; s.value_counts(dropna=False)\n |      3.0    2\n |      1.0    1\n |      2.0    1\n |      4.0    1\n |      NaN    1\n |      Name: count, dtype: int64\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties inherited from pandas.core.base.IndexOpsMixin:\n |  \n |  T\n |      Return the transpose, which is by definition self.\n |  \n |  empty\n |  \n |  is_monotonic_decreasing\n |      Return boolean if values in the object are monotonically decreasing.\n |      \n |      Returns\n |      -------\n |      bool\n |  \n |  is_monotonic_increasing\n |      Return boolean if values in the object are monotonically increasing.\n |      \n |      Returns\n |      -------\n |      bool\n |  \n |  is_unique\n |      Return boolean if values in the object are unique.\n |      \n |      Returns\n |      -------\n |      bool\n |  \n |  nbytes\n |      Return the number of bytes in the underlying data.\n |  \n |  ndim\n |      Number of dimensions of the underlying data, by definition 1.\n |  \n |  shape\n |      Return a tuple of the shape of the underlying data.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; s = pd.Series([1, 2, 3])\n |      &gt;&gt;&gt; s.shape\n |      (3,)\n |  \n |  size\n |      Return the number of elements in the underlying data.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pandas.core.base.IndexOpsMixin:\n |  \n |  __array_priority__ = 1000\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pandas.core.arraylike.OpsMixin:\n |  \n |  __add__(self, other)\n |      Get Addition of DataFrame and other, column-wise.\n |      \n |      Equivalent to ``DataFrame.add(other)``.\n |      \n |      Parameters\n |      ----------\n |      other : scalar, sequence, Series, dict or DataFrame\n |          Object to be added to the DataFrame.\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          The result of adding ``other`` to DataFrame.\n |      \n |      See Also\n |      --------\n |      DataFrame.add : Add a DataFrame and another object, with option for index-\n |          or column-oriented addition.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; df = pd.DataFrame({'height': [1.5, 2.6], 'weight': [500, 800]},\n |      ...                   index=['elk', 'moose'])\n |      &gt;&gt;&gt; df\n |             height  weight\n |      elk       1.5     500\n |      moose     2.6     800\n |      \n |      Adding a scalar affects all rows and columns.\n |      \n |      &gt;&gt;&gt; df[['height', 'weight']] + 1.5\n |             height  weight\n |      elk       3.0   501.5\n |      moose     4.1   801.5\n |      \n |      Each element of a list is added to a column of the DataFrame, in order.\n |      \n |      &gt;&gt;&gt; df[['height', 'weight']] + [0.5, 1.5]\n |             height  weight\n |      elk       2.0   501.5\n |      moose     3.1   801.5\n |      \n |      Keys of a dictionary are aligned to the DataFrame, based on column names;\n |      each value in the dictionary is added to the corresponding column.\n |      \n |      &gt;&gt;&gt; df[['height', 'weight']] + {'height': 0.5, 'weight': 1.5}\n |             height  weight\n |      elk       2.0   501.5\n |      moose     3.1   801.5\n |      \n |      When `other` is a :class:`Series`, the index of `other` is aligned with the\n |      columns of the DataFrame.\n |      \n |      &gt;&gt;&gt; s1 = pd.Series([0.5, 1.5], index=['weight', 'height'])\n |      &gt;&gt;&gt; df[['height', 'weight']] + s1\n |             height  weight\n |      elk       3.0   500.5\n |      moose     4.1   800.5\n |      \n |      Even when the index of `other` is the same as the index of the DataFrame,\n |      the :class:`Series` will not be reoriented. If index-wise alignment is desired,\n |      :meth:`DataFrame.add` should be used with `axis='index'`.\n |      \n |      &gt;&gt;&gt; s2 = pd.Series([0.5, 1.5], index=['elk', 'moose'])\n |      &gt;&gt;&gt; df[['height', 'weight']] + s2\n |             elk  height  moose  weight\n |      elk    NaN     NaN    NaN     NaN\n |      moose  NaN     NaN    NaN     NaN\n |      \n |      &gt;&gt;&gt; df[['height', 'weight']].add(s2, axis='index')\n |             height  weight\n |      elk       2.0   500.5\n |      moose     4.1   801.5\n |      \n |      When `other` is a :class:`DataFrame`, both columns names and the\n |      index are aligned.\n |      \n |      &gt;&gt;&gt; other = pd.DataFrame({'height': [0.2, 0.4, 0.6]},\n |      ...                      index=['elk', 'moose', 'deer'])\n |      &gt;&gt;&gt; df[['height', 'weight']] + other\n |             height  weight\n |      deer      NaN     NaN\n |      elk       1.7     NaN\n |      moose     3.0     NaN\n |  \n |  __and__(self, other)\n |  \n |  __divmod__(self, other)\n |  \n |  __eq__(self, other)\n |      Return self==value.\n |  \n |  __floordiv__(self, other)\n |  \n |  __ge__(self, other)\n |      Return self&gt;=value.\n |  \n |  __gt__(self, other)\n |      Return self&gt;value.\n |  \n |  __le__(self, other)\n |      Return self&lt;=value.\n |  \n |  __lt__(self, other)\n |      Return self&lt;value.\n |  \n |  __mod__(self, other)\n |  \n |  __mul__(self, other)\n |  \n |  __ne__(self, other)\n |      Return self!=value.\n |  \n |  __or__(self, other)\n |  \n |  __pow__(self, other)\n |  \n |  __radd__(self, other)\n |  \n |  __rand__(self, other)\n |  \n |  __rdivmod__(self, other)\n |  \n |  __rfloordiv__(self, other)\n |  \n |  __rmod__(self, other)\n |  \n |  __rmul__(self, other)\n |  \n |  __ror__(self, other)\n |  \n |  __rpow__(self, other)\n |  \n |  __rsub__(self, other)\n |  \n |  __rtruediv__(self, other)\n |  \n |  __rxor__(self, other)\n |  \n |  __sub__(self, other)\n |  \n |  __truediv__(self, other)\n |  \n |  __xor__(self, other)\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from pandas.core.arraylike.OpsMixin:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pandas.core.arraylike.OpsMixin:\n |  \n |  __hash__ = None\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pandas.core.generic.NDFrame:\n |  \n |  __abs__(self: 'NDFrameT') -&gt; 'NDFrameT'\n |  \n |  __array_ufunc__(self, ufunc: 'np.ufunc', method: 'str', *inputs: 'Any', **kwargs: 'Any')\n |  \n |  __bool__ = __nonzero__(self) -&gt; 'NoReturn'\n |  \n |  __contains__(self, key) -&gt; 'bool_t'\n |      True if the key is in the info axis\n |  \n |  __copy__(self: 'NDFrameT', deep: 'bool_t' = True) -&gt; 'NDFrameT'\n |  \n |  __deepcopy__(self: 'NDFrameT', memo=None) -&gt; 'NDFrameT'\n |      Parameters\n |      ----------\n |      memo, default None\n |          Standard signature. Unused\n |  \n |  __delitem__(self, key) -&gt; 'None'\n |      Delete item\n |  \n |  __finalize__(self: 'NDFrameT', other, method: 'str | None' = None, **kwargs) -&gt; 'NDFrameT'\n |      Propagate metadata from other to self.\n |      \n |      Parameters\n |      ----------\n |      other : the object from which to get the attributes that we are going\n |          to propagate\n |      method : str, optional\n |          A passed method name providing context on where ``__finalize__``\n |          was called.\n |      \n |          .. warning::\n |      \n |             The value passed as `method` are not currently considered\n |             stable across pandas releases.\n |  \n |  __getattr__(self, name: 'str')\n |      After regular attribute access, try looking up the name\n |      This allows simpler access to columns for interactive use.\n |  \n |  __getstate__(self) -&gt; 'dict[str, Any]'\n |  \n |  __iadd__(self: 'NDFrameT', other) -&gt; 'NDFrameT'\n |  \n |  __iand__(self: 'NDFrameT', other) -&gt; 'NDFrameT'\n |  \n |  __ifloordiv__(self: 'NDFrameT', other) -&gt; 'NDFrameT'\n |  \n |  __imod__(self: 'NDFrameT', other) -&gt; 'NDFrameT'\n |  \n |  __imul__(self: 'NDFrameT', other) -&gt; 'NDFrameT'\n |  \n |  __invert__(self: 'NDFrameT') -&gt; 'NDFrameT'\n |  \n |  __ior__(self: 'NDFrameT', other) -&gt; 'NDFrameT'\n |  \n |  __ipow__(self: 'NDFrameT', other) -&gt; 'NDFrameT'\n |  \n |  __isub__(self: 'NDFrameT', other) -&gt; 'NDFrameT'\n |  \n |  __itruediv__(self: 'NDFrameT', other) -&gt; 'NDFrameT'\n |  \n |  __ixor__(self: 'NDFrameT', other) -&gt; 'NDFrameT'\n |  \n |  __neg__(self: 'NDFrameT') -&gt; 'NDFrameT'\n |  \n |  __nonzero__(self) -&gt; 'NoReturn'\n |  \n |  __pos__(self: 'NDFrameT') -&gt; 'NDFrameT'\n |  \n |  __round__(self: 'NDFrameT', decimals: 'int' = 0) -&gt; 'NDFrameT'\n |  \n |  __setattr__(self, name: 'str', value) -&gt; 'None'\n |      After regular attribute access, try setting the name\n |      This allows simpler access to columns for interactive use.\n |  \n |  __setstate__(self, state) -&gt; 'None'\n |  \n |  abs(self: 'NDFrameT') -&gt; 'NDFrameT'\n |      Return a Series/DataFrame with absolute numeric value of each element.\n |      \n |      This function only applies to elements that are all numeric.\n |      \n |      Returns\n |      -------\n |      abs\n |          Series/DataFrame containing the absolute value of each element.\n |      \n |      See Also\n |      --------\n |      numpy.absolute : Calculate the absolute value element-wise.\n |      \n |      Notes\n |      -----\n |      For ``complex`` inputs, ``1.2 + 1j``, the absolute value is\n |      :math:`\\sqrt{ a^2 + b^2 }`.\n |      \n |      Examples\n |      --------\n |      Absolute numeric values in a Series.\n |      \n |      &gt;&gt;&gt; s = pd.Series([-1.10, 2, -3.33, 4])\n |      &gt;&gt;&gt; s.abs()\n |      0    1.10\n |      1    2.00\n |      2    3.33\n |      3    4.00\n |      dtype: float64\n |      \n |      Absolute numeric values in a Series with complex numbers.\n |      \n |      &gt;&gt;&gt; s = pd.Series([1.2 + 1j])\n |      &gt;&gt;&gt; s.abs()\n |      0    1.56205\n |      dtype: float64\n |      \n |      Absolute numeric values in a Series with a Timedelta element.\n |      \n |      &gt;&gt;&gt; s = pd.Series([pd.Timedelta('1 days')])\n |      &gt;&gt;&gt; s.abs()\n |      0   1 days\n |      dtype: timedelta64[ns]\n |      \n |      Select rows with data closest to certain value using argsort (from\n |      `StackOverflow &lt;https://stackoverflow.com/a/17758115&gt;`__).\n |      \n |      &gt;&gt;&gt; df = pd.DataFrame({\n |      ...     'a': [4, 5, 6, 7],\n |      ...     'b': [10, 20, 30, 40],\n |      ...     'c': [100, 50, -30, -50]\n |      ... })\n |      &gt;&gt;&gt; df\n |           a    b    c\n |      0    4   10  100\n |      1    5   20   50\n |      2    6   30  -30\n |      3    7   40  -50\n |      &gt;&gt;&gt; df.loc[(df.c - 43).abs().argsort()]\n |           a    b    c\n |      1    5   20   50\n |      0    4   10  100\n |      2    6   30  -30\n |      3    7   40  -50\n |  \n |  add_prefix(self: 'NDFrameT', prefix: 'str', axis: 'Axis | None' = None) -&gt; 'NDFrameT'\n |      Prefix labels with string `prefix`.\n |      \n |      For Series, the row labels are prefixed.\n |      For DataFrame, the column labels are prefixed.\n |      \n |      Parameters\n |      ----------\n |      prefix : str\n |          The string to add before each label.\n |      axis : {{0 or 'index', 1 or 'columns', None}}, default None\n |          Axis to add prefix on\n |      \n |          .. versionadded:: 2.0.0\n |      \n |      Returns\n |      -------\n |      Series or DataFrame\n |          New Series or DataFrame with updated labels.\n |      \n |      See Also\n |      --------\n |      Series.add_suffix: Suffix row labels with string `suffix`.\n |      DataFrame.add_suffix: Suffix column labels with string `suffix`.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; s = pd.Series([1, 2, 3, 4])\n |      &gt;&gt;&gt; s\n |      0    1\n |      1    2\n |      2    3\n |      3    4\n |      dtype: int64\n |      \n |      &gt;&gt;&gt; s.add_prefix('item_')\n |      item_0    1\n |      item_1    2\n |      item_2    3\n |      item_3    4\n |      dtype: int64\n |      \n |      &gt;&gt;&gt; df = pd.DataFrame({'A': [1, 2, 3, 4], 'B': [3, 4, 5, 6]})\n |      &gt;&gt;&gt; df\n |         A  B\n |      0  1  3\n |      1  2  4\n |      2  3  5\n |      3  4  6\n |      \n |      &gt;&gt;&gt; df.add_prefix('col_')\n |           col_A  col_B\n |      0       1       3\n |      1       2       4\n |      2       3       5\n |      3       4       6\n |  \n |  add_suffix(self: 'NDFrameT', suffix: 'str', axis: 'Axis | None' = None) -&gt; 'NDFrameT'\n |      Suffix labels with string `suffix`.\n |      \n |      For Series, the row labels are suffixed.\n |      For DataFrame, the column labels are suffixed.\n |      \n |      Parameters\n |      ----------\n |      suffix : str\n |          The string to add after each label.\n |      axis : {{0 or 'index', 1 or 'columns', None}}, default None\n |          Axis to add suffix on\n |      \n |          .. versionadded:: 2.0.0\n |      \n |      Returns\n |      -------\n |      Series or DataFrame\n |          New Series or DataFrame with updated labels.\n |      \n |      See Also\n |      --------\n |      Series.add_prefix: Prefix row labels with string `prefix`.\n |      DataFrame.add_prefix: Prefix column labels with string `prefix`.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; s = pd.Series([1, 2, 3, 4])\n |      &gt;&gt;&gt; s\n |      0    1\n |      1    2\n |      2    3\n |      3    4\n |      dtype: int64\n |      \n |      &gt;&gt;&gt; s.add_suffix('_item')\n |      0_item    1\n |      1_item    2\n |      2_item    3\n |      3_item    4\n |      dtype: int64\n |      \n |      &gt;&gt;&gt; df = pd.DataFrame({'A': [1, 2, 3, 4], 'B': [3, 4, 5, 6]})\n |      &gt;&gt;&gt; df\n |         A  B\n |      0  1  3\n |      1  2  4\n |      2  3  5\n |      3  4  6\n |      \n |      &gt;&gt;&gt; df.add_suffix('_col')\n |           A_col  B_col\n |      0       1       3\n |      1       2       4\n |      2       3       5\n |      3       4       6\n |  \n |  asof(self, where, subset=None)\n |      Return the last row(s) without any NaNs before `where`.\n |      \n |      The last row (for each element in `where`, if list) without any\n |      NaN is taken.\n |      In case of a :class:`~pandas.DataFrame`, the last row without NaN\n |      considering only the subset of columns (if not `None`)\n |      \n |      If there is no good value, NaN is returned for a Series or\n |      a Series of NaN values for a DataFrame\n |      \n |      Parameters\n |      ----------\n |      where : date or array-like of dates\n |          Date(s) before which the last row(s) are returned.\n |      subset : str or array-like of str, default `None`\n |          For DataFrame, if not `None`, only use these columns to\n |          check for NaNs.\n |      \n |      Returns\n |      -------\n |      scalar, Series, or DataFrame\n |      \n |          The return can be:\n |      \n |          * scalar : when `self` is a Series and `where` is a scalar\n |          * Series: when `self` is a Series and `where` is an array-like,\n |            or when `self` is a DataFrame and `where` is a scalar\n |          * DataFrame : when `self` is a DataFrame and `where` is an\n |            array-like\n |      \n |          Return scalar, Series, or DataFrame.\n |      \n |      See Also\n |      --------\n |      merge_asof : Perform an asof merge. Similar to left join.\n |      \n |      Notes\n |      -----\n |      Dates are assumed to be sorted. Raises if this is not the case.\n |      \n |      Examples\n |      --------\n |      A Series and a scalar `where`.\n |      \n |      &gt;&gt;&gt; s = pd.Series([1, 2, np.nan, 4], index=[10, 20, 30, 40])\n |      &gt;&gt;&gt; s\n |      10    1.0\n |      20    2.0\n |      30    NaN\n |      40    4.0\n |      dtype: float64\n |      \n |      &gt;&gt;&gt; s.asof(20)\n |      2.0\n |      \n |      For a sequence `where`, a Series is returned. The first value is\n |      NaN, because the first element of `where` is before the first\n |      index value.\n |      \n |      &gt;&gt;&gt; s.asof([5, 20])\n |      5     NaN\n |      20    2.0\n |      dtype: float64\n |      \n |      Missing values are not considered. The following is ``2.0``, not\n |      NaN, even though NaN is at the index location for ``30``.\n |      \n |      &gt;&gt;&gt; s.asof(30)\n |      2.0\n |      \n |      Take all columns into consideration\n |      \n |      &gt;&gt;&gt; df = pd.DataFrame({'a': [10, 20, 30, 40, 50],\n |      ...                    'b': [None, None, None, None, 500]},\n |      ...                   index=pd.DatetimeIndex(['2018-02-27 09:01:00',\n |      ...                                           '2018-02-27 09:02:00',\n |      ...                                           '2018-02-27 09:03:00',\n |      ...                                           '2018-02-27 09:04:00',\n |      ...                                           '2018-02-27 09:05:00']))\n |      &gt;&gt;&gt; df.asof(pd.DatetimeIndex(['2018-02-27 09:03:30',\n |      ...                           '2018-02-27 09:04:30']))\n |                            a   b\n |      2018-02-27 09:03:30 NaN NaN\n |      2018-02-27 09:04:30 NaN NaN\n |      \n |      Take a single column into consideration\n |      \n |      &gt;&gt;&gt; df.asof(pd.DatetimeIndex(['2018-02-27 09:03:30',\n |      ...                           '2018-02-27 09:04:30']),\n |      ...         subset=['a'])\n |                            a   b\n |      2018-02-27 09:03:30  30 NaN\n |      2018-02-27 09:04:30  40 NaN\n |  \n |  astype(self: 'NDFrameT', dtype, copy: 'bool_t | None' = None, errors: 'IgnoreRaise' = 'raise') -&gt; 'NDFrameT'\n |      Cast a pandas object to a specified dtype ``dtype``.\n |      \n |      Parameters\n |      ----------\n |      dtype : str, data type, Series or Mapping of column name -&gt; data type\n |          Use a str, numpy.dtype, pandas.ExtensionDtype or Python type to\n |          cast entire pandas object to the same type. Alternatively, use a\n |          mapping, e.g. {col: dtype, ...}, where col is a column label and dtype is\n |          a numpy.dtype or Python type to cast one or more of the DataFrame's\n |          columns to column-specific types.\n |      copy : bool, default True\n |          Return a copy when ``copy=True`` (be very careful setting\n |          ``copy=False`` as changes to values then may propagate to other\n |          pandas objects).\n |      errors : {'raise', 'ignore'}, default 'raise'\n |          Control raising of exceptions on invalid data for provided dtype.\n |      \n |          - ``raise`` : allow exceptions to be raised\n |          - ``ignore`` : suppress exceptions. On error return original object.\n |      \n |      Returns\n |      -------\n |      same type as caller\n |      \n |      See Also\n |      --------\n |      to_datetime : Convert argument to datetime.\n |      to_timedelta : Convert argument to timedelta.\n |      to_numeric : Convert argument to a numeric type.\n |      numpy.ndarray.astype : Cast a numpy array to a specified type.\n |      \n |      Notes\n |      -----\n |      .. versionchanged:: 2.0.0\n |      \n |          Using ``astype`` to convert from timezone-naive dtype to\n |          timezone-aware dtype will raise an exception.\n |          Use :meth:`Series.dt.tz_localize` instead.\n |      \n |      Examples\n |      --------\n |      Create a DataFrame:\n |      \n |      &gt;&gt;&gt; d = {'col1': [1, 2], 'col2': [3, 4]}\n |      &gt;&gt;&gt; df = pd.DataFrame(data=d)\n |      &gt;&gt;&gt; df.dtypes\n |      col1    int64\n |      col2    int64\n |      dtype: object\n |      \n |      Cast all columns to int32:\n |      \n |      &gt;&gt;&gt; df.astype('int32').dtypes\n |      col1    int32\n |      col2    int32\n |      dtype: object\n |      \n |      Cast col1 to int32 using a dictionary:\n |      \n |      &gt;&gt;&gt; df.astype({'col1': 'int32'}).dtypes\n |      col1    int32\n |      col2    int64\n |      dtype: object\n |      \n |      Create a series:\n |      \n |      &gt;&gt;&gt; ser = pd.Series([1, 2], dtype='int32')\n |      &gt;&gt;&gt; ser\n |      0    1\n |      1    2\n |      dtype: int32\n |      &gt;&gt;&gt; ser.astype('int64')\n |      0    1\n |      1    2\n |      dtype: int64\n |      \n |      Convert to categorical type:\n |      \n |      &gt;&gt;&gt; ser.astype('category')\n |      0    1\n |      1    2\n |      dtype: category\n |      Categories (2, int32): [1, 2]\n |      \n |      Convert to ordered categorical type with custom ordering:\n |      \n |      &gt;&gt;&gt; from pandas.api.types import CategoricalDtype\n |      &gt;&gt;&gt; cat_dtype = CategoricalDtype(\n |      ...     categories=[2, 1], ordered=True)\n |      &gt;&gt;&gt; ser.astype(cat_dtype)\n |      0    1\n |      1    2\n |      dtype: category\n |      Categories (2, int64): [2 &lt; 1]\n |      \n |      Create a series of dates:\n |      \n |      &gt;&gt;&gt; ser_date = pd.Series(pd.date_range('20200101', periods=3))\n |      &gt;&gt;&gt; ser_date\n |      0   2020-01-01\n |      1   2020-01-02\n |      2   2020-01-03\n |      dtype: datetime64[ns]\n |  \n |  at_time(self: 'NDFrameT', time, asof: 'bool_t' = False, axis: 'Axis | None' = None) -&gt; 'NDFrameT'\n |      Select values at particular time of day (e.g., 9:30AM).\n |      \n |      Parameters\n |      ----------\n |      time : datetime.time or str\n |          The values to select.\n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |          For `Series` this parameter is unused and defaults to 0.\n |      \n |      Returns\n |      -------\n |      Series or DataFrame\n |      \n |      Raises\n |      ------\n |      TypeError\n |          If the index is not  a :class:`DatetimeIndex`\n |      \n |      See Also\n |      --------\n |      between_time : Select values between particular times of the day.\n |      first : Select initial periods of time series based on a date offset.\n |      last : Select final periods of time series based on a date offset.\n |      DatetimeIndex.indexer_at_time : Get just the index locations for\n |          values at particular time of the day.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; i = pd.date_range('2018-04-09', periods=4, freq='12H')\n |      &gt;&gt;&gt; ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i)\n |      &gt;&gt;&gt; ts\n |                           A\n |      2018-04-09 00:00:00  1\n |      2018-04-09 12:00:00  2\n |      2018-04-10 00:00:00  3\n |      2018-04-10 12:00:00  4\n |      \n |      &gt;&gt;&gt; ts.at_time('12:00')\n |                           A\n |      2018-04-09 12:00:00  2\n |      2018-04-10 12:00:00  4\n |  \n |  backfill(self: 'NDFrameT', *, axis: 'None | Axis' = None, inplace: 'bool_t' = False, limit: 'None | int' = None, downcast: 'dict | None' = None) -&gt; 'NDFrameT | None'\n |      Synonym for :meth:`DataFrame.fillna` with ``method='bfill'``.\n |      \n |      .. deprecated:: 2.0\n |      \n |          Series/DataFrame.backfill is deprecated. Use Series/DataFrame.bfill instead.\n |      \n |      Returns\n |      -------\n |      Series/DataFrame or None\n |          Object with missing values filled or None if ``inplace=True``.\n |  \n |  between_time(self: 'NDFrameT', start_time, end_time, inclusive: 'IntervalClosedType' = 'both', axis: 'Axis | None' = None) -&gt; 'NDFrameT'\n |      Select values between particular times of the day (e.g., 9:00-9:30 AM).\n |      \n |      By setting ``start_time`` to be later than ``end_time``,\n |      you can get the times that are *not* between the two times.\n |      \n |      Parameters\n |      ----------\n |      start_time : datetime.time or str\n |          Initial time as a time filter limit.\n |      end_time : datetime.time or str\n |          End time as a time filter limit.\n |      inclusive : {\"both\", \"neither\", \"left\", \"right\"}, default \"both\"\n |          Include boundaries; whether to set each bound as closed or open.\n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |          Determine range time on index or columns value.\n |          For `Series` this parameter is unused and defaults to 0.\n |      \n |      Returns\n |      -------\n |      Series or DataFrame\n |          Data from the original object filtered to the specified dates range.\n |      \n |      Raises\n |      ------\n |      TypeError\n |          If the index is not  a :class:`DatetimeIndex`\n |      \n |      See Also\n |      --------\n |      at_time : Select values at a particular time of the day.\n |      first : Select initial periods of time series based on a date offset.\n |      last : Select final periods of time series based on a date offset.\n |      DatetimeIndex.indexer_between_time : Get just the index locations for\n |          values between particular times of the day.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; i = pd.date_range('2018-04-09', periods=4, freq='1D20min')\n |      &gt;&gt;&gt; ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i)\n |      &gt;&gt;&gt; ts\n |                           A\n |      2018-04-09 00:00:00  1\n |      2018-04-10 00:20:00  2\n |      2018-04-11 00:40:00  3\n |      2018-04-12 01:00:00  4\n |      \n |      &gt;&gt;&gt; ts.between_time('0:15', '0:45')\n |                           A\n |      2018-04-10 00:20:00  2\n |      2018-04-11 00:40:00  3\n |      \n |      You get the times that are *not* between two times by setting\n |      ``start_time`` later than ``end_time``:\n |      \n |      &gt;&gt;&gt; ts.between_time('0:45', '0:15')\n |                           A\n |      2018-04-09 00:00:00  1\n |      2018-04-12 01:00:00  4\n |  \n |  bool(self) -&gt; 'bool_t'\n |      Return the bool of a single element Series or DataFrame.\n |      \n |      This must be a boolean scalar value, either True or False. It will raise a\n |      ValueError if the Series or DataFrame does not have exactly 1 element, or that\n |      element is not boolean (integer values 0 and 1 will also raise an exception).\n |      \n |      Returns\n |      -------\n |      bool\n |          The value in the Series or DataFrame.\n |      \n |      See Also\n |      --------\n |      Series.astype : Change the data type of a Series, including to boolean.\n |      DataFrame.astype : Change the data type of a DataFrame, including to boolean.\n |      numpy.bool_ : NumPy boolean data type, used by pandas for boolean values.\n |      \n |      Examples\n |      --------\n |      The method will only work for single element objects with a boolean value:\n |      \n |      &gt;&gt;&gt; pd.Series([True]).bool()\n |      True\n |      &gt;&gt;&gt; pd.Series([False]).bool()\n |      False\n |      \n |      &gt;&gt;&gt; pd.DataFrame({'col': [True]}).bool()\n |      True\n |      &gt;&gt;&gt; pd.DataFrame({'col': [False]}).bool()\n |      False\n |  \n |  convert_dtypes(self: 'NDFrameT', infer_objects: 'bool_t' = True, convert_string: 'bool_t' = True, convert_integer: 'bool_t' = True, convert_boolean: 'bool_t' = True, convert_floating: 'bool_t' = True, dtype_backend: 'DtypeBackend' = 'numpy_nullable') -&gt; 'NDFrameT'\n |      Convert columns to the best possible dtypes using dtypes supporting ``pd.NA``.\n |      \n |      Parameters\n |      ----------\n |      infer_objects : bool, default True\n |          Whether object dtypes should be converted to the best possible types.\n |      convert_string : bool, default True\n |          Whether object dtypes should be converted to ``StringDtype()``.\n |      convert_integer : bool, default True\n |          Whether, if possible, conversion can be done to integer extension types.\n |      convert_boolean : bool, defaults True\n |          Whether object dtypes should be converted to ``BooleanDtypes()``.\n |      convert_floating : bool, defaults True\n |          Whether, if possible, conversion can be done to floating extension types.\n |          If `convert_integer` is also True, preference will be give to integer\n |          dtypes if the floats can be faithfully casted to integers.\n |      \n |          .. versionadded:: 1.2.0\n |      dtype_backend : {\"numpy_nullable\", \"pyarrow\"}, default \"numpy_nullable\"\n |          Which dtype_backend to use, e.g. whether a DataFrame should use nullable\n |          dtypes for all dtypes that have a nullable\n |          implementation when \"numpy_nullable\" is set, pyarrow is used for all\n |          dtypes if \"pyarrow\" is set.\n |      \n |          The dtype_backends are still experimential.\n |      \n |          .. versionadded:: 2.0\n |      \n |      Returns\n |      -------\n |      Series or DataFrame\n |          Copy of input object with new dtype.\n |      \n |      See Also\n |      --------\n |      infer_objects : Infer dtypes of objects.\n |      to_datetime : Convert argument to datetime.\n |      to_timedelta : Convert argument to timedelta.\n |      to_numeric : Convert argument to a numeric type.\n |      \n |      Notes\n |      -----\n |      By default, ``convert_dtypes`` will attempt to convert a Series (or each\n |      Series in a DataFrame) to dtypes that support ``pd.NA``. By using the options\n |      ``convert_string``, ``convert_integer``, ``convert_boolean`` and\n |      ``convert_floating``, it is possible to turn off individual conversions\n |      to ``StringDtype``, the integer extension types, ``BooleanDtype``\n |      or floating extension types, respectively.\n |      \n |      For object-dtyped columns, if ``infer_objects`` is ``True``, use the inference\n |      rules as during normal Series/DataFrame construction.  Then, if possible,\n |      convert to ``StringDtype``, ``BooleanDtype`` or an appropriate integer\n |      or floating extension type, otherwise leave as ``object``.\n |      \n |      If the dtype is integer, convert to an appropriate integer extension type.\n |      \n |      If the dtype is numeric, and consists of all integers, convert to an\n |      appropriate integer extension type. Otherwise, convert to an\n |      appropriate floating extension type.\n |      \n |      .. versionchanged:: 1.2\n |          Starting with pandas 1.2, this method also converts float columns\n |          to the nullable floating extension type.\n |      \n |      In the future, as new dtypes are added that support ``pd.NA``, the results\n |      of this method will change to support those new dtypes.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; df = pd.DataFrame(\n |      ...     {\n |      ...         \"a\": pd.Series([1, 2, 3], dtype=np.dtype(\"int32\")),\n |      ...         \"b\": pd.Series([\"x\", \"y\", \"z\"], dtype=np.dtype(\"O\")),\n |      ...         \"c\": pd.Series([True, False, np.nan], dtype=np.dtype(\"O\")),\n |      ...         \"d\": pd.Series([\"h\", \"i\", np.nan], dtype=np.dtype(\"O\")),\n |      ...         \"e\": pd.Series([10, np.nan, 20], dtype=np.dtype(\"float\")),\n |      ...         \"f\": pd.Series([np.nan, 100.5, 200], dtype=np.dtype(\"float\")),\n |      ...     }\n |      ... )\n |      \n |      Start with a DataFrame with default dtypes.\n |      \n |      &gt;&gt;&gt; df\n |         a  b      c    d     e      f\n |      0  1  x   True    h  10.0    NaN\n |      1  2  y  False    i   NaN  100.5\n |      2  3  z    NaN  NaN  20.0  200.0\n |      \n |      &gt;&gt;&gt; df.dtypes\n |      a      int32\n |      b     object\n |      c     object\n |      d     object\n |      e    float64\n |      f    float64\n |      dtype: object\n |      \n |      Convert the DataFrame to use best possible dtypes.\n |      \n |      &gt;&gt;&gt; dfn = df.convert_dtypes()\n |      &gt;&gt;&gt; dfn\n |         a  b      c     d     e      f\n |      0  1  x   True     h    10   &lt;NA&gt;\n |      1  2  y  False     i  &lt;NA&gt;  100.5\n |      2  3  z   &lt;NA&gt;  &lt;NA&gt;    20  200.0\n |      \n |      &gt;&gt;&gt; dfn.dtypes\n |      a             Int32\n |      b    string[python]\n |      c           boolean\n |      d    string[python]\n |      e             Int64\n |      f           Float64\n |      dtype: object\n |      \n |      Start with a Series of strings and missing data represented by ``np.nan``.\n |      \n |      &gt;&gt;&gt; s = pd.Series([\"a\", \"b\", np.nan])\n |      &gt;&gt;&gt; s\n |      0      a\n |      1      b\n |      2    NaN\n |      dtype: object\n |      \n |      Obtain a Series with dtype ``StringDtype``.\n |      \n |      &gt;&gt;&gt; s.convert_dtypes()\n |      0       a\n |      1       b\n |      2    &lt;NA&gt;\n |      dtype: string\n |  \n |  copy(self: 'NDFrameT', deep: 'bool_t | None' = True) -&gt; 'NDFrameT'\n |      Make a copy of this object's indices and data.\n |      \n |      When ``deep=True`` (default), a new object will be created with a\n |      copy of the calling object's data and indices. Modifications to\n |      the data or indices of the copy will not be reflected in the\n |      original object (see notes below).\n |      \n |      When ``deep=False``, a new object will be created without copying\n |      the calling object's data or index (only references to the data\n |      and index are copied). Any changes to the data of the original\n |      will be reflected in the shallow copy (and vice versa).\n |      \n |      Parameters\n |      ----------\n |      deep : bool, default True\n |          Make a deep copy, including a copy of the data and the indices.\n |          With ``deep=False`` neither the indices nor the data are copied.\n |      \n |      Returns\n |      -------\n |      Series or DataFrame\n |          Object type matches caller.\n |      \n |      Notes\n |      -----\n |      When ``deep=True``, data is copied but actual Python objects\n |      will not be copied recursively, only the reference to the object.\n |      This is in contrast to `copy.deepcopy` in the Standard Library,\n |      which recursively copies object data (see examples below).\n |      \n |      While ``Index`` objects are copied when ``deep=True``, the underlying\n |      numpy array is not copied for performance reasons. Since ``Index`` is\n |      immutable, the underlying data can be safely shared and a copy\n |      is not needed.\n |      \n |      Since pandas is not thread safe, see the\n |      :ref:`gotchas &lt;gotchas.thread-safety&gt;` when copying in a threading\n |      environment.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; s = pd.Series([1, 2], index=[\"a\", \"b\"])\n |      &gt;&gt;&gt; s\n |      a    1\n |      b    2\n |      dtype: int64\n |      \n |      &gt;&gt;&gt; s_copy = s.copy()\n |      &gt;&gt;&gt; s_copy\n |      a    1\n |      b    2\n |      dtype: int64\n |      \n |      **Shallow copy versus default (deep) copy:**\n |      \n |      &gt;&gt;&gt; s = pd.Series([1, 2], index=[\"a\", \"b\"])\n |      &gt;&gt;&gt; deep = s.copy()\n |      &gt;&gt;&gt; shallow = s.copy(deep=False)\n |      \n |      Shallow copy shares data and index with original.\n |      \n |      &gt;&gt;&gt; s is shallow\n |      False\n |      &gt;&gt;&gt; s.values is shallow.values and s.index is shallow.index\n |      True\n |      \n |      Deep copy has own copy of data and index.\n |      \n |      &gt;&gt;&gt; s is deep\n |      False\n |      &gt;&gt;&gt; s.values is deep.values or s.index is deep.index\n |      False\n |      \n |      Updates to the data shared by shallow copy and original is reflected\n |      in both; deep copy remains unchanged.\n |      \n |      &gt;&gt;&gt; s[0] = 3\n |      &gt;&gt;&gt; shallow[1] = 4\n |      &gt;&gt;&gt; s\n |      a    3\n |      b    4\n |      dtype: int64\n |      &gt;&gt;&gt; shallow\n |      a    3\n |      b    4\n |      dtype: int64\n |      &gt;&gt;&gt; deep\n |      a    1\n |      b    2\n |      dtype: int64\n |      \n |      Note that when copying an object containing Python objects, a deep copy\n |      will copy the data, but will not do so recursively. Updating a nested\n |      data object will be reflected in the deep copy.\n |      \n |      &gt;&gt;&gt; s = pd.Series([[1, 2], [3, 4]])\n |      &gt;&gt;&gt; deep = s.copy()\n |      &gt;&gt;&gt; s[0][0] = 10\n |      &gt;&gt;&gt; s\n |      0    [10, 2]\n |      1     [3, 4]\n |      dtype: object\n |      &gt;&gt;&gt; deep\n |      0    [10, 2]\n |      1     [3, 4]\n |      dtype: object\n |  \n |  describe(self: 'NDFrameT', percentiles=None, include=None, exclude=None) -&gt; 'NDFrameT'\n |      Generate descriptive statistics.\n |      \n |      Descriptive statistics include those that summarize the central\n |      tendency, dispersion and shape of a\n |      dataset's distribution, excluding ``NaN`` values.\n |      \n |      Analyzes both numeric and object series, as well\n |      as ``DataFrame`` column sets of mixed data types. The output\n |      will vary depending on what is provided. Refer to the notes\n |      below for more detail.\n |      \n |      Parameters\n |      ----------\n |      percentiles : list-like of numbers, optional\n |          The percentiles to include in the output. All should\n |          fall between 0 and 1. The default is\n |          ``[.25, .5, .75]``, which returns the 25th, 50th, and\n |          75th percentiles.\n |      include : 'all', list-like of dtypes or None (default), optional\n |          A white list of data types to include in the result. Ignored\n |          for ``Series``. Here are the options:\n |      \n |          - 'all' : All columns of the input will be included in the output.\n |          - A list-like of dtypes : Limits the results to the\n |            provided data types.\n |            To limit the result to numeric types submit\n |            ``numpy.number``. To limit it instead to object columns submit\n |            the ``numpy.object`` data type. Strings\n |            can also be used in the style of\n |            ``select_dtypes`` (e.g. ``df.describe(include=['O'])``). To\n |            select pandas categorical columns, use ``'category'``\n |          - None (default) : The result will include all numeric columns.\n |      exclude : list-like of dtypes or None (default), optional,\n |          A black list of data types to omit from the result. Ignored\n |          for ``Series``. Here are the options:\n |      \n |          - A list-like of dtypes : Excludes the provided data types\n |            from the result. To exclude numeric types submit\n |            ``numpy.number``. To exclude object columns submit the data\n |            type ``numpy.object``. Strings can also be used in the style of\n |            ``select_dtypes`` (e.g. ``df.describe(exclude=['O'])``). To\n |            exclude pandas categorical columns, use ``'category'``\n |          - None (default) : The result will exclude nothing.\n |      \n |      Returns\n |      -------\n |      Series or DataFrame\n |          Summary statistics of the Series or Dataframe provided.\n |      \n |      See Also\n |      --------\n |      DataFrame.count: Count number of non-NA/null observations.\n |      DataFrame.max: Maximum of the values in the object.\n |      DataFrame.min: Minimum of the values in the object.\n |      DataFrame.mean: Mean of the values.\n |      DataFrame.std: Standard deviation of the observations.\n |      DataFrame.select_dtypes: Subset of a DataFrame including/excluding\n |          columns based on their dtype.\n |      \n |      Notes\n |      -----\n |      For numeric data, the result's index will include ``count``,\n |      ``mean``, ``std``, ``min``, ``max`` as well as lower, ``50`` and\n |      upper percentiles. By default the lower percentile is ``25`` and the\n |      upper percentile is ``75``. The ``50`` percentile is the\n |      same as the median.\n |      \n |      For object data (e.g. strings or timestamps), the result's index\n |      will include ``count``, ``unique``, ``top``, and ``freq``. The ``top``\n |      is the most common value. The ``freq`` is the most common value's\n |      frequency. Timestamps also include the ``first`` and ``last`` items.\n |      \n |      If multiple object values have the highest count, then the\n |      ``count`` and ``top`` results will be arbitrarily chosen from\n |      among those with the highest count.\n |      \n |      For mixed data types provided via a ``DataFrame``, the default is to\n |      return only an analysis of numeric columns. If the dataframe consists\n |      only of object and categorical data without any numeric columns, the\n |      default is to return an analysis of both the object and categorical\n |      columns. If ``include='all'`` is provided as an option, the result\n |      will include a union of attributes of each type.\n |      \n |      The `include` and `exclude` parameters can be used to limit\n |      which columns in a ``DataFrame`` are analyzed for the output.\n |      The parameters are ignored when analyzing a ``Series``.\n |      \n |      Examples\n |      --------\n |      Describing a numeric ``Series``.\n |      \n |      &gt;&gt;&gt; s = pd.Series([1, 2, 3])\n |      &gt;&gt;&gt; s.describe()\n |      count    3.0\n |      mean     2.0\n |      std      1.0\n |      min      1.0\n |      25%      1.5\n |      50%      2.0\n |      75%      2.5\n |      max      3.0\n |      dtype: float64\n |      \n |      Describing a categorical ``Series``.\n |      \n |      &gt;&gt;&gt; s = pd.Series(['a', 'a', 'b', 'c'])\n |      &gt;&gt;&gt; s.describe()\n |      count     4\n |      unique    3\n |      top       a\n |      freq      2\n |      dtype: object\n |      \n |      Describing a timestamp ``Series``.\n |      \n |      &gt;&gt;&gt; s = pd.Series([\n |      ...     np.datetime64(\"2000-01-01\"),\n |      ...     np.datetime64(\"2010-01-01\"),\n |      ...     np.datetime64(\"2010-01-01\")\n |      ... ])\n |      &gt;&gt;&gt; s.describe()\n |      count                      3\n |      mean     2006-09-01 08:00:00\n |      min      2000-01-01 00:00:00\n |      25%      2004-12-31 12:00:00\n |      50%      2010-01-01 00:00:00\n |      75%      2010-01-01 00:00:00\n |      max      2010-01-01 00:00:00\n |      dtype: object\n |      \n |      Describing a ``DataFrame``. By default only numeric fields\n |      are returned.\n |      \n |      &gt;&gt;&gt; df = pd.DataFrame({'categorical': pd.Categorical(['d','e','f']),\n |      ...                    'numeric': [1, 2, 3],\n |      ...                    'object': ['a', 'b', 'c']\n |      ...                   })\n |      &gt;&gt;&gt; df.describe()\n |             numeric\n |      count      3.0\n |      mean       2.0\n |      std        1.0\n |      min        1.0\n |      25%        1.5\n |      50%        2.0\n |      75%        2.5\n |      max        3.0\n |      \n |      Describing all columns of a ``DataFrame`` regardless of data type.\n |      \n |      &gt;&gt;&gt; df.describe(include='all')  # doctest: +SKIP\n |             categorical  numeric object\n |      count            3      3.0      3\n |      unique           3      NaN      3\n |      top              f      NaN      a\n |      freq             1      NaN      1\n |      mean           NaN      2.0    NaN\n |      std            NaN      1.0    NaN\n |      min            NaN      1.0    NaN\n |      25%            NaN      1.5    NaN\n |      50%            NaN      2.0    NaN\n |      75%            NaN      2.5    NaN\n |      max            NaN      3.0    NaN\n |      \n |      Describing a column from a ``DataFrame`` by accessing it as\n |      an attribute.\n |      \n |      &gt;&gt;&gt; df.numeric.describe()\n |      count    3.0\n |      mean     2.0\n |      std      1.0\n |      min      1.0\n |      25%      1.5\n |      50%      2.0\n |      75%      2.5\n |      max      3.0\n |      Name: numeric, dtype: float64\n |      \n |      Including only numeric columns in a ``DataFrame`` description.\n |      \n |      &gt;&gt;&gt; df.describe(include=[np.number])\n |             numeric\n |      count      3.0\n |      mean       2.0\n |      std        1.0\n |      min        1.0\n |      25%        1.5\n |      50%        2.0\n |      75%        2.5\n |      max        3.0\n |      \n |      Including only string columns in a ``DataFrame`` description.\n |      \n |      &gt;&gt;&gt; df.describe(include=[object])  # doctest: +SKIP\n |             object\n |      count       3\n |      unique      3\n |      top         a\n |      freq        1\n |      \n |      Including only categorical columns from a ``DataFrame`` description.\n |      \n |      &gt;&gt;&gt; df.describe(include=['category'])\n |             categorical\n |      count            3\n |      unique           3\n |      top              d\n |      freq             1\n |      \n |      Excluding numeric columns from a ``DataFrame`` description.\n |      \n |      &gt;&gt;&gt; df.describe(exclude=[np.number])  # doctest: +SKIP\n |             categorical object\n |      count            3      3\n |      unique           3      3\n |      top              f      a\n |      freq             1      1\n |      \n |      Excluding object columns from a ``DataFrame`` description.\n |      \n |      &gt;&gt;&gt; df.describe(exclude=[object])  # doctest: +SKIP\n |             categorical  numeric\n |      count            3      3.0\n |      unique           3      NaN\n |      top              f      NaN\n |      freq             1      NaN\n |      mean           NaN      2.0\n |      std            NaN      1.0\n |      min            NaN      1.0\n |      25%            NaN      1.5\n |      50%            NaN      2.0\n |      75%            NaN      2.5\n |      max            NaN      3.0\n |  \n |  droplevel(self: 'NDFrameT', level: 'IndexLabel', axis: 'Axis' = 0) -&gt; 'NDFrameT'\n |      Return Series/DataFrame with requested index / column level(s) removed.\n |      \n |      Parameters\n |      ----------\n |      level : int, str, or list-like\n |          If a string is given, must be the name of a level\n |          If list-like, elements must be names or positional indexes\n |          of levels.\n |      \n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |          Axis along which the level(s) is removed:\n |      \n |          * 0 or 'index': remove level(s) in column.\n |          * 1 or 'columns': remove level(s) in row.\n |      \n |          For `Series` this parameter is unused and defaults to 0.\n |      \n |      Returns\n |      -------\n |      Series/DataFrame\n |          Series/DataFrame with requested index / column level(s) removed.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; df = pd.DataFrame([\n |      ...     [1, 2, 3, 4],\n |      ...     [5, 6, 7, 8],\n |      ...     [9, 10, 11, 12]\n |      ... ]).set_index([0, 1]).rename_axis(['a', 'b'])\n |      \n |      &gt;&gt;&gt; df.columns = pd.MultiIndex.from_tuples([\n |      ...     ('c', 'e'), ('d', 'f')\n |      ... ], names=['level_1', 'level_2'])\n |      \n |      &gt;&gt;&gt; df\n |      level_1   c   d\n |      level_2   e   f\n |      a b\n |      1 2      3   4\n |      5 6      7   8\n |      9 10    11  12\n |      \n |      &gt;&gt;&gt; df.droplevel('a')\n |      level_1   c   d\n |      level_2   e   f\n |      b\n |      2        3   4\n |      6        7   8\n |      10      11  12\n |      \n |      &gt;&gt;&gt; df.droplevel('level_2', axis=1)\n |      level_1   c   d\n |      a b\n |      1 2      3   4\n |      5 6      7   8\n |      9 10    11  12\n |  \n |  equals(self, other: 'object') -&gt; 'bool_t'\n |      Test whether two objects contain the same elements.\n |      \n |      This function allows two Series or DataFrames to be compared against\n |      each other to see if they have the same shape and elements. NaNs in\n |      the same location are considered equal.\n |      \n |      The row/column index do not need to have the same type, as long\n |      as the values are considered equal. Corresponding columns must be of\n |      the same dtype.\n |      \n |      Parameters\n |      ----------\n |      other : Series or DataFrame\n |          The other Series or DataFrame to be compared with the first.\n |      \n |      Returns\n |      -------\n |      bool\n |          True if all elements are the same in both objects, False\n |          otherwise.\n |      \n |      See Also\n |      --------\n |      Series.eq : Compare two Series objects of the same length\n |          and return a Series where each element is True if the element\n |          in each Series is equal, False otherwise.\n |      DataFrame.eq : Compare two DataFrame objects of the same shape and\n |          return a DataFrame where each element is True if the respective\n |          element in each DataFrame is equal, False otherwise.\n |      testing.assert_series_equal : Raises an AssertionError if left and\n |          right are not equal. Provides an easy interface to ignore\n |          inequality in dtypes, indexes and precision among others.\n |      testing.assert_frame_equal : Like assert_series_equal, but targets\n |          DataFrames.\n |      numpy.array_equal : Return True if two arrays have the same shape\n |          and elements, False otherwise.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; df = pd.DataFrame({1: [10], 2: [20]})\n |      &gt;&gt;&gt; df\n |          1   2\n |      0  10  20\n |      \n |      DataFrames df and exactly_equal have the same types and values for\n |      their elements and column labels, which will return True.\n |      \n |      &gt;&gt;&gt; exactly_equal = pd.DataFrame({1: [10], 2: [20]})\n |      &gt;&gt;&gt; exactly_equal\n |          1   2\n |      0  10  20\n |      &gt;&gt;&gt; df.equals(exactly_equal)\n |      True\n |      \n |      DataFrames df and different_column_type have the same element\n |      types and values, but have different types for the column labels,\n |      which will still return True.\n |      \n |      &gt;&gt;&gt; different_column_type = pd.DataFrame({1.0: [10], 2.0: [20]})\n |      &gt;&gt;&gt; different_column_type\n |         1.0  2.0\n |      0   10   20\n |      &gt;&gt;&gt; df.equals(different_column_type)\n |      True\n |      \n |      DataFrames df and different_data_type have different types for the\n |      same values for their elements, and will return False even though\n |      their column labels are the same values and types.\n |      \n |      &gt;&gt;&gt; different_data_type = pd.DataFrame({1: [10.0], 2: [20.0]})\n |      &gt;&gt;&gt; different_data_type\n |            1     2\n |      0  10.0  20.0\n |      &gt;&gt;&gt; df.equals(different_data_type)\n |      False\n |  \n |  ewm(self, com: 'float | None' = None, span: 'float | None' = None, halflife: 'float | TimedeltaConvertibleTypes | None' = None, alpha: 'float | None' = None, min_periods: 'int | None' = 0, adjust: 'bool_t' = True, ignore_na: 'bool_t' = False, axis: 'Axis' = 0, times: 'np.ndarray | DataFrame | Series | None' = None, method: 'str' = 'single') -&gt; 'ExponentialMovingWindow'\n |      Provide exponentially weighted (EW) calculations.\n |      \n |      Exactly one of ``com``, ``span``, ``halflife``, or ``alpha`` must be\n |      provided if ``times`` is not provided. If ``times`` is provided,\n |      ``halflife`` and one of ``com``, ``span`` or ``alpha`` may be provided.\n |      \n |      Parameters\n |      ----------\n |      com : float, optional\n |          Specify decay in terms of center of mass\n |      \n |          :math:`\\alpha = 1 / (1 + com)`, for :math:`com \\geq 0`.\n |      \n |      span : float, optional\n |          Specify decay in terms of span\n |      \n |          :math:`\\alpha = 2 / (span + 1)`, for :math:`span \\geq 1`.\n |      \n |      halflife : float, str, timedelta, optional\n |          Specify decay in terms of half-life\n |      \n |          :math:`\\alpha = 1 - \\exp\\left(-\\ln(2) / halflife\\right)`, for\n |          :math:`halflife &gt; 0`.\n |      \n |          If ``times`` is specified, a timedelta convertible unit over which an\n |          observation decays to half its value. Only applicable to ``mean()``,\n |          and halflife value will not apply to the other functions.\n |      \n |          .. versionadded:: 1.1.0\n |      \n |      alpha : float, optional\n |          Specify smoothing factor :math:`\\alpha` directly\n |      \n |          :math:`0 &lt; \\alpha \\leq 1`.\n |      \n |      min_periods : int, default 0\n |          Minimum number of observations in window required to have a value;\n |          otherwise, result is ``np.nan``.\n |      \n |      adjust : bool, default True\n |          Divide by decaying adjustment factor in beginning periods to account\n |          for imbalance in relative weightings (viewing EWMA as a moving average).\n |      \n |          - When ``adjust=True`` (default), the EW function is calculated using weights\n |            :math:`w_i = (1 - \\alpha)^i`. For example, the EW moving average of the series\n |            [:math:`x_0, x_1, ..., x_t`] would be:\n |      \n |          .. math::\n |              y_t = \\frac{x_t + (1 - \\alpha)x_{t-1} + (1 - \\alpha)^2 x_{t-2} + ... + (1 -\n |              \\alpha)^t x_0}{1 + (1 - \\alpha) + (1 - \\alpha)^2 + ... + (1 - \\alpha)^t}\n |      \n |          - When ``adjust=False``, the exponentially weighted function is calculated\n |            recursively:\n |      \n |          .. math::\n |              \\begin{split}\n |                  y_0 &= x_0\\\\\n |                  y_t &= (1 - \\alpha) y_{t-1} + \\alpha x_t,\n |              \\end{split}\n |      ignore_na : bool, default False\n |          Ignore missing values when calculating weights.\n |      \n |          - When ``ignore_na=False`` (default), weights are based on absolute positions.\n |            For example, the weights of :math:`x_0` and :math:`x_2` used in calculating\n |            the final weighted average of [:math:`x_0`, None, :math:`x_2`] are\n |            :math:`(1-\\alpha)^2` and :math:`1` if ``adjust=True``, and\n |            :math:`(1-\\alpha)^2` and :math:`\\alpha` if ``adjust=False``.\n |      \n |          - When ``ignore_na=True``, weights are based\n |            on relative positions. For example, the weights of :math:`x_0` and :math:`x_2`\n |            used in calculating the final weighted average of\n |            [:math:`x_0`, None, :math:`x_2`] are :math:`1-\\alpha` and :math:`1` if\n |            ``adjust=True``, and :math:`1-\\alpha` and :math:`\\alpha` if ``adjust=False``.\n |      \n |      axis : {0, 1}, default 0\n |          If ``0`` or ``'index'``, calculate across the rows.\n |      \n |          If ``1`` or ``'columns'``, calculate across the columns.\n |      \n |          For `Series` this parameter is unused and defaults to 0.\n |      \n |      times : np.ndarray, Series, default None\n |      \n |          .. versionadded:: 1.1.0\n |      \n |          Only applicable to ``mean()``.\n |      \n |          Times corresponding to the observations. Must be monotonically increasing and\n |          ``datetime64[ns]`` dtype.\n |      \n |          If 1-D array like, a sequence with the same shape as the observations.\n |      \n |      method : str {'single', 'table'}, default 'single'\n |          .. versionadded:: 1.4.0\n |      \n |          Execute the rolling operation per single column or row (``'single'``)\n |          or over the entire object (``'table'``).\n |      \n |          This argument is only implemented when specifying ``engine='numba'``\n |          in the method call.\n |      \n |          Only applicable to ``mean()``\n |      \n |      Returns\n |      -------\n |      ``ExponentialMovingWindow`` subclass\n |      \n |      See Also\n |      --------\n |      rolling : Provides rolling window calculations.\n |      expanding : Provides expanding transformations.\n |      \n |      Notes\n |      -----\n |      See :ref:`Windowing Operations &lt;window.exponentially_weighted&gt;`\n |      for further usage details and examples.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; df = pd.DataFrame({'B': [0, 1, 2, np.nan, 4]})\n |      &gt;&gt;&gt; df\n |           B\n |      0  0.0\n |      1  1.0\n |      2  2.0\n |      3  NaN\n |      4  4.0\n |      \n |      &gt;&gt;&gt; df.ewm(com=0.5).mean()\n |                B\n |      0  0.000000\n |      1  0.750000\n |      2  1.615385\n |      3  1.615385\n |      4  3.670213\n |      &gt;&gt;&gt; df.ewm(alpha=2 / 3).mean()\n |                B\n |      0  0.000000\n |      1  0.750000\n |      2  1.615385\n |      3  1.615385\n |      4  3.670213\n |      \n |      **adjust**\n |      \n |      &gt;&gt;&gt; df.ewm(com=0.5, adjust=True).mean()\n |                B\n |      0  0.000000\n |      1  0.750000\n |      2  1.615385\n |      3  1.615385\n |      4  3.670213\n |      &gt;&gt;&gt; df.ewm(com=0.5, adjust=False).mean()\n |                B\n |      0  0.000000\n |      1  0.666667\n |      2  1.555556\n |      3  1.555556\n |      4  3.650794\n |      \n |      **ignore_na**\n |      \n |      &gt;&gt;&gt; df.ewm(com=0.5, ignore_na=True).mean()\n |                B\n |      0  0.000000\n |      1  0.750000\n |      2  1.615385\n |      3  1.615385\n |      4  3.225000\n |      &gt;&gt;&gt; df.ewm(com=0.5, ignore_na=False).mean()\n |                B\n |      0  0.000000\n |      1  0.750000\n |      2  1.615385\n |      3  1.615385\n |      4  3.670213\n |      \n |      **times**\n |      \n |      Exponentially weighted mean with weights calculated with a timedelta ``halflife``\n |      relative to ``times``.\n |      \n |      &gt;&gt;&gt; times = ['2020-01-01', '2020-01-03', '2020-01-10', '2020-01-15', '2020-01-17']\n |      &gt;&gt;&gt; df.ewm(halflife='4 days', times=pd.DatetimeIndex(times)).mean()\n |                B\n |      0  0.000000\n |      1  0.585786\n |      2  1.523889\n |      3  1.523889\n |      4  3.233686\n |  \n |  expanding(self, min_periods: 'int' = 1, axis: 'Axis' = 0, method: 'str' = 'single') -&gt; 'Expanding'\n |      Provide expanding window calculations.\n |      \n |      Parameters\n |      ----------\n |      min_periods : int, default 1\n |          Minimum number of observations in window required to have a value;\n |          otherwise, result is ``np.nan``.\n |      \n |      axis : int or str, default 0\n |          If ``0`` or ``'index'``, roll across the rows.\n |      \n |          If ``1`` or ``'columns'``, roll across the columns.\n |      \n |          For `Series` this parameter is unused and defaults to 0.\n |      \n |      method : str {'single', 'table'}, default 'single'\n |          Execute the rolling operation per single column or row (``'single'``)\n |          or over the entire object (``'table'``).\n |      \n |          This argument is only implemented when specifying ``engine='numba'``\n |          in the method call.\n |      \n |          .. versionadded:: 1.3.0\n |      \n |      Returns\n |      -------\n |      ``Expanding`` subclass\n |      \n |      See Also\n |      --------\n |      rolling : Provides rolling window calculations.\n |      ewm : Provides exponential weighted functions.\n |      \n |      Notes\n |      -----\n |      See :ref:`Windowing Operations &lt;window.expanding&gt;` for further usage details\n |      and examples.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; df = pd.DataFrame({\"B\": [0, 1, 2, np.nan, 4]})\n |      &gt;&gt;&gt; df\n |           B\n |      0  0.0\n |      1  1.0\n |      2  2.0\n |      3  NaN\n |      4  4.0\n |      \n |      **min_periods**\n |      \n |      Expanding sum with 1 vs 3 observations needed to calculate a value.\n |      \n |      &gt;&gt;&gt; df.expanding(1).sum()\n |           B\n |      0  0.0\n |      1  1.0\n |      2  3.0\n |      3  3.0\n |      4  7.0\n |      &gt;&gt;&gt; df.expanding(3).sum()\n |           B\n |      0  NaN\n |      1  NaN\n |      2  3.0\n |      3  3.0\n |      4  7.0\n |  \n |  filter(self: 'NDFrameT', items=None, like: 'str | None' = None, regex: 'str | None' = None, axis: 'Axis | None' = None) -&gt; 'NDFrameT'\n |      Subset the dataframe rows or columns according to the specified index labels.\n |      \n |      Note that this routine does not filter a dataframe on its\n |      contents. The filter is applied to the labels of the index.\n |      \n |      Parameters\n |      ----------\n |      items : list-like\n |          Keep labels from axis which are in items.\n |      like : str\n |          Keep labels from axis for which \"like in label == True\".\n |      regex : str (regular expression)\n |          Keep labels from axis for which re.search(regex, label) == True.\n |      axis : {0 or ‘index’, 1 or ‘columns’, None}, default None\n |          The axis to filter on, expressed either as an index (int)\n |          or axis name (str). By default this is the info axis, 'columns' for\n |          DataFrame. For `Series` this parameter is unused and defaults to `None`.\n |      \n |      Returns\n |      -------\n |      same type as input object\n |      \n |      See Also\n |      --------\n |      DataFrame.loc : Access a group of rows and columns\n |          by label(s) or a boolean array.\n |      \n |      Notes\n |      -----\n |      The ``items``, ``like``, and ``regex`` parameters are\n |      enforced to be mutually exclusive.\n |      \n |      ``axis`` defaults to the info axis that is used when indexing\n |      with ``[]``.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; df = pd.DataFrame(np.array(([1, 2, 3], [4, 5, 6])),\n |      ...                   index=['mouse', 'rabbit'],\n |      ...                   columns=['one', 'two', 'three'])\n |      &gt;&gt;&gt; df\n |              one  two  three\n |      mouse     1    2      3\n |      rabbit    4    5      6\n |      \n |      &gt;&gt;&gt; # select columns by name\n |      &gt;&gt;&gt; df.filter(items=['one', 'three'])\n |               one  three\n |      mouse     1      3\n |      rabbit    4      6\n |      \n |      &gt;&gt;&gt; # select columns by regular expression\n |      &gt;&gt;&gt; df.filter(regex='e$', axis=1)\n |               one  three\n |      mouse     1      3\n |      rabbit    4      6\n |      \n |      &gt;&gt;&gt; # select rows containing 'bbi'\n |      &gt;&gt;&gt; df.filter(like='bbi', axis=0)\n |               one  two  three\n |      rabbit    4    5      6\n |  \n |  first(self: 'NDFrameT', offset) -&gt; 'NDFrameT'\n |      Select initial periods of time series data based on a date offset.\n |      \n |      For a DataFrame with a sorted DatetimeIndex, this function can\n |      select the first few rows based on a date offset.\n |      \n |      Parameters\n |      ----------\n |      offset : str, DateOffset or dateutil.relativedelta\n |          The offset length of the data that will be selected. For instance,\n |          '1M' will display all the rows having their index within the first month.\n |      \n |      Returns\n |      -------\n |      Series or DataFrame\n |          A subset of the caller.\n |      \n |      Raises\n |      ------\n |      TypeError\n |          If the index is not  a :class:`DatetimeIndex`\n |      \n |      See Also\n |      --------\n |      last : Select final periods of time series based on a date offset.\n |      at_time : Select values at a particular time of the day.\n |      between_time : Select values between particular times of the day.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; i = pd.date_range('2018-04-09', periods=4, freq='2D')\n |      &gt;&gt;&gt; ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i)\n |      &gt;&gt;&gt; ts\n |                  A\n |      2018-04-09  1\n |      2018-04-11  2\n |      2018-04-13  3\n |      2018-04-15  4\n |      \n |      Get the rows for the first 3 days:\n |      \n |      &gt;&gt;&gt; ts.first('3D')\n |                  A\n |      2018-04-09  1\n |      2018-04-11  2\n |      \n |      Notice the data for 3 first calendar days were returned, not the first\n |      3 days observed in the dataset, and therefore data for 2018-04-13 was\n |      not returned.\n |  \n |  first_valid_index(self) -&gt; 'Hashable | None'\n |      Return index for first non-NA value or None, if no non-NA value is found.\n |      \n |      Returns\n |      -------\n |      type of index\n |      \n |      Notes\n |      -----\n |      If all elements are non-NA/null, returns None.\n |      Also returns None for empty Series/DataFrame.\n |  \n |  get(self, key, default=None)\n |      Get item from object for given key (ex: DataFrame column).\n |      \n |      Returns default value if not found.\n |      \n |      Parameters\n |      ----------\n |      key : object\n |      \n |      Returns\n |      -------\n |      same type as items contained in object\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; df = pd.DataFrame(\n |      ...     [\n |      ...         [24.3, 75.7, \"high\"],\n |      ...         [31, 87.8, \"high\"],\n |      ...         [22, 71.6, \"medium\"],\n |      ...         [35, 95, \"medium\"],\n |      ...     ],\n |      ...     columns=[\"temp_celsius\", \"temp_fahrenheit\", \"windspeed\"],\n |      ...     index=pd.date_range(start=\"2014-02-12\", end=\"2014-02-15\", freq=\"D\"),\n |      ... )\n |      \n |      &gt;&gt;&gt; df\n |                  temp_celsius  temp_fahrenheit windspeed\n |      2014-02-12          24.3             75.7      high\n |      2014-02-13          31.0             87.8      high\n |      2014-02-14          22.0             71.6    medium\n |      2014-02-15          35.0             95.0    medium\n |      \n |      &gt;&gt;&gt; df.get([\"temp_celsius\", \"windspeed\"])\n |                  temp_celsius windspeed\n |      2014-02-12          24.3      high\n |      2014-02-13          31.0      high\n |      2014-02-14          22.0    medium\n |      2014-02-15          35.0    medium\n |      \n |      &gt;&gt;&gt; ser = df['windspeed']\n |      &gt;&gt;&gt; ser.get('2014-02-13')\n |      'high'\n |      \n |      If the key isn't found, the default value will be used.\n |      \n |      &gt;&gt;&gt; df.get([\"temp_celsius\", \"temp_kelvin\"], default=\"default_value\")\n |      'default_value'\n |      \n |      &gt;&gt;&gt; ser.get('2014-02-10', '[unknown]')\n |      '[unknown]'\n |  \n |  head(self: 'NDFrameT', n: 'int' = 5) -&gt; 'NDFrameT'\n |      Return the first `n` rows.\n |      \n |      This function returns the first `n` rows for the object based\n |      on position. It is useful for quickly testing if your object\n |      has the right type of data in it.\n |      \n |      For negative values of `n`, this function returns all rows except\n |      the last `|n|` rows, equivalent to ``df[:n]``.\n |      \n |      If n is larger than the number of rows, this function returns all rows.\n |      \n |      Parameters\n |      ----------\n |      n : int, default 5\n |          Number of rows to select.\n |      \n |      Returns\n |      -------\n |      same type as caller\n |          The first `n` rows of the caller object.\n |      \n |      See Also\n |      --------\n |      DataFrame.tail: Returns the last `n` rows.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; df = pd.DataFrame({'animal': ['alligator', 'bee', 'falcon', 'lion',\n |      ...                    'monkey', 'parrot', 'shark', 'whale', 'zebra']})\n |      &gt;&gt;&gt; df\n |            animal\n |      0  alligator\n |      1        bee\n |      2     falcon\n |      3       lion\n |      4     monkey\n |      5     parrot\n |      6      shark\n |      7      whale\n |      8      zebra\n |      \n |      Viewing the first 5 lines\n |      \n |      &gt;&gt;&gt; df.head()\n |            animal\n |      0  alligator\n |      1        bee\n |      2     falcon\n |      3       lion\n |      4     monkey\n |      \n |      Viewing the first `n` lines (three in this case)\n |      \n |      &gt;&gt;&gt; df.head(3)\n |            animal\n |      0  alligator\n |      1        bee\n |      2     falcon\n |      \n |      For negative values of `n`\n |      \n |      &gt;&gt;&gt; df.head(-3)\n |            animal\n |      0  alligator\n |      1        bee\n |      2     falcon\n |      3       lion\n |      4     monkey\n |      5     parrot\n |  \n |  infer_objects(self: 'NDFrameT', copy: 'bool_t | None' = None) -&gt; 'NDFrameT'\n |      Attempt to infer better dtypes for object columns.\n |      \n |      Attempts soft conversion of object-dtyped\n |      columns, leaving non-object and unconvertible\n |      columns unchanged. The inference rules are the\n |      same as during normal Series/DataFrame construction.\n |      \n |      Parameters\n |      ----------\n |      copy : bool, default True\n |          Whether to make a copy for non-object or non-inferrable columns\n |          or Series.\n |      \n |      Returns\n |      -------\n |      same type as input object\n |      \n |      See Also\n |      --------\n |      to_datetime : Convert argument to datetime.\n |      to_timedelta : Convert argument to timedelta.\n |      to_numeric : Convert argument to numeric type.\n |      convert_dtypes : Convert argument to best possible dtype.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; df = pd.DataFrame({\"A\": [\"a\", 1, 2, 3]})\n |      &gt;&gt;&gt; df = df.iloc[1:]\n |      &gt;&gt;&gt; df\n |         A\n |      1  1\n |      2  2\n |      3  3\n |      \n |      &gt;&gt;&gt; df.dtypes\n |      A    object\n |      dtype: object\n |      \n |      &gt;&gt;&gt; df.infer_objects().dtypes\n |      A    int64\n |      dtype: object\n |  \n |  last(self: 'NDFrameT', offset) -&gt; 'NDFrameT'\n |      Select final periods of time series data based on a date offset.\n |      \n |      For a DataFrame with a sorted DatetimeIndex, this function\n |      selects the last few rows based on a date offset.\n |      \n |      Parameters\n |      ----------\n |      offset : str, DateOffset, dateutil.relativedelta\n |          The offset length of the data that will be selected. For instance,\n |          '3D' will display all the rows having their index within the last 3 days.\n |      \n |      Returns\n |      -------\n |      Series or DataFrame\n |          A subset of the caller.\n |      \n |      Raises\n |      ------\n |      TypeError\n |          If the index is not  a :class:`DatetimeIndex`\n |      \n |      See Also\n |      --------\n |      first : Select initial periods of time series based on a date offset.\n |      at_time : Select values at a particular time of the day.\n |      between_time : Select values between particular times of the day.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; i = pd.date_range('2018-04-09', periods=4, freq='2D')\n |      &gt;&gt;&gt; ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i)\n |      &gt;&gt;&gt; ts\n |                  A\n |      2018-04-09  1\n |      2018-04-11  2\n |      2018-04-13  3\n |      2018-04-15  4\n |      \n |      Get the rows for the last 3 days:\n |      \n |      &gt;&gt;&gt; ts.last('3D')\n |                  A\n |      2018-04-13  3\n |      2018-04-15  4\n |      \n |      Notice the data for 3 last calendar days were returned, not the last\n |      3 observed days in the dataset, and therefore data for 2018-04-11 was\n |      not returned.\n |  \n |  last_valid_index(self) -&gt; 'Hashable | None'\n |      Return index for last non-NA value or None, if no non-NA value is found.\n |      \n |      Returns\n |      -------\n |      type of index\n |      \n |      Notes\n |      -----\n |      If all elements are non-NA/null, returns None.\n |      Also returns None for empty Series/DataFrame.\n |  \n |  pad(self: 'NDFrameT', *, axis: 'None | Axis' = None, inplace: 'bool_t' = False, limit: 'None | int' = None, downcast: 'dict | None' = None) -&gt; 'NDFrameT | None'\n |      Synonym for :meth:`DataFrame.fillna` with ``method='ffill'``.\n |      \n |      .. deprecated:: 2.0\n |      \n |          Series/DataFrame.pad is deprecated. Use Series/DataFrame.ffill instead.\n |      \n |      Returns\n |      -------\n |      Series/DataFrame or None\n |          Object with missing values filled or None if ``inplace=True``.\n |  \n |  pct_change(self: 'NDFrameT', periods: 'int' = 1, fill_method: \"Literal['backfill', 'bfill', 'pad', 'ffill'] | None\" = 'pad', limit=None, freq=None, **kwargs) -&gt; 'NDFrameT'\n |      Percentage change between the current and a prior element.\n |      \n |      Computes the percentage change from the immediately previous row by\n |      default. This is useful in comparing the percentage of change in a time\n |      series of elements.\n |      \n |      Parameters\n |      ----------\n |      periods : int, default 1\n |          Periods to shift for forming percent change.\n |      fill_method : {'backfill', 'bfill', 'pad', 'ffill', None}, default 'pad'\n |          How to handle NAs **before** computing percent changes.\n |      limit : int, default None\n |          The number of consecutive NAs to fill before stopping.\n |      freq : DateOffset, timedelta, or str, optional\n |          Increment to use from time series API (e.g. 'M' or BDay()).\n |      **kwargs\n |          Additional keyword arguments are passed into\n |          `DataFrame.shift` or `Series.shift`.\n |      \n |      Returns\n |      -------\n |      Series or DataFrame\n |          The same type as the calling object.\n |      \n |      See Also\n |      --------\n |      Series.diff : Compute the difference of two elements in a Series.\n |      DataFrame.diff : Compute the difference of two elements in a DataFrame.\n |      Series.shift : Shift the index by some number of periods.\n |      DataFrame.shift : Shift the index by some number of periods.\n |      \n |      Examples\n |      --------\n |      **Series**\n |      \n |      &gt;&gt;&gt; s = pd.Series([90, 91, 85])\n |      &gt;&gt;&gt; s\n |      0    90\n |      1    91\n |      2    85\n |      dtype: int64\n |      \n |      &gt;&gt;&gt; s.pct_change()\n |      0         NaN\n |      1    0.011111\n |      2   -0.065934\n |      dtype: float64\n |      \n |      &gt;&gt;&gt; s.pct_change(periods=2)\n |      0         NaN\n |      1         NaN\n |      2   -0.055556\n |      dtype: float64\n |      \n |      See the percentage change in a Series where filling NAs with last\n |      valid observation forward to next valid.\n |      \n |      &gt;&gt;&gt; s = pd.Series([90, 91, None, 85])\n |      &gt;&gt;&gt; s\n |      0    90.0\n |      1    91.0\n |      2     NaN\n |      3    85.0\n |      dtype: float64\n |      \n |      &gt;&gt;&gt; s.pct_change(fill_method='ffill')\n |      0         NaN\n |      1    0.011111\n |      2    0.000000\n |      3   -0.065934\n |      dtype: float64\n |      \n |      **DataFrame**\n |      \n |      Percentage change in French franc, Deutsche Mark, and Italian lira from\n |      1980-01-01 to 1980-03-01.\n |      \n |      &gt;&gt;&gt; df = pd.DataFrame({\n |      ...     'FR': [4.0405, 4.0963, 4.3149],\n |      ...     'GR': [1.7246, 1.7482, 1.8519],\n |      ...     'IT': [804.74, 810.01, 860.13]},\n |      ...     index=['1980-01-01', '1980-02-01', '1980-03-01'])\n |      &gt;&gt;&gt; df\n |                      FR      GR      IT\n |      1980-01-01  4.0405  1.7246  804.74\n |      1980-02-01  4.0963  1.7482  810.01\n |      1980-03-01  4.3149  1.8519  860.13\n |      \n |      &gt;&gt;&gt; df.pct_change()\n |                        FR        GR        IT\n |      1980-01-01       NaN       NaN       NaN\n |      1980-02-01  0.013810  0.013684  0.006549\n |      1980-03-01  0.053365  0.059318  0.061876\n |      \n |      Percentage of change in GOOG and APPL stock volume. Shows computing\n |      the percentage change between columns.\n |      \n |      &gt;&gt;&gt; df = pd.DataFrame({\n |      ...     '2016': [1769950, 30586265],\n |      ...     '2015': [1500923, 40912316],\n |      ...     '2014': [1371819, 41403351]},\n |      ...     index=['GOOG', 'APPL'])\n |      &gt;&gt;&gt; df\n |                2016      2015      2014\n |      GOOG   1769950   1500923   1371819\n |      APPL  30586265  40912316  41403351\n |      \n |      &gt;&gt;&gt; df.pct_change(axis='columns', periods=-1)\n |                2016      2015  2014\n |      GOOG  0.179241  0.094112   NaN\n |      APPL -0.252395 -0.011860   NaN\n |  \n |  pipe(self, func: 'Callable[..., T] | tuple[Callable[..., T], str]', *args, **kwargs) -&gt; 'T'\n |      Apply chainable functions that expect Series or DataFrames.\n |      \n |      Parameters\n |      ----------\n |      func : function\n |          Function to apply to the Series/DataFrame.\n |          ``args``, and ``kwargs`` are passed into ``func``.\n |          Alternatively a ``(callable, data_keyword)`` tuple where\n |          ``data_keyword`` is a string indicating the keyword of\n |          ``callable`` that expects the Series/DataFrame.\n |      args : iterable, optional\n |          Positional arguments passed into ``func``.\n |      kwargs : mapping, optional\n |          A dictionary of keyword arguments passed into ``func``.\n |      \n |      Returns\n |      -------\n |      the return type of ``func``.\n |      \n |      See Also\n |      --------\n |      DataFrame.apply : Apply a function along input axis of DataFrame.\n |      DataFrame.applymap : Apply a function elementwise on a whole DataFrame.\n |      Series.map : Apply a mapping correspondence on a\n |          :class:`~pandas.Series`.\n |      \n |      Notes\n |      -----\n |      Use ``.pipe`` when chaining together functions that expect\n |      Series, DataFrames or GroupBy objects. Instead of writing\n |      \n |      &gt;&gt;&gt; func(g(h(df), arg1=a), arg2=b, arg3=c)  # doctest: +SKIP\n |      \n |      You can write\n |      \n |      &gt;&gt;&gt; (df.pipe(h)\n |      ...    .pipe(g, arg1=a)\n |      ...    .pipe(func, arg2=b, arg3=c)\n |      ... )  # doctest: +SKIP\n |      \n |      If you have a function that takes the data as (say) the second\n |      argument, pass a tuple indicating which keyword expects the\n |      data. For example, suppose ``func`` takes its data as ``arg2``:\n |      \n |      &gt;&gt;&gt; (df.pipe(h)\n |      ...    .pipe(g, arg1=a)\n |      ...    .pipe((func, 'arg2'), arg1=a, arg3=c)\n |      ...  )  # doctest: +SKIP\n |  \n |  rank(self: 'NDFrameT', axis: 'Axis' = 0, method: 'str' = 'average', numeric_only: 'bool_t' = False, na_option: 'str' = 'keep', ascending: 'bool_t' = True, pct: 'bool_t' = False) -&gt; 'NDFrameT'\n |      Compute numerical data ranks (1 through n) along axis.\n |      \n |      By default, equal values are assigned a rank that is the average of the\n |      ranks of those values.\n |      \n |      Parameters\n |      ----------\n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |          Index to direct ranking.\n |          For `Series` this parameter is unused and defaults to 0.\n |      method : {'average', 'min', 'max', 'first', 'dense'}, default 'average'\n |          How to rank the group of records that have the same value (i.e. ties):\n |      \n |          * average: average rank of the group\n |          * min: lowest rank in the group\n |          * max: highest rank in the group\n |          * first: ranks assigned in order they appear in the array\n |          * dense: like 'min', but rank always increases by 1 between groups.\n |      \n |      numeric_only : bool, default False\n |          For DataFrame objects, rank only numeric columns if set to True.\n |      \n |          .. versionchanged:: 2.0.0\n |              The default value of ``numeric_only`` is now ``False``.\n |      \n |      na_option : {'keep', 'top', 'bottom'}, default 'keep'\n |          How to rank NaN values:\n |      \n |          * keep: assign NaN rank to NaN values\n |          * top: assign lowest rank to NaN values\n |          * bottom: assign highest rank to NaN values\n |      \n |      ascending : bool, default True\n |          Whether or not the elements should be ranked in ascending order.\n |      pct : bool, default False\n |          Whether or not to display the returned rankings in percentile\n |          form.\n |      \n |      Returns\n |      -------\n |      same type as caller\n |          Return a Series or DataFrame with data ranks as values.\n |      \n |      See Also\n |      --------\n |      core.groupby.DataFrameGroupBy.rank : Rank of values within each group.\n |      core.groupby.SeriesGroupBy.rank : Rank of values within each group.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; df = pd.DataFrame(data={'Animal': ['cat', 'penguin', 'dog',\n |      ...                                    'spider', 'snake'],\n |      ...                         'Number_legs': [4, 2, 4, 8, np.nan]})\n |      &gt;&gt;&gt; df\n |          Animal  Number_legs\n |      0      cat          4.0\n |      1  penguin          2.0\n |      2      dog          4.0\n |      3   spider          8.0\n |      4    snake          NaN\n |      \n |      Ties are assigned the mean of the ranks (by default) for the group.\n |      \n |      &gt;&gt;&gt; s = pd.Series(range(5), index=list(\"abcde\"))\n |      &gt;&gt;&gt; s[\"d\"] = s[\"b\"]\n |      &gt;&gt;&gt; s.rank()\n |      a    1.0\n |      b    2.5\n |      c    4.0\n |      d    2.5\n |      e    5.0\n |      dtype: float64\n |      \n |      The following example shows how the method behaves with the above\n |      parameters:\n |      \n |      * default_rank: this is the default behaviour obtained without using\n |        any parameter.\n |      * max_rank: setting ``method = 'max'`` the records that have the\n |        same values are ranked using the highest rank (e.g.: since 'cat'\n |        and 'dog' are both in the 2nd and 3rd position, rank 3 is assigned.)\n |      * NA_bottom: choosing ``na_option = 'bottom'``, if there are records\n |        with NaN values they are placed at the bottom of the ranking.\n |      * pct_rank: when setting ``pct = True``, the ranking is expressed as\n |        percentile rank.\n |      \n |      &gt;&gt;&gt; df['default_rank'] = df['Number_legs'].rank()\n |      &gt;&gt;&gt; df['max_rank'] = df['Number_legs'].rank(method='max')\n |      &gt;&gt;&gt; df['NA_bottom'] = df['Number_legs'].rank(na_option='bottom')\n |      &gt;&gt;&gt; df['pct_rank'] = df['Number_legs'].rank(pct=True)\n |      &gt;&gt;&gt; df\n |          Animal  Number_legs  default_rank  max_rank  NA_bottom  pct_rank\n |      0      cat          4.0           2.5       3.0        2.5     0.625\n |      1  penguin          2.0           1.0       1.0        1.0     0.250\n |      2      dog          4.0           2.5       3.0        2.5     0.625\n |      3   spider          8.0           4.0       4.0        4.0     1.000\n |      4    snake          NaN           NaN       NaN        5.0       NaN\n |  \n |  reindex_like(self: 'NDFrameT', other, method: \"Literal['backfill', 'bfill', 'pad', 'ffill', 'nearest'] | None\" = None, copy: 'bool_t | None' = None, limit=None, tolerance=None) -&gt; 'NDFrameT'\n |      Return an object with matching indices as other object.\n |      \n |      Conform the object to the same index on all axes. Optional\n |      filling logic, placing NaN in locations having no value\n |      in the previous index. A new object is produced unless the\n |      new index is equivalent to the current one and copy=False.\n |      \n |      Parameters\n |      ----------\n |      other : Object of the same data type\n |          Its row and column indices are used to define the new indices\n |          of this object.\n |      method : {None, 'backfill'/'bfill', 'pad'/'ffill', 'nearest'}\n |          Method to use for filling holes in reindexed DataFrame.\n |          Please note: this is only applicable to DataFrames/Series with a\n |          monotonically increasing/decreasing index.\n |      \n |          * None (default): don't fill gaps\n |          * pad / ffill: propagate last valid observation forward to next\n |            valid\n |          * backfill / bfill: use next valid observation to fill gap\n |          * nearest: use nearest valid observations to fill gap.\n |      \n |      copy : bool, default True\n |          Return a new object, even if the passed indexes are the same.\n |      limit : int, default None\n |          Maximum number of consecutive labels to fill for inexact matches.\n |      tolerance : optional\n |          Maximum distance between original and new labels for inexact\n |          matches. The values of the index at the matching locations must\n |          satisfy the equation ``abs(index[indexer] - target) &lt;= tolerance``.\n |      \n |          Tolerance may be a scalar value, which applies the same tolerance\n |          to all values, or list-like, which applies variable tolerance per\n |          element. List-like includes list, tuple, array, Series, and must be\n |          the same size as the index and its dtype must exactly match the\n |          index's type.\n |      \n |      Returns\n |      -------\n |      Series or DataFrame\n |          Same type as caller, but with changed indices on each axis.\n |      \n |      See Also\n |      --------\n |      DataFrame.set_index : Set row labels.\n |      DataFrame.reset_index : Remove row labels or move them to new columns.\n |      DataFrame.reindex : Change to new indices or expand indices.\n |      \n |      Notes\n |      -----\n |      Same as calling\n |      ``.reindex(index=other.index, columns=other.columns,...)``.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; df1 = pd.DataFrame([[24.3, 75.7, 'high'],\n |      ...                     [31, 87.8, 'high'],\n |      ...                     [22, 71.6, 'medium'],\n |      ...                     [35, 95, 'medium']],\n |      ...                    columns=['temp_celsius', 'temp_fahrenheit',\n |      ...                             'windspeed'],\n |      ...                    index=pd.date_range(start='2014-02-12',\n |      ...                                        end='2014-02-15', freq='D'))\n |      \n |      &gt;&gt;&gt; df1\n |                  temp_celsius  temp_fahrenheit windspeed\n |      2014-02-12          24.3             75.7      high\n |      2014-02-13          31.0             87.8      high\n |      2014-02-14          22.0             71.6    medium\n |      2014-02-15          35.0             95.0    medium\n |      \n |      &gt;&gt;&gt; df2 = pd.DataFrame([[28, 'low'],\n |      ...                     [30, 'low'],\n |      ...                     [35.1, 'medium']],\n |      ...                    columns=['temp_celsius', 'windspeed'],\n |      ...                    index=pd.DatetimeIndex(['2014-02-12', '2014-02-13',\n |      ...                                            '2014-02-15']))\n |      \n |      &gt;&gt;&gt; df2\n |                  temp_celsius windspeed\n |      2014-02-12          28.0       low\n |      2014-02-13          30.0       low\n |      2014-02-15          35.1    medium\n |      \n |      &gt;&gt;&gt; df2.reindex_like(df1)\n |                  temp_celsius  temp_fahrenheit windspeed\n |      2014-02-12          28.0              NaN       low\n |      2014-02-13          30.0              NaN       low\n |      2014-02-14           NaN              NaN       NaN\n |      2014-02-15          35.1              NaN    medium\n |  \n |  rolling(self, window: 'int | dt.timedelta | str | BaseOffset | BaseIndexer', min_periods: 'int | None' = None, center: 'bool_t' = False, win_type: 'str | None' = None, on: 'str | None' = None, axis: 'Axis' = 0, closed: 'str | None' = None, step: 'int | None' = None, method: 'str' = 'single') -&gt; 'Window | Rolling'\n |      Provide rolling window calculations.\n |      \n |      Parameters\n |      ----------\n |      window : int, timedelta, str, offset, or BaseIndexer subclass\n |          Size of the moving window.\n |      \n |          If an integer, the fixed number of observations used for\n |          each window.\n |      \n |          If a timedelta, str, or offset, the time period of each window. Each\n |          window will be a variable sized based on the observations included in\n |          the time-period. This is only valid for datetimelike indexes.\n |          To learn more about the offsets & frequency strings, please see `this link\n |          &lt;https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases&gt;`__.\n |      \n |          If a BaseIndexer subclass, the window boundaries\n |          based on the defined ``get_window_bounds`` method. Additional rolling\n |          keyword arguments, namely ``min_periods``, ``center``, ``closed`` and\n |          ``step`` will be passed to ``get_window_bounds``.\n |      \n |      min_periods : int, default None\n |          Minimum number of observations in window required to have a value;\n |          otherwise, result is ``np.nan``.\n |      \n |          For a window that is specified by an offset, ``min_periods`` will default to 1.\n |      \n |          For a window that is specified by an integer, ``min_periods`` will default\n |          to the size of the window.\n |      \n |      center : bool, default False\n |          If False, set the window labels as the right edge of the window index.\n |      \n |          If True, set the window labels as the center of the window index.\n |      \n |      win_type : str, default None\n |          If ``None``, all points are evenly weighted.\n |      \n |          If a string, it must be a valid `scipy.signal window function\n |          &lt;https://docs.scipy.org/doc/scipy/reference/signal.windows.html#module-scipy.signal.windows&gt;`__.\n |      \n |          Certain Scipy window types require additional parameters to be passed\n |          in the aggregation function. The additional parameters must match\n |          the keywords specified in the Scipy window type method signature.\n |      \n |      on : str, optional\n |          For a DataFrame, a column label or Index level on which\n |          to calculate the rolling window, rather than the DataFrame's index.\n |      \n |          Provided integer column is ignored and excluded from result since\n |          an integer index is not used to calculate the rolling window.\n |      \n |      axis : int or str, default 0\n |          If ``0`` or ``'index'``, roll across the rows.\n |      \n |          If ``1`` or ``'columns'``, roll across the columns.\n |      \n |          For `Series` this parameter is unused and defaults to 0.\n |      \n |      closed : str, default None\n |          If ``'right'``, the first point in the window is excluded from calculations.\n |      \n |          If ``'left'``, the last point in the window is excluded from calculations.\n |      \n |          If ``'both'``, the no points in the window are excluded from calculations.\n |      \n |          If ``'neither'``, the first and last points in the window are excluded\n |          from calculations.\n |      \n |          Default ``None`` (``'right'``).\n |      \n |          .. versionchanged:: 1.2.0\n |      \n |              The closed parameter with fixed windows is now supported.\n |      \n |      step : int, default None\n |      \n |          .. versionadded:: 1.5.0\n |      \n |          Evaluate the window at every ``step`` result, equivalent to slicing as\n |          ``[::step]``. ``window`` must be an integer. Using a step argument other\n |          than None or 1 will produce a result with a different shape than the input.\n |      \n |      method : str {'single', 'table'}, default 'single'\n |      \n |          .. versionadded:: 1.3.0\n |      \n |          Execute the rolling operation per single column or row (``'single'``)\n |          or over the entire object (``'table'``).\n |      \n |          This argument is only implemented when specifying ``engine='numba'``\n |          in the method call.\n |      \n |      Returns\n |      -------\n |      ``Window`` subclass if a ``win_type`` is passed\n |      \n |      ``Rolling`` subclass if ``win_type`` is not passed\n |      \n |      See Also\n |      --------\n |      expanding : Provides expanding transformations.\n |      ewm : Provides exponential weighted functions.\n |      \n |      Notes\n |      -----\n |      See :ref:`Windowing Operations &lt;window.generic&gt;` for further usage details\n |      and examples.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; df = pd.DataFrame({'B': [0, 1, 2, np.nan, 4]})\n |      &gt;&gt;&gt; df\n |           B\n |      0  0.0\n |      1  1.0\n |      2  2.0\n |      3  NaN\n |      4  4.0\n |      \n |      **window**\n |      \n |      Rolling sum with a window length of 2 observations.\n |      \n |      &gt;&gt;&gt; df.rolling(2).sum()\n |           B\n |      0  NaN\n |      1  1.0\n |      2  3.0\n |      3  NaN\n |      4  NaN\n |      \n |      Rolling sum with a window span of 2 seconds.\n |      \n |      &gt;&gt;&gt; df_time = pd.DataFrame({'B': [0, 1, 2, np.nan, 4]},\n |      ...                        index = [pd.Timestamp('20130101 09:00:00'),\n |      ...                                 pd.Timestamp('20130101 09:00:02'),\n |      ...                                 pd.Timestamp('20130101 09:00:03'),\n |      ...                                 pd.Timestamp('20130101 09:00:05'),\n |      ...                                 pd.Timestamp('20130101 09:00:06')])\n |      \n |      &gt;&gt;&gt; df_time\n |                             B\n |      2013-01-01 09:00:00  0.0\n |      2013-01-01 09:00:02  1.0\n |      2013-01-01 09:00:03  2.0\n |      2013-01-01 09:00:05  NaN\n |      2013-01-01 09:00:06  4.0\n |      \n |      &gt;&gt;&gt; df_time.rolling('2s').sum()\n |                             B\n |      2013-01-01 09:00:00  0.0\n |      2013-01-01 09:00:02  1.0\n |      2013-01-01 09:00:03  3.0\n |      2013-01-01 09:00:05  NaN\n |      2013-01-01 09:00:06  4.0\n |      \n |      Rolling sum with forward looking windows with 2 observations.\n |      \n |      &gt;&gt;&gt; indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=2)\n |      &gt;&gt;&gt; df.rolling(window=indexer, min_periods=1).sum()\n |           B\n |      0  1.0\n |      1  3.0\n |      2  2.0\n |      3  4.0\n |      4  4.0\n |      \n |      **min_periods**\n |      \n |      Rolling sum with a window length of 2 observations, but only needs a minimum of 1\n |      observation to calculate a value.\n |      \n |      &gt;&gt;&gt; df.rolling(2, min_periods=1).sum()\n |           B\n |      0  0.0\n |      1  1.0\n |      2  3.0\n |      3  2.0\n |      4  4.0\n |      \n |      **center**\n |      \n |      Rolling sum with the result assigned to the center of the window index.\n |      \n |      &gt;&gt;&gt; df.rolling(3, min_periods=1, center=True).sum()\n |           B\n |      0  1.0\n |      1  3.0\n |      2  3.0\n |      3  6.0\n |      4  4.0\n |      \n |      &gt;&gt;&gt; df.rolling(3, min_periods=1, center=False).sum()\n |           B\n |      0  0.0\n |      1  1.0\n |      2  3.0\n |      3  3.0\n |      4  6.0\n |      \n |      **step**\n |      \n |      Rolling sum with a window length of 2 observations, minimum of 1 observation to\n |      calculate a value, and a step of 2.\n |      \n |      &gt;&gt;&gt; df.rolling(2, min_periods=1, step=2).sum()\n |           B\n |      0  0.0\n |      2  3.0\n |      4  4.0\n |      \n |      **win_type**\n |      \n |      Rolling sum with a window length of 2, using the Scipy ``'gaussian'``\n |      window type. ``std`` is required in the aggregation function.\n |      \n |      &gt;&gt;&gt; df.rolling(2, win_type='gaussian').sum(std=3)\n |                B\n |      0       NaN\n |      1  0.986207\n |      2  2.958621\n |      3       NaN\n |      4       NaN\n |      \n |      **on**\n |      \n |      Rolling sum with a window length of 2 days.\n |      \n |      &gt;&gt;&gt; df = pd.DataFrame({\n |      ...     'A': [pd.to_datetime('2020-01-01'),\n |      ...           pd.to_datetime('2020-01-01'),\n |      ...           pd.to_datetime('2020-01-02'),],\n |      ...     'B': [1, 2, 3], },\n |      ...     index=pd.date_range('2020', periods=3))\n |      \n |      &gt;&gt;&gt; df\n |                          A  B\n |      2020-01-01 2020-01-01  1\n |      2020-01-02 2020-01-01  2\n |      2020-01-03 2020-01-02  3\n |      \n |      &gt;&gt;&gt; df.rolling('2D', on='A').sum()\n |                          A    B\n |      2020-01-01 2020-01-01  1.0\n |      2020-01-02 2020-01-01  3.0\n |      2020-01-03 2020-01-02  6.0\n |  \n |  sample(self: 'NDFrameT', n: 'int | None' = None, frac: 'float | None' = None, replace: 'bool_t' = False, weights=None, random_state: 'RandomState | None' = None, axis: 'Axis | None' = None, ignore_index: 'bool_t' = False) -&gt; 'NDFrameT'\n |      Return a random sample of items from an axis of object.\n |      \n |      You can use `random_state` for reproducibility.\n |      \n |      Parameters\n |      ----------\n |      n : int, optional\n |          Number of items from axis to return. Cannot be used with `frac`.\n |          Default = 1 if `frac` = None.\n |      frac : float, optional\n |          Fraction of axis items to return. Cannot be used with `n`.\n |      replace : bool, default False\n |          Allow or disallow sampling of the same row more than once.\n |      weights : str or ndarray-like, optional\n |          Default 'None' results in equal probability weighting.\n |          If passed a Series, will align with target object on index. Index\n |          values in weights not found in sampled object will be ignored and\n |          index values in sampled object not in weights will be assigned\n |          weights of zero.\n |          If called on a DataFrame, will accept the name of a column\n |          when axis = 0.\n |          Unless weights are a Series, weights must be same length as axis\n |          being sampled.\n |          If weights do not sum to 1, they will be normalized to sum to 1.\n |          Missing values in the weights column will be treated as zero.\n |          Infinite values not allowed.\n |      random_state : int, array-like, BitGenerator, np.random.RandomState, np.random.Generator, optional\n |          If int, array-like, or BitGenerator, seed for random number generator.\n |          If np.random.RandomState or np.random.Generator, use as given.\n |      \n |          .. versionchanged:: 1.1.0\n |      \n |              array-like and BitGenerator object now passed to np.random.RandomState()\n |              as seed\n |      \n |          .. versionchanged:: 1.4.0\n |      \n |              np.random.Generator objects now accepted\n |      \n |      axis : {0 or ‘index’, 1 or ‘columns’, None}, default None\n |          Axis to sample. Accepts axis number or name. Default is stat axis\n |          for given data type. For `Series` this parameter is unused and defaults to `None`.\n |      ignore_index : bool, default False\n |          If True, the resulting index will be labeled 0, 1, …, n - 1.\n |      \n |          .. versionadded:: 1.3.0\n |      \n |      Returns\n |      -------\n |      Series or DataFrame\n |          A new object of same type as caller containing `n` items randomly\n |          sampled from the caller object.\n |      \n |      See Also\n |      --------\n |      DataFrameGroupBy.sample: Generates random samples from each group of a\n |          DataFrame object.\n |      SeriesGroupBy.sample: Generates random samples from each group of a\n |          Series object.\n |      numpy.random.choice: Generates a random sample from a given 1-D numpy\n |          array.\n |      \n |      Notes\n |      -----\n |      If `frac` &gt; 1, `replacement` should be set to `True`.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; df = pd.DataFrame({'num_legs': [2, 4, 8, 0],\n |      ...                    'num_wings': [2, 0, 0, 0],\n |      ...                    'num_specimen_seen': [10, 2, 1, 8]},\n |      ...                   index=['falcon', 'dog', 'spider', 'fish'])\n |      &gt;&gt;&gt; df\n |              num_legs  num_wings  num_specimen_seen\n |      falcon         2          2                 10\n |      dog            4          0                  2\n |      spider         8          0                  1\n |      fish           0          0                  8\n |      \n |      Extract 3 random elements from the ``Series`` ``df['num_legs']``:\n |      Note that we use `random_state` to ensure the reproducibility of\n |      the examples.\n |      \n |      &gt;&gt;&gt; df['num_legs'].sample(n=3, random_state=1)\n |      fish      0\n |      spider    8\n |      falcon    2\n |      Name: num_legs, dtype: int64\n |      \n |      A random 50% sample of the ``DataFrame`` with replacement:\n |      \n |      &gt;&gt;&gt; df.sample(frac=0.5, replace=True, random_state=1)\n |            num_legs  num_wings  num_specimen_seen\n |      dog          4          0                  2\n |      fish         0          0                  8\n |      \n |      An upsample sample of the ``DataFrame`` with replacement:\n |      Note that `replace` parameter has to be `True` for `frac` parameter &gt; 1.\n |      \n |      &gt;&gt;&gt; df.sample(frac=2, replace=True, random_state=1)\n |              num_legs  num_wings  num_specimen_seen\n |      dog            4          0                  2\n |      fish           0          0                  8\n |      falcon         2          2                 10\n |      falcon         2          2                 10\n |      fish           0          0                  8\n |      dog            4          0                  2\n |      fish           0          0                  8\n |      dog            4          0                  2\n |      \n |      Using a DataFrame column as weights. Rows with larger value in the\n |      `num_specimen_seen` column are more likely to be sampled.\n |      \n |      &gt;&gt;&gt; df.sample(n=2, weights='num_specimen_seen', random_state=1)\n |              num_legs  num_wings  num_specimen_seen\n |      falcon         2          2                 10\n |      fish           0          0                  8\n |  \n |  set_flags(self: 'NDFrameT', *, copy: 'bool_t' = False, allows_duplicate_labels: 'bool_t | None' = None) -&gt; 'NDFrameT'\n |      Return a new object with updated flags.\n |      \n |      Parameters\n |      ----------\n |      copy : bool, default False\n |          Specify if a copy of the object should be made.\n |      allows_duplicate_labels : bool, optional\n |          Whether the returned object allows duplicate labels.\n |      \n |      Returns\n |      -------\n |      Series or DataFrame\n |          The same type as the caller.\n |      \n |      See Also\n |      --------\n |      DataFrame.attrs : Global metadata applying to this dataset.\n |      DataFrame.flags : Global flags applying to this object.\n |      \n |      Notes\n |      -----\n |      This method returns a new object that's a view on the same data\n |      as the input. Mutating the input or the output values will be reflected\n |      in the other.\n |      \n |      This method is intended to be used in method chains.\n |      \n |      \"Flags\" differ from \"metadata\". Flags reflect properties of the\n |      pandas object (the Series or DataFrame). Metadata refer to properties\n |      of the dataset, and should be stored in :attr:`DataFrame.attrs`.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; df = pd.DataFrame({\"A\": [1, 2]})\n |      &gt;&gt;&gt; df.flags.allows_duplicate_labels\n |      True\n |      &gt;&gt;&gt; df2 = df.set_flags(allows_duplicate_labels=False)\n |      &gt;&gt;&gt; df2.flags.allows_duplicate_labels\n |      False\n |  \n |  squeeze(self, axis: 'Axis | None' = None)\n |      Squeeze 1 dimensional axis objects into scalars.\n |      \n |      Series or DataFrames with a single element are squeezed to a scalar.\n |      DataFrames with a single column or a single row are squeezed to a\n |      Series. Otherwise the object is unchanged.\n |      \n |      This method is most useful when you don't know if your\n |      object is a Series or DataFrame, but you do know it has just a single\n |      column. In that case you can safely call `squeeze` to ensure you have a\n |      Series.\n |      \n |      Parameters\n |      ----------\n |      axis : {0 or 'index', 1 or 'columns', None}, default None\n |          A specific axis to squeeze. By default, all length-1 axes are\n |          squeezed. For `Series` this parameter is unused and defaults to `None`.\n |      \n |      Returns\n |      -------\n |      DataFrame, Series, or scalar\n |          The projection after squeezing `axis` or all the axes.\n |      \n |      See Also\n |      --------\n |      Series.iloc : Integer-location based indexing for selecting scalars.\n |      DataFrame.iloc : Integer-location based indexing for selecting Series.\n |      Series.to_frame : Inverse of DataFrame.squeeze for a\n |          single-column DataFrame.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; primes = pd.Series([2, 3, 5, 7])\n |      \n |      Slicing might produce a Series with a single value:\n |      \n |      &gt;&gt;&gt; even_primes = primes[primes % 2 == 0]\n |      &gt;&gt;&gt; even_primes\n |      0    2\n |      dtype: int64\n |      \n |      &gt;&gt;&gt; even_primes.squeeze()\n |      2\n |      \n |      Squeezing objects with more than one value in every axis does nothing:\n |      \n |      &gt;&gt;&gt; odd_primes = primes[primes % 2 == 1]\n |      &gt;&gt;&gt; odd_primes\n |      1    3\n |      2    5\n |      3    7\n |      dtype: int64\n |      \n |      &gt;&gt;&gt; odd_primes.squeeze()\n |      1    3\n |      2    5\n |      3    7\n |      dtype: int64\n |      \n |      Squeezing is even more effective when used with DataFrames.\n |      \n |      &gt;&gt;&gt; df = pd.DataFrame([[1, 2], [3, 4]], columns=['a', 'b'])\n |      &gt;&gt;&gt; df\n |         a  b\n |      0  1  2\n |      1  3  4\n |      \n |      Slicing a single column will produce a DataFrame with the columns\n |      having only one value:\n |      \n |      &gt;&gt;&gt; df_a = df[['a']]\n |      &gt;&gt;&gt; df_a\n |         a\n |      0  1\n |      1  3\n |      \n |      So the columns can be squeezed down, resulting in a Series:\n |      \n |      &gt;&gt;&gt; df_a.squeeze('columns')\n |      0    1\n |      1    3\n |      Name: a, dtype: int64\n |      \n |      Slicing a single row from a single column will produce a single\n |      scalar DataFrame:\n |      \n |      &gt;&gt;&gt; df_0a = df.loc[df.index &lt; 1, ['a']]\n |      &gt;&gt;&gt; df_0a\n |         a\n |      0  1\n |      \n |      Squeezing the rows produces a single scalar Series:\n |      \n |      &gt;&gt;&gt; df_0a.squeeze('rows')\n |      a    1\n |      Name: 0, dtype: int64\n |      \n |      Squeezing all axes will project directly into a scalar:\n |      \n |      &gt;&gt;&gt; df_0a.squeeze()\n |      1\n |  \n |  swapaxes(self: 'NDFrameT', axis1: 'Axis', axis2: 'Axis', copy: 'bool_t | None' = None) -&gt; 'NDFrameT'\n |      Interchange axes and swap values axes appropriately.\n |      \n |      Returns\n |      -------\n |      same as input\n |  \n |  tail(self: 'NDFrameT', n: 'int' = 5) -&gt; 'NDFrameT'\n |      Return the last `n` rows.\n |      \n |      This function returns last `n` rows from the object based on\n |      position. It is useful for quickly verifying data, for example,\n |      after sorting or appending rows.\n |      \n |      For negative values of `n`, this function returns all rows except\n |      the first `|n|` rows, equivalent to ``df[|n|:]``.\n |      \n |      If n is larger than the number of rows, this function returns all rows.\n |      \n |      Parameters\n |      ----------\n |      n : int, default 5\n |          Number of rows to select.\n |      \n |      Returns\n |      -------\n |      type of caller\n |          The last `n` rows of the caller object.\n |      \n |      See Also\n |      --------\n |      DataFrame.head : The first `n` rows of the caller object.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; df = pd.DataFrame({'animal': ['alligator', 'bee', 'falcon', 'lion',\n |      ...                    'monkey', 'parrot', 'shark', 'whale', 'zebra']})\n |      &gt;&gt;&gt; df\n |            animal\n |      0  alligator\n |      1        bee\n |      2     falcon\n |      3       lion\n |      4     monkey\n |      5     parrot\n |      6      shark\n |      7      whale\n |      8      zebra\n |      \n |      Viewing the last 5 lines\n |      \n |      &gt;&gt;&gt; df.tail()\n |         animal\n |      4  monkey\n |      5  parrot\n |      6   shark\n |      7   whale\n |      8   zebra\n |      \n |      Viewing the last `n` lines (three in this case)\n |      \n |      &gt;&gt;&gt; df.tail(3)\n |        animal\n |      6  shark\n |      7  whale\n |      8  zebra\n |      \n |      For negative values of `n`\n |      \n |      &gt;&gt;&gt; df.tail(-3)\n |         animal\n |      3    lion\n |      4  monkey\n |      5  parrot\n |      6   shark\n |      7   whale\n |      8   zebra\n |  \n |  to_clipboard(self, excel: 'bool_t' = True, sep: 'str | None' = None, **kwargs) -&gt; 'None'\n |      Copy object to the system clipboard.\n |      \n |      Write a text representation of object to the system clipboard.\n |      This can be pasted into Excel, for example.\n |      \n |      Parameters\n |      ----------\n |      excel : bool, default True\n |          Produce output in a csv format for easy pasting into excel.\n |      \n |          - True, use the provided separator for csv pasting.\n |          - False, write a string representation of the object to the clipboard.\n |      \n |      sep : str, default ``'\\t'``\n |          Field delimiter.\n |      **kwargs\n |          These parameters will be passed to DataFrame.to_csv.\n |      \n |      See Also\n |      --------\n |      DataFrame.to_csv : Write a DataFrame to a comma-separated values\n |          (csv) file.\n |      read_clipboard : Read text from clipboard and pass to read_csv.\n |      \n |      Notes\n |      -----\n |      Requirements for your platform.\n |      \n |        - Linux : `xclip`, or `xsel` (with `PyQt4` modules)\n |        - Windows : none\n |        - macOS : none\n |      \n |      This method uses the processes developed for the package `pyperclip`. A\n |      solution to render any output string format is given in the examples.\n |      \n |      Examples\n |      --------\n |      Copy the contents of a DataFrame to the clipboard.\n |      \n |      &gt;&gt;&gt; df = pd.DataFrame([[1, 2, 3], [4, 5, 6]], columns=['A', 'B', 'C'])\n |      \n |      &gt;&gt;&gt; df.to_clipboard(sep=',')  # doctest: +SKIP\n |      ... # Wrote the following to the system clipboard:\n |      ... # ,A,B,C\n |      ... # 0,1,2,3\n |      ... # 1,4,5,6\n |      \n |      We can omit the index by passing the keyword `index` and setting\n |      it to false.\n |      \n |      &gt;&gt;&gt; df.to_clipboard(sep=',', index=False)  # doctest: +SKIP\n |      ... # Wrote the following to the system clipboard:\n |      ... # A,B,C\n |      ... # 1,2,3\n |      ... # 4,5,6\n |      \n |      Using the original `pyperclip` package for any string output format.\n |      \n |      .. code-block:: python\n |      \n |         import pyperclip\n |         html = df.style.to_html()\n |         pyperclip.copy(html)\n |  \n |  to_csv(self, path_or_buf: 'FilePath | WriteBuffer[bytes] | WriteBuffer[str] | None' = None, sep: 'str' = ',', na_rep: 'str' = '', float_format: 'str | Callable | None' = None, columns: 'Sequence[Hashable] | None' = None, header: 'bool_t | list[str]' = True, index: 'bool_t' = True, index_label: 'IndexLabel | None' = None, mode: 'str' = 'w', encoding: 'str | None' = None, compression: 'CompressionOptions' = 'infer', quoting: 'int | None' = None, quotechar: 'str' = '\"', lineterminator: 'str | None' = None, chunksize: 'int | None' = None, date_format: 'str | None' = None, doublequote: 'bool_t' = True, escapechar: 'str | None' = None, decimal: 'str' = '.', errors: 'str' = 'strict', storage_options: 'StorageOptions' = None) -&gt; 'str | None'\n |      Write object to a comma-separated values (csv) file.\n |      \n |      Parameters\n |      ----------\n |      path_or_buf : str, path object, file-like object, or None, default None\n |          String, path object (implementing os.PathLike[str]), or file-like\n |          object implementing a write() function. If None, the result is\n |          returned as a string. If a non-binary file object is passed, it should\n |          be opened with `newline=''`, disabling universal newlines. If a binary\n |          file object is passed, `mode` might need to contain a `'b'`.\n |      \n |          .. versionchanged:: 1.2.0\n |      \n |             Support for binary file objects was introduced.\n |      \n |      sep : str, default ','\n |          String of length 1. Field delimiter for the output file.\n |      na_rep : str, default ''\n |          Missing data representation.\n |      float_format : str, Callable, default None\n |          Format string for floating point numbers. If a Callable is given, it takes\n |          precedence over other numeric formatting parameters, like decimal.\n |      columns : sequence, optional\n |          Columns to write.\n |      header : bool or list of str, default True\n |          Write out the column names. If a list of strings is given it is\n |          assumed to be aliases for the column names.\n |      index : bool, default True\n |          Write row names (index).\n |      index_label : str or sequence, or False, default None\n |          Column label for index column(s) if desired. If None is given, and\n |          `header` and `index` are True, then the index names are used. A\n |          sequence should be given if the object uses MultiIndex. If\n |          False do not print fields for index names. Use index_label=False\n |          for easier importing in R.\n |      mode : str, default 'w'\n |          Python write mode. The available write modes are the same as\n |          :py:func:`open`.\n |      encoding : str, optional\n |          A string representing the encoding to use in the output file,\n |          defaults to 'utf-8'. `encoding` is not supported if `path_or_buf`\n |          is a non-binary file object.\n |      compression : str or dict, default 'infer'\n |          For on-the-fly compression of the output data. If 'infer' and 'path_or_buf' is\n |          path-like, then detect compression from the following extensions: '.gz',\n |          '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n |          (otherwise no compression).\n |          Set to ``None`` for no compression.\n |          Can also be a dict with key ``'method'`` set\n |          to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'tar'``} and other\n |          key-value pairs are forwarded to\n |          ``zipfile.ZipFile``, ``gzip.GzipFile``,\n |          ``bz2.BZ2File``, ``zstandard.ZstdCompressor`` or\n |          ``tarfile.TarFile``, respectively.\n |          As an example, the following could be passed for faster compression and to create\n |          a reproducible gzip archive:\n |          ``compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}``.\n |      \n |          .. versionadded:: 1.5.0\n |              Added support for `.tar` files.\n |      \n |          .. versionchanged:: 1.0.0\n |      \n |             May now be a dict with key 'method' as compression mode\n |             and other entries as additional compression options if\n |             compression mode is 'zip'.\n |      \n |          .. versionchanged:: 1.1.0\n |      \n |             Passing compression options as keys in dict is\n |             supported for compression modes 'gzip', 'bz2', 'zstd', and 'zip'.\n |      \n |          .. versionchanged:: 1.2.0\n |      \n |              Compression is supported for binary file objects.\n |      \n |          .. versionchanged:: 1.2.0\n |      \n |              Previous versions forwarded dict entries for 'gzip' to\n |              `gzip.open` instead of `gzip.GzipFile` which prevented\n |              setting `mtime`.\n |      \n |      quoting : optional constant from csv module\n |          Defaults to csv.QUOTE_MINIMAL. If you have set a `float_format`\n |          then floats are converted to strings and thus csv.QUOTE_NONNUMERIC\n |          will treat them as non-numeric.\n |      quotechar : str, default '\\\"'\n |          String of length 1. Character used to quote fields.\n |      lineterminator : str, optional\n |          The newline character or character sequence to use in the output\n |          file. Defaults to `os.linesep`, which depends on the OS in which\n |          this method is called ('\\\\n' for linux, '\\\\r\\\\n' for Windows, i.e.).\n |      \n |          .. versionchanged:: 1.5.0\n |      \n |              Previously was line_terminator, changed for consistency with\n |              read_csv and the standard library 'csv' module.\n |      \n |      chunksize : int or None\n |          Rows to write at a time.\n |      date_format : str, default None\n |          Format string for datetime objects.\n |      doublequote : bool, default True\n |          Control quoting of `quotechar` inside a field.\n |      escapechar : str, default None\n |          String of length 1. Character used to escape `sep` and `quotechar`\n |          when appropriate.\n |      decimal : str, default '.'\n |          Character recognized as decimal separator. E.g. use ',' for\n |          European data.\n |      errors : str, default 'strict'\n |          Specifies how encoding and decoding errors are to be handled.\n |          See the errors argument for :func:`open` for a full list\n |          of options.\n |      \n |          .. versionadded:: 1.1.0\n |      \n |      storage_options : dict, optional\n |          Extra options that make sense for a particular storage connection, e.g.\n |          host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n |          are forwarded to ``urllib.request.Request`` as header options. For other\n |          URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n |          forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n |          details, and for more examples on storage options refer `here\n |          &lt;https://pandas.pydata.org/docs/user_guide/io.html?\n |          highlight=storage_options#reading-writing-remote-files&gt;`_.\n |      \n |          .. versionadded:: 1.2.0\n |      \n |      Returns\n |      -------\n |      None or str\n |          If path_or_buf is None, returns the resulting csv format as a\n |          string. Otherwise returns None.\n |      \n |      See Also\n |      --------\n |      read_csv : Load a CSV file into a DataFrame.\n |      to_excel : Write DataFrame to an Excel file.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; df = pd.DataFrame({'name': ['Raphael', 'Donatello'],\n |      ...                    'mask': ['red', 'purple'],\n |      ...                    'weapon': ['sai', 'bo staff']})\n |      &gt;&gt;&gt; df.to_csv(index=False)\n |      'name,mask,weapon\\nRaphael,red,sai\\nDonatello,purple,bo staff\\n'\n |      \n |      Create 'out.zip' containing 'out.csv'\n |      \n |      &gt;&gt;&gt; compression_opts = dict(method='zip',\n |      ...                         archive_name='out.csv')  # doctest: +SKIP\n |      &gt;&gt;&gt; df.to_csv('out.zip', index=False,\n |      ...           compression=compression_opts)  # doctest: +SKIP\n |      \n |      To write a csv file to a new folder or nested folder you will first\n |      need to create it using either Pathlib or os:\n |      \n |      &gt;&gt;&gt; from pathlib import Path  # doctest: +SKIP\n |      &gt;&gt;&gt; filepath = Path('folder/subfolder/out.csv')  # doctest: +SKIP\n |      &gt;&gt;&gt; filepath.parent.mkdir(parents=True, exist_ok=True)  # doctest: +SKIP\n |      &gt;&gt;&gt; df.to_csv(filepath)  # doctest: +SKIP\n |      \n |      &gt;&gt;&gt; import os  # doctest: +SKIP\n |      &gt;&gt;&gt; os.makedirs('folder/subfolder', exist_ok=True)  # doctest: +SKIP\n |      &gt;&gt;&gt; df.to_csv('folder/subfolder/out.csv')  # doctest: +SKIP\n |  \n |  to_excel(self, excel_writer, sheet_name: 'str' = 'Sheet1', na_rep: 'str' = '', float_format: 'str | None' = None, columns: 'Sequence[Hashable] | None' = None, header: 'Sequence[Hashable] | bool_t' = True, index: 'bool_t' = True, index_label: 'IndexLabel' = None, startrow: 'int' = 0, startcol: 'int' = 0, engine: 'str | None' = None, merge_cells: 'bool_t' = True, inf_rep: 'str' = 'inf', freeze_panes: 'tuple[int, int] | None' = None, storage_options: 'StorageOptions' = None) -&gt; 'None'\n |      Write object to an Excel sheet.\n |      \n |      To write a single object to an Excel .xlsx file it is only necessary to\n |      specify a target file name. To write to multiple sheets it is necessary to\n |      create an `ExcelWriter` object with a target file name, and specify a sheet\n |      in the file to write to.\n |      \n |      Multiple sheets may be written to by specifying unique `sheet_name`.\n |      With all data written to the file it is necessary to save the changes.\n |      Note that creating an `ExcelWriter` object with a file name that already\n |      exists will result in the contents of the existing file being erased.\n |      \n |      Parameters\n |      ----------\n |      excel_writer : path-like, file-like, or ExcelWriter object\n |          File path or existing ExcelWriter.\n |      sheet_name : str, default 'Sheet1'\n |          Name of sheet which will contain DataFrame.\n |      na_rep : str, default ''\n |          Missing data representation.\n |      float_format : str, optional\n |          Format string for floating point numbers. For example\n |          ``float_format=\"%.2f\"`` will format 0.1234 to 0.12.\n |      columns : sequence or list of str, optional\n |          Columns to write.\n |      header : bool or list of str, default True\n |          Write out the column names. If a list of string is given it is\n |          assumed to be aliases for the column names.\n |      index : bool, default True\n |          Write row names (index).\n |      index_label : str or sequence, optional\n |          Column label for index column(s) if desired. If not specified, and\n |          `header` and `index` are True, then the index names are used. A\n |          sequence should be given if the DataFrame uses MultiIndex.\n |      startrow : int, default 0\n |          Upper left cell row to dump data frame.\n |      startcol : int, default 0\n |          Upper left cell column to dump data frame.\n |      engine : str, optional\n |          Write engine to use, 'openpyxl' or 'xlsxwriter'. You can also set this\n |          via the options ``io.excel.xlsx.writer`` or\n |          ``io.excel.xlsm.writer``.\n |      \n |      merge_cells : bool, default True\n |          Write MultiIndex and Hierarchical Rows as merged cells.\n |      inf_rep : str, default 'inf'\n |          Representation for infinity (there is no native representation for\n |          infinity in Excel).\n |      freeze_panes : tuple of int (length 2), optional\n |          Specifies the one-based bottommost row and rightmost column that\n |          is to be frozen.\n |      storage_options : dict, optional\n |          Extra options that make sense for a particular storage connection, e.g.\n |          host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n |          are forwarded to ``urllib.request.Request`` as header options. For other\n |          URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n |          forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n |          details, and for more examples on storage options refer `here\n |          &lt;https://pandas.pydata.org/docs/user_guide/io.html?\n |          highlight=storage_options#reading-writing-remote-files&gt;`_.\n |      \n |          .. versionadded:: 1.2.0\n |      \n |      See Also\n |      --------\n |      to_csv : Write DataFrame to a comma-separated values (csv) file.\n |      ExcelWriter : Class for writing DataFrame objects into excel sheets.\n |      read_excel : Read an Excel file into a pandas DataFrame.\n |      read_csv : Read a comma-separated values (csv) file into DataFrame.\n |      io.formats.style.Styler.to_excel : Add styles to Excel sheet.\n |      \n |      Notes\n |      -----\n |      For compatibility with :meth:`~DataFrame.to_csv`,\n |      to_excel serializes lists and dicts to strings before writing.\n |      \n |      Once a workbook has been saved it is not possible to write further\n |      data without rewriting the whole workbook.\n |      \n |      Examples\n |      --------\n |      \n |      Create, write to and save a workbook:\n |      \n |      &gt;&gt;&gt; df1 = pd.DataFrame([['a', 'b'], ['c', 'd']],\n |      ...                    index=['row 1', 'row 2'],\n |      ...                    columns=['col 1', 'col 2'])\n |      &gt;&gt;&gt; df1.to_excel(\"output.xlsx\")  # doctest: +SKIP\n |      \n |      To specify the sheet name:\n |      \n |      &gt;&gt;&gt; df1.to_excel(\"output.xlsx\",\n |      ...              sheet_name='Sheet_name_1')  # doctest: +SKIP\n |      \n |      If you wish to write to more than one sheet in the workbook, it is\n |      necessary to specify an ExcelWriter object:\n |      \n |      &gt;&gt;&gt; df2 = df1.copy()\n |      &gt;&gt;&gt; with pd.ExcelWriter('output.xlsx') as writer:  # doctest: +SKIP\n |      ...     df1.to_excel(writer, sheet_name='Sheet_name_1')\n |      ...     df2.to_excel(writer, sheet_name='Sheet_name_2')\n |      \n |      ExcelWriter can also be used to append to an existing Excel file:\n |      \n |      &gt;&gt;&gt; with pd.ExcelWriter('output.xlsx',\n |      ...                     mode='a') as writer:  # doctest: +SKIP\n |      ...     df.to_excel(writer, sheet_name='Sheet_name_3')\n |      \n |      To set the library that is used to write the Excel file,\n |      you can pass the `engine` keyword (the default engine is\n |      automatically chosen depending on the file extension):\n |      \n |      &gt;&gt;&gt; df1.to_excel('output1.xlsx', engine='xlsxwriter')  # doctest: +SKIP\n |  \n |  to_hdf(self, path_or_buf: 'FilePath | HDFStore', key: 'str', mode: 'str' = 'a', complevel: 'int | None' = None, complib: 'str | None' = None, append: 'bool_t' = False, format: 'str | None' = None, index: 'bool_t' = True, min_itemsize: 'int | dict[str, int] | None' = None, nan_rep=None, dropna: 'bool_t | None' = None, data_columns: 'Literal[True] | list[str] | None' = None, errors: 'str' = 'strict', encoding: 'str' = 'UTF-8') -&gt; 'None'\n |      Write the contained data to an HDF5 file using HDFStore.\n |      \n |      Hierarchical Data Format (HDF) is self-describing, allowing an\n |      application to interpret the structure and contents of a file with\n |      no outside information. One HDF file can hold a mix of related objects\n |      which can be accessed as a group or as individual objects.\n |      \n |      In order to add another DataFrame or Series to an existing HDF file\n |      please use append mode and a different a key.\n |      \n |      .. warning::\n |      \n |         One can store a subclass of ``DataFrame`` or ``Series`` to HDF5,\n |         but the type of the subclass is lost upon storing.\n |      \n |      For more information see the :ref:`user guide &lt;io.hdf5&gt;`.\n |      \n |      Parameters\n |      ----------\n |      path_or_buf : str or pandas.HDFStore\n |          File path or HDFStore object.\n |      key : str\n |          Identifier for the group in the store.\n |      mode : {'a', 'w', 'r+'}, default 'a'\n |          Mode to open file:\n |      \n |          - 'w': write, a new file is created (an existing file with\n |            the same name would be deleted).\n |          - 'a': append, an existing file is opened for reading and\n |            writing, and if the file does not exist it is created.\n |          - 'r+': similar to 'a', but the file must already exist.\n |      complevel : {0-9}, default None\n |          Specifies a compression level for data.\n |          A value of 0 or None disables compression.\n |      complib : {'zlib', 'lzo', 'bzip2', 'blosc'}, default 'zlib'\n |          Specifies the compression library to be used.\n |          As of v0.20.2 these additional compressors for Blosc are supported\n |          (default if no compressor specified: 'blosc:blosclz'):\n |          {'blosc:blosclz', 'blosc:lz4', 'blosc:lz4hc', 'blosc:snappy',\n |          'blosc:zlib', 'blosc:zstd'}.\n |          Specifying a compression library which is not available issues\n |          a ValueError.\n |      append : bool, default False\n |          For Table formats, append the input data to the existing.\n |      format : {'fixed', 'table', None}, default 'fixed'\n |          Possible values:\n |      \n |          - 'fixed': Fixed format. Fast writing/reading. Not-appendable,\n |            nor searchable.\n |          - 'table': Table format. Write as a PyTables Table structure\n |            which may perform worse but allow more flexible operations\n |            like searching / selecting subsets of the data.\n |          - If None, pd.get_option('io.hdf.default_format') is checked,\n |            followed by fallback to \"fixed\".\n |      index : bool, default True\n |          Write DataFrame index as a column.\n |      min_itemsize : dict or int, optional\n |          Map column names to minimum string sizes for columns.\n |      nan_rep : Any, optional\n |          How to represent null values as str.\n |          Not allowed with append=True.\n |      dropna : bool, default False, optional\n |          Remove missing values.\n |      data_columns : list of columns or True, optional\n |          List of columns to create as indexed data columns for on-disk\n |          queries, or True to use all columns. By default only the axes\n |          of the object are indexed. See\n |          :ref:`Query via data columns&lt;io.hdf5-query-data-columns&gt;`. for\n |          more information.\n |          Applicable only to format='table'.\n |      errors : str, default 'strict'\n |          Specifies how encoding and decoding errors are to be handled.\n |          See the errors argument for :func:`open` for a full list\n |          of options.\n |      encoding : str, default \"UTF-8\"\n |      \n |      See Also\n |      --------\n |      read_hdf : Read from HDF file.\n |      DataFrame.to_orc : Write a DataFrame to the binary orc format.\n |      DataFrame.to_parquet : Write a DataFrame to the binary parquet format.\n |      DataFrame.to_sql : Write to a SQL table.\n |      DataFrame.to_feather : Write out feather-format for DataFrames.\n |      DataFrame.to_csv : Write out to a csv file.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]},\n |      ...                   index=['a', 'b', 'c'])  # doctest: +SKIP\n |      &gt;&gt;&gt; df.to_hdf('data.h5', key='df', mode='w')  # doctest: +SKIP\n |      \n |      We can add another object to the same file:\n |      \n |      &gt;&gt;&gt; s = pd.Series([1, 2, 3, 4])  # doctest: +SKIP\n |      &gt;&gt;&gt; s.to_hdf('data.h5', key='s')  # doctest: +SKIP\n |      \n |      Reading from HDF file:\n |      \n |      &gt;&gt;&gt; pd.read_hdf('data.h5', 'df')  # doctest: +SKIP\n |      A  B\n |      a  1  4\n |      b  2  5\n |      c  3  6\n |      &gt;&gt;&gt; pd.read_hdf('data.h5', 's')  # doctest: +SKIP\n |      0    1\n |      1    2\n |      2    3\n |      3    4\n |      dtype: int64\n |  \n |  to_json(self, path_or_buf: 'FilePath | WriteBuffer[bytes] | WriteBuffer[str] | None' = None, orient: 'str | None' = None, date_format: 'str | None' = None, double_precision: 'int' = 10, force_ascii: 'bool_t' = True, date_unit: 'str' = 'ms', default_handler: 'Callable[[Any], JSONSerializable] | None' = None, lines: 'bool_t' = False, compression: 'CompressionOptions' = 'infer', index: 'bool_t' = True, indent: 'int | None' = None, storage_options: 'StorageOptions' = None, mode: \"Literal['a', 'w']\" = 'w') -&gt; 'str | None'\n |      Convert the object to a JSON string.\n |      \n |      Note NaN's and None will be converted to null and datetime objects\n |      will be converted to UNIX timestamps.\n |      \n |      Parameters\n |      ----------\n |      path_or_buf : str, path object, file-like object, or None, default None\n |          String, path object (implementing os.PathLike[str]), or file-like\n |          object implementing a write() function. If None, the result is\n |          returned as a string.\n |      orient : str\n |          Indication of expected JSON string format.\n |      \n |          * Series:\n |      \n |              - default is 'index'\n |              - allowed values are: {'split', 'records', 'index', 'table'}.\n |      \n |          * DataFrame:\n |      \n |              - default is 'columns'\n |              - allowed values are: {'split', 'records', 'index', 'columns',\n |                'values', 'table'}.\n |      \n |          * The format of the JSON string:\n |      \n |              - 'split' : dict like {'index' -&gt; [index], 'columns' -&gt; [columns],\n |                'data' -&gt; [values]}\n |              - 'records' : list like [{column -&gt; value}, ... , {column -&gt; value}]\n |              - 'index' : dict like {index -&gt; {column -&gt; value}}\n |              - 'columns' : dict like {column -&gt; {index -&gt; value}}\n |              - 'values' : just the values array\n |              - 'table' : dict like {'schema': {schema}, 'data': {data}}\n |      \n |              Describing the data, where data component is like ``orient='records'``.\n |      \n |      date_format : {None, 'epoch', 'iso'}\n |          Type of date conversion. 'epoch' = epoch milliseconds,\n |          'iso' = ISO8601. The default depends on the `orient`. For\n |          ``orient='table'``, the default is 'iso'. For all other orients,\n |          the default is 'epoch'.\n |      double_precision : int, default 10\n |          The number of decimal places to use when encoding\n |          floating point values.\n |      force_ascii : bool, default True\n |          Force encoded string to be ASCII.\n |      date_unit : str, default 'ms' (milliseconds)\n |          The time unit to encode to, governs timestamp and ISO8601\n |          precision.  One of 's', 'ms', 'us', 'ns' for second, millisecond,\n |          microsecond, and nanosecond respectively.\n |      default_handler : callable, default None\n |          Handler to call if object cannot otherwise be converted to a\n |          suitable format for JSON. Should receive a single argument which is\n |          the object to convert and return a serialisable object.\n |      lines : bool, default False\n |          If 'orient' is 'records' write out line-delimited json format. Will\n |          throw ValueError if incorrect 'orient' since others are not\n |          list-like.\n |      compression : str or dict, default 'infer'\n |          For on-the-fly compression of the output data. If 'infer' and 'path_or_buf' is\n |          path-like, then detect compression from the following extensions: '.gz',\n |          '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n |          (otherwise no compression).\n |          Set to ``None`` for no compression.\n |          Can also be a dict with key ``'method'`` set\n |          to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'tar'``} and other\n |          key-value pairs are forwarded to\n |          ``zipfile.ZipFile``, ``gzip.GzipFile``,\n |          ``bz2.BZ2File``, ``zstandard.ZstdCompressor`` or\n |          ``tarfile.TarFile``, respectively.\n |          As an example, the following could be passed for faster compression and to create\n |          a reproducible gzip archive:\n |          ``compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}``.\n |      \n |          .. versionadded:: 1.5.0\n |              Added support for `.tar` files.\n |      \n |          .. versionchanged:: 1.4.0 Zstandard support.\n |      \n |      index : bool, default True\n |          Whether to include the index values in the JSON string. Not\n |          including the index (``index=False``) is only supported when\n |          orient is 'split' or 'table'.\n |      indent : int, optional\n |         Length of whitespace used to indent each record.\n |      \n |      storage_options : dict, optional\n |          Extra options that make sense for a particular storage connection, e.g.\n |          host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n |          are forwarded to ``urllib.request.Request`` as header options. For other\n |          URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n |          forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n |          details, and for more examples on storage options refer `here\n |          &lt;https://pandas.pydata.org/docs/user_guide/io.html?\n |          highlight=storage_options#reading-writing-remote-files&gt;`_.\n |      \n |          .. versionadded:: 1.2.0\n |      \n |      mode : str, default 'w' (writing)\n |          Specify the IO mode for output when supplying a path_or_buf.\n |          Accepted args are 'w' (writing) and 'a' (append) only.\n |          mode='a' is only supported when lines is True and orient is 'records'.\n |      \n |      Returns\n |      -------\n |      None or str\n |          If path_or_buf is None, returns the resulting json format as a\n |          string. Otherwise returns None.\n |      \n |      See Also\n |      --------\n |      read_json : Convert a JSON string to pandas object.\n |      \n |      Notes\n |      -----\n |      The behavior of ``indent=0`` varies from the stdlib, which does not\n |      indent the output but does insert newlines. Currently, ``indent=0``\n |      and the default ``indent=None`` are equivalent in pandas, though this\n |      may change in a future release.\n |      \n |      ``orient='table'`` contains a 'pandas_version' field under 'schema'.\n |      This stores the version of `pandas` used in the latest revision of the\n |      schema.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; from json import loads, dumps\n |      &gt;&gt;&gt; df = pd.DataFrame(\n |      ...     [[\"a\", \"b\"], [\"c\", \"d\"]],\n |      ...     index=[\"row 1\", \"row 2\"],\n |      ...     columns=[\"col 1\", \"col 2\"],\n |      ... )\n |      \n |      &gt;&gt;&gt; result = df.to_json(orient=\"split\")\n |      &gt;&gt;&gt; parsed = loads(result)\n |      &gt;&gt;&gt; dumps(parsed, indent=4)  # doctest: +SKIP\n |      {\n |          \"columns\": [\n |              \"col 1\",\n |              \"col 2\"\n |          ],\n |          \"index\": [\n |              \"row 1\",\n |              \"row 2\"\n |          ],\n |          \"data\": [\n |              [\n |                  \"a\",\n |                  \"b\"\n |              ],\n |              [\n |                  \"c\",\n |                  \"d\"\n |              ]\n |          ]\n |      }\n |      \n |      Encoding/decoding a Dataframe using ``'records'`` formatted JSON.\n |      Note that index labels are not preserved with this encoding.\n |      \n |      &gt;&gt;&gt; result = df.to_json(orient=\"records\")\n |      &gt;&gt;&gt; parsed = loads(result)\n |      &gt;&gt;&gt; dumps(parsed, indent=4)  # doctest: +SKIP\n |      [\n |          {\n |              \"col 1\": \"a\",\n |              \"col 2\": \"b\"\n |          },\n |          {\n |              \"col 1\": \"c\",\n |              \"col 2\": \"d\"\n |          }\n |      ]\n |      \n |      Encoding/decoding a Dataframe using ``'index'`` formatted JSON:\n |      \n |      &gt;&gt;&gt; result = df.to_json(orient=\"index\")\n |      &gt;&gt;&gt; parsed = loads(result)\n |      &gt;&gt;&gt; dumps(parsed, indent=4)  # doctest: +SKIP\n |      {\n |          \"row 1\": {\n |              \"col 1\": \"a\",\n |              \"col 2\": \"b\"\n |          },\n |          \"row 2\": {\n |              \"col 1\": \"c\",\n |              \"col 2\": \"d\"\n |          }\n |      }\n |      \n |      Encoding/decoding a Dataframe using ``'columns'`` formatted JSON:\n |      \n |      &gt;&gt;&gt; result = df.to_json(orient=\"columns\")\n |      &gt;&gt;&gt; parsed = loads(result)\n |      &gt;&gt;&gt; dumps(parsed, indent=4)  # doctest: +SKIP\n |      {\n |          \"col 1\": {\n |              \"row 1\": \"a\",\n |              \"row 2\": \"c\"\n |          },\n |          \"col 2\": {\n |              \"row 1\": \"b\",\n |              \"row 2\": \"d\"\n |          }\n |      }\n |      \n |      Encoding/decoding a Dataframe using ``'values'`` formatted JSON:\n |      \n |      &gt;&gt;&gt; result = df.to_json(orient=\"values\")\n |      &gt;&gt;&gt; parsed = loads(result)\n |      &gt;&gt;&gt; dumps(parsed, indent=4)  # doctest: +SKIP\n |      [\n |          [\n |              \"a\",\n |              \"b\"\n |          ],\n |          [\n |              \"c\",\n |              \"d\"\n |          ]\n |      ]\n |      \n |      Encoding with Table Schema:\n |      \n |      &gt;&gt;&gt; result = df.to_json(orient=\"table\")\n |      &gt;&gt;&gt; parsed = loads(result)\n |      &gt;&gt;&gt; dumps(parsed, indent=4)  # doctest: +SKIP\n |      {\n |          \"schema\": {\n |              \"fields\": [\n |                  {\n |                      \"name\": \"index\",\n |                      \"type\": \"string\"\n |                  },\n |                  {\n |                      \"name\": \"col 1\",\n |                      \"type\": \"string\"\n |                  },\n |                  {\n |                      \"name\": \"col 2\",\n |                      \"type\": \"string\"\n |                  }\n |              ],\n |              \"primaryKey\": [\n |                  \"index\"\n |              ],\n |              \"pandas_version\": \"1.4.0\"\n |          },\n |          \"data\": [\n |              {\n |                  \"index\": \"row 1\",\n |                  \"col 1\": \"a\",\n |                  \"col 2\": \"b\"\n |              },\n |              {\n |                  \"index\": \"row 2\",\n |                  \"col 1\": \"c\",\n |                  \"col 2\": \"d\"\n |              }\n |          ]\n |      }\n |  \n |  to_latex(self, buf: 'FilePath | WriteBuffer[str] | None' = None, columns: 'Sequence[Hashable] | None' = None, header: 'bool_t | Sequence[str]' = True, index: 'bool_t' = True, na_rep: 'str' = 'NaN', formatters: 'FormattersType | None' = None, float_format: 'FloatFormatType | None' = None, sparsify: 'bool_t | None' = None, index_names: 'bool_t' = True, bold_rows: 'bool_t' = False, column_format: 'str | None' = None, longtable: 'bool_t | None' = None, escape: 'bool_t | None' = None, encoding: 'str | None' = None, decimal: 'str' = '.', multicolumn: 'bool_t | None' = None, multicolumn_format: 'str | None' = None, multirow: 'bool_t | None' = None, caption: 'str | tuple[str, str] | None' = None, label: 'str | None' = None, position: 'str | None' = None) -&gt; 'str | None'\n |      Render object to a LaTeX tabular, longtable, or nested table.\n |      \n |      Requires ``\\usepackage{{booktabs}}``.  The output can be copy/pasted\n |      into a main LaTeX document or read from an external file\n |      with ``\\input{{table.tex}}``.\n |      \n |      .. versionchanged:: 1.2.0\n |         Added position argument, changed meaning of caption argument.\n |      \n |      .. versionchanged:: 2.0.0\n |         Refactored to use the Styler implementation via jinja2 templating.\n |      \n |      Parameters\n |      ----------\n |      buf : str, Path or StringIO-like, optional, default None\n |          Buffer to write to. If None, the output is returned as a string.\n |      columns : list of label, optional\n |          The subset of columns to write. Writes all columns by default.\n |      header : bool or list of str, default True\n |          Write out the column names. If a list of strings is given,\n |          it is assumed to be aliases for the column names.\n |      index : bool, default True\n |          Write row names (index).\n |      na_rep : str, default 'NaN'\n |          Missing data representation.\n |      formatters : list of functions or dict of {{str: function}}, optional\n |          Formatter functions to apply to columns' elements by position or\n |          name. The result of each function must be a unicode string.\n |          List must be of length equal to the number of columns.\n |      float_format : one-parameter function or str, optional, default None\n |          Formatter for floating point numbers. For example\n |          ``float_format=\"%.2f\"`` and ``float_format=\"{{:0.2f}}\".format`` will\n |          both result in 0.1234 being formatted as 0.12.\n |      sparsify : bool, optional\n |          Set to False for a DataFrame with a hierarchical index to print\n |          every multiindex key at each row. By default, the value will be\n |          read from the config module.\n |      index_names : bool, default True\n |          Prints the names of the indexes.\n |      bold_rows : bool, default False\n |          Make the row labels bold in the output.\n |      column_format : str, optional\n |          The columns format as specified in `LaTeX table format\n |          &lt;https://en.wikibooks.org/wiki/LaTeX/Tables&gt;`__ e.g. 'rcl' for 3\n |          columns. By default, 'l' will be used for all columns except\n |          columns of numbers, which default to 'r'.\n |      longtable : bool, optional\n |          Use a longtable environment instead of tabular. Requires\n |          adding a \\usepackage{{longtable}} to your LaTeX preamble.\n |          By default, the value will be read from the pandas config\n |          module, and set to `True` if the option ``styler.latex.environment`` is\n |          `\"longtable\"`.\n |      \n |          .. versionchanged:: 2.0.0\n |             The pandas option affecting this argument has changed.\n |      escape : bool, optional\n |          By default, the value will be read from the pandas config\n |          module and set to `True` if the option ``styler.format.escape`` is\n |          `\"latex\"`. When set to False prevents from escaping latex special\n |          characters in column names.\n |      \n |          .. versionchanged:: 2.0.0\n |             The pandas option affecting this argument has changed, as has the\n |             default value to `False`.\n |      encoding : str, optional\n |          A string representing the encoding to use in the output file,\n |          defaults to 'utf-8'.\n |      decimal : str, default '.'\n |          Character recognized as decimal separator, e.g. ',' in Europe.\n |      multicolumn : bool, default True\n |          Use \\multicolumn to enhance MultiIndex columns.\n |          The default will be read from the config module, and is set\n |          as the option ``styler.sparse.columns``.\n |      \n |          .. versionchanged:: 2.0.0\n |             The pandas option affecting this argument has changed.\n |      multicolumn_format : str, default 'r'\n |          The alignment for multicolumns, similar to `column_format`\n |          The default will be read from the config module, and is set as the option\n |          ``styler.latex.multicol_align``.\n |      \n |          .. versionchanged:: 2.0.0\n |             The pandas option affecting this argument has changed, as has the\n |             default value to \"r\".\n |      multirow : bool, default True\n |          Use \\multirow to enhance MultiIndex rows. Requires adding a\n |          \\usepackage{{multirow}} to your LaTeX preamble. Will print\n |          centered labels (instead of top-aligned) across the contained\n |          rows, separating groups via clines. The default will be read\n |          from the pandas config module, and is set as the option\n |          ``styler.sparse.index``.\n |      \n |          .. versionchanged:: 2.0.0\n |             The pandas option affecting this argument has changed, as has the\n |             default value to `True`.\n |      caption : str or tuple, optional\n |          Tuple (full_caption, short_caption),\n |          which results in ``\\caption[short_caption]{{full_caption}}``;\n |          if a single string is passed, no short caption will be set.\n |      \n |          .. versionchanged:: 1.2.0\n |             Optionally allow caption to be a tuple ``(full_caption, short_caption)``.\n |      \n |      label : str, optional\n |          The LaTeX label to be placed inside ``\\label{{}}`` in the output.\n |          This is used with ``\\ref{{}}`` in the main ``.tex`` file.\n |      \n |      position : str, optional\n |          The LaTeX positional argument for tables, to be placed after\n |          ``\\begin{{}}`` in the output.\n |      \n |          .. versionadded:: 1.2.0\n |      \n |      Returns\n |      -------\n |      str or None\n |          If buf is None, returns the result as a string. Otherwise returns None.\n |      \n |      See Also\n |      --------\n |      io.formats.style.Styler.to_latex : Render a DataFrame to LaTeX\n |          with conditional formatting.\n |      DataFrame.to_string : Render a DataFrame to a console-friendly\n |          tabular output.\n |      DataFrame.to_html : Render a DataFrame as an HTML table.\n |      \n |      Notes\n |      -----\n |      As of v2.0.0 this method has changed to use the Styler implementation as\n |      part of :meth:`.Styler.to_latex` via ``jinja2`` templating. This means\n |      that ``jinja2`` is a requirement, and needs to be installed, for this method\n |      to function. It is advised that users switch to using Styler, since that\n |      implementation is more frequently updated and contains much more\n |      flexibility with the output.\n |      \n |      Examples\n |      --------\n |      Convert a general DataFrame to LaTeX with formatting:\n |      \n |      &gt;&gt;&gt; df = pd.DataFrame(dict(name=['Raphael', 'Donatello'],\n |      ...                        age=[26, 45],\n |      ...                        height=[181.23, 177.65]))\n |      &gt;&gt;&gt; print(df.to_latex(index=False,\n |      ...                   formatters={\"name\": str.upper},\n |      ...                   float_format=\"{:.1f}\".format,\n |      ... ))  # doctest: +SKIP\n |      \\begin{tabular}{lrr}\n |      \\toprule\n |      name & age & height \\\\\n |      \\midrule\n |      RAPHAEL & 26 & 181.2 \\\\\n |      DONATELLO & 45 & 177.7 \\\\\n |      \\bottomrule\n |      \\end{tabular}\n |  \n |  to_pickle(self, path: 'FilePath | WriteBuffer[bytes]', compression: 'CompressionOptions' = 'infer', protocol: 'int' = 5, storage_options: 'StorageOptions' = None) -&gt; 'None'\n |      Pickle (serialize) object to file.\n |      \n |      Parameters\n |      ----------\n |      path : str, path object, or file-like object\n |          String, path object (implementing ``os.PathLike[str]``), or file-like\n |          object implementing a binary ``write()`` function. File path where\n |          the pickled object will be stored.\n |      compression : str or dict, default 'infer'\n |          For on-the-fly compression of the output data. If 'infer' and 'path' is\n |          path-like, then detect compression from the following extensions: '.gz',\n |          '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n |          (otherwise no compression).\n |          Set to ``None`` for no compression.\n |          Can also be a dict with key ``'method'`` set\n |          to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'tar'``} and other\n |          key-value pairs are forwarded to\n |          ``zipfile.ZipFile``, ``gzip.GzipFile``,\n |          ``bz2.BZ2File``, ``zstandard.ZstdCompressor`` or\n |          ``tarfile.TarFile``, respectively.\n |          As an example, the following could be passed for faster compression and to create\n |          a reproducible gzip archive:\n |          ``compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}``.\n |      \n |          .. versionadded:: 1.5.0\n |              Added support for `.tar` files.\n |      protocol : int\n |          Int which indicates which protocol should be used by the pickler,\n |          default HIGHEST_PROTOCOL (see [1]_ paragraph 12.1.2). The possible\n |          values are 0, 1, 2, 3, 4, 5. A negative value for the protocol\n |          parameter is equivalent to setting its value to HIGHEST_PROTOCOL.\n |      \n |          .. [1] https://docs.python.org/3/library/pickle.html.\n |      \n |      storage_options : dict, optional\n |          Extra options that make sense for a particular storage connection, e.g.\n |          host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n |          are forwarded to ``urllib.request.Request`` as header options. For other\n |          URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n |          forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n |          details, and for more examples on storage options refer `here\n |          &lt;https://pandas.pydata.org/docs/user_guide/io.html?\n |          highlight=storage_options#reading-writing-remote-files&gt;`_.\n |      \n |          .. versionadded:: 1.2.0\n |      \n |      See Also\n |      --------\n |      read_pickle : Load pickled pandas object (or any object) from file.\n |      DataFrame.to_hdf : Write DataFrame to an HDF5 file.\n |      DataFrame.to_sql : Write DataFrame to a SQL database.\n |      DataFrame.to_parquet : Write a DataFrame to the binary parquet format.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; original_df = pd.DataFrame({\"foo\": range(5), \"bar\": range(5, 10)})  # doctest: +SKIP\n |      &gt;&gt;&gt; original_df  # doctest: +SKIP\n |         foo  bar\n |      0    0    5\n |      1    1    6\n |      2    2    7\n |      3    3    8\n |      4    4    9\n |      &gt;&gt;&gt; original_df.to_pickle(\"./dummy.pkl\")  # doctest: +SKIP\n |      \n |      &gt;&gt;&gt; unpickled_df = pd.read_pickle(\"./dummy.pkl\")  # doctest: +SKIP\n |      &gt;&gt;&gt; unpickled_df  # doctest: +SKIP\n |         foo  bar\n |      0    0    5\n |      1    1    6\n |      2    2    7\n |      3    3    8\n |      4    4    9\n |  \n |  to_sql(self, name: 'str', con, schema: 'str | None' = None, if_exists: \"Literal['fail', 'replace', 'append']\" = 'fail', index: 'bool_t' = True, index_label: 'IndexLabel' = None, chunksize: 'int | None' = None, dtype: 'DtypeArg | None' = None, method: 'str | None' = None) -&gt; 'int | None'\n |      Write records stored in a DataFrame to a SQL database.\n |      \n |      Databases supported by SQLAlchemy [1]_ are supported. Tables can be\n |      newly created, appended to, or overwritten.\n |      \n |      Parameters\n |      ----------\n |      name : str\n |          Name of SQL table.\n |      con : sqlalchemy.engine.(Engine or Connection) or sqlite3.Connection\n |          Using SQLAlchemy makes it possible to use any DB supported by that\n |          library. Legacy support is provided for sqlite3.Connection objects. The user\n |          is responsible for engine disposal and connection closure for the SQLAlchemy\n |          connectable. See `here                 &lt;https://docs.sqlalchemy.org/en/20/core/connections.html&gt;`_.\n |          If passing a sqlalchemy.engine.Connection which is already in a transaction,\n |          the transaction will not be committed.  If passing a sqlite3.Connection,\n |          it will not be possible to roll back the record insertion.\n |      \n |      schema : str, optional\n |          Specify the schema (if database flavor supports this). If None, use\n |          default schema.\n |      if_exists : {'fail', 'replace', 'append'}, default 'fail'\n |          How to behave if the table already exists.\n |      \n |          * fail: Raise a ValueError.\n |          * replace: Drop the table before inserting new values.\n |          * append: Insert new values to the existing table.\n |      \n |      index : bool, default True\n |          Write DataFrame index as a column. Uses `index_label` as the column\n |          name in the table.\n |      index_label : str or sequence, default None\n |          Column label for index column(s). If None is given (default) and\n |          `index` is True, then the index names are used.\n |          A sequence should be given if the DataFrame uses MultiIndex.\n |      chunksize : int, optional\n |          Specify the number of rows in each batch to be written at a time.\n |          By default, all rows will be written at once.\n |      dtype : dict or scalar, optional\n |          Specifying the datatype for columns. If a dictionary is used, the\n |          keys should be the column names and the values should be the\n |          SQLAlchemy types or strings for the sqlite3 legacy mode. If a\n |          scalar is provided, it will be applied to all columns.\n |      method : {None, 'multi', callable}, optional\n |          Controls the SQL insertion clause used:\n |      \n |          * None : Uses standard SQL ``INSERT`` clause (one per row).\n |          * 'multi': Pass multiple values in a single ``INSERT`` clause.\n |          * callable with signature ``(pd_table, conn, keys, data_iter)``.\n |      \n |          Details and a sample callable implementation can be found in the\n |          section :ref:`insert method &lt;io.sql.method&gt;`.\n |      \n |      Returns\n |      -------\n |      None or int\n |          Number of rows affected by to_sql. None is returned if the callable\n |          passed into ``method`` does not return an integer number of rows.\n |      \n |          The number of returned rows affected is the sum of the ``rowcount``\n |          attribute of ``sqlite3.Cursor`` or SQLAlchemy connectable which may not\n |          reflect the exact number of written rows as stipulated in the\n |          `sqlite3 &lt;https://docs.python.org/3/library/sqlite3.html#sqlite3.Cursor.rowcount&gt;`__ or\n |          `SQLAlchemy &lt;https://docs.sqlalchemy.org/en/20/core/connections.html#sqlalchemy.engine.CursorResult.rowcount&gt;`__.\n |      \n |          .. versionadded:: 1.4.0\n |      \n |      Raises\n |      ------\n |      ValueError\n |          When the table already exists and `if_exists` is 'fail' (the\n |          default).\n |      \n |      See Also\n |      --------\n |      read_sql : Read a DataFrame from a table.\n |      \n |      Notes\n |      -----\n |      Timezone aware datetime columns will be written as\n |      ``Timestamp with timezone`` type with SQLAlchemy if supported by the\n |      database. Otherwise, the datetimes will be stored as timezone unaware\n |      timestamps local to the original timezone.\n |      \n |      References\n |      ----------\n |      .. [1] https://docs.sqlalchemy.org\n |      .. [2] https://www.python.org/dev/peps/pep-0249/\n |      \n |      Examples\n |      --------\n |      Create an in-memory SQLite database.\n |      \n |      &gt;&gt;&gt; from sqlalchemy import create_engine\n |      &gt;&gt;&gt; engine = create_engine('sqlite://', echo=False)\n |      \n |      Create a table from scratch with 3 rows.\n |      \n |      &gt;&gt;&gt; df = pd.DataFrame({'name' : ['User 1', 'User 2', 'User 3']})\n |      &gt;&gt;&gt; df\n |           name\n |      0  User 1\n |      1  User 2\n |      2  User 3\n |      \n |      &gt;&gt;&gt; df.to_sql('users', con=engine)\n |      3\n |      &gt;&gt;&gt; from sqlalchemy import text\n |      &gt;&gt;&gt; with engine.connect() as conn:\n |      ...    conn.execute(text(\"SELECT * FROM users\")).fetchall()\n |      [(0, 'User 1'), (1, 'User 2'), (2, 'User 3')]\n |      \n |      An `sqlalchemy.engine.Connection` can also be passed to `con`:\n |      \n |      &gt;&gt;&gt; with engine.begin() as connection:\n |      ...     df1 = pd.DataFrame({'name' : ['User 4', 'User 5']})\n |      ...     df1.to_sql('users', con=connection, if_exists='append')\n |      2\n |      \n |      This is allowed to support operations that require that the same\n |      DBAPI connection is used for the entire operation.\n |      \n |      &gt;&gt;&gt; df2 = pd.DataFrame({'name' : ['User 6', 'User 7']})\n |      &gt;&gt;&gt; df2.to_sql('users', con=engine, if_exists='append')\n |      2\n |      &gt;&gt;&gt; with engine.connect() as conn:\n |      ...    conn.execute(text(\"SELECT * FROM users\")).fetchall()\n |      [(0, 'User 1'), (1, 'User 2'), (2, 'User 3'),\n |       (0, 'User 4'), (1, 'User 5'), (0, 'User 6'),\n |       (1, 'User 7')]\n |      \n |      Overwrite the table with just ``df2``.\n |      \n |      &gt;&gt;&gt; df2.to_sql('users', con=engine, if_exists='replace',\n |      ...            index_label='id')\n |      2\n |      &gt;&gt;&gt; with engine.connect() as conn:\n |      ...    conn.execute(text(\"SELECT * FROM users\")).fetchall()\n |      [(0, 'User 6'), (1, 'User 7')]\n |      \n |      Specify the dtype (especially useful for integers with missing values).\n |      Notice that while pandas is forced to store the data as floating point,\n |      the database supports nullable integers. When fetching the data with\n |      Python, we get back integer scalars.\n |      \n |      &gt;&gt;&gt; df = pd.DataFrame({\"A\": [1, None, 2]})\n |      &gt;&gt;&gt; df\n |           A\n |      0  1.0\n |      1  NaN\n |      2  2.0\n |      \n |      &gt;&gt;&gt; from sqlalchemy.types import Integer\n |      &gt;&gt;&gt; df.to_sql('integers', con=engine, index=False,\n |      ...           dtype={\"A\": Integer()})\n |      3\n |      \n |      &gt;&gt;&gt; with engine.connect() as conn:\n |      ...   conn.execute(text(\"SELECT * FROM integers\")).fetchall()\n |      [(1,), (None,), (2,)]\n |  \n |  to_xarray(self)\n |      Return an xarray object from the pandas object.\n |      \n |      Returns\n |      -------\n |      xarray.DataArray or xarray.Dataset\n |          Data in the pandas structure converted to Dataset if the object is\n |          a DataFrame, or a DataArray if the object is a Series.\n |      \n |      See Also\n |      --------\n |      DataFrame.to_hdf : Write DataFrame to an HDF5 file.\n |      DataFrame.to_parquet : Write a DataFrame to the binary parquet format.\n |      \n |      Notes\n |      -----\n |      See the `xarray docs &lt;https://xarray.pydata.org/en/stable/&gt;`__\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; df = pd.DataFrame([('falcon', 'bird', 389.0, 2),\n |      ...                    ('parrot', 'bird', 24.0, 2),\n |      ...                    ('lion', 'mammal', 80.5, 4),\n |      ...                    ('monkey', 'mammal', np.nan, 4)],\n |      ...                   columns=['name', 'class', 'max_speed',\n |      ...                            'num_legs'])\n |      &gt;&gt;&gt; df\n |           name   class  max_speed  num_legs\n |      0  falcon    bird      389.0         2\n |      1  parrot    bird       24.0         2\n |      2    lion  mammal       80.5         4\n |      3  monkey  mammal        NaN         4\n |      \n |      &gt;&gt;&gt; df.to_xarray()\n |      &lt;xarray.Dataset&gt;\n |      Dimensions:    (index: 4)\n |      Coordinates:\n |        * index      (index) int64 0 1 2 3\n |      Data variables:\n |          name       (index) object 'falcon' 'parrot' 'lion' 'monkey'\n |          class      (index) object 'bird' 'bird' 'mammal' 'mammal'\n |          max_speed  (index) float64 389.0 24.0 80.5 nan\n |          num_legs   (index) int64 2 2 4 4\n |      \n |      &gt;&gt;&gt; df['max_speed'].to_xarray()\n |      &lt;xarray.DataArray 'max_speed' (index: 4)&gt;\n |      array([389. ,  24. ,  80.5,   nan])\n |      Coordinates:\n |        * index    (index) int64 0 1 2 3\n |      \n |      &gt;&gt;&gt; dates = pd.to_datetime(['2018-01-01', '2018-01-01',\n |      ...                         '2018-01-02', '2018-01-02'])\n |      &gt;&gt;&gt; df_multiindex = pd.DataFrame({'date': dates,\n |      ...                               'animal': ['falcon', 'parrot',\n |      ...                                          'falcon', 'parrot'],\n |      ...                               'speed': [350, 18, 361, 15]})\n |      &gt;&gt;&gt; df_multiindex = df_multiindex.set_index(['date', 'animal'])\n |      \n |      &gt;&gt;&gt; df_multiindex\n |                         speed\n |      date       animal\n |      2018-01-01 falcon    350\n |                 parrot     18\n |      2018-01-02 falcon    361\n |                 parrot     15\n |      \n |      &gt;&gt;&gt; df_multiindex.to_xarray()\n |      &lt;xarray.Dataset&gt;\n |      Dimensions:  (date: 2, animal: 2)\n |      Coordinates:\n |        * date     (date) datetime64[ns] 2018-01-01 2018-01-02\n |        * animal   (animal) object 'falcon' 'parrot'\n |      Data variables:\n |          speed    (date, animal) int64 350 18 361 15\n |  \n |  truncate(self: 'NDFrameT', before=None, after=None, axis: 'Axis | None' = None, copy: 'bool_t | None' = None) -&gt; 'NDFrameT'\n |      Truncate a Series or DataFrame before and after some index value.\n |      \n |      This is a useful shorthand for boolean indexing based on index\n |      values above or below certain thresholds.\n |      \n |      Parameters\n |      ----------\n |      before : date, str, int\n |          Truncate all rows before this index value.\n |      after : date, str, int\n |          Truncate all rows after this index value.\n |      axis : {0 or 'index', 1 or 'columns'}, optional\n |          Axis to truncate. Truncates the index (rows) by default.\n |          For `Series` this parameter is unused and defaults to 0.\n |      copy : bool, default is True,\n |          Return a copy of the truncated section.\n |      \n |      Returns\n |      -------\n |      type of caller\n |          The truncated Series or DataFrame.\n |      \n |      See Also\n |      --------\n |      DataFrame.loc : Select a subset of a DataFrame by label.\n |      DataFrame.iloc : Select a subset of a DataFrame by position.\n |      \n |      Notes\n |      -----\n |      If the index being truncated contains only datetime values,\n |      `before` and `after` may be specified as strings instead of\n |      Timestamps.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; df = pd.DataFrame({'A': ['a', 'b', 'c', 'd', 'e'],\n |      ...                    'B': ['f', 'g', 'h', 'i', 'j'],\n |      ...                    'C': ['k', 'l', 'm', 'n', 'o']},\n |      ...                   index=[1, 2, 3, 4, 5])\n |      &gt;&gt;&gt; df\n |         A  B  C\n |      1  a  f  k\n |      2  b  g  l\n |      3  c  h  m\n |      4  d  i  n\n |      5  e  j  o\n |      \n |      &gt;&gt;&gt; df.truncate(before=2, after=4)\n |         A  B  C\n |      2  b  g  l\n |      3  c  h  m\n |      4  d  i  n\n |      \n |      The columns of a DataFrame can be truncated.\n |      \n |      &gt;&gt;&gt; df.truncate(before=\"A\", after=\"B\", axis=\"columns\")\n |         A  B\n |      1  a  f\n |      2  b  g\n |      3  c  h\n |      4  d  i\n |      5  e  j\n |      \n |      For Series, only rows can be truncated.\n |      \n |      &gt;&gt;&gt; df['A'].truncate(before=2, after=4)\n |      2    b\n |      3    c\n |      4    d\n |      Name: A, dtype: object\n |      \n |      The index values in ``truncate`` can be datetimes or string\n |      dates.\n |      \n |      &gt;&gt;&gt; dates = pd.date_range('2016-01-01', '2016-02-01', freq='s')\n |      &gt;&gt;&gt; df = pd.DataFrame(index=dates, data={'A': 1})\n |      &gt;&gt;&gt; df.tail()\n |                           A\n |      2016-01-31 23:59:56  1\n |      2016-01-31 23:59:57  1\n |      2016-01-31 23:59:58  1\n |      2016-01-31 23:59:59  1\n |      2016-02-01 00:00:00  1\n |      \n |      &gt;&gt;&gt; df.truncate(before=pd.Timestamp('2016-01-05'),\n |      ...             after=pd.Timestamp('2016-01-10')).tail()\n |                           A\n |      2016-01-09 23:59:56  1\n |      2016-01-09 23:59:57  1\n |      2016-01-09 23:59:58  1\n |      2016-01-09 23:59:59  1\n |      2016-01-10 00:00:00  1\n |      \n |      Because the index is a DatetimeIndex containing only dates, we can\n |      specify `before` and `after` as strings. They will be coerced to\n |      Timestamps before truncation.\n |      \n |      &gt;&gt;&gt; df.truncate('2016-01-05', '2016-01-10').tail()\n |                           A\n |      2016-01-09 23:59:56  1\n |      2016-01-09 23:59:57  1\n |      2016-01-09 23:59:58  1\n |      2016-01-09 23:59:59  1\n |      2016-01-10 00:00:00  1\n |      \n |      Note that ``truncate`` assumes a 0 value for any unspecified time\n |      component (midnight). This differs from partial string slicing, which\n |      returns any partially matching dates.\n |      \n |      &gt;&gt;&gt; df.loc['2016-01-05':'2016-01-10', :].tail()\n |                           A\n |      2016-01-10 23:59:55  1\n |      2016-01-10 23:59:56  1\n |      2016-01-10 23:59:57  1\n |      2016-01-10 23:59:58  1\n |      2016-01-10 23:59:59  1\n |  \n |  tz_convert(self: 'NDFrameT', tz, axis: 'Axis' = 0, level=None, copy: 'bool_t | None' = None) -&gt; 'NDFrameT'\n |      Convert tz-aware axis to target time zone.\n |      \n |      Parameters\n |      ----------\n |      tz : str or tzinfo object or None\n |          Target time zone. Passing ``None`` will convert to\n |          UTC and remove the timezone information.\n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |          The axis to convert\n |      level : int, str, default None\n |          If axis is a MultiIndex, convert a specific level. Otherwise\n |          must be None.\n |      copy : bool, default True\n |          Also make a copy of the underlying data.\n |      \n |      Returns\n |      -------\n |      Series/DataFrame\n |          Object with time zone converted axis.\n |      \n |      Raises\n |      ------\n |      TypeError\n |          If the axis is tz-naive.\n |      \n |      Examples\n |      --------\n |      Change to another time zone:\n |      \n |      &gt;&gt;&gt; s = pd.Series(\n |      ...     [1],\n |      ...     index=pd.DatetimeIndex(['2018-09-15 01:30:00+02:00']),\n |      ... )\n |      &gt;&gt;&gt; s.tz_convert('Asia/Shanghai')\n |      2018-09-15 07:30:00+08:00    1\n |      dtype: int64\n |      \n |      Pass None to convert to UTC and get a tz-naive index:\n |      \n |      &gt;&gt;&gt; s = pd.Series([1],\n |      ...     index=pd.DatetimeIndex(['2018-09-15 01:30:00+02:00']))\n |      &gt;&gt;&gt; s.tz_convert(None)\n |      2018-09-14 23:30:00    1\n |      dtype: int64\n |  \n |  tz_localize(self: 'NDFrameT', tz, axis: 'Axis' = 0, level=None, copy: 'bool_t | None' = None, ambiguous: 'TimeAmbiguous' = 'raise', nonexistent: 'TimeNonexistent' = 'raise') -&gt; 'NDFrameT'\n |      Localize tz-naive index of a Series or DataFrame to target time zone.\n |      \n |      This operation localizes the Index. To localize the values in a\n |      timezone-naive Series, use :meth:`Series.dt.tz_localize`.\n |      \n |      Parameters\n |      ----------\n |      tz : str or tzinfo or None\n |          Time zone to localize. Passing ``None`` will remove the\n |          time zone information and preserve local time.\n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |          The axis to localize\n |      level : int, str, default None\n |          If axis ia a MultiIndex, localize a specific level. Otherwise\n |          must be None.\n |      copy : bool, default True\n |          Also make a copy of the underlying data.\n |      ambiguous : 'infer', bool-ndarray, 'NaT', default 'raise'\n |          When clocks moved backward due to DST, ambiguous times may arise.\n |          For example in Central European Time (UTC+01), when going from\n |          03:00 DST to 02:00 non-DST, 02:30:00 local time occurs both at\n |          00:30:00 UTC and at 01:30:00 UTC. In such a situation, the\n |          `ambiguous` parameter dictates how ambiguous times should be\n |          handled.\n |      \n |          - 'infer' will attempt to infer fall dst-transition hours based on\n |            order\n |          - bool-ndarray where True signifies a DST time, False designates\n |            a non-DST time (note that this flag is only applicable for\n |            ambiguous times)\n |          - 'NaT' will return NaT where there are ambiguous times\n |          - 'raise' will raise an AmbiguousTimeError if there are ambiguous\n |            times.\n |      nonexistent : str, default 'raise'\n |          A nonexistent time does not exist in a particular timezone\n |          where clocks moved forward due to DST. Valid values are:\n |      \n |          - 'shift_forward' will shift the nonexistent time forward to the\n |            closest existing time\n |          - 'shift_backward' will shift the nonexistent time backward to the\n |            closest existing time\n |          - 'NaT' will return NaT where there are nonexistent times\n |          - timedelta objects will shift nonexistent times by the timedelta\n |          - 'raise' will raise an NonExistentTimeError if there are\n |            nonexistent times.\n |      \n |      Returns\n |      -------\n |      Series/DataFrame\n |          Same type as the input.\n |      \n |      Raises\n |      ------\n |      TypeError\n |          If the TimeSeries is tz-aware and tz is not None.\n |      \n |      Examples\n |      --------\n |      Localize local times:\n |      \n |      &gt;&gt;&gt; s = pd.Series(\n |      ...     [1],\n |      ...     index=pd.DatetimeIndex(['2018-09-15 01:30:00']),\n |      ... )\n |      &gt;&gt;&gt; s.tz_localize('CET')\n |      2018-09-15 01:30:00+02:00    1\n |      dtype: int64\n |      \n |      Pass None to convert to tz-naive index and preserve local time:\n |      \n |      &gt;&gt;&gt; s = pd.Series([1],\n |      ...     index=pd.DatetimeIndex(['2018-09-15 01:30:00+02:00']))\n |      &gt;&gt;&gt; s.tz_localize(None)\n |      2018-09-15 01:30:00    1\n |      dtype: int64\n |      \n |      Be careful with DST changes. When there is sequential data, pandas\n |      can infer the DST time:\n |      \n |      &gt;&gt;&gt; s = pd.Series(range(7),\n |      ...               index=pd.DatetimeIndex(['2018-10-28 01:30:00',\n |      ...                                       '2018-10-28 02:00:00',\n |      ...                                       '2018-10-28 02:30:00',\n |      ...                                       '2018-10-28 02:00:00',\n |      ...                                       '2018-10-28 02:30:00',\n |      ...                                       '2018-10-28 03:00:00',\n |      ...                                       '2018-10-28 03:30:00']))\n |      &gt;&gt;&gt; s.tz_localize('CET', ambiguous='infer')\n |      2018-10-28 01:30:00+02:00    0\n |      2018-10-28 02:00:00+02:00    1\n |      2018-10-28 02:30:00+02:00    2\n |      2018-10-28 02:00:00+01:00    3\n |      2018-10-28 02:30:00+01:00    4\n |      2018-10-28 03:00:00+01:00    5\n |      2018-10-28 03:30:00+01:00    6\n |      dtype: int64\n |      \n |      In some cases, inferring the DST is impossible. In such cases, you can\n |      pass an ndarray to the ambiguous parameter to set the DST explicitly\n |      \n |      &gt;&gt;&gt; s = pd.Series(range(3),\n |      ...               index=pd.DatetimeIndex(['2018-10-28 01:20:00',\n |      ...                                       '2018-10-28 02:36:00',\n |      ...                                       '2018-10-28 03:46:00']))\n |      &gt;&gt;&gt; s.tz_localize('CET', ambiguous=np.array([True, True, False]))\n |      2018-10-28 01:20:00+02:00    0\n |      2018-10-28 02:36:00+02:00    1\n |      2018-10-28 03:46:00+01:00    2\n |      dtype: int64\n |      \n |      If the DST transition causes nonexistent times, you can shift these\n |      dates forward or backward with a timedelta object or `'shift_forward'`\n |      or `'shift_backward'`.\n |      \n |      &gt;&gt;&gt; s = pd.Series(range(2),\n |      ...               index=pd.DatetimeIndex(['2015-03-29 02:30:00',\n |      ...                                       '2015-03-29 03:30:00']))\n |      &gt;&gt;&gt; s.tz_localize('Europe/Warsaw', nonexistent='shift_forward')\n |      2015-03-29 03:00:00+02:00    0\n |      2015-03-29 03:30:00+02:00    1\n |      dtype: int64\n |      &gt;&gt;&gt; s.tz_localize('Europe/Warsaw', nonexistent='shift_backward')\n |      2015-03-29 01:59:59.999999999+01:00    0\n |      2015-03-29 03:30:00+02:00              1\n |      dtype: int64\n |      &gt;&gt;&gt; s.tz_localize('Europe/Warsaw', nonexistent=pd.Timedelta('1H'))\n |      2015-03-29 03:30:00+02:00    0\n |      2015-03-29 03:30:00+02:00    1\n |      dtype: int64\n |  \n |  xs(self: 'NDFrameT', key: 'IndexLabel', axis: 'Axis' = 0, level: 'IndexLabel' = None, drop_level: 'bool_t' = True) -&gt; 'NDFrameT'\n |      Return cross-section from the Series/DataFrame.\n |      \n |      This method takes a `key` argument to select data at a particular\n |      level of a MultiIndex.\n |      \n |      Parameters\n |      ----------\n |      key : label or tuple of label\n |          Label contained in the index, or partially in a MultiIndex.\n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |          Axis to retrieve cross-section on.\n |      level : object, defaults to first n levels (n=1 or len(key))\n |          In case of a key partially contained in a MultiIndex, indicate\n |          which levels are used. Levels can be referred by label or position.\n |      drop_level : bool, default True\n |          If False, returns object with same levels as self.\n |      \n |      Returns\n |      -------\n |      Series or DataFrame\n |          Cross-section from the original Series or DataFrame\n |          corresponding to the selected index levels.\n |      \n |      See Also\n |      --------\n |      DataFrame.loc : Access a group of rows and columns\n |          by label(s) or a boolean array.\n |      DataFrame.iloc : Purely integer-location based indexing\n |          for selection by position.\n |      \n |      Notes\n |      -----\n |      `xs` can not be used to set values.\n |      \n |      MultiIndex Slicers is a generic way to get/set values on\n |      any level or levels.\n |      It is a superset of `xs` functionality, see\n |      :ref:`MultiIndex Slicers &lt;advanced.mi_slicers&gt;`.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; d = {'num_legs': [4, 4, 2, 2],\n |      ...      'num_wings': [0, 0, 2, 2],\n |      ...      'class': ['mammal', 'mammal', 'mammal', 'bird'],\n |      ...      'animal': ['cat', 'dog', 'bat', 'penguin'],\n |      ...      'locomotion': ['walks', 'walks', 'flies', 'walks']}\n |      &gt;&gt;&gt; df = pd.DataFrame(data=d)\n |      &gt;&gt;&gt; df = df.set_index(['class', 'animal', 'locomotion'])\n |      &gt;&gt;&gt; df\n |                                 num_legs  num_wings\n |      class  animal  locomotion\n |      mammal cat     walks              4          0\n |             dog     walks              4          0\n |             bat     flies              2          2\n |      bird   penguin walks              2          2\n |      \n |      Get values at specified index\n |      \n |      &gt;&gt;&gt; df.xs('mammal')\n |                         num_legs  num_wings\n |      animal locomotion\n |      cat    walks              4          0\n |      dog    walks              4          0\n |      bat    flies              2          2\n |      \n |      Get values at several indexes\n |      \n |      &gt;&gt;&gt; df.xs(('mammal', 'dog', 'walks'))\n |      num_legs     4\n |      num_wings    0\n |      Name: (mammal, dog, walks), dtype: int64\n |      \n |      Get values at specified index and level\n |      \n |      &gt;&gt;&gt; df.xs('cat', level=1)\n |                         num_legs  num_wings\n |      class  locomotion\n |      mammal walks              4          0\n |      \n |      Get values at several indexes and levels\n |      \n |      &gt;&gt;&gt; df.xs(('bird', 'walks'),\n |      ...       level=[0, 'locomotion'])\n |               num_legs  num_wings\n |      animal\n |      penguin         2          2\n |      \n |      Get values at specified column and axis\n |      \n |      &gt;&gt;&gt; df.xs('num_wings', axis=1)\n |      class   animal   locomotion\n |      mammal  cat      walks         0\n |              dog      walks         0\n |              bat      flies         2\n |      bird    penguin  walks         2\n |      Name: num_wings, dtype: int64\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties inherited from pandas.core.generic.NDFrame:\n |  \n |  flags\n |      Get the properties associated with this pandas object.\n |      \n |      The available flags are\n |      \n |      * :attr:`Flags.allows_duplicate_labels`\n |      \n |      See Also\n |      --------\n |      Flags : Flags that apply to pandas objects.\n |      DataFrame.attrs : Global metadata applying to this dataset.\n |      \n |      Notes\n |      -----\n |      \"Flags\" differ from \"metadata\". Flags reflect properties of the\n |      pandas object (the Series or DataFrame). Metadata refer to properties\n |      of the dataset, and should be stored in :attr:`DataFrame.attrs`.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; df = pd.DataFrame({\"A\": [1, 2]})\n |      &gt;&gt;&gt; df.flags\n |      &lt;Flags(allows_duplicate_labels=True)&gt;\n |      \n |      Flags can be get or set using ``.``\n |      \n |      &gt;&gt;&gt; df.flags.allows_duplicate_labels\n |      True\n |      &gt;&gt;&gt; df.flags.allows_duplicate_labels = False\n |      \n |      Or by slicing with a key\n |      \n |      &gt;&gt;&gt; df.flags[\"allows_duplicate_labels\"]\n |      False\n |      &gt;&gt;&gt; df.flags[\"allows_duplicate_labels\"] = True\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from pandas.core.generic.NDFrame:\n |  \n |  attrs\n |      Dictionary of global attributes of this dataset.\n |      \n |      .. warning::\n |      \n |         attrs is experimental and may change without warning.\n |      \n |      See Also\n |      --------\n |      DataFrame.flags : Global flags applying to this object.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pandas.core.base.PandasObject:\n |  \n |  __sizeof__(self) -&gt; 'int'\n |      Generates the total memory usage for an object that returns\n |      either a value or Series of values\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pandas.core.accessor.DirNamesMixin:\n |  \n |  __dir__(self) -&gt; 'list[str]'\n |      Provide method name lookup and completion.\n |      \n |      Notes\n |      -----\n |      Only provide 'public' methods.\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties inherited from pandas.core.indexing.IndexingMixin:\n |  \n |  at\n |      Access a single value for a row/column label pair.\n |      \n |      Similar to ``loc``, in that both provide label-based lookups. Use\n |      ``at`` if you only need to get or set a single value in a DataFrame\n |      or Series.\n |      \n |      Raises\n |      ------\n |      KeyError\n |          * If getting a value and 'label' does not exist in a DataFrame or\n |              Series.\n |      ValueError\n |          * If row/column label pair is not a tuple or if any label from\n |              the pair is not a scalar for DataFrame.\n |          * If label is list-like (*excluding* NamedTuple) for Series.\n |      \n |      See Also\n |      --------\n |      DataFrame.at : Access a single value for a row/column pair by label.\n |      DataFrame.iat : Access a single value for a row/column pair by integer\n |          position.\n |      DataFrame.loc : Access a group of rows and columns by label(s).\n |      DataFrame.iloc : Access a group of rows and columns by integer\n |          position(s).\n |      Series.at : Access a single value by label.\n |      Series.iat : Access a single value by integer position.\n |      Series.loc : Access a group of rows by label(s).\n |      Series.iloc : Access a group of rows by integer position(s).\n |      \n |      Notes\n |      -----\n |      See :ref:`Fast scalar value getting and setting &lt;indexing.basics.get_value&gt;`\n |      for more details.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30]],\n |      ...                   index=[4, 5, 6], columns=['A', 'B', 'C'])\n |      &gt;&gt;&gt; df\n |          A   B   C\n |      4   0   2   3\n |      5   0   4   1\n |      6  10  20  30\n |      \n |      Get value at specified row/column pair\n |      \n |      &gt;&gt;&gt; df.at[4, 'B']\n |      2\n |      \n |      Set value at specified row/column pair\n |      \n |      &gt;&gt;&gt; df.at[4, 'B'] = 10\n |      &gt;&gt;&gt; df.at[4, 'B']\n |      10\n |      \n |      Get value within a Series\n |      \n |      &gt;&gt;&gt; df.loc[5].at['B']\n |      4\n |  \n |  iat\n |      Access a single value for a row/column pair by integer position.\n |      \n |      Similar to ``iloc``, in that both provide integer-based lookups. Use\n |      ``iat`` if you only need to get or set a single value in a DataFrame\n |      or Series.\n |      \n |      Raises\n |      ------\n |      IndexError\n |          When integer position is out of bounds.\n |      \n |      See Also\n |      --------\n |      DataFrame.at : Access a single value for a row/column label pair.\n |      DataFrame.loc : Access a group of rows and columns by label(s).\n |      DataFrame.iloc : Access a group of rows and columns by integer position(s).\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30]],\n |      ...                   columns=['A', 'B', 'C'])\n |      &gt;&gt;&gt; df\n |          A   B   C\n |      0   0   2   3\n |      1   0   4   1\n |      2  10  20  30\n |      \n |      Get value at specified row/column pair\n |      \n |      &gt;&gt;&gt; df.iat[1, 2]\n |      1\n |      \n |      Set value at specified row/column pair\n |      \n |      &gt;&gt;&gt; df.iat[1, 2] = 10\n |      &gt;&gt;&gt; df.iat[1, 2]\n |      10\n |      \n |      Get value within a series\n |      \n |      &gt;&gt;&gt; df.loc[0].iat[1]\n |      2\n |  \n |  iloc\n |      Purely integer-location based indexing for selection by position.\n |      \n |      ``.iloc[]`` is primarily integer position based (from ``0`` to\n |      ``length-1`` of the axis), but may also be used with a boolean\n |      array.\n |      \n |      Allowed inputs are:\n |      \n |      - An integer, e.g. ``5``.\n |      - A list or array of integers, e.g. ``[4, 3, 0]``.\n |      - A slice object with ints, e.g. ``1:7``.\n |      - A boolean array.\n |      - A ``callable`` function with one argument (the calling Series or\n |        DataFrame) and that returns valid output for indexing (one of the above).\n |        This is useful in method chains, when you don't have a reference to the\n |        calling object, but would like to base your selection on some value.\n |      - A tuple of row and column indexes. The tuple elements consist of one of the\n |        above inputs, e.g. ``(0, 1)``.\n |      \n |      ``.iloc`` will raise ``IndexError`` if a requested indexer is\n |      out-of-bounds, except *slice* indexers which allow out-of-bounds\n |      indexing (this conforms with python/numpy *slice* semantics).\n |      \n |      See more at :ref:`Selection by Position &lt;indexing.integer&gt;`.\n |      \n |      See Also\n |      --------\n |      DataFrame.iat : Fast integer location scalar accessor.\n |      DataFrame.loc : Purely label-location based indexer for selection by label.\n |      Series.iloc : Purely integer-location based indexing for\n |                     selection by position.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; mydict = [{'a': 1, 'b': 2, 'c': 3, 'd': 4},\n |      ...           {'a': 100, 'b': 200, 'c': 300, 'd': 400},\n |      ...           {'a': 1000, 'b': 2000, 'c': 3000, 'd': 4000 }]\n |      &gt;&gt;&gt; df = pd.DataFrame(mydict)\n |      &gt;&gt;&gt; df\n |            a     b     c     d\n |      0     1     2     3     4\n |      1   100   200   300   400\n |      2  1000  2000  3000  4000\n |      \n |      **Indexing just the rows**\n |      \n |      With a scalar integer.\n |      \n |      &gt;&gt;&gt; type(df.iloc[0])\n |      &lt;class 'pandas.core.series.Series'&gt;\n |      &gt;&gt;&gt; df.iloc[0]\n |      a    1\n |      b    2\n |      c    3\n |      d    4\n |      Name: 0, dtype: int64\n |      \n |      With a list of integers.\n |      \n |      &gt;&gt;&gt; df.iloc[[0]]\n |         a  b  c  d\n |      0  1  2  3  4\n |      &gt;&gt;&gt; type(df.iloc[[0]])\n |      &lt;class 'pandas.core.frame.DataFrame'&gt;\n |      \n |      &gt;&gt;&gt; df.iloc[[0, 1]]\n |           a    b    c    d\n |      0    1    2    3    4\n |      1  100  200  300  400\n |      \n |      With a `slice` object.\n |      \n |      &gt;&gt;&gt; df.iloc[:3]\n |            a     b     c     d\n |      0     1     2     3     4\n |      1   100   200   300   400\n |      2  1000  2000  3000  4000\n |      \n |      With a boolean mask the same length as the index.\n |      \n |      &gt;&gt;&gt; df.iloc[[True, False, True]]\n |            a     b     c     d\n |      0     1     2     3     4\n |      2  1000  2000  3000  4000\n |      \n |      With a callable, useful in method chains. The `x` passed\n |      to the ``lambda`` is the DataFrame being sliced. This selects\n |      the rows whose index label even.\n |      \n |      &gt;&gt;&gt; df.iloc[lambda x: x.index % 2 == 0]\n |            a     b     c     d\n |      0     1     2     3     4\n |      2  1000  2000  3000  4000\n |      \n |      **Indexing both axes**\n |      \n |      You can mix the indexer types for the index and columns. Use ``:`` to\n |      select the entire axis.\n |      \n |      With scalar integers.\n |      \n |      &gt;&gt;&gt; df.iloc[0, 1]\n |      2\n |      \n |      With lists of integers.\n |      \n |      &gt;&gt;&gt; df.iloc[[0, 2], [1, 3]]\n |            b     d\n |      0     2     4\n |      2  2000  4000\n |      \n |      With `slice` objects.\n |      \n |      &gt;&gt;&gt; df.iloc[1:3, 0:3]\n |            a     b     c\n |      1   100   200   300\n |      2  1000  2000  3000\n |      \n |      With a boolean array whose length matches the columns.\n |      \n |      &gt;&gt;&gt; df.iloc[:, [True, False, True, False]]\n |            a     c\n |      0     1     3\n |      1   100   300\n |      2  1000  3000\n |      \n |      With a callable function that expects the Series or DataFrame.\n |      \n |      &gt;&gt;&gt; df.iloc[:, lambda df: [0, 2]]\n |            a     c\n |      0     1     3\n |      1   100   300\n |      2  1000  3000\n |  \n |  loc\n |      Access a group of rows and columns by label(s) or a boolean array.\n |      \n |      ``.loc[]`` is primarily label based, but may also be used with a\n |      boolean array.\n |      \n |      Allowed inputs are:\n |      \n |      - A single label, e.g. ``5`` or ``'a'``, (note that ``5`` is\n |        interpreted as a *label* of the index, and **never** as an\n |        integer position along the index).\n |      - A list or array of labels, e.g. ``['a', 'b', 'c']``.\n |      - A slice object with labels, e.g. ``'a':'f'``.\n |      \n |        .. warning:: Note that contrary to usual python slices, **both** the\n |            start and the stop are included\n |      \n |      - A boolean array of the same length as the axis being sliced,\n |        e.g. ``[True, False, True]``.\n |      - An alignable boolean Series. The index of the key will be aligned before\n |        masking.\n |      - An alignable Index. The Index of the returned selection will be the input.\n |      - A ``callable`` function with one argument (the calling Series or\n |        DataFrame) and that returns valid output for indexing (one of the above)\n |      \n |      See more at :ref:`Selection by Label &lt;indexing.label&gt;`.\n |      \n |      Raises\n |      ------\n |      KeyError\n |          If any items are not found.\n |      IndexingError\n |          If an indexed key is passed and its index is unalignable to the frame index.\n |      \n |      See Also\n |      --------\n |      DataFrame.at : Access a single value for a row/column label pair.\n |      DataFrame.iloc : Access group of rows and columns by integer position(s).\n |      DataFrame.xs : Returns a cross-section (row(s) or column(s)) from the\n |          Series/DataFrame.\n |      Series.loc : Access group of values using labels.\n |      \n |      Examples\n |      --------\n |      **Getting values**\n |      \n |      &gt;&gt;&gt; df = pd.DataFrame([[1, 2], [4, 5], [7, 8]],\n |      ...      index=['cobra', 'viper', 'sidewinder'],\n |      ...      columns=['max_speed', 'shield'])\n |      &gt;&gt;&gt; df\n |                  max_speed  shield\n |      cobra               1       2\n |      viper               4       5\n |      sidewinder          7       8\n |      \n |      Single label. Note this returns the row as a Series.\n |      \n |      &gt;&gt;&gt; df.loc['viper']\n |      max_speed    4\n |      shield       5\n |      Name: viper, dtype: int64\n |      \n |      List of labels. Note using ``[[]]`` returns a DataFrame.\n |      \n |      &gt;&gt;&gt; df.loc[['viper', 'sidewinder']]\n |                  max_speed  shield\n |      viper               4       5\n |      sidewinder          7       8\n |      \n |      Single label for row and column\n |      \n |      &gt;&gt;&gt; df.loc['cobra', 'shield']\n |      2\n |      \n |      Slice with labels for row and single label for column. As mentioned\n |      above, note that both the start and stop of the slice are included.\n |      \n |      &gt;&gt;&gt; df.loc['cobra':'viper', 'max_speed']\n |      cobra    1\n |      viper    4\n |      Name: max_speed, dtype: int64\n |      \n |      Boolean list with the same length as the row axis\n |      \n |      &gt;&gt;&gt; df.loc[[False, False, True]]\n |                  max_speed  shield\n |      sidewinder          7       8\n |      \n |      Alignable boolean Series:\n |      \n |      &gt;&gt;&gt; df.loc[pd.Series([False, True, False],\n |      ...        index=['viper', 'sidewinder', 'cobra'])]\n |                  max_speed  shield\n |      sidewinder          7       8\n |      \n |      Index (same behavior as ``df.reindex``)\n |      \n |      &gt;&gt;&gt; df.loc[pd.Index([\"cobra\", \"viper\"], name=\"foo\")]\n |             max_speed  shield\n |      foo\n |      cobra          1       2\n |      viper          4       5\n |      \n |      Conditional that returns a boolean Series\n |      \n |      &gt;&gt;&gt; df.loc[df['shield'] &gt; 6]\n |                  max_speed  shield\n |      sidewinder          7       8\n |      \n |      Conditional that returns a boolean Series with column labels specified\n |      \n |      &gt;&gt;&gt; df.loc[df['shield'] &gt; 6, ['max_speed']]\n |                  max_speed\n |      sidewinder          7\n |      \n |      Callable that returns a boolean Series\n |      \n |      &gt;&gt;&gt; df.loc[lambda df: df['shield'] == 8]\n |                  max_speed  shield\n |      sidewinder          7       8\n |      \n |      **Setting values**\n |      \n |      Set value for all items matching the list of labels\n |      \n |      &gt;&gt;&gt; df.loc[['viper', 'sidewinder'], ['shield']] = 50\n |      &gt;&gt;&gt; df\n |                  max_speed  shield\n |      cobra               1       2\n |      viper               4      50\n |      sidewinder          7      50\n |      \n |      Set value for an entire row\n |      \n |      &gt;&gt;&gt; df.loc['cobra'] = 10\n |      &gt;&gt;&gt; df\n |                  max_speed  shield\n |      cobra              10      10\n |      viper               4      50\n |      sidewinder          7      50\n |      \n |      Set value for an entire column\n |      \n |      &gt;&gt;&gt; df.loc[:, 'max_speed'] = 30\n |      &gt;&gt;&gt; df\n |                  max_speed  shield\n |      cobra              30      10\n |      viper              30      50\n |      sidewinder         30      50\n |      \n |      Set value for rows matching callable condition\n |      \n |      &gt;&gt;&gt; df.loc[df['shield'] &gt; 35] = 0\n |      &gt;&gt;&gt; df\n |                  max_speed  shield\n |      cobra              30      10\n |      viper               0       0\n |      sidewinder          0       0\n |      \n |      **Getting values on a DataFrame with an index that has integer labels**\n |      \n |      Another example using integers for the index\n |      \n |      &gt;&gt;&gt; df = pd.DataFrame([[1, 2], [4, 5], [7, 8]],\n |      ...      index=[7, 8, 9], columns=['max_speed', 'shield'])\n |      &gt;&gt;&gt; df\n |         max_speed  shield\n |      7          1       2\n |      8          4       5\n |      9          7       8\n |      \n |      Slice with integer labels for rows. As mentioned above, note that both\n |      the start and stop of the slice are included.\n |      \n |      &gt;&gt;&gt; df.loc[7:9]\n |         max_speed  shield\n |      7          1       2\n |      8          4       5\n |      9          7       8\n |      \n |      **Getting values with a MultiIndex**\n |      \n |      A number of examples using a DataFrame with a MultiIndex\n |      \n |      &gt;&gt;&gt; tuples = [\n |      ...    ('cobra', 'mark i'), ('cobra', 'mark ii'),\n |      ...    ('sidewinder', 'mark i'), ('sidewinder', 'mark ii'),\n |      ...    ('viper', 'mark ii'), ('viper', 'mark iii')\n |      ... ]\n |      &gt;&gt;&gt; index = pd.MultiIndex.from_tuples(tuples)\n |      &gt;&gt;&gt; values = [[12, 2], [0, 4], [10, 20],\n |      ...         [1, 4], [7, 1], [16, 36]]\n |      &gt;&gt;&gt; df = pd.DataFrame(values, columns=['max_speed', 'shield'], index=index)\n |      &gt;&gt;&gt; df\n |                           max_speed  shield\n |      cobra      mark i           12       2\n |                 mark ii           0       4\n |      sidewinder mark i           10      20\n |                 mark ii           1       4\n |      viper      mark ii           7       1\n |                 mark iii         16      36\n |      \n |      Single label. Note this returns a DataFrame with a single index.\n |      \n |      &gt;&gt;&gt; df.loc['cobra']\n |               max_speed  shield\n |      mark i          12       2\n |      mark ii          0       4\n |      \n |      Single index tuple. Note this returns a Series.\n |      \n |      &gt;&gt;&gt; df.loc[('cobra', 'mark ii')]\n |      max_speed    0\n |      shield       4\n |      Name: (cobra, mark ii), dtype: int64\n |      \n |      Single label for row and column. Similar to passing in a tuple, this\n |      returns a Series.\n |      \n |      &gt;&gt;&gt; df.loc['cobra', 'mark i']\n |      max_speed    12\n |      shield        2\n |      Name: (cobra, mark i), dtype: int64\n |      \n |      Single tuple. Note using ``[[]]`` returns a DataFrame.\n |      \n |      &gt;&gt;&gt; df.loc[[('cobra', 'mark ii')]]\n |                     max_speed  shield\n |      cobra mark ii          0       4\n |      \n |      Single tuple for the index with a single label for the column\n |      \n |      &gt;&gt;&gt; df.loc[('cobra', 'mark i'), 'shield']\n |      2\n |      \n |      Slice from index tuple to single label\n |      \n |      &gt;&gt;&gt; df.loc[('cobra', 'mark i'):'viper']\n |                           max_speed  shield\n |      cobra      mark i           12       2\n |                 mark ii           0       4\n |      sidewinder mark i           10      20\n |                 mark ii           1       4\n |      viper      mark ii           7       1\n |                 mark iii         16      36\n |      \n |      Slice from index tuple to index tuple\n |      \n |      &gt;&gt;&gt; df.loc[('cobra', 'mark i'):('viper', 'mark ii')]\n |                          max_speed  shield\n |      cobra      mark i          12       2\n |                 mark ii          0       4\n |      sidewinder mark i          10      20\n |                 mark ii          1       4\n |      viper      mark ii          7       1\n |      \n |      Please see the :ref:`user guide&lt;advanced.advanced_hierarchical&gt;`\n |      for more details and explanations of advanced indexing.\n\n\n\n\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n-1.716945\nNaN\n0.198477\n\n\n1\n2.815565\nNaN\n-1.749359\n\n\n2\n-0.073905\nNaN\n0.826025\n\n\n3\n1.122968\nNaN\n0.932057\n\n\n4\n-0.037637\n1.100561\n-0.328430\n\n\n5\n-0.077328\n-1.032715\n0.157982\n\n\n6\n0.363370\n1.845914\n-0.172841\n\n\n\n\n\n\n\n\ndf.iloc[2:, 1]= np.nan   #all rows from third onwards, second column\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n-1.716945\nNaN\n0.198477\n\n\n1\n2.815565\nNaN\n-1.749359\n\n\n2\n-0.073905\nNaN\n0.826025\n\n\n3\n1.122968\nNaN\n0.932057\n\n\n4\n-0.037637\nNaN\n-0.328430\n\n\n5\n-0.077328\nNaN\n0.157982\n\n\n6\n0.363370\nNaN\n-0.172841\n\n\n\n\n\n\n\n\ndf.iloc[4:, 2] = np.nan\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n-1.716945\nNaN\n0.198477\n\n\n1\n2.815565\nNaN\n-1.749359\n\n\n2\n-0.073905\nNaN\n0.826025\n\n\n3\n1.122968\nNaN\n0.932057\n\n\n4\n-0.037637\nNaN\nNaN\n\n\n5\n-0.077328\nNaN\nNaN\n\n\n6\n0.363370\nNaN\nNaN\n\n\n\n\n\n\n\n\ndf.fillna(method = \"ffill\")\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n-1.716945\nNaN\n0.198477\n\n\n1\n2.815565\nNaN\n-1.749359\n\n\n2\n-0.073905\nNaN\n0.826025\n\n\n3\n1.122968\nNaN\n0.932057\n\n\n4\n-0.037637\nNaN\n0.932057\n\n\n5\n-0.077328\nNaN\n0.932057\n\n\n6\n0.363370\nNaN\n0.932057\n\n\n\n\n\n\n\n\ndf.fillna({1:0.5})\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n-1.716945\n0.5\n0.198477\n\n\n1\n2.815565\n0.5\n-1.749359\n\n\n2\n-0.073905\n0.5\n0.826025\n\n\n3\n1.122968\n0.5\n0.932057\n\n\n4\n-0.037637\n0.5\nNaN\n\n\n5\n-0.077328\n0.5\nNaN\n\n\n6\n0.363370\n0.5\nNaN\n\n\n\n\n\n\n\n\ndf.fillna(method='ffill', limit=2) # 2 values of third column gets filled\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n-1.716945\nNaN\n0.198477\n\n\n1\n2.815565\nNaN\n-1.749359\n\n\n2\n-0.073905\nNaN\n0.826025\n\n\n3\n1.122968\nNaN\n0.932057\n\n\n4\n-0.037637\nNaN\n0.932057\n\n\n5\n-0.077328\nNaN\n0.932057\n\n\n6\n0.363370\nNaN\nNaN"
  },
  {
    "objectID": "posts/python/data_cleaning+prepration.html#imputations-with-fillna",
    "href": "posts/python/data_cleaning+prepration.html#imputations-with-fillna",
    "title": "Data Cleaning",
    "section": "imputations with fillna()",
    "text": "imputations with fillna()\n\nfucntion arguments (value, method, axis, limit)\n\n\n# imputations with fillna\ndata = pd.Series([1., np.nan, 3.5, np.nan, 7])\n\ndata\n\n0    1.0\n1    NaN\n2    3.5\n3    NaN\n4    7.0\ndtype: float64\n\n\n\ndata.fillna(data.mean())\n\n0    1.000000\n1    3.833333\n2    3.500000\n3    3.833333\n4    7.000000\ndtype: float64"
  },
  {
    "objectID": "posts/python/data_cleaning+prepration.html#removing-duplicates",
    "href": "posts/python/data_cleaning+prepration.html#removing-duplicates",
    "title": "Data Cleaning",
    "section": "Removing duplicates",
    "text": "Removing duplicates\n\ndata = pd.DataFrame({\"k1\":['one', 'two']* 3 + ['two'],\n                   \"k2\": [1, 22, 1, 22, 3, 22, 3,]})\n\ndata\n\n\n\n\n\n\n\n\nk1\nk2\n\n\n\n\n0\none\n1\n\n\n1\ntwo\n22\n\n\n2\none\n1\n\n\n3\ntwo\n22\n\n\n4\none\n3\n\n\n5\ntwo\n22\n\n\n6\ntwo\n3\n\n\n\n\n\n\n\n\ndata.duplicated()\n\n0    False\n1    False\n2     True\n3     True\n4    False\n5     True\n6    False\ndtype: bool\n\n\n\ndata.drop_duplicates()\n\n\n\n\n\n\n\n\nk1\nk2\n\n\n\n\n0\none\n1\n\n\n1\ntwo\n22\n\n\n4\none\n3\n\n\n6\ntwo\n3\n\n\n\n\n\n\n\n\n# removing duplicates based on column\ndata['k3'] = range(7)   # adding column\n\n\ndata\n\n\n\n\n\n\n\n\nk1\nk2\nk3\n\n\n\n\n0\none\n1\n0\n\n\n1\ntwo\n22\n1\n\n\n2\none\n1\n2\n\n\n3\ntwo\n22\n3\n\n\n4\none\n3\n4\n\n\n5\ntwo\n22\n5\n\n\n6\ntwo\n3\n6\n\n\n\n\n\n\n\n\ndata.drop_duplicates(subset=[\"k1\"])\n\n\n\n\n\n\n\n\nk1\nk2\nk3\n\n\n\n\n0\none\n1\n0\n\n\n1\ntwo\n22\n1"
  },
  {
    "objectID": "posts/python/data_cleaning+prepration.html#transforming-data-using-a-function-or-mapping",
    "href": "posts/python/data_cleaning+prepration.html#transforming-data-using-a-function-or-mapping",
    "title": "Data Cleaning",
    "section": "Transforming data using a Function or mapping",
    "text": "Transforming data using a Function or mapping\n\ndata = pd.DataFrame({'food': ['poisson', 'boeuf', 'mouton', 'bacon', 'poulet'],\n                     'quantité - ounces' : [4, 5, 3, 4, 2]})\n\ndata\n\n\n\n\n\n\n\n\nfood\nquantité - ounces\n\n\n\n\n0\npoisson\n4\n\n\n1\nboeuf\n5\n\n\n2\nmouton\n3\n\n\n3\nbacon\n4\n\n\n4\npoulet\n2\n\n\n\n\n\n\n\n\n# writing animal names in english alongside\nmeat_of_animal = {'poisson': 'fish',\n                 'boeuf' : 'beef',\n                 'mouton' : 'sheep',\n                 'bacon': 'pig',\n                 'poulet': 'chicken'}\n\n\ndata['english'] = data['food'].map(meat_of_animal)\n\n\ndata\n\n\n\n\n\n\n\n\nfood\nquantité - ounces\nenglish\n\n\n\n\n0\npoisson\n4\nfish\n\n\n1\nboeuf\n5\nbeef\n\n\n2\nmouton\n3\nsheep\n\n\n3\nbacon\n4\npig\n\n\n4\npoulet\n2\nchicken\n\n\n\n\n\n\n\n\n# creating a function for the same and using map() \n\ndef animal_english (x):\n    return meat_of_animal[x]\n\ndata['food'].map(animal_english)\n\n0       fish\n1       beef\n2      sheep\n3        pig\n4    chicken\nName: food, dtype: object"
  },
  {
    "objectID": "posts/python/data_cleaning+prepration.html#replacing-values",
    "href": "posts/python/data_cleaning+prepration.html#replacing-values",
    "title": "Data Cleaning",
    "section": "Replacing values",
    "text": "Replacing values\n\ndata\n\n\n\n\n\n\n\n\nfood\nquantité - ounces\nenglish\n\n\n\n\n0\npoisson\n4\nfish\n\n\n1\nboeuf\n5\nbeef\n\n\n2\nmouton\n3\nsheep\n\n\n3\nbacon\n4\npig\n\n\n4\npoulet\n2\nchicken\n\n\n\n\n\n\n\n\ndata2 = pd.Series([1., - 323, -.32 , 4.])\n\ndata2\n\n0      1.00\n1   -323.00\n2     -0.32\n3      4.00\ndtype: float64\n\n\n\ndata2.replace(-323, np.nan)\n\n0    1.00\n1     NaN\n2   -0.32\n3    4.00\ndtype: float64\n\n\n\n# replacing multiple values\ndata2.replace([-323, -.32], np.nan)\n\n0    1.0\n1    NaN\n2    NaN\n3    4.0\ndtype: float64\n\n\n\n# using different replacement values for different substitutes\ndata2.replace([-323, -.32], [np.nan, 0])\n\n0    1.0\n1    NaN\n2    0.0\n3    4.0\ndtype: float64\n\n\n\n# argument passed can alse be a dictionary\ndata2.replace({-323: np.nan, -.32:55555})\n\n0        1.0\n1        NaN\n2    55555.0\n3        4.0\ndtype: float64"
  },
  {
    "objectID": "posts/python/data_cleaning+prepration.html#renaming-axis-indexes",
    "href": "posts/python/data_cleaning+prepration.html#renaming-axis-indexes",
    "title": "Data Cleaning",
    "section": "Renaming Axis indexes",
    "text": "Renaming Axis indexes\n\ndata\n\n\n\n\n\n\n\n\nfood\nquantité - ounces\nenglish\n\n\n\n\n0\npoisson\n4\nfish\n\n\n1\nboeuf\n5\nbeef\n\n\n2\nmouton\n3\nsheep\n\n\n3\nbacon\n4\npig\n\n\n4\npoulet\n2\nchicken\n\n\n\n\n\n\n\n\n# transforming first four letters of column\ndef transform(x):\n    return x[:5].upper()\n\ndata.columns.map(transform)  \n\nIndex(['FOOD', 'QUANT', 'ENGLI'], dtype='object')\n\n\n\n# changing the titles of the DataFrame\ndata.columns = data.columns.map(transform)\n\ndata\n\n\n\n\n\n\n\n\nFOOD\nQUANT\nENGLI\n\n\n\n\n0\npoisson\n4\nfish\n\n\n1\nboeuf\n5\nbeef\n\n\n2\nmouton\n3\nsheep\n\n\n3\nbacon\n4\npig\n\n\n4\npoulet\n2\nchicken\n\n\n\n\n\n\n\n\n# tranformed version of dataset without modifying the orignal\n\ndata.rename(columns = str.lower)\n\n\n\n\n\n\n\n\nfood\nquant\nengli\n\n\n\n\n0\npoisson\n4\nfish\n\n\n1\nboeuf\n5\nbeef\n\n\n2\nmouton\n3\nsheep\n\n\n3\nbacon\n4\npig\n\n\n4\npoulet\n2\nchicken"
  },
  {
    "objectID": "posts/python/data_cleaning+prepration.html#discretization-and-binning",
    "href": "posts/python/data_cleaning+prepration.html#discretization-and-binning",
    "title": "Data Cleaning",
    "section": "Discretization and Binning",
    "text": "Discretization and Binning\n\npandas.cut\npandas.value_counts\n\n\nages = [21, 33, 12, 33, 32, 21, 44, 55]\n\n\nbins = [12, 30, 40, 50, 60, 70, 100]\n\n\nage_categories = pd.cut(ages, bins)\n\n\nage_categories\n\n[(12.0, 30.0], (30.0, 40.0], NaN, (30.0, 40.0], (30.0, 40.0], (12.0, 30.0], (40.0, 50.0], (50.0, 60.0]]\nCategories (6, interval[int64, right]): [(12, 30] &lt; (30, 40] &lt; (40, 50] &lt; (50, 60] &lt; (60, 70] &lt; (70, 100]]\n\n\n\npd.value_counts(age_categories)\n\n(30, 40]     3\n(12, 30]     2\n(40, 50]     1\n(50, 60]     1\n(60, 70]     0\n(70, 100]    0\nName: count, dtype: int64\n\n\n\nhelp(pd.value_counts)\n\nHelp on function value_counts in module pandas.core.algorithms:\n\nvalue_counts(values, sort: 'bool' = True, ascending: 'bool' = False, normalize: 'bool' = False, bins=None, dropna: 'bool' = True) -&gt; 'Series'\n    Compute a histogram of the counts of non-null values.\n    \n    Parameters\n    ----------\n    values : ndarray (1-d)\n    sort : bool, default True\n        Sort by values\n    ascending : bool, default False\n        Sort in ascending order\n    normalize: bool, default False\n        If True then compute a relative histogram\n    bins : integer, optional\n        Rather than count values, group them into half-open bins,\n        convenience for pd.cut, only works with numeric data\n    dropna : bool, default True\n        Don't include counts of NaN\n    \n    Returns\n    -------\n    Series\n\n\n\n\n# parnethesis is towards open side\n\n# changing side\n\npd.cut(ages, bins, right= False)\n\n[[12, 30), [30, 40), [12, 30), [30, 40), [30, 40), [12, 30), [40, 50), [50, 60)]\nCategories (6, interval[int64, left]): [[12, 30) &lt; [30, 40) &lt; [40, 50) &lt; [50, 60) &lt; [60, 70) &lt; [70, 100)]\n\n\n\n# changing labels\n\ngroup_names = ['ados','youth', 'youngsters', 'midaged', 'senior', 'retired', ]\n\npd.cut(ages, bins, labels = group_names)\n\n['ados', 'youth', NaN, 'youth', 'youth', 'ados', 'youngsters', 'midaged']\nCategories (6, object): ['ados' &lt; 'youth' &lt; 'youngsters' &lt; 'midaged' &lt; 'senior' &lt; 'retired']\n\n\n\ndata3 = np.random.uniform (size = 20)\n\ncategories = pd.cut(data3, 4, precision=2)   # 4 is the number of bins\n\n# precision limits decmical point to two decimal places\n\n\npd.value_counts(categories)\n\n(0.011, 0.25]    8\n(0.49, 0.73]     6\n(0.73, 0.97]     5\n(0.25, 0.49]     1\nName: count, dtype: int64\n\n\n\n# for equal sized bins\ncategories_1 = pd.qcut(data3, 4, precision = 2)\n\n\npd.value_counts(categories_1)\n\n(0.002, 0.14]    5\n(0.14, 0.51]     5\n(0.51, 0.69]     5\n(0.69, 0.97]     5\nName: count, dtype: int64"
  },
  {
    "objectID": "posts/python/data_cleaning+prepration.html#detecting-and-filtering-outliers",
    "href": "posts/python/data_cleaning+prepration.html#detecting-and-filtering-outliers",
    "title": "Data Cleaning",
    "section": "Detecting and Filtering Outliers",
    "text": "Detecting and Filtering Outliers\n\ndata4 = pd.DataFrame(np.random.standard_normal((1000, 4)))\n\ndata4.describe()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\ncount\n1000.000000\n1000.000000\n1000.000000\n1000.000000\n\n\nmean\n0.016561\n-0.031729\n0.027078\n0.064505\n\n\nstd\n1.007664\n1.047383\n1.001156\n0.993706\n\n\nmin\n-3.592826\n-3.066364\n-3.149016\n-3.644190\n\n\n25%\n-0.627244\n-0.749349\n-0.603733\n-0.592056\n\n\n50%\n0.046336\n-0.056226\n0.010209\n0.030974\n\n\n75%\n0.686210\n0.658083\n0.737684\n0.765335\n\n\nmax\n3.262253\n3.918947\n2.981393\n3.029627\n\n\n\n\n\n\n\n\n# find values of columns exceeding 3 in absolute value\n\ncol = data4[2]\n\ncol[col.abs() &gt; 3]\n\n614   -3.149016\n701   -3.093409\nName: 2, dtype: float64\n\n\n\n# selecting all the columns\n\ndata4[(data4.abs() &gt; 3).any(axis = 'columns')]\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n78\n-3.591639\n-0.652692\n2.002021\n-0.233262\n\n\n105\n1.620532\n3.918947\n-0.014565\n0.483555\n\n\n178\n-0.642515\n-3.020820\n-1.899087\n-0.244327\n\n\n184\n-3.592826\n-1.979726\n0.371343\n1.022697\n\n\n240\n-0.331648\n3.687370\n-0.459598\n0.994123\n\n\n267\n1.236309\n-3.066364\n-0.132627\n0.551233\n\n\n274\n-1.350742\n3.505783\n0.908839\n-0.219144\n\n\n306\n-0.419746\n3.068152\n-0.085927\n1.446228\n\n\n346\n0.174638\n3.035741\n1.034076\n1.208338\n\n\n414\n1.224208\n3.060689\n0.439984\n-1.323200\n\n\n433\n1.493799\n-0.528079\n-0.241609\n3.029627\n\n\n543\n3.262253\n-0.713190\n0.583197\n1.791658\n\n\n614\n0.535075\n-1.450999\n-3.149016\n1.604812\n\n\n617\n0.240876\n-0.628409\n0.151670\n-3.455415\n\n\n701\n-0.618249\n0.618510\n-3.093409\n1.118688\n\n\n817\n-0.376220\n-0.653628\n-1.408788\n-3.644190\n\n\n\n\n\n\n\n\n# code to cap values outsite the inteval -3 to 3\n\ndata4[data4.abs() &gt; 3] = np.sign(data4) * 3\n\ndata4.describe()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\ncount\n1000.000000\n1000.000000\n1000.000000\n1000.000000\n\n\nmean\n0.017483\n-0.033918\n0.027320\n0.065575\n\n\nstd\n1.002943\n1.039751\n1.000406\n0.989902\n\n\nmin\n-3.000000\n-3.000000\n-3.000000\n-3.000000\n\n\n25%\n-0.627244\n-0.749349\n-0.603733\n-0.592056\n\n\n50%\n0.046336\n-0.056226\n0.010209\n0.030974\n\n\n75%\n0.686210\n0.658083\n0.737684\n0.765335\n\n\nmax\n3.000000\n3.000000\n2.981393\n3.000000\n\n\n\n\n\n\n\n\n# np.sign(data4) produces 1 and -1 values\n\nnp.sign(data4).head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n-1.0\n1.0\n-1.0\n1.0\n\n\n1\n-1.0\n-1.0\n-1.0\n1.0\n\n\n2\n1.0\n1.0\n-1.0\n-1.0\n\n\n3\n-1.0\n1.0\n-1.0\n-1.0\n\n\n4\n-1.0\n-1.0\n1.0\n-1.0"
  },
  {
    "objectID": "posts/python/data_cleaning+prepration.html#permutation-and-random-sampling",
    "href": "posts/python/data_cleaning+prepration.html#permutation-and-random-sampling",
    "title": "Data Cleaning",
    "section": "Permutation and Random Sampling",
    "text": "Permutation and Random Sampling\n\ndf2 = pd.DataFrame (np.arange(5*7).reshape(5,7))\ndf2\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n\n\n\n\n0\n0\n1\n2\n3\n4\n5\n6\n\n\n1\n7\n8\n9\n10\n11\n12\n13\n\n\n2\n14\n15\n16\n17\n18\n19\n20\n\n\n3\n21\n22\n23\n24\n25\n26\n27\n\n\n4\n28\n29\n30\n31\n32\n33\n34\n\n\n\n\n\n\n\n\nsampler = np.random.permutation(5)\nsampler\n\narray([4, 1, 2, 0, 3])\n\n\n\n# take function or 'iloc' based indexing\n\ndf2.take(sampler)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n\n\n\n\n4\n28\n29\n30\n31\n32\n33\n34\n\n\n1\n7\n8\n9\n10\n11\n12\n13\n\n\n2\n14\n15\n16\n17\n18\n19\n20\n\n\n0\n0\n1\n2\n3\n4\n5\n6\n\n\n3\n21\n22\n23\n24\n25\n26\n27\n\n\n\n\n\n\n\n\ndf2.iloc[sampler]\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n\n\n\n\n4\n28\n29\n30\n31\n32\n33\n34\n\n\n1\n7\n8\n9\n10\n11\n12\n13\n\n\n2\n14\n15\n16\n17\n18\n19\n20\n\n\n0\n0\n1\n2\n3\n4\n5\n6\n\n\n3\n21\n22\n23\n24\n25\n26\n27\n\n\n\n\n\n\n\n\n# selecting random subset without replacement\n\ndf2.sample(n=3)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n\n\n\n\n1\n7\n8\n9\n10\n11\n12\n13\n\n\n4\n28\n29\n30\n31\n32\n33\n34\n\n\n3\n21\n22\n23\n24\n25\n26\n27\n\n\n\n\n\n\n\n\n# with replacement\n\ndf2.sample(n =4, replace = True)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n\n\n\n\n4\n28\n29\n30\n31\n32\n33\n34\n\n\n4\n28\n29\n30\n31\n32\n33\n34\n\n\n2\n14\n15\n16\n17\n18\n19\n20\n\n\n1\n7\n8\n9\n10\n11\n12\n13"
  },
  {
    "objectID": "posts/python/data_cleaning+prepration.html#computing-indicatordummay-variables",
    "href": "posts/python/data_cleaning+prepration.html#computing-indicatordummay-variables",
    "title": "Data Cleaning",
    "section": "Computing Indicator/Dummay Variables",
    "text": "Computing Indicator/Dummay Variables\n\npandas.get_dummies function\nused for statistical modelling or machine learning applications\nDataFrame.join() method\ncombine get_dummies() with pandas.cut()\n\n\ndf_dict = pd.DataFrame({'key': ['a', 'b', 'c', 'd', 'e',],\n                       'data1': range(5)})\n\ndf_dict\n\n\n\n\n\n\n\n\nkey\ndata1\n\n\n\n\n0\na\n0\n\n\n1\nb\n1\n\n\n2\nc\n2\n\n\n3\nd\n3\n\n\n4\ne\n4\n\n\n\n\n\n\n\n\npd.get_dummies(df_dict['key'])\n\n\n\n\n\n\n\n\na\nb\nc\nd\ne\n\n\n\n\n0\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n1\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n2\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n3\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n4\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n\n\n\n\n\n\ndummies = pd.get_dummies(df_dict['key'], prefix = 'key')\n\ndf_with_dummy = df_dict[['data1']].join(dummies)\n\ndf_with_dummy\n\n\n\n\n\n\n\n\ndata1\nkey_a\nkey_b\nkey_c\nkey_d\nkey_e\n\n\n\n\n0\n0\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n1\n1\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n2\n2\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n3\n3\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n4\n4\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n\n\n\n\n\n\n# combine pd.get_dummies() with pd.cut()\n\nnp.random.seed(123)\n\nvalues = np.random.uniform(size = 10)\n\nvalues\n\narray([0.69646919, 0.28613933, 0.22685145, 0.55131477, 0.71946897,\n       0.42310646, 0.9807642 , 0.68482974, 0.4809319 , 0.39211752])\n\n\n\nbins = [0, 0.2, 0.4, 0.6, 0.8, 1.0]\n\npd.get_dummies(pd.cut(values, bins))\n\n\n\n\n\n\n\n\n(0.0, 0.2]\n(0.2, 0.4]\n(0.4, 0.6]\n(0.6, 0.8]\n(0.8, 1.0]\n\n\n\n\n0\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n1\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n2\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n3\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n4\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n5\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n6\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n7\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n8\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n9\nFalse\nTrue\nFalse\nFalse\nFalse"
  },
  {
    "objectID": "posts/python/data_cleaning+prepration.html#extension-data-types-1",
    "href": "posts/python/data_cleaning+prepration.html#extension-data-types-1",
    "title": "Data Cleaning",
    "section": "Extension data types",
    "text": "Extension data types\n\ns = pd.Series([1, 2, 4, None])\n\n\ns\n\n0    1.0\n1    2.0\n2    4.0\n3    NaN\ndtype: float64\n\n\n\ns.dtype\n\ndtype('float64')\n\n\n\ns.isna()\n\n0    False\n1    False\n2    False\n3     True\ndtype: bool\n\n\n\ns_int = pd.Series(['one', 'two', 'three', None, 'four'],\n                 dtype = pd.StringDtype())\n\n\ns_int\n\n0      one\n1      two\n2    three\n3     &lt;NA&gt;\n4     four\ndtype: string\n\n\n\ndf3 = pd.DataFrame({'A': [1,2, None, 4],\n                   'B': ['one', 'two', 'three', None],\n                   'C': [False, True, None, True]})\n\ndf3\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n1.0\none\nFalse\n\n\n1\n2.0\ntwo\nTrue\n\n\n2\nNaN\nthree\nNone\n\n\n3\n4.0\nNone\nTrue\n\n\n\n\n\n\n\n\n# changing to their respective categories\ndf3['A'] = df3['A'].astype('Int64')\ndf3['B'] = df3['B'].astype('string')\ndf3['C'] = df3['C'].astype('boolean')\n\n\ndf3\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n1\none\nFalse\n\n\n1\n2\ntwo\nTrue\n\n\n2\n&lt;NA&gt;\nthree\n&lt;NA&gt;\n\n\n3\n4\n&lt;NA&gt;\nTrue"
  },
  {
    "objectID": "posts/python/data_cleaning+prepration.html#string-manipulation-1",
    "href": "posts/python/data_cleaning+prepration.html#string-manipulation-1",
    "title": "Data Cleaning",
    "section": "String manipulation",
    "text": "String manipulation\n\nval = 'a, b, kuch v'\n\nval.split(',')\n\n['a', ' b', ' kuch v']\n\n\n\n# removing white space\nremove = [x.strip() for x in val.split(',')]\n\n\nremove\n\n['a', 'b', 'kuch v']\n\n\n\n# two colon concatenation\nfirst, second, third = remove\nfirst + '::' + second + '::' + third\n\n'a::b::kuch v'\n\n\n\n'::'.join(val)\n'::'.join(remove)\n\n'a::b::kuch v'\n\n\n\n# in keyword\n'kuch v' in val\n\nTrue\n\n\n\nval.index(',')\n\n1\n\n\n\nval.find(':')\n\n# remark- index doesnot give an error if the value is not found\n\n-1"
  },
  {
    "objectID": "posts/python/data_cleaning+prepration.html#regular-expressions",
    "href": "posts/python/data_cleaning+prepration.html#regular-expressions",
    "title": "Data Cleaning",
    "section": "Regular expressions",
    "text": "Regular expressions\n\nre saves CPU cycles\n\n\nimport re\n\ntext = 'bla bla bla ta ta ta bar\\t baz aja \\t nai fer'\n\ntext\n\n'bla bla bla ta ta ta bar\\t baz aja \\t nai fer'\n\n\n\nre.split(r\"\\s+\", text)\n\n['bla', 'bla', 'bla', 'ta', 'ta', 'ta', 'bar', 'baz', 'aja', 'nai', 'fer']\n\n\n\n# doing it with re\nwithre = re.compile(r\"\\s+\")\n\nwithre.split(text)\n\n['bla', 'bla', 'bla', 'ta', 'ta', 'ta', 'bar', 'baz', 'aja', 'nai', 'fer']\n\n\n\n# finding pattern\n\nwithre.findall(text)\n\n[' ', ' ', ' ', ' ', ' ', ' ', '\\t ', ' ', ' \\t ', ' ']\n\n\n\ntext2 = \"\"\"\nKunal Khuranasoilpau@gmail.com\nSonakshi mehra.43@gmail.com\nKaran gill007@outlook.ca\nSmriti cuti3_43@ourkut.ca\n\"\"\"\n\n\npattern = r\"[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,4}\"\n\n\n# using re.IGNORECASE\nfinal_text = re.compile(pattern, flags=re.IGNORECASE)\n\nfinal_text.findall(text2)\n\n['Khuranasoilpau@gmail.com',\n 'mehra.43@gmail.com',\n 'gill007@outlook.ca',\n 'cuti3_43@ourkut.ca']\n\n\n\nprint(final_text.sub('REDACTED', text2))\n\n\nKunal REDACTED\nSonakshi REDACTED\nKaran REDACTED\nSmriti REDACTED"
  },
  {
    "objectID": "posts/python/data_cleaning+prepration.html#string-functions-in-pandas",
    "href": "posts/python/data_cleaning+prepration.html#string-functions-in-pandas",
    "title": "Data Cleaning",
    "section": "String functions in pandas",
    "text": "String functions in pandas\n\nconvert dictionary to Series with pandas\npandas string methods\n\n\ndata = {'Kunal': 'Khuranasoilpau@gmail.com', 'Robin': 'aryan_robin@yahoo.com',\n       'Deepika': 'padukone.deepi@outlook.ca', 'Ranbir' : \"singh.cool7@yahoo.com\",\n       'kabir': np.nan}\n        \n\ndata = pd.Series(data)\ndata\n\nKunal       Khuranasoilpau@gmail.com\nRobin          aryan_robin@yahoo.com\nDeepika    padukone.deepi@outlook.ca\nRanbir         singh.cool7@yahoo.com\nkabir                            NaN\ndtype: object\n\n\n\ndata.str.findall(pattern, flags=re.IGNORECASE)\n\nKunal       [Khuranasoilpau@gmail.com]\nRobin          [aryan_robin@yahoo.com]\nDeepika    [padukone.deepi@outlook.ca]\nRanbir         [singh.cool7@yahoo.com]\nkabir                              NaN\ndtype: object\n\n\n\n# slice\ndata.str[:7]\n\nKunal      Khurana\nRobin      aryan_r\nDeepika    padukon\nRanbir     singh.c\nkabir          NaN\ndtype: object"
  },
  {
    "objectID": "posts/python/data_cleaning+prepration.html#categorical-data-1",
    "href": "posts/python/data_cleaning+prepration.html#categorical-data-1",
    "title": "Data Cleaning",
    "section": "Categorical data",
    "text": "Categorical data\n\nvalues = pd.Series(['apple', 'orange', 'apple', 'mango']* 2)\n\n\nvalues\n\n0     apple\n1    orange\n2     apple\n3     mango\n4     apple\n5    orange\n6     apple\n7     mango\ndtype: object\n\n\n\npd.unique(values)\n\narray(['apple', 'orange', 'mango'], dtype=object)\n\n\n\nvalues2 = pd.Series([0,1,0, 0] * 2)\n\ndim = pd.Series(['apple', 'orange'])\n\n\nvalues2\n\n0    0\n1    1\n2    0\n3    0\n4    0\n5    1\n6    0\n7    0\ndtype: int64\n\n\n\ndim\n\n0     apple\n1    orange\ndtype: object\n\n\n\n# take method to restore orignal set of strings\ndim.take(values2)\n\n0     apple\n1    orange\n0     apple\n0     apple\n0     apple\n1    orange\n0     apple\n0     apple\ndtype: object"
  },
  {
    "objectID": "posts/python/data_cleaning+prepration.html#categorical-extension-type-in-pandas",
    "href": "posts/python/data_cleaning+prepration.html#categorical-extension-type-in-pandas",
    "title": "Data Cleaning",
    "section": "Categorical Extension type in pandas",
    "text": "Categorical Extension type in pandas\n\nfruits = ['apple', 'orange', 'apple', 'papaya'] * 2\n\nN = len(fruits)\n\nrng = np.random.default_rng(seed = 123)\n\n\ndf = pd.DataFrame({'fruit': fruits,\n                  'basket_id': np.arange(N),\n                   'count': rng.integers(3, 15, size = N),\n                  'weight': rng.uniform(0, 4, size = N)},\n                 columns = ['basket_id', 'fruit', 'count', 'weight'])\n\ndf\n\n\n\n\n\n\n\n\nbasket_id\nfruit\ncount\nweight\n\n\n\n\n0\n0\napple\n9\n0.694527\n\n\n1\n1\norange\n3\n1.250969\n\n\n2\n2\napple\n7\n0.057898\n\n\n3\n3\npapaya\n9\n0.130208\n\n\n4\n4\napple\n14\n1.986807\n\n\n5\n5\norange\n5\n1.873250\n\n\n6\n6\napple\n10\n0.510761\n\n\n7\n7\npapaya\n12\n1.030250\n\n\n\n\n\n\n\n\n# converting df to categorical\nfruit_cat = df['fruit'].astype('category')\nfruit_cat\n\n0     apple\n1    orange\n2     apple\n3    papaya\n4     apple\n5    orange\n6     apple\n7    papaya\nName: fruit, dtype: category\nCategories (3, object): ['apple', 'orange', 'papaya']\n\n\n\nc = fruit_cat.array\n\n\ntype(c)\n\npandas.core.arrays.categorical.Categorical\n\n\n\nc.categories\n\nIndex(['apple', 'orange', 'papaya'], dtype='object')\n\n\n\nc.codes\n\narray([0, 1, 0, 2, 0, 1, 0, 2], dtype=int8)\n\n\n\n# how to get mapping between code and categories\n\ndict(enumerate(c.categories))\n\n{0: 'apple', 1: 'orange', 2: 'papaya'}\n\n\n\npd.unique(values)\n\narray([0, 1], dtype=int64)"
  },
  {
    "objectID": "posts/python/data_cleaning+prepration.html#categorical-data-2",
    "href": "posts/python/data_cleaning+prepration.html#categorical-data-2",
    "title": "Data Cleaning",
    "section": "Categorical data",
    "text": "Categorical data\n\nvalues3 = pd.Series(['apple', 'orange', 'apple',\n                    'apple'] * 2)\n\n\nvalues3\n\n0     apple\n1    orange\n2     apple\n3     apple\n4     apple\n5    orange\n6     apple\n7     apple\ndtype: object\n\n\n\npd.unique(values3)\n\narray(['apple', 'orange'], dtype=object)\n\n\n\npd.value_counts(values3)\n\napple     6\norange    2\nName: count, dtype: int64\n\n\n\ncategories = ['foo', 'bar', 'baz']\n\ncodes = [0, 1, 2, 0, 2, 0, 1]\n\nmy_cats2 = pd.Categorical.from_codes(codes, categories)\n\nmy_cats2\n\n['foo', 'bar', 'baz', 'foo', 'baz', 'foo', 'bar']\nCategories (3, object): ['foo', 'bar', 'baz']\n\n\n\nordered_cat = pd.Categorical.from_codes(codes, categories, \n                                 ordered = True)\n\nordered_cat\n\n['foo', 'bar', 'baz', 'foo', 'baz', 'foo', 'bar']\nCategories (3, object): ['foo' &lt; 'bar' &lt; 'baz']"
  },
  {
    "objectID": "posts/python/data_cleaning+prepration.html#computations-with-categoricals",
    "href": "posts/python/data_cleaning+prepration.html#computations-with-categoricals",
    "title": "Data Cleaning",
    "section": "Computations with categoricals",
    "text": "Computations with categoricals\n\nrng = np.random.default_rng(seed = 123)\n\ndraws = rng.standard_normal(1000)\n\ndraws[:5]\n\narray([-0.98912135, -0.36778665,  1.28792526,  0.19397442,  0.9202309 ])\n\n\n\nbins = pd.qcut(draws, 4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n\nbins\n\n['Q1', 'Q2', 'Q4', 'Q3', 'Q4', ..., 'Q1', 'Q3', 'Q3', 'Q1', 'Q3']\nLength: 1000\nCategories (4, object): ['Q1' &lt; 'Q2' &lt; 'Q3' &lt; 'Q4']\n\n\n\nusing groupby for summary statistics\n\nbins = pd.Series(bins, name= 'quartile')\n\nresults = (pd.Series(draws)\n           .groupby(bins)\n           .agg(['count', 'min', 'max'])\n           .reset_index())\n\nresults\n\n\n\n\n\n\n\n\nquartile\ncount\nmin\nmax\n\n\n\n\n0\nQ1\n250\n-3.298281\n-0.626241\n\n\n1\nQ2\n250\n-0.622043\n0.040753\n\n\n2\nQ3\n250\n0.043084\n0.736086\n\n\n3\nQ4\n250\n0.738013\n3.058244\n\n\n\n\n\n\n\n\n\nBetter performance with categoricals\n\nN = 10_000_000\n\nlabels = pd.Series(['foo', 'bar', 'baz', 'qux']) * (N //4)\n\n\n# convert labels to categoricals\ncategories = labels.astype('category')\n\n\n# memory use\nlabels.memory_usage(deep = True)\n\n30000356\n\n\n\ncategories.memory_usage(deep = True)\n\n30000532"
  },
  {
    "objectID": "posts/python/data_cleaning+prepration.html#categorical-methods",
    "href": "posts/python/data_cleaning+prepration.html#categorical-methods",
    "title": "Data Cleaning",
    "section": "Categorical Methods",
    "text": "Categorical Methods\n\nlist of categorical methods\n\n\ns = pd.Series(['a', 'b', 'c', 'd'] * 2)\n\ncat_s = s.astype('category')\n\ncat_s\n\n0    a\n1    b\n2    c\n3    d\n4    a\n5    b\n6    c\n7    d\ndtype: category\nCategories (4, object): ['a', 'b', 'c', 'd']\n\n\n\n# using special accessor attribute cat\n\ncat_s.cat.codes\n\n0    0\n1    1\n2    2\n3    3\n4    0\n5    1\n6    2\n7    3\ndtype: int8\n\n\n\nactual_categories = ['a', 'b', 'c', 'd', 'e']\n\ncat_s2 = cat_s.cat.set_categories(actual_categories)\n\ncat_s2\n\n0    a\n1    b\n2    c\n3    d\n4    a\n5    b\n6    c\n7    d\ndtype: category\nCategories (5, object): ['a', 'b', 'c', 'd', 'e']\n\n\n\ncat_s.value_counts()\n\na    2\nb    2\nc    2\nd    2\nName: count, dtype: int64\n\n\n\ncat_s2.value_counts()\n\na    2\nb    2\nc    2\nd    2\ne    0\nName: count, dtype: int64\n\n\n\ncat_s3 = cat_s[cat_s.isin(['a', 'b'])]\n\ncat_s3\n\n0    a\n1    b\n4    a\n5    b\ndtype: category\nCategories (4, object): ['a', 'b', 'c', 'd']\n\n\n\n# removing unused categories\ncat_s3.cat.remove_unused_categories()\n\n0    a\n1    b\n4    a\n5    b\ndtype: category\nCategories (2, object): ['a', 'b']"
  },
  {
    "objectID": "posts/python/data_cleaning+prepration.html#creating-dummy-variables-for-modelling",
    "href": "posts/python/data_cleaning+prepration.html#creating-dummy-variables-for-modelling",
    "title": "Data Cleaning",
    "section": "Creating dummy variables for modelling",
    "text": "Creating dummy variables for modelling\n\ncat_s = pd.Series(['a', 'b', 'c', 'd'] * 2,\n                 dtype= 'category')\n\n\npd.get_dummies(cat_s)\n\n\n\n\n\n\n\n\na\nb\nc\nd\n\n\n\n\n0\nTrue\nFalse\nFalse\nFalse\n\n\n1\nFalse\nTrue\nFalse\nFalse\n\n\n2\nFalse\nFalse\nTrue\nFalse\n\n\n3\nFalse\nFalse\nFalse\nTrue\n\n\n4\nTrue\nFalse\nFalse\nFalse\n\n\n5\nFalse\nTrue\nFalse\nFalse\n\n\n6\nFalse\nFalse\nTrue\nFalse\n\n\n7\nFalse\nFalse\nFalse\nTrue"
  },
  {
    "objectID": "posts/python/Functions.html",
    "href": "posts/python/Functions.html",
    "title": "Functions",
    "section": "",
    "text": "evit repetition\nscope and local function\n\n\ndef func():\n    a = []\n    for i in range(5):\n        a.append(i)\n\n\na\n\nNameError: name 'a' is not defined\n\n\n\n# making the function global to fix it\n\n#a = None\n\ndef bind_a_variable():\n    global a\n    a = []\n    bind_a_variable()\n    \nprint(a)\n\nNone\n\n\n\nc = []\ndef func1():\n    for i in range(5):\n        c.append(i)\n\n\nc\n\n[0, 1, 2, 3, 4]\n\n\n\n\n\ndef f():\n    a = 5\n    b = 6\n    c = 7\n    return a, b, c\n\na, b, c = f()\n\n\nf()\n\n(5, 6, 7)\n\n\n\nf(1, 2, 3)  # traceback because 0 positional argument should have been provided\n\nTypeError: f() takes 0 positional arguments but 3 were given\n\n\n\ndef g():\n    a = 5\n    b = 3\n    c = 5\n    return {'a':a, 'b': b, 'c': c}\n\n\ng()\n\n{'a': 5, 'b': 3, 'c': 5}\n\n\n\nstates = [\"   Alabama \", \"Georgia!\", \"Georgia\", \"georgia\", \"FlOrIda\",          \"south   carolina##\", \"West virginia?\"]\n\n\n### data cleaning with functions\n\nimport re\n\ndef clean_strings(strings):\n    result = []\n    for value in strings:\n        value = value.strip()\n        value = re.sub(\"[!#?]\", \"\", value)\n        value = value.title()\n        result.append(value)\n    return result\n\n\nclean_strings(states)\n\n['Alabama',\n 'Georgia',\n 'Georgia',\n 'Georgia',\n 'Florida',\n 'South   Carolina',\n 'West Virginia']\n\n\n\n\n\n\ndef short_function(x):\n    return x * 2\n\n\n\nequiv_anon = lambda x: x * 2\n\n\nshort_function(3)\n\n6\n\n\n\nequiv_anan = 2\n\n\ndef apply_to_list (some_list, f):\n    return [f(x) for x in some_list]\n\n\n\nints = [4, 0, 1, 4, 6]\napply_to_list (ints, lambda x: x * 2)\n\n[8, 0, 2, 8, 12]\n\n\n\nstrings = ['foo', 'card', 'bar', 'aaa', 'abab']\n\nstrings.sort(key = lambda x : len(set(x)))\n\nstrings\n\n['aaa', 'foo', 'abab', 'bar', 'card']\n\n\n\n\n\n\nsome_dict = {'a': 1, 'b': 2, 'c': 3}\n\nfor key in some_dict:\n    print(key)\n\na\nb\nc\n\n\n\ndict_iterator = iter(some_dict)\n\ndict_iterator\n\n&lt;dict_keyiterator at 0x193c70f6040&gt;\n\n\n\nlist(dict_iterator)\n\n['a', 'b', 'c']\n\n\n\ntuple(dict_iterator)\n\n()\n\n\n\ndef squares(n = 10):\n    print(f\"Generating squares from 1 to {n **2}\")\n    for i in range (1, n + 1):\n        yield i ** 2\n\n\ngen = squares()\n\n\ngen\n\n&lt;generator object squares at 0x00000193C6733120&gt;\n\n\n\n# use for loop to see the output from generator\nfor x in gen:\n    print(x, end= ' ')\n\nGenerating squares from 1 to 100\n1 4 9 16 25 36 49 64 81 100 \n\n\n\n\n\n\ngen2 = (x ** 2 for x in range(100))\n\ngen2\n\n&lt;generator object &lt;genexpr&gt; at 0x00000193C6733D60&gt;\n\n\n\nfor x in gen2:\n    print(x, end= \" \")\n\n0 1 4 9 16 25 36 49 64 81 100 121 144 169 196 225 256 289 324 361 400 441 484 529 576 625 676 729 784 841 900 961 1024 1089 1156 1225 1296 1369 1444 1521 1600 1681 1764 1849 1936 2025 2116 2209 2304 2401 2500 2601 2704 2809 2916 3025 3136 3249 3364 3481 3600 3721 3844 3969 4096 4225 4356 4489 4624 4761 4900 5041 5184 5329 5476 5625 5776 5929 6084 6241 6400 6561 6724 6889 7056 7225 7396 7569 7744 7921 8100 8281 8464 8649 8836 9025 9216 9409 9604 9801 \n\n\n\n# verbose generator\n\ndef _make_gen():\n    for x in range(100):\n        yield x** 2\n\ngen = _make_gen()\n\n\n# using gen expressions as function arguments\n\nsum (x ** 2 for x in range(100))\n\n328350\n\n\n\ndict((i, i**2 ) for i in range(5))\n\n{0: 0, 1: 1, 2: 4, 3: 9, 4: 16}\n\n\n\n\n\n\ncontains collection of generators for many data algorithms\n\n\nimport itertools\n\ndef first_letter(x):\n    return x[0]\n\nnames =  ['Alan', 'Adam', 'Wes', 'Will', 'Albert']\n\nfor letter, names in itertools.groupby(names, first_letter):\n    print(letter, list(names))\n\nA ['Alan', 'Adam']\nW ['Wes', 'Will']\nA ['Albert']\n\n\n\n\n\n\ndef attempt_float(x):\n    try:\n        return float(x)\n    except:\n        return x\n\n\n\n\n1.22\n\n\n\nattempt_float('flower')\n\n'flower'\n\n\n\ndef attempt_float1(x):\n    try:\n        return float(x)\n    except(TypeError, ValueError):\n        return x\n\n\nattempt_float1('nice')\n\n'nice'"
  },
  {
    "objectID": "posts/python/Functions.html#functions",
    "href": "posts/python/Functions.html#functions",
    "title": "Functions",
    "section": "",
    "text": "evit repetition\nscope and local function\n\n\ndef func():\n    a = []\n    for i in range(5):\n        a.append(i)\n\n\na\n\nNameError: name 'a' is not defined\n\n\n\n# making the function global to fix it\n\n#a = None\n\ndef bind_a_variable():\n    global a\n    a = []\n    bind_a_variable()\n    \nprint(a)\n\nNone\n\n\n\nc = []\ndef func1():\n    for i in range(5):\n        c.append(i)\n\n\nc\n\n[0, 1, 2, 3, 4]\n\n\n\n\n\ndef f():\n    a = 5\n    b = 6\n    c = 7\n    return a, b, c\n\na, b, c = f()\n\n\nf()\n\n(5, 6, 7)\n\n\n\nf(1, 2, 3)  # traceback because 0 positional argument should have been provided\n\nTypeError: f() takes 0 positional arguments but 3 were given\n\n\n\ndef g():\n    a = 5\n    b = 3\n    c = 5\n    return {'a':a, 'b': b, 'c': c}\n\n\ng()\n\n{'a': 5, 'b': 3, 'c': 5}\n\n\n\nstates = [\"   Alabama \", \"Georgia!\", \"Georgia\", \"georgia\", \"FlOrIda\",          \"south   carolina##\", \"West virginia?\"]\n\n\n### data cleaning with functions\n\nimport re\n\ndef clean_strings(strings):\n    result = []\n    for value in strings:\n        value = value.strip()\n        value = re.sub(\"[!#?]\", \"\", value)\n        value = value.title()\n        result.append(value)\n    return result\n\n\nclean_strings(states)\n\n['Alabama',\n 'Georgia',\n 'Georgia',\n 'Georgia',\n 'Florida',\n 'South   Carolina',\n 'West Virginia']\n\n\n\n\n\n\ndef short_function(x):\n    return x * 2\n\n\n\nequiv_anon = lambda x: x * 2\n\n\nshort_function(3)\n\n6\n\n\n\nequiv_anan = 2\n\n\ndef apply_to_list (some_list, f):\n    return [f(x) for x in some_list]\n\n\n\nints = [4, 0, 1, 4, 6]\napply_to_list (ints, lambda x: x * 2)\n\n[8, 0, 2, 8, 12]\n\n\n\nstrings = ['foo', 'card', 'bar', 'aaa', 'abab']\n\nstrings.sort(key = lambda x : len(set(x)))\n\nstrings\n\n['aaa', 'foo', 'abab', 'bar', 'card']\n\n\n\n\n\n\nsome_dict = {'a': 1, 'b': 2, 'c': 3}\n\nfor key in some_dict:\n    print(key)\n\na\nb\nc\n\n\n\ndict_iterator = iter(some_dict)\n\ndict_iterator\n\n&lt;dict_keyiterator at 0x193c70f6040&gt;\n\n\n\nlist(dict_iterator)\n\n['a', 'b', 'c']\n\n\n\ntuple(dict_iterator)\n\n()\n\n\n\ndef squares(n = 10):\n    print(f\"Generating squares from 1 to {n **2}\")\n    for i in range (1, n + 1):\n        yield i ** 2\n\n\ngen = squares()\n\n\ngen\n\n&lt;generator object squares at 0x00000193C6733120&gt;\n\n\n\n# use for loop to see the output from generator\nfor x in gen:\n    print(x, end= ' ')\n\nGenerating squares from 1 to 100\n1 4 9 16 25 36 49 64 81 100 \n\n\n\n\n\n\ngen2 = (x ** 2 for x in range(100))\n\ngen2\n\n&lt;generator object &lt;genexpr&gt; at 0x00000193C6733D60&gt;\n\n\n\nfor x in gen2:\n    print(x, end= \" \")\n\n0 1 4 9 16 25 36 49 64 81 100 121 144 169 196 225 256 289 324 361 400 441 484 529 576 625 676 729 784 841 900 961 1024 1089 1156 1225 1296 1369 1444 1521 1600 1681 1764 1849 1936 2025 2116 2209 2304 2401 2500 2601 2704 2809 2916 3025 3136 3249 3364 3481 3600 3721 3844 3969 4096 4225 4356 4489 4624 4761 4900 5041 5184 5329 5476 5625 5776 5929 6084 6241 6400 6561 6724 6889 7056 7225 7396 7569 7744 7921 8100 8281 8464 8649 8836 9025 9216 9409 9604 9801 \n\n\n\n# verbose generator\n\ndef _make_gen():\n    for x in range(100):\n        yield x** 2\n\ngen = _make_gen()\n\n\n# using gen expressions as function arguments\n\nsum (x ** 2 for x in range(100))\n\n328350\n\n\n\ndict((i, i**2 ) for i in range(5))\n\n{0: 0, 1: 1, 2: 4, 3: 9, 4: 16}\n\n\n\n\n\n\ncontains collection of generators for many data algorithms\n\n\nimport itertools\n\ndef first_letter(x):\n    return x[0]\n\nnames =  ['Alan', 'Adam', 'Wes', 'Will', 'Albert']\n\nfor letter, names in itertools.groupby(names, first_letter):\n    print(letter, list(names))\n\nA ['Alan', 'Adam']\nW ['Wes', 'Will']\nA ['Albert']\n\n\n\n\n\n\ndef attempt_float(x):\n    try:\n        return float(x)\n    except:\n        return x\n\n\n\n\n1.22\n\n\n\nattempt_float('flower')\n\n'flower'\n\n\n\ndef attempt_float1(x):\n    try:\n        return float(x)\n    except(TypeError, ValueError):\n        return x\n\n\nattempt_float1('nice')\n\n'nice'"
  },
  {
    "objectID": "posts/python/Geopandas2.html",
    "href": "posts/python/Geopandas2.html",
    "title": "Geopandas2",
    "section": "",
    "text": "import geopandas as gpd\nimport pandas as pd\nimport numpy as np\nfrom shapely.geometry import LineString\n\n# for interactive maps\nimport folium\nfrom folium import Choropleth, Circle, Marker\nfrom folium.plugins import HeatMap, MarkerCluster\nimport math\n\n# for Geocoding\nfrom geopy.geocoders import Nominatim"
  },
  {
    "objectID": "posts/python/Geopandas2.html#geocoding",
    "href": "posts/python/Geopandas2.html#geocoding",
    "title": "Geopandas2",
    "section": "Geocoding",
    "text": "Geocoding\n\nprocess of converting name of a place or an address to a location on a Map\n\n\nexample location=[54, 15];\n\nLatidute coordinate, 54 (ranges from -90 to 90 degrees); positive value - northern hemisphere)\n\n\nLongitude coordinate, 15 (ranges from -180 to 180 degrees; positive value- eastern hemisphere)\n\ngeolocator = Nominatim (user_agent = 'Kunal Khurana')\nlocation = geolocator.geocode('Taj Mahal')\n\nprint(location.point)\nprint(location.address)\n\n27 10m 30.027s N, 78 2m 31.5645s E\nTaj Mahal, Taj East Gate Road, Taj Ganj, Agra, Agra District, Uttar Pradesh, 282004, India\n\n\n\npoint = location.point\nprint(\"Latitude:\", point.latitude)\nprint(\"Longitude:\", point.longitude)\n\nLatitude: 27.1750075\nLongitude: 78.04210126365584\n\n\n\n# top universities\nuniversities = pd.read_csv('data_for_all_courses\\\\top_universities.csv')\nuniversities.head()\n\n\n\n\n\n\n\n\nName\n\n\n\n\n0\nUniversity of Oxford\n\n\n1\nUniversity of Cambridge\n\n\n2\nImperial College London\n\n\n3\nETH Zurich\n\n\n4\nUCL\n\n\n\n\n\n\n\n\n# using lambda function to apply geocoder to every row in DataFrame\ndef my_geocoder(row):\n    #use try/excepth where geocoding is unsuccessful\n    try:\n        point = geolocator.geocode(row).point\n        return pd.Series({'Latitude': point.latitude, 'Longitude': point.longitude})\n    except:\n        return None\n    \nuniversities[['Latitude', 'Longitude']] = universities.apply(lambda x: my_geocoder(x['Name']), axis = 1)\n\nprint(f\"{(1-sum(np.isnan(universities['Latitude'])) / len(universities)) * 100}% of addresses were geocoded!\")\n    \n    \n\n95.0% of addresses were geocoded!\n\n\n\n# frop universities that were not successfully geocoded\nuniversities = universities.dropna(subset= ['Latitude']).copy()\nuniversities = gpd.GeoDataFrame(universities, \n                               geometry = gpd.points_from_xy (universities.Longitude, universities.Latitude))\nuniversities.crs\nuniversities.head()\n\n\n\n\n\n\n\n\nName\nLatitude\nLongitude\ngeometry\n\n\n\n\n0\nUniversity of Oxford\n51.758708\n-1.255668\nPOINT (-1.25567 51.75871)\n\n\n1\nUniversity of Cambridge\n52.210946\n0.092005\nPOINT (0.09200 52.21095)\n\n\n2\nImperial College London\n51.498959\n-0.175641\nPOINT (-0.17564 51.49896)\n\n\n3\nETH Zurich\n47.413218\n8.537491\nPOINT (8.53749 47.41322)\n\n\n4\nUCL\n51.521785\n-0.135151\nPOINT (-0.13515 51.52179)\n\n\n\n\n\n\n\n\n# create a map\nm = folium.Map(location = [20, 79], titles = 'openstreetmap', zoom_start=2)\n\n# add points\nfor idx, row in universities.iterrows():\n    Marker([row['Latitude'], row['Longitude']], popup= row['Name']).add_to(m)\n    \n# display\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "posts/python/Geopandas2.html#geodataframe-table-joins",
    "href": "posts/python/Geopandas2.html#geodataframe-table-joins",
    "title": "Geopandas2",
    "section": "GeoDataFrame Table joins",
    "text": "GeoDataFrame Table joins\n\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\neurope = world.loc[world.continent == \"Europe\"].reset_index(drop = True)\n\nC:\\Users\\Khurana_Kunal\\AppData\\Local\\Temp\\ipykernel_13672\\2106073632.py:1: FutureWarning: The geopandas.dataset module is deprecated and will be removed in GeoPandas 1.0. You can get the original 'naturalearth_lowres' data from https://www.naturalearthdata.com/downloads/110m-cultural-vectors/.\n  world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n\n\n\neurope.head()\n\n\n\n\n\n\n\n\npop_est\ncontinent\nname\niso_a3\ngdp_md_est\ngeometry\n\n\n\n\n0\n144373535.0\nEurope\nRussia\nRUS\n1699876\nMULTIPOLYGON (((180.00000 71.51571, 180.00000 ...\n\n\n1\n5347896.0\nEurope\nNorway\nNOR\n403336\nMULTIPOLYGON (((15.14282 79.67431, 15.52255 80...\n\n\n2\n67059887.0\nEurope\nFrance\nFRA\n2715518\nMULTIPOLYGON (((-51.65780 4.15623, -52.24934 3...\n\n\n3\n10285453.0\nEurope\nSweden\nSWE\n530883\nPOLYGON ((11.02737 58.85615, 11.46827 59.43239...\n\n\n4\n9466856.0\nEurope\nBelarus\nBLR\n63080\nPOLYGON ((28.17671 56.16913, 29.22951 55.91834...\n\n\n\n\n\n\n\n\neurope_stats = europe[['name', 'pop_est', 'gdp_md_est']]\neurope_boundaries = europe [['name', 'geometry']]\n\n\neurope_stats.head()\n\n\n\n\n\n\n\n\nname\npop_est\ngdp_md_est\n\n\n\n\n0\nRussia\n144373535.0\n1699876\n\n\n1\nNorway\n5347896.0\n403336\n\n\n2\nFrance\n67059887.0\n2715518\n\n\n3\nSweden\n10285453.0\n530883\n\n\n4\nBelarus\n9466856.0\n63080\n\n\n\n\n\n\n\n\neurope_boundaries.head()\n\n\n\n\n\n\n\n\nname\ngeometry\n\n\n\n\n0\nRussia\nMULTIPOLYGON (((180.00000 71.51571, 180.00000 ...\n\n\n1\nNorway\nMULTIPOLYGON (((15.14282 79.67431, 15.52255 80...\n\n\n2\nFrance\nMULTIPOLYGON (((-51.65780 4.15623, -52.24934 3...\n\n\n3\nSweden\nPOLYGON ((11.02737 58.85615, 11.46827 59.43239...\n\n\n4\nBelarus\nPOLYGON ((28.17671 56.16913, 29.22951 55.91834...\n\n\n\n\n\n\n\n\n# use'name' to merge europe_boundaries and europe_stats\n\neurope2 = europe_boundaries.merge(europe_stats, on= 'name')\neurope2.head()\n\n\n\n\n\n\n\n\nname\ngeometry\npop_est\ngdp_md_est\n\n\n\n\n0\nRussia\nMULTIPOLYGON (((180.00000 71.51571, 180.00000 ...\n144373535.0\n1699876\n\n\n1\nNorway\nMULTIPOLYGON (((15.14282 79.67431, 15.52255 80...\n5347896.0\n403336\n\n\n2\nFrance\nMULTIPOLYGON (((-51.65780 4.15623, -52.24934 3...\n67059887.0\n2715518\n\n\n3\nSweden\nPOLYGON ((11.02737 58.85615, 11.46827 59.43239...\n10285453.0\n530883\n\n\n4\nBelarus\nPOLYGON ((28.17671 56.16913, 29.22951 55.91834...\n9466856.0\n63080"
  },
  {
    "objectID": "posts/python/Geopandas2.html#spatial-join",
    "href": "posts/python/Geopandas2.html#spatial-join",
    "title": "Geopandas2",
    "section": "Spatial join",
    "text": "Spatial join\n\njoining data based on geometry\n\n\n# match universities to countries\n\neuropean_universities = gpd.sjoin(universities, europe2)\n\nC:\\Users\\Khurana_Kunal\\AppData\\Local\\Temp\\ipykernel_13672\\546710102.py:3: UserWarning: CRS mismatch between the CRS of left geometries and the CRS of right geometries.\nUse `to_crs()` to reproject one of the input geometries to match the CRS of the other.\n\nLeft CRS: None\nRight CRS: EPSG:4326\n\n  european_universities = gpd.sjoin(universities, europe2)\n\n\n\n# check the crs for europe2\neurope2.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\n# lets provide same crs value to universities\nuniversities = universities.set_crs('epsg: 4326')\n\n\neuropean_unviersities = gpd.sjoin(universities, europe2)\neuropean_universities.head()\n\n\n\n\n\n\n\n\nName\nLatitude\nLongitude\ngeometry\nindex_right\nname\npop_est\ngdp_md_est\n\n\n\n\n0\nUniversity of Oxford\n51.758708\n-1.255668\nPOINT (-1.25567 51.75871)\n28\nUnited Kingdom\n66834405.0\n2829108\n\n\n1\nUniversity of Cambridge\n52.210946\n0.092005\nPOINT (0.09200 52.21095)\n28\nUnited Kingdom\n66834405.0\n2829108\n\n\n2\nImperial College London\n51.498959\n-0.175641\nPOINT (-0.17564 51.49896)\n28\nUnited Kingdom\n66834405.0\n2829108\n\n\n4\nUCL\n51.521785\n-0.135151\nPOINT (-0.13515 51.52179)\n28\nUnited Kingdom\n66834405.0\n2829108\n\n\n5\nLondon School of Economics and Political Science\n51.514261\n-0.116734\nPOINT (-0.11673 51.51426)\n28\nUnited Kingdom\n66834405.0\n2829108\n\n\n\n\n\n\n\n\n# investigate the result\nprint (f\"We located {len(universities)} universities.\")\n\nprint (f\"Out of 95,  {len(european_universities)} \"\n       f\"of the universities were located in Europe in \"\n       f\"{len(european_universities.name.unique())}\"\n       f\" different countries.\" )\n\nWe located 95 universities.\nOut of 95,  89 of the universities were located in Europe in 15 different countries.\n\n\n\neuropean_universities.head()\n\n\n\n\n\n\n\n\nName\nLatitude\nLongitude\ngeometry\nindex_right\nname\npop_est\ngdp_md_est\n\n\n\n\n0\nUniversity of Oxford\n51.758708\n-1.255668\nPOINT (-1.25567 51.75871)\n28\nUnited Kingdom\n66834405.0\n2829108\n\n\n1\nUniversity of Cambridge\n52.210946\n0.092005\nPOINT (0.09200 52.21095)\n28\nUnited Kingdom\n66834405.0\n2829108\n\n\n2\nImperial College London\n51.498959\n-0.175641\nPOINT (-0.17564 51.49896)\n28\nUnited Kingdom\n66834405.0\n2829108\n\n\n4\nUCL\n51.521785\n-0.135151\nPOINT (-0.13515 51.52179)\n28\nUnited Kingdom\n66834405.0\n2829108\n\n\n5\nLondon School of Economics and Political Science\n51.514261\n-0.116734\nPOINT (-0.11673 51.51426)\n28\nUnited Kingdom\n66834405.0\n2829108"
  },
  {
    "objectID": "posts/python/Geopandas2.html#potentially-suitable-areas-for-starbucks",
    "href": "posts/python/Geopandas2.html#potentially-suitable-areas-for-starbucks",
    "title": "Geopandas2",
    "section": "Potentially suitable areas for starbucks",
    "text": "Potentially suitable areas for starbucks\n\n# for maps visualization\ndef embed_map(m, file_name):\n    from IPython.display import IFrame\n    m.save(file_name)\n    return IFrame(file_name, width='100%', height='500px')\n\n\nstarbucks = pd.read_csv('data_for_all_courses\\starbucks_locations.csv')\nstarbucks.head()\n\n\n\n\n\n\n\n\nStore Number\nStore Name\nAddress\nCity\nLongitude\nLatitude\n\n\n\n\n0\n10429-100710\nPalmdale & Hwy 395\n14136 US Hwy 395 Adelanto CA\nAdelanto\n-117.40\n34.51\n\n\n1\n635-352\nKanan & Thousand Oaks\n5827 Kanan Road Agoura CA\nAgoura\n-118.76\n34.16\n\n\n2\n74510-27669\nVons-Agoura Hills #2001\n5671 Kanan Rd. Agoura Hills CA\nAgoura Hills\n-118.76\n34.15\n\n\n3\n29839-255026\nTarget Anaheim T-0677\n8148 E SANTA ANA CANYON ROAD AHAHEIM CA\nAHAHEIM\n-117.75\n33.87\n\n\n4\n23463-230284\nSafeway - Alameda 3281\n2600 5th Street Alameda CA\nAlameda\n-122.28\n37.79\n\n\n\n\n\n\n\n\n# print individual missing values\nprint(f\"The DataFrame starbucks has {starbucks.isnull().sum()} missing values\")\n\nThe DataFrame starbucks has Store Number    0\nStore Name      0\nAddress         0\nCity            0\nLongitude       5\nLatitude        5\ndtype: int64 missing values\n\n\n\n# printing missing rows\nmissing_value_rows = starbucks[starbucks.isnull().any(axis=1)]\nprint(missing_value_rows)\n\n     Store Number                Store Name  \\\n153      5406-945  2224 Shattuck - Berkeley   \n154       570-512                Solano Ave   \n155  17877-164526   Safeway - Berkeley #691   \n156  19864-202264         Telegraph & Ashby   \n157     9217-9253           2128 Oxford St.   \n\n                               Address      City  Longitude  Latitude  \n153   2224 Shattuck Avenue Berkeley CA  Berkeley        NaN       NaN  \n154     1799 Solano Avenue Berkeley CA  Berkeley        NaN       NaN  \n155    1444 Shattuck Place Berkeley CA  Berkeley        NaN       NaN  \n156  3001 Telegraph Avenue Berkeley CA  Berkeley        NaN       NaN  \n157     2128 Oxford Street Berkeley CA  Berkeley        NaN       NaN  \n\n\n\n# print total missing values\nprint(f\"The DataFrame starbucks has {starbucks.isnull().sum().sum()} missing values\")\n\nThe DataFrame starbucks has 10 missing values\n\n\n\n# removing missing values\nstarbucks_clean = print (f\"We are removing missing values from the starbucks \"\n                         f\"{starbucks.dropna(inplace = True)} dataframe.\")\n\nWe are removing missing values from the starbucks None dataframe.\n\n\n\n# checking\nprint(starbucks.isnull().sum())\n\nStore Number    0\nStore Name      0\nAddress         0\nCity            0\nLongitude       0\nLatitude        0\ndtype: int64\n\n\n\ninference\n\nAll the missing values are in Berkley city.\n\n\n# create geocoder that adds latitude and longitude values\n\ngeolocator = Nominatim (user_agent = \"Kunal Khurana \")\n\ndef my_geocoder(row):\n    point = geolocator.geocode(row).point\n    return pd.Series({'Latitude': point.latitude, 'Longitude': point.longitude})\n\nberkley_locations = missing_value_rows.apply(lambda x: my_geocoder(x['Address']), axis = 1)\nstarbucks.update(berkley_locations)\n\n\nstarbucks.describe\n\n&lt;bound method NDFrame.describe of       Store Number                  Store Name  \\\n0     10429-100710          Palmdale & Hwy 395   \n1          635-352       Kanan & Thousand Oaks   \n2      74510-27669     Vons-Agoura Hills #2001   \n3     29839-255026       Target Anaheim T-0677   \n4     23463-230284      Safeway - Alameda 3281   \n...            ...                         ...   \n2816  14071-108147  Hwy 20 & Tharp - Yuba City   \n2817    9974-98559  Yucaipa & Hampton, Yucaipa   \n2818  79654-108478        Vons - Yucaipa #1796   \n2819   6438-245084               Yucaipa & 6th   \n2820    6829-82142   Highway 62 & Warren Vista   \n\n                                      Address          City  Longitude  \\\n0                14136 US Hwy 395 Adelanto CA      Adelanto    -117.40   \n1                   5827 Kanan Road Agoura CA        Agoura    -118.76   \n2              5671 Kanan Rd. Agoura Hills CA  Agoura Hills    -118.76   \n3     8148 E SANTA ANA CANYON ROAD AHAHEIM CA       AHAHEIM    -117.75   \n4                  2600 5th Street Alameda CA       Alameda    -122.28   \n...                                       ...           ...        ...   \n2816    1615 Colusa Hwy, Ste 100 Yuba City CA     Yuba City    -121.64   \n2817        31364 Yucaipa Blvd., A Yucaipa CA       Yucaipa    -117.12   \n2818            33644 YUCAIPA BLVD YUCAIPA CA       YUCAIPA    -117.07   \n2819      34050 Yucaipa Blvd., 200 Yucaipa CA       Yucaipa    -117.06   \n2820  57744  29 Palms Highway Yucca Valley CA  Yucca Valley    -116.40   \n\n      Latitude  \n0        34.51  \n1        34.16  \n2        34.15  \n3        33.87  \n4        37.79  \n...        ...  \n2816     39.14  \n2817     34.03  \n2818     34.04  \n2819     34.03  \n2820     34.13  \n\n[2816 rows x 6 columns]&gt;\n\n\n\nstarbucks.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 2816 entries, 0 to 2820\nData columns (total 6 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   Store Number  2816 non-null   object \n 1   Store Name    2816 non-null   object \n 2   Address       2816 non-null   object \n 3   City          2816 non-null   object \n 4   Longitude     2816 non-null   float64\n 5   Latitude      2816 non-null   float64\ndtypes: float64(2), object(4)\nmemory usage: 154.0+ KB\n\n\n\n# base map\nm_base = folium.Map(location= [37.88, -122.26], tiles='openstreetmap', zoom_start = 12)\n\n# add markers\nfor idx, row in starbucks[starbucks['City']=='Berkeley'].iterrows():\n    Marker([row['Latitude'], row['Longitude']]).add_to(m_base)    \n    \n# display\nm_base\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nstarbucks.head()\n\n\n\n\n\n\n\n\nStore Number\nStore Name\nAddress\nCity\nLongitude\nLatitude\n\n\n\n\n0\n10429-100710\nPalmdale & Hwy 395\n14136 US Hwy 395 Adelanto CA\nAdelanto\n-117.40\n34.51\n\n\n1\n635-352\nKanan & Thousand Oaks\n5827 Kanan Road Agoura CA\nAgoura\n-118.76\n34.16\n\n\n2\n74510-27669\nVons-Agoura Hills #2001\n5671 Kanan Rd. Agoura Hills CA\nAgoura Hills\n-118.76\n34.15\n\n\n3\n29839-255026\nTarget Anaheim T-0677\n8148 E SANTA ANA CANYON ROAD AHAHEIM CA\nAHAHEIM\n-117.75\n33.87\n\n\n4\n23463-230284\nSafeway - Alameda 3281\n2600 5th Street Alameda CA\nAlameda\n-122.28\n37.79"
  },
  {
    "objectID": "posts/python/p11.html",
    "href": "posts/python/p11.html",
    "title": "Working with APIs",
    "section": "",
    "text": "with web applicaiton programming interphase request specific information from a website to generate a visualization\nwrite programs that gather data they need and create a visualiztion\nuse Github’s API to explore the most starred projects on GitHub\nUse Requests package to issue and process results.\nUse plotly to generate and customize the appearance of charts"
  },
  {
    "objectID": "posts/python/p11.html#resources",
    "href": "posts/python/p11.html#resources",
    "title": "Working with APIs",
    "section": "Resources",
    "text": "Resources\n\nPlotly guide\nConfigure plotly visualizations\nAPI documentation\nHacker news API\n\n\nimport requests\n\n# make an API and store the response\n\nurl = 'https://api.github.com/search/repositories?q=language:python&sort=stars'\nheaders = {'Accept': 'application/vnd.github.v3+json'}  #uses specific version\nr = requests.get(url, headers = headers)  #using requests to make a call to API\nprint(f\"Status code: {r.status_code}\")\n\n#store API response in a variable\n\nresponse_dict = r.json()  #using json method to convert it into a pyton dictionary\n\n# process results\n\nprint(response_dict.keys())\n\nStatus code: 200\ndict_keys(['total_count', 'incomplete_results', 'items'])\n\n\n\nresponse_dict = r.json()  \n\nprint (f\"Total repositories: {response_dict['total_count']}\")  #prints total count\n\n# explore information about repos\nrepo_dicts = response_dict ['items']  #storing the list in repo_dicts\nprint(f\"Repositories returned: {len(repo_dicts)}\")\n\n# examine the first repository\nrepo_dict = repo_dicts[0]\nprint(f\"\\nKeys: {len(repo_dict)}\")\nfor key in sorted(repo_dict.keys()):\n    print(key)\n\nTotal repositories: 9344484\nRepositories returned: 30\n\nKeys: 80\nallow_forking\narchive_url\narchived\nassignees_url\nblobs_url\nbranches_url\nclone_url\ncollaborators_url\ncomments_url\ncommits_url\ncompare_url\ncontents_url\ncontributors_url\ncreated_at\ndefault_branch\ndeployments_url\ndescription\ndisabled\ndownloads_url\nevents_url\nfork\nforks\nforks_count\nforks_url\nfull_name\ngit_commits_url\ngit_refs_url\ngit_tags_url\ngit_url\nhas_discussions\nhas_downloads\nhas_issues\nhas_pages\nhas_projects\nhas_wiki\nhomepage\nhooks_url\nhtml_url\nid\nis_template\nissue_comment_url\nissue_events_url\nissues_url\nkeys_url\nlabels_url\nlanguage\nlanguages_url\nlicense\nmerges_url\nmilestones_url\nmirror_url\nname\nnode_id\nnotifications_url\nopen_issues\nopen_issues_count\nowner\nprivate\npulls_url\npushed_at\nreleases_url\nscore\nsize\nssh_url\nstargazers_count\nstargazers_url\nstatuses_url\nsubscribers_url\nsubscription_url\nsvn_url\ntags_url\nteams_url\ntopics\ntrees_url\nupdated_at\nurl\nvisibility\nwatchers\nwatchers_count\nweb_commit_signoff_required\n\n\n\nPull out values from keys in repo_dict\n\n# explore information about repos\nrepo_dicts = response_dict ['items']  \n\n# examine first\nrepo_dict = repo_dicts[0]\n\nprint (\"\\n Selected information about first repository:\")\nprint(f\"Name: {repo_dict['name']}\")\nprint(f\"Owner: {repo_dict['owner']['login']}\")\nprint(f\"Stars: {repo_dict['stargazers_count']}\")\nprint (f\"Repository: {repo_dict['html_url']}\")\nprint(f\"Created: {repo_dict['created_at']}\")\nprint(f\"Updated: {repo_dict['updated_at']}\")\nprint(f\"Description: {repo_dict['description']}\")\n\n\n Selected information about first repository:\nName: Python-100-Days\nOwner: jackfrued\nStars: 142124\nRepository: https://github.com/jackfrued/Python-100-Days\nCreated: 2018-03-01T16:05:52Z\nUpdated: 2023-11-13T16:14:16Z\nDescription: Python - 100天从新手到大师\n\n\n\n# examine second\nrepo_dict = repo_dicts[1]\n\nprint (\"\\n Selected information about second repository:\")\nprint(f\"Name: {repo_dict['name']}\")\nprint(f\"Owner: {repo_dict['owner']['login']}\")\nprint(f\"Stars: {repo_dict['stargazers_count']}\")\nprint (f\"Repository: {repo_dict['html_url']}\")\nprint(f\"Created: {repo_dict['created_at']}\")\nprint(f\"Updated: {repo_dict['updated_at']}\")\nprint(f\"Description: {repo_dict['description']}\")\n\n\n Selected information about second repository:\nName: ColossalAI\nOwner: hpcaitech\nStars: 35236\nRepository: https://github.com/hpcaitech/ColossalAI\nCreated: 2021-10-28T16:19:44Z\nUpdated: 2023-11-13T20:52:01Z\nDescription: Making large AI models cheaper, faster and more accessible\n\n\n\n\nSummarizing top repositories\n\n#prints total count first\nresponse_dict = r.json()  \n\nprint (f\"Total repositories: {response_dict['total_count']}\")  \n\n# explore information first\nrepo_dicts = response_dict['items']\nprint(f\"\\nRepositories returned: {len(repo_dicts)}\")\n\nprint(\"\\nSelected information about each repository:\")\nfor repo_dict in repo_dicts:\n    print(f\"Name: {repo_dict['name']}\")\n    print(f\"Owner: {repo_dict['owner']['login']}\")\n    print(f\"Stars: {repo_dict['stargazers_count']}\")\n    print (f\"Repository: {repo_dict['html_url']}\")\n    print(f\"Description: {repo_dict['description']}\")\n\nTotal repositories: 9344484\n\nRepositories returned: 30\n\nSelected information about each repository:\nName: Python-100-Days\nOwner: jackfrued\nStars: 142124\nRepository: https://github.com/jackfrued/Python-100-Days\nDescription: Python - 100天从新手到大师\nName: ColossalAI\nOwner: hpcaitech\nStars: 35236\nRepository: https://github.com/hpcaitech/ColossalAI\nDescription: Making large AI models cheaper, faster and more accessible\nName: DragGAN\nOwner: XingangPan\nStars: 33710\nRepository: https://github.com/XingangPan/DragGAN\nDescription: Official Code for DragGAN (SIGGRAPH 2023)\nName: open-interpreter\nOwner: KillianLucas\nStars: 33035\nRepository: https://github.com/KillianLucas/open-interpreter\nDescription: OpenAI's Code Interpreter in your terminal, running locally\nName: XX-Net\nOwner: XX-net\nStars: 32300\nRepository: https://github.com/XX-net/XX-Net\nDescription: A proxy tool to bypass GFW.\nName: MockingBird\nOwner: babysor\nStars: 31783\nRepository: https://github.com/babysor/MockingBird\nDescription: 🚀AI拟声: 5秒内克隆您的声音并生成任意语音内容 Clone a voice in 5 seconds to generate arbitrary speech in real-time\nName: HanLP\nOwner: hankcs\nStars: 30751\nRepository: https://github.com/hankcs/HanLP\nDescription: Natural Language Processing for the next decade. Tokenization, Part-of-Speech Tagging, Named Entity Recognition, Syntactic & Semantic Dependency Parsing, Document Classification\nName: ray\nOwner: ray-project\nStars: 28605\nRepository: https://github.com/ray-project/ray\nDescription: Ray is a unified framework for scaling AI and Python applications. Ray consists of a core distributed runtime and a set of AI Libraries for accelerating ML workloads.\nName: ItChat\nOwner: littlecodersh\nStars: 24369\nRepository: https://github.com/littlecodersh/ItChat\nDescription: A complete and graceful API for Wechat. 微信个人号接口、微信机器人及命令行微信，三十行即可自定义个人号机器人。\nName: hosts\nOwner: StevenBlack\nStars: 24161\nRepository: https://github.com/StevenBlack/hosts\nDescription: 🔒 Consolidating and extending hosts files from several well-curated sources. Optionally pick extensions for porn, social media, and other categories.\nName: dash\nOwner: plotly\nStars: 19624\nRepository: https://github.com/plotly/dash\nDescription: Data Apps & Dashboards for Python. No JavaScript Required.\nName: chatgpt-on-wechat\nOwner: zhayujie\nStars: 17214\nRepository: https://github.com/zhayujie/chatgpt-on-wechat\nDescription: Wechat robot based on ChatGPT,  which using OpenAI api and itchat library. 使用大模型搭建微信聊天机器人，基于 GPT3.5/GPT4.0/Claude/文心一言/讯飞星火/LinkAI，支持个人微信、公众号、企业微信部署，能处理文本、语音和图片，访问操作系统和互联网，支持基于知识库定制专属机器人。\nName: Hitomi-Downloader\nOwner: KurtBestor\nStars: 17151\nRepository: https://github.com/KurtBestor/Hitomi-Downloader\nDescription: :cake: Desktop utility to download images/videos/music/text from various websites, and more.\nName: recommenders\nOwner: recommenders-team\nStars: 16663\nRepository: https://github.com/recommenders-team/recommenders\nDescription: Best Practices on Recommendation Systems\nName: loguru\nOwner: Delgan\nStars: 16585\nRepository: https://github.com/Delgan/loguru\nDescription: Python logging made (stupidly) simple\nName: awesome-oss-alternatives\nOwner: RunaCapital\nStars: 14225\nRepository: https://github.com/RunaCapital/awesome-oss-alternatives\nDescription: Awesome list of open-source startup alternatives to well-known SaaS products 🚀\nName: learn_python3_spider\nOwner: wistbean\nStars: 13936\nRepository: https://github.com/wistbean/learn_python3_spider\nDescription: python爬虫教程系列、从0到1学习python爬虫，包括浏览器抓包，手机APP抓包，如 fiddler、mitmproxy，各种爬虫涉及的模块的使用，如：requests、beautifulSoup、selenium、appium、scrapy等，以及IP代理，验证码识别，Mysql，MongoDB数据库的python使用，多线程多进程爬虫的使用，css 爬虫加密逆向破解，JS爬虫逆向，分布式爬虫，爬虫项目实战实例等\nName: mlc-llm\nOwner: mlc-ai\nStars: 13821\nRepository: https://github.com/mlc-ai/mlc-llm\nDescription: Enable everyone to develop, optimize and deploy AI models natively on everyone's devices.\nName: mackup\nOwner: lra\nStars: 13780\nRepository: https://github.com/lra/mackup\nDescription: Keep your application settings in sync (OS X/Linux)\nName: ChuanhuChatGPT\nOwner: GaiZhenbiao\nStars: 13206\nRepository: https://github.com/GaiZhenbiao/ChuanhuChatGPT\nDescription: GUI for ChatGPT API and many LLMs. Supports agents, file-based QA, GPT finetuning and query with web search. All with a neat UI.\nName: searx\nOwner: searx\nStars: 13193\nRepository: https://github.com/searx/searx\nDescription: Privacy-respecting metasearch engine\nName: PySimpleGUI\nOwner: PySimpleGUI\nStars: 12208\nRepository: https://github.com/PySimpleGUI/PySimpleGUI\nDescription: Launched in 2018. It's 2023 and PySimpleGUI is actively developed & supported. Create complex windows simply. Supports tkinter, Qt, WxPython, Remi (in browser). Create GUI applications trivially with a full set of widgets. Multi-Window applications are also simple. 3.4 to 3.11 supported. 325+ Demo programs & Cookbook for rapid start. Extensive docs\nName: redis-py\nOwner: redis\nStars: 11878\nRepository: https://github.com/redis/redis-py\nDescription: Redis Python Client\nName: pelican\nOwner: getpelican\nStars: 11872\nRepository: https://github.com/getpelican/pelican\nDescription: Static site generator that supports Markdown and reST syntax. Powered by Python.\nName: awesome-aws\nOwner: donnemartin\nStars: 11799\nRepository: https://github.com/donnemartin/awesome-aws\nDescription: A curated list of awesome Amazon Web Services (AWS) libraries, open source repos, guides, blogs, and other resources.  Featuring the Fiery Meter of AWSome.\nName: numba\nOwner: numba\nStars: 9026\nRepository: https://github.com/numba/numba\nDescription: NumPy aware dynamic Python compiler using LLVM\nName: kedro\nOwner: kedro-org\nStars: 8981\nRepository: https://github.com/kedro-org/kedro\nDescription: Kedro is a toolbox for production-ready data science. It uses software engineering best practices to help you create data engineering and data science pipelines that are reproducible, maintainable, and modular.\nName: OpenChatKit\nOwner: togethercomputer\nStars: 8928\nRepository: https://github.com/togethercomputer/OpenChatKit\nDescription: None\nName: Python\nOwner: injetlee\nStars: 8825\nRepository: https://github.com/injetlee/Python\nDescription: Python脚本。模拟登录知乎， 爬虫，操作excel，微信公众号，远程开机\nName: Reinforcement-learning-with-tensorflow\nOwner: MorvanZhou\nStars: 8362\nRepository: https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow\nDescription: Simple Reinforcement learning tutorials, 莫烦Python 中文AI教学\n\n\n\n\nVisualizing repositories using Plotly\n\nimport requests\n\nfrom plotly.graph_objs import Bar\nfrom plotly import offline\n\n# make an API call and store the response\nurl = 'https://api.github.com/search/repositories?q=language:python&sort=stars'\nheaders = {'Accept': 'application/vnd.github.v3+json'}  #uses specific version\nr = requests.get(url, headers = headers)  #using requests to make a call to API\nprint(f\"Status code: {r.status_code}\")\n\n# process results\nresponse_dict = r.json()\nrepo_dicts = response_dict['items']\nrepo_names, stars = [], []  #empty lists\nfor repo_dict in repo_dicts:\n    repo_names.append(repo_dict['name'])\n    stars.append(repo_dict['stargazers_count'])\n\n# make visualization\ndata = [{\n    'type': 'bar',\n    'x': repo_names,\n    'y': stars, \n}]\nmy_layout = {\n    'title': 'Most-starred python projects in Github',\n    'xaxis': {'title': 'Repository'},\n    'yaxis': {'title' : 'Stars'},\n}\n\nfig = {'data': data, 'layout': my_layout}\noffline.plot(fig, filename= 'python_repos.html')\n\nStatus code: 200\n\n\n'python_repos.html'\n\n\n\nRefining plotly charts\n\n# make visualization\n## modifying data\ndata_1 = [{\n    'type': 'bar',\n    'x': repo_names,\n    'y': stars,\n    'marker': {\n        'color' : 'rgb(255,0,0)',\n        'line' : {'width' : 1.5, 'color' : 'rgb(255,0,1)'}\n    },\n    'opacity' : 0.6,\n}]\nmy_layout = {\n    'title': 'Most-starred python projects in Github',\n    'xaxis': {'title': 'Repository'},\n    'yaxis': {'title' : 'Stars'},\n}\n\nfig = {'data': data_1, 'layout': my_layout}\noffline.plot(fig, filename= 'python_repos_1.html')\n\n'python_repos_1.html'\n\n\n\n# make visualization\ndata_1 = [{\n    'type': 'bar',\n    'x': repo_names,\n    'y': stars,\n    'marker': {\n        'color' : 'rgb(255,0,0)',\n        'line' : {'width' : 1.5, 'color' : 'rgb(255,0,1)'}\n    },\n    'opacity' : 0.6,\n}]\n## modifying layout\nmy_layout_1 = {\n    'title': 'Most-starred python projects in Github',\n    'titlefont': {'size': 28},\n    'xaxis': {\n        'title': 'Repository',\n        'titlefont' : {'size': 24},\n        'tickfont' : {'size': 14},\n    },\n    'yaxis': {\n        'title' : 'Stars',\n        'titlefont' : {'size' : 24},\n        'tickfont' : {'size' : 14},\n     },\n}\n\nfig = {'data': data_1, 'layout': my_layout_1}\noffline.plot(fig, filename= 'python_repos_2.html')\n\n'python_repos_2.html'\n\n\n\n\nAdding Custom Tooltips\n\n# process results\nresponse_dict = r.json()\nrepo_dicts = response_dict['items']\nrepo_names, stars, labels = [], [], []  #empty lists\nfor repo_dict in repo_dicts:\n    repo_names.append(repo_dict['name'])\n    stars.append(repo_dict['stargazers_count'])\n\n    owner = repo_dict['owner']['login']\n    description = repo_dict['description']\n    label = f\"{owner}&lt;br /&gt;{description}\"\n    labels.append(label)\n    \n# make visualization\ndata = [{\n    'type': 'bar',\n    'x': repo_names,\n    'y': stars, \n    'hovertext': labels,\n    'marker': {\n        'color' : 'rgb(250,0,0)',\n        'line' : {'width' : 1.5, 'color' : 'rgb(255,0,1)'}\n    },\n    'opacity' : 0.6,\n}]\nmy_layout = {\n    'title': 'Most-starred python projects in Github',\n    'xaxis': {'title': 'Repository'},\n    'yaxis': {'title' : 'Stars'},\n}\n\nfig = {'data': data, 'layout': my_layout}\noffline.plot(fig, filename= 'python_repos.html')\n\n'python_repos.html'\n\n\n\n\nAdding clickable links\n\n# process results\nresponse_dict = r.json()\nrepo_dicts = response_dict['items']\nrepo_links, stars, labels = [], [], []  #empty lists\nfor repo_dict in repo_dicts:\n    repo_name = repo_dict['name']\n    repo_names.append(repo_dict['name'])\n    repo_url = repo_dict['html_url']\n    repo_link = f\"&lt;a href='{repo_url}'&gt;{repo_name}&lt;/a&gt;\"\n    repo_links.append(repo_link)\n    stars.append(repo_dict['stargazers_count'])\n\n    owner = repo_dict['owner']['login']\n    description = repo_dict['description']\n    label = f\"{owner}&lt;br /&gt;{description}\"\n    labels.append(label)\n    \n# make visualization\ndata = [{\n    'type': 'bar',\n    'x': repo_names,\n    'y': stars, \n    'hovertext': labels,\n    'marker': {\n        'color' : 'rgb(250,0,0)',\n        'line' : {'width' : 1.5, 'color' : 'rgb(255,0,1)'}\n    },\n    'opacity' : 0.6,\n}]\nmy_layout = {\n    'title': 'Most-starred python projects in Github',\n    'xaxis': {'title': 'Repository'},\n    'yaxis': {'title' : 'Stars'},\n}\n\nfig = {'data': data, 'layout': my_layout}\noffline.plot(fig, filename= 'python_repos.html')\n\n'python_repos.html'\n\n\n\n# improved version\nimport plotly.graph_objs as go\nfrom plotly.offline import plot as offline_plot\n\n# Extract data from the JSON response\nresponse_dict = r.json()\nrepo_dicts = response_dict['items']\n\n# Initialize empty lists\nrepo_names, repo_links, stars, labels = [], [], [], []\n\n# Process each repository in the response\nfor repo_dict in repo_dicts:\n    repo_name = repo_dict['name']\n    repo_url = repo_dict['html_url']\n    repo_link = f\"&lt;a href='{repo_url}' target='_blank'&gt;{repo_name}&lt;/a&gt;\"\n    \n    # Append data to lists\n    repo_names.append(repo_name)\n    repo_links.append(repo_link)\n    stars.append(repo_dict['stargazers_count'])\n\n    owner = repo_dict['owner']['login']\n    description = repo_dict['description']\n    label = f\"{owner}&lt;br /&gt;{description}\"\n    labels.append(label)\n\n# Create visualization data\ndata = [{\n    'type': 'bar',\n    'x': repo_links,  # Use repo_links for clickable links in the chart\n    'y': stars,\n    'hovertext': labels,\n    'marker': {\n        'color': 'rgb(250, 0, 0)',\n        'line': {'width': 1.5, 'color': 'rgb(255, 0, 1)'}\n    },\n    'opacity': 0.6,\n}]\n\n# Configure layout\nmy_layout = {\n    'title': 'Most-starred Python projects on GitHub',\n    'xaxis': {'title': 'Repository'},\n    'yaxis': {'title': 'Stars'},\n}\n\n# Create figure\nfig = go.Figure(data=data, layout=my_layout)\n\n# Save the interactive chart to an HTML file\noffline_plot(fig, filename='python_repos.html')\n\n'python_repos.html'\n\n\n\n\n\nThe Hacker News API\n\ncontains articles about programming and technology (http://news.ycombinator.com/)\n\n\nimport requests \nimport json \n\n# make an API call, and store the response\n \nurl = 'https://hacker-news.firebaseio.com/v0/item/19155826.json'\nr = requests.get(url)\nprint (f\"Status code: {r.status_code}\")\n\n# Explore data structure\n#filename = \"E:\\\\machine learning projects\\\\readable_hn_data.json\"\n#with open(filename, encoding = 'utf-8') as f:\n#    all_eq_data = json.load(f)\n\n# opening the readable file that we just created\n\nreadable_file = 'E:\\\\machine learning projects\\\\readable_hn_data.json'\nwith open(readable_file, 'r', encoding = 'utf-8') as f:\n    content = f.read()\n    print(content)\n\nStatus code: 200\n{\n    \"by\": \"jimktrains2\",\n    \"descendants\": 221,\n    \"id\": 19155826,\n    \"kids\": [\n        19156572,\n        19158857,\n        19156773,\n        19157251,\n        19156415,\n        19159820,\n        19157154,\n        19156385,\n        19156489,\n        19158522,\n        19156755,\n        19156974,\n        19158319,\n        19157034,\n        19156935,\n        19158935,\n        19157531,\n        19158638,\n        19156466,\n        19156758,\n        19156565,\n        19156498,\n        19156335,\n        19156041,\n        19156704,\n        19159047,\n        19159127,\n        19156217,\n        19156375,\n        19157945\n    ],\n    \"score\": 728,\n    \"time\": 1550085414,\n    \"title\": \"Nasa\\u2019s Mars Rover Opportunity Concludes a 15-Year Mission\",\n    \"type\": \"story\",\n    \"url\": \"https://www.nytimes.com/2019/02/13/science/mars-opportunity-rover-dead.html\"\n}\n\n\n\n\nAnalysing all the top articles from Hackernews\n\nfrom operator import itemgetter\n\nimport requests\n\n# make an API and store the response\nurl = 'https://hacker-news.firebaseio.com/v0/topstories.json'\nr = requests.get(url)\nprint(f\"Status code: {r.status_code}\")\n\n#process information about each submission\nsubmission_ids = r.json()\nsubmission_dicts = []\n\nfor submission_id in submission_ids[:30]:\n    #make a seperate API call for each submission\n    url = f\"https://hacker-news.firebaseio.com/v0/item/{submission_id}.json\"\n    r = requests.get(url)\n    print(f\"id: {submission_id}/tstatus: {r.status_code}\")\n    response_dict = r.json()\n    \n    #build a dictionary for each article\n    submission_dict = {\n        'title' : response_dict['title'],\n        'hn_link' : f\"http://news.ycombinator.com/item?id={submission_id}\",\n        'comments': response_dict['descendants'],\n    }\n    submission_dicts.append(submission_dict)\n    \n    submission_dicts = sorted(submission_dicts, key=itemgetter('comments'), reverse= True)\n    \n    for sumbission_dict in submission_dicts:\n        print(f\"\\nTitle: {submission_dict['title']}\")\n        print(f\"Discussion link: {submission_dict['hn_link']}\")\n        print(f\"Comments: {submission_dict['comments']}\")\n    \n    \n\nStatus code: 200\nid: 38290145/tstatus: 200\n\nTitle: The real realtime preemption end game\nDiscussion link: http://news.ycombinator.com/item?id=38290145\nComments: 163\nid: 38292553/tstatus: 200\n\nTitle: Migrating to OpenTelemetry\nDiscussion link: http://news.ycombinator.com/item?id=38292553\nComments: 27\n\nTitle: Migrating to OpenTelemetry\nDiscussion link: http://news.ycombinator.com/item?id=38292553\nComments: 27\nid: 38295179/tstatus: 200\n\nTitle: Show HN: Tiny LLMs – Browser-based private AI models for a wide array of tasks\nDiscussion link: http://news.ycombinator.com/item?id=38295179\nComments: 0\n\nTitle: Show HN: Tiny LLMs – Browser-based private AI models for a wide array of tasks\nDiscussion link: http://news.ycombinator.com/item?id=38295179\nComments: 0\n\nTitle: Show HN: Tiny LLMs – Browser-based private AI models for a wide array of tasks\nDiscussion link: http://news.ycombinator.com/item?id=38295179\nComments: 0\nid: 38290613/tstatus: 200\n\nTitle: From email to phone number, a new OSINT approach (2019)\nDiscussion link: http://news.ycombinator.com/item?id=38290613\nComments: 76\n\nTitle: From email to phone number, a new OSINT approach (2019)\nDiscussion link: http://news.ycombinator.com/item?id=38290613\nComments: 76\n\nTitle: From email to phone number, a new OSINT approach (2019)\nDiscussion link: http://news.ycombinator.com/item?id=38290613\nComments: 76\n\nTitle: From email to phone number, a new OSINT approach (2019)\nDiscussion link: http://news.ycombinator.com/item?id=38290613\nComments: 76\nid: 38291139/tstatus: 200\n\nTitle: Emu Video and Emu Edit, our latest generative AI research milestones\nDiscussion link: http://news.ycombinator.com/item?id=38291139\nComments: 15\n\nTitle: Emu Video and Emu Edit, our latest generative AI research milestones\nDiscussion link: http://news.ycombinator.com/item?id=38291139\nComments: 15\n\nTitle: Emu Video and Emu Edit, our latest generative AI research milestones\nDiscussion link: http://news.ycombinator.com/item?id=38291139\nComments: 15\n\nTitle: Emu Video and Emu Edit, our latest generative AI research milestones\nDiscussion link: http://news.ycombinator.com/item?id=38291139\nComments: 15\n\nTitle: Emu Video and Emu Edit, our latest generative AI research milestones\nDiscussion link: http://news.ycombinator.com/item?id=38291139\nComments: 15\nid: 38291199/tstatus: 200\n\nTitle: Zimbra 0-day used to steal email data from government organizations\nDiscussion link: http://news.ycombinator.com/item?id=38291199\nComments: 13\n\nTitle: Zimbra 0-day used to steal email data from government organizations\nDiscussion link: http://news.ycombinator.com/item?id=38291199\nComments: 13\n\nTitle: Zimbra 0-day used to steal email data from government organizations\nDiscussion link: http://news.ycombinator.com/item?id=38291199\nComments: 13\n\nTitle: Zimbra 0-day used to steal email data from government organizations\nDiscussion link: http://news.ycombinator.com/item?id=38291199\nComments: 13\n\nTitle: Zimbra 0-day used to steal email data from government organizations\nDiscussion link: http://news.ycombinator.com/item?id=38291199\nComments: 13\n\nTitle: Zimbra 0-day used to steal email data from government organizations\nDiscussion link: http://news.ycombinator.com/item?id=38291199\nComments: 13\nid: 38295638/tstatus: 200\n\n\nKeyError: 'descendants'\n\n\n\n## improved code\n\nfrom operator import itemgetter\nimport requests\n\n# Make an API call to get the top story IDs\nurl_top_stories = 'https://hacker-news.firebaseio.com/v0/topstories.json'\nresponse_top_stories = requests.get(url_top_stories)\nprint(f\"Status code: {response_top_stories.status_code}\")\n\n# Process information about each submission\nsubmission_ids = response_top_stories.json()\nsubmission_dicts = []\n\n# Make a separate API call for each submission and store relevant information\nfor submission_id in submission_ids[:30]:\n    url_submission = f\"https://hacker-news.firebaseio.com/v0/item/{submission_id}.json\"\n    response_submission = requests.get(url_submission)\n    print(f\"id: {submission_id}\\tstatus: {response_submission.status_code}\")\n\n    # Check if the API call was successful\n    if response_submission.status_code == 200:\n        submission_dict = {\n            'title': response_submission.json().get('title', 'N/A'),\n            'hn_link': f\"http://news.ycombinator.com/item?id={submission_id}\",\n            'comments': response_submission.json().get('descendants', 0),\n        }\n        submission_dicts.append(submission_dict)\n\n# Sort the submission dictionaries based on the number of comments in descending order\nsubmission_dicts = sorted(submission_dicts, key=itemgetter('comments'), reverse=True)\n\n# Print information about each submission\nfor submission_dict in submission_dicts:\n    print(f\"\\nTitle: {submission_dict['title']}\")\n    print(f\"Discussion link: {submission_dict['hn_link']}\")\n    print(f\"Comments: {submission_dict['comments']}\")\n\nStatus code: 200\nid: 38290145    status: 200\nid: 38295179    status: 200\nid: 38292553    status: 200\nid: 38290613    status: 200\nid: 38291139    status: 200\nid: 38291199    status: 200\nid: 38295638    status: 200\nid: 38291015    status: 200\nid: 38276418    status: 200\nid: 38294723    status: 200\nid: 38295524    status: 200\nid: 38294569    status: 200\nid: 38292102    status: 200\nid: 38294203    status: 200\nid: 38289327    status: 200\nid: 38291735    status: 200\nid: 38291880    status: 200\nid: 38291427    status: 200\nid: 38294623    status: 200\nid: 38293817    status: 200\nid: 38276727    status: 200\nid: 38287257    status: 200\nid: 38275698    status: 200\nid: 38269866    status: 200\nid: 38295819    status: 200\nid: 38288980    status: 200\nid: 38288130    status: 200\nid: 38287299    status: 200\nid: 38291399    status: 200\nid: 38288743    status: 200\n\nTitle: Privacy is priceless, but Signal is expensive\nDiscussion link: http://news.ycombinator.com/item?id=38291427\nComments: 557\n\nTitle: I think I need to go lie down\nDiscussion link: http://news.ycombinator.com/item?id=38288130\nComments: 391\n\nTitle: A failed AI girlfriend product, and my lessons\nDiscussion link: http://news.ycombinator.com/item?id=38287299\nComments: 198\n\nTitle: Smart drugs reduce quality of effort, and slow decision-making\nDiscussion link: http://news.ycombinator.com/item?id=38287257\nComments: 173\n\nTitle: The real realtime preemption end game\nDiscussion link: http://news.ycombinator.com/item?id=38290145\nComments: 163\n\nTitle: Operating on a minimal two-core Postgres instance: Query optimization insights\nDiscussion link: http://news.ycombinator.com/item?id=38276727\nComments: 128\n\nTitle: Sweden Gov Announces 'Massive Expansion' of Nuclear Energy\nDiscussion link: http://news.ycombinator.com/item?id=38291015\nComments: 122\n\nTitle: Moving from AWS to Bare-Metal saved us 230k$ /yr\nDiscussion link: http://news.ycombinator.com/item?id=38294569\nComments: 80\n\nTitle: From email to phone number, a new OSINT approach (2019)\nDiscussion link: http://news.ycombinator.com/item?id=38290613\nComments: 76\n\nTitle: Why thinking hard makes us feel tired\nDiscussion link: http://news.ycombinator.com/item?id=38294723\nComments: 59\n\nTitle: Ransomware Group Files SEC Complaint over Victim's Failure Disclose Data Breach\nDiscussion link: http://news.ycombinator.com/item?id=38291399\nComments: 45\n\nTitle: Hackers know everything is securities fraud\nDiscussion link: http://news.ycombinator.com/item?id=38293817\nComments: 40\n\nTitle: Frutiger Aero\nDiscussion link: http://news.ycombinator.com/item?id=38276418\nComments: 38\n\nTitle: Printed robots with bones, ligaments, and tendons\nDiscussion link: http://news.ycombinator.com/item?id=38288980\nComments: 38\n\nTitle: You don't need a CRDT to build a collaborative experience\nDiscussion link: http://news.ycombinator.com/item?id=38289327\nComments: 35\n\nTitle: std::source_location Is Broken\nDiscussion link: http://news.ycombinator.com/item?id=38292102\nComments: 28\n\nTitle: Migrating to OpenTelemetry\nDiscussion link: http://news.ycombinator.com/item?id=38292553\nComments: 27\n\nTitle: Hello World on the GPU (2019)\nDiscussion link: http://news.ycombinator.com/item?id=38275698\nComments: 22\n\nTitle: Emu Video and Emu Edit, our latest generative AI research milestones\nDiscussion link: http://news.ycombinator.com/item?id=38291139\nComments: 15\n\nTitle: Federated finetuning of Whisper on Raspberry Pi 5\nDiscussion link: http://news.ycombinator.com/item?id=38294203\nComments: 15\n\nTitle: Zimbra 0-day used to steal email data from government organizations\nDiscussion link: http://news.ycombinator.com/item?id=38291199\nComments: 13\n\nTitle: AI-Exploits: Repo of multiple unauthenticated RCEs in AI tools\nDiscussion link: http://news.ycombinator.com/item?id=38291880\nComments: 10\n\nTitle: Serverless development experience for embedded computer vision\nDiscussion link: http://news.ycombinator.com/item?id=38288743\nComments: 8\n\nTitle: The CWEB System of Structured Documentation\nDiscussion link: http://news.ycombinator.com/item?id=38291735\nComments: 2\n\nTitle: OCapN, Interoperable Capabilities over the Network\nDiscussion link: http://news.ycombinator.com/item?id=38295524\nComments: 1\n\nTitle: A floating solar-powered device produces clean water and clean fuel\nDiscussion link: http://news.ycombinator.com/item?id=38269866\nComments: 1\n\nTitle: Microsoft support 'cracks' Windows for customer after activation fails\nDiscussion link: http://news.ycombinator.com/item?id=38295819\nComments: 1\n\nTitle: Show HN: Tiny LLMs – Browser-based private AI models for a wide array of tasks\nDiscussion link: http://news.ycombinator.com/item?id=38295179\nComments: 0\n\nTitle: In-person YC Startup Tech Talk and hiring mixer on 12/4 in SF\nDiscussion link: http://news.ycombinator.com/item?id=38295638\nComments: 0\n\nTitle: Planning for Unplanned Work in Linear\nDiscussion link: http://news.ycombinator.com/item?id=38294623\nComments: 0"
  },
  {
    "objectID": "posts/python/Proximityanalysis.html",
    "href": "posts/python/Proximityanalysis.html",
    "title": "Proximity Analysis",
    "section": "",
    "text": "measure distance between points on map\n\n\nselect all points within same radius\n\n\nimport folium\nfrom folium import Marker, GeoJson\nfrom folium.plugins import HeatMap\nfrom shapely.geometry import MultiPolygon\n\nimport math\nimport pandas as pd\nimport geopandas as gpd\n\n\nreleases = gpd.read_file('data_for_all_courses\\\\toxic_release_pennsylvania\\\\toxic_release_pennsylvania.shp')\nreleases.head()\n\n\n\n\n\n\n\n\nYEAR\nCITY\nCOUNTY\nST\nLATITUDE\nLONGITUDE\nCHEMICAL\nUNIT_OF_ME\nTOTAL_RELE\ngeometry\n\n\n\n\n0\n2016\nPHILADELPHIA\nPHILADELPHIA\nPA\n40.005901\n-75.072103\nFORMIC ACID\nPounds\n0.160\nPOINT (2718560.227 256380.179)\n\n\n1\n2016\nPHILADELPHIA\nPHILADELPHIA\nPA\n39.920120\n-75.146410\nETHYLENE GLYCOL\nPounds\n13353.480\nPOINT (2698674.606 224522.905)\n\n\n2\n2016\nPHILADELPHIA\nPHILADELPHIA\nPA\n40.023880\n-75.220450\nCERTAIN GLYCOL ETHERS\nPounds\n104.135\nPOINT (2676833.394 261701.856)\n\n\n3\n2016\nPHILADELPHIA\nPHILADELPHIA\nPA\n39.913540\n-75.198890\nLEAD COMPOUNDS\nPounds\n1730.280\nPOINT (2684030.004 221697.388)\n\n\n4\n2016\nPHILADELPHIA\nPHILADELPHIA\nPA\n39.913540\n-75.198890\nBENZENE\nPounds\n39863.290\nPOINT (2684030.004 221697.388)\n\n\n\n\n\n\n\n\nreleases.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nRangeIndex: 4663 entries, 0 to 4662\nData columns (total 10 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   YEAR        4663 non-null   object  \n 1   CITY        4663 non-null   object  \n 2   COUNTY      4663 non-null   object  \n 3   ST          4663 non-null   object  \n 4   LATITUDE    4663 non-null   float64 \n 5   LONGITUDE   4663 non-null   float64 \n 6   CHEMICAL    4663 non-null   object  \n 7   UNIT_OF_ME  4663 non-null   object  \n 8   TOTAL_RELE  4663 non-null   float64 \n 9   geometry    4663 non-null   geometry\ndtypes: float64(3), geometry(1), object(6)\nmemory usage: 364.4+ KB\n\n\n\n# air quality data\nstations = gpd.read_file('data_for_all_courses\\\\toxic_release_pennsylvania.shx')\nstations.head()\n\n\n\n\n\n\n\n\ngeometry\n\n\n\n\n0\nPOINT (2718560.227 256380.179)\n\n\n1\nPOINT (2698674.606 224522.905)\n\n\n2\nPOINT (2676833.394 261701.856)\n\n\n3\nPOINT (2684030.004 221697.388)\n\n\n4\nPOINT (2684030.004 221697.388)\n\n\n\n\n\n\n\n\nstations.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nRangeIndex: 4663 entries, 0 to 4662\nData columns (total 1 columns):\n #   Column    Non-Null Count  Dtype   \n---  ------    --------------  -----   \n 0   geometry  4663 non-null   geometry\ndtypes: geometry(1)\nmemory usage: 36.6 KB\n\n\n\nstations2 = gpd.read_file('data_for_all_courses\\\\toxic_release_pennsylvania.shp')\nstations2.head()\n\n\n\n\n\n\n\n\ngeometry\n\n\n\n\n0\nPOINT (2718560.227 256380.179)\n\n\n1\nPOINT (2698674.606 224522.905)\n\n\n2\nPOINT (2676833.394 261701.856)\n\n\n3\nPOINT (2684030.004 221697.388)\n\n\n4\nPOINT (2684030.004 221697.388)\n\n\n\n\n\n\n\n\nstations3 = gpd.read_file('data_for_all_courses\\\\toxic_release_pennsylvania.dbf')\nstations3.head()\n\n\n\n\n\n\n\n\nYEAR\nCITY\nCOUNTY\nST\nLATITUDE\nLONGITUDE\nCHEMICAL\nUNIT_OF_ME\nTOTAL_RELE\ngeometry\n\n\n\n\n0\n2016\nPHILADELPHIA\nPHILADELPHIA\nPA\n40.005901\n-75.072103\nFORMIC ACID\nPounds\n0.160\nPOINT (2718560.227 256380.179)\n\n\n1\n2016\nPHILADELPHIA\nPHILADELPHIA\nPA\n39.920120\n-75.146410\nETHYLENE GLYCOL\nPounds\n13353.480\nPOINT (2698674.606 224522.905)\n\n\n2\n2016\nPHILADELPHIA\nPHILADELPHIA\nPA\n40.023880\n-75.220450\nCERTAIN GLYCOL ETHERS\nPounds\n104.135\nPOINT (2676833.394 261701.856)\n\n\n3\n2016\nPHILADELPHIA\nPHILADELPHIA\nPA\n39.913540\n-75.198890\nLEAD COMPOUNDS\nPounds\n1730.280\nPOINT (2684030.004 221697.388)\n\n\n4\n2016\nPHILADELPHIA\nPHILADELPHIA\nPA\n39.913540\n-75.198890\nBENZENE\nPounds\n39863.290\nPOINT (2684030.004 221697.388)\n\n\n\n\n\n\n\n\nstations3.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nRangeIndex: 4663 entries, 0 to 4662\nData columns (total 10 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   YEAR        4663 non-null   object  \n 1   CITY        4663 non-null   object  \n 2   COUNTY      4663 non-null   object  \n 3   ST          4663 non-null   object  \n 4   LATITUDE    4663 non-null   float64 \n 5   LONGITUDE   4663 non-null   float64 \n 6   CHEMICAL    4663 non-null   object  \n 7   UNIT_OF_ME  4663 non-null   object  \n 8   TOTAL_RELE  4663 non-null   float64 \n 9   geometry    4663 non-null   geometry\ndtypes: float64(3), geometry(1), object(6)\nmemory usage: 364.4+ KB\n\n\n\n# checking crs coordinates for both\nprint(stations3.crs)\nprint(releases.crs)\n\nNone\nEPSG:2272\n\n\n\n#  stations3 = stations3.set_crs\nstations3 = stations3.set_crs(epsg=2272, inplace=True)\n\n\n# checking crs coordinates for both\nprint(stations3.crs)\nprint(releases.crs)\n\nEPSG:2272\nEPSG:2272\n\n\n\n# select one release incident in particular\nrecent_release = releases.iloc[360]\n\n# measure distances from each station\ndistances = stations3.geometry.distance(recent_release.geometry)\ndistances\n\n0       48941.110275\n1       14914.687505\n2       40646.631420\n3           0.000000\n4           0.000000\n            ...     \n4658    41735.245165\n4659    40909.967527\n4660     4519.771240\n4661    32442.454868\n4662    20534.504851\nLength: 4663, dtype: float64\n\n\n\n# mean distance\nprint(f'Mean distance to monitoring stations: {distances.mean()} feet')\n\nMean distance to monitoring stations: 35350.82207483399 feet\n\n\n\n# print minimum\nprint(stations3.iloc[distances.idxmin()][['COUNTY', 'LATITUDE', 'LONGITUDE']])\n\nCOUNTY       PHILADELPHIA\nLATITUDE         39.91354\nLONGITUDE       -75.19889\nName: 3, dtype: object"
  },
  {
    "objectID": "posts/python/Proximityanalysis.html#techniques",
    "href": "posts/python/Proximityanalysis.html#techniques",
    "title": "Proximity Analysis",
    "section": "",
    "text": "measure distance between points on map\n\n\nselect all points within same radius\n\n\nimport folium\nfrom folium import Marker, GeoJson\nfrom folium.plugins import HeatMap\nfrom shapely.geometry import MultiPolygon\n\nimport math\nimport pandas as pd\nimport geopandas as gpd\n\n\nreleases = gpd.read_file('data_for_all_courses\\\\toxic_release_pennsylvania\\\\toxic_release_pennsylvania.shp')\nreleases.head()\n\n\n\n\n\n\n\n\nYEAR\nCITY\nCOUNTY\nST\nLATITUDE\nLONGITUDE\nCHEMICAL\nUNIT_OF_ME\nTOTAL_RELE\ngeometry\n\n\n\n\n0\n2016\nPHILADELPHIA\nPHILADELPHIA\nPA\n40.005901\n-75.072103\nFORMIC ACID\nPounds\n0.160\nPOINT (2718560.227 256380.179)\n\n\n1\n2016\nPHILADELPHIA\nPHILADELPHIA\nPA\n39.920120\n-75.146410\nETHYLENE GLYCOL\nPounds\n13353.480\nPOINT (2698674.606 224522.905)\n\n\n2\n2016\nPHILADELPHIA\nPHILADELPHIA\nPA\n40.023880\n-75.220450\nCERTAIN GLYCOL ETHERS\nPounds\n104.135\nPOINT (2676833.394 261701.856)\n\n\n3\n2016\nPHILADELPHIA\nPHILADELPHIA\nPA\n39.913540\n-75.198890\nLEAD COMPOUNDS\nPounds\n1730.280\nPOINT (2684030.004 221697.388)\n\n\n4\n2016\nPHILADELPHIA\nPHILADELPHIA\nPA\n39.913540\n-75.198890\nBENZENE\nPounds\n39863.290\nPOINT (2684030.004 221697.388)\n\n\n\n\n\n\n\n\nreleases.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nRangeIndex: 4663 entries, 0 to 4662\nData columns (total 10 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   YEAR        4663 non-null   object  \n 1   CITY        4663 non-null   object  \n 2   COUNTY      4663 non-null   object  \n 3   ST          4663 non-null   object  \n 4   LATITUDE    4663 non-null   float64 \n 5   LONGITUDE   4663 non-null   float64 \n 6   CHEMICAL    4663 non-null   object  \n 7   UNIT_OF_ME  4663 non-null   object  \n 8   TOTAL_RELE  4663 non-null   float64 \n 9   geometry    4663 non-null   geometry\ndtypes: float64(3), geometry(1), object(6)\nmemory usage: 364.4+ KB\n\n\n\n# air quality data\nstations = gpd.read_file('data_for_all_courses\\\\toxic_release_pennsylvania.shx')\nstations.head()\n\n\n\n\n\n\n\n\ngeometry\n\n\n\n\n0\nPOINT (2718560.227 256380.179)\n\n\n1\nPOINT (2698674.606 224522.905)\n\n\n2\nPOINT (2676833.394 261701.856)\n\n\n3\nPOINT (2684030.004 221697.388)\n\n\n4\nPOINT (2684030.004 221697.388)\n\n\n\n\n\n\n\n\nstations.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nRangeIndex: 4663 entries, 0 to 4662\nData columns (total 1 columns):\n #   Column    Non-Null Count  Dtype   \n---  ------    --------------  -----   \n 0   geometry  4663 non-null   geometry\ndtypes: geometry(1)\nmemory usage: 36.6 KB\n\n\n\nstations2 = gpd.read_file('data_for_all_courses\\\\toxic_release_pennsylvania.shp')\nstations2.head()\n\n\n\n\n\n\n\n\ngeometry\n\n\n\n\n0\nPOINT (2718560.227 256380.179)\n\n\n1\nPOINT (2698674.606 224522.905)\n\n\n2\nPOINT (2676833.394 261701.856)\n\n\n3\nPOINT (2684030.004 221697.388)\n\n\n4\nPOINT (2684030.004 221697.388)\n\n\n\n\n\n\n\n\nstations3 = gpd.read_file('data_for_all_courses\\\\toxic_release_pennsylvania.dbf')\nstations3.head()\n\n\n\n\n\n\n\n\nYEAR\nCITY\nCOUNTY\nST\nLATITUDE\nLONGITUDE\nCHEMICAL\nUNIT_OF_ME\nTOTAL_RELE\ngeometry\n\n\n\n\n0\n2016\nPHILADELPHIA\nPHILADELPHIA\nPA\n40.005901\n-75.072103\nFORMIC ACID\nPounds\n0.160\nPOINT (2718560.227 256380.179)\n\n\n1\n2016\nPHILADELPHIA\nPHILADELPHIA\nPA\n39.920120\n-75.146410\nETHYLENE GLYCOL\nPounds\n13353.480\nPOINT (2698674.606 224522.905)\n\n\n2\n2016\nPHILADELPHIA\nPHILADELPHIA\nPA\n40.023880\n-75.220450\nCERTAIN GLYCOL ETHERS\nPounds\n104.135\nPOINT (2676833.394 261701.856)\n\n\n3\n2016\nPHILADELPHIA\nPHILADELPHIA\nPA\n39.913540\n-75.198890\nLEAD COMPOUNDS\nPounds\n1730.280\nPOINT (2684030.004 221697.388)\n\n\n4\n2016\nPHILADELPHIA\nPHILADELPHIA\nPA\n39.913540\n-75.198890\nBENZENE\nPounds\n39863.290\nPOINT (2684030.004 221697.388)\n\n\n\n\n\n\n\n\nstations3.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nRangeIndex: 4663 entries, 0 to 4662\nData columns (total 10 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   YEAR        4663 non-null   object  \n 1   CITY        4663 non-null   object  \n 2   COUNTY      4663 non-null   object  \n 3   ST          4663 non-null   object  \n 4   LATITUDE    4663 non-null   float64 \n 5   LONGITUDE   4663 non-null   float64 \n 6   CHEMICAL    4663 non-null   object  \n 7   UNIT_OF_ME  4663 non-null   object  \n 8   TOTAL_RELE  4663 non-null   float64 \n 9   geometry    4663 non-null   geometry\ndtypes: float64(3), geometry(1), object(6)\nmemory usage: 364.4+ KB\n\n\n\n# checking crs coordinates for both\nprint(stations3.crs)\nprint(releases.crs)\n\nNone\nEPSG:2272\n\n\n\n#  stations3 = stations3.set_crs\nstations3 = stations3.set_crs(epsg=2272, inplace=True)\n\n\n# checking crs coordinates for both\nprint(stations3.crs)\nprint(releases.crs)\n\nEPSG:2272\nEPSG:2272\n\n\n\n# select one release incident in particular\nrecent_release = releases.iloc[360]\n\n# measure distances from each station\ndistances = stations3.geometry.distance(recent_release.geometry)\ndistances\n\n0       48941.110275\n1       14914.687505\n2       40646.631420\n3           0.000000\n4           0.000000\n            ...     \n4658    41735.245165\n4659    40909.967527\n4660     4519.771240\n4661    32442.454868\n4662    20534.504851\nLength: 4663, dtype: float64\n\n\n\n# mean distance\nprint(f'Mean distance to monitoring stations: {distances.mean()} feet')\n\nMean distance to monitoring stations: 35350.82207483399 feet\n\n\n\n# print minimum\nprint(stations3.iloc[distances.idxmin()][['COUNTY', 'LATITUDE', 'LONGITUDE']])\n\nCOUNTY       PHILADELPHIA\nLATITUDE         39.91354\nLONGITUDE       -75.19889\nName: 3, dtype: object"
  },
  {
    "objectID": "posts/python/Proximityanalysis.html#creating-a-buffer",
    "href": "posts/python/Proximityanalysis.html#creating-a-buffer",
    "title": "Proximity Analysis",
    "section": "Creating a buffer",
    "text": "Creating a buffer\n\nto understand some points on the map that are some distance away from the reference point\n\n\nuse folium.GeoJson() to plot each polygon\n\n\ntwo_mile_buffer = stations3.geometry.buffer(2*5280)\ntwo_mile_buffer.head()\n\n0    POLYGON ((2729120.227 256380.179, 2729069.378 ...\n1    POLYGON ((2709234.606 224522.905, 2709183.756 ...\n2    POLYGON ((2687393.394 261701.856, 2687342.544 ...\n3    POLYGON ((2694590.004 221697.388, 2694539.155 ...\n4    POLYGON ((2694590.004 221697.388, 2694539.155 ...\ndtype: geometry\n\n\n\n# create base map\nm = folium.Map(location=[39.9526,-75.1652], zoom_start=11)\nHeatMap(data = releases[[\"LATITUDE\", 'LONGITUDE']], radius= 12).add_to(m)\n\nfor idx, row in stations3.iterrows():\n    Marker([row['LATITUDE'], row['LONGITUDE']]).add_to(m)\n    \n# plot\nGeoJson(two_mile_buffer.set_crs(epsg=2272)).add_to(m)\n\n\n# show\n#m\n\n&lt;folium.features.GeoJson at 0x1ccf09ef2d0&gt;\n\n\n\n# turn a group of polygons into a single polygon\n\nmy_union = two_mile_buffer.geometry.unary_union\nprint('Type: ', type(my_union))\n\n# show\nmy_union\n\nType:  &lt;class 'shapely.geometry.multipolygon.MultiPolygon'&gt;"
  },
  {
    "objectID": "posts/python/p_10.html",
    "href": "posts/python/p_10.html",
    "title": "Downloading data",
    "section": "",
    "text": "Learning outcomes\n\nworking with real-world data sets.\nprocessing CSV and JSON files.\nextracting data that needs to be focussed on.\nUse of datetime module with Matplotlib, and plot multiple data series on one chart.\nStyle plotly charts and maps\n\n\n\nCSV File Headers\n\nimport csv\n\nfilename = \"E:\\machine learning projects\\sitka_weather_07-2018_simple.csv\"\nwith open(filename) as f:\n    reader = csv.reader(f)\n    header_row = next(reader) #reads next/first line\n    print(header_row)\n\n['STATION', 'NAME', 'DATE', 'PRCP', 'TAVG', 'TMAX', 'TMIN']\n\n\n\nPrinting the headers and their Positions\n\nimport csv\n\nfilename = \"E:\\machine learning projects\\sitka_weather_07-2018_simple.csv\"\nwith open(filename) as f:\n    reader = csv.reader(f)\n    header_row = next(reader) #reads next/first line\n    \n    for a, b in enumerate(header_row):\n        print(a, b)\n    \n    \n\n0 STATION\n1 NAME\n2 DATE\n3 PRCP\n4 TAVG\n5 TMAX\n6 TMIN\n\n\n\n\n\nExtracting and reading data\n\nhigh temperatures for the day\n\n\nfilename = \"E:\\machine learning projects\\sitka_weather_07-2018_simple.csv\"\nwith open(filename) as f:\n    reader = csv.reader(f)\n    header_row = next(reader) #reads next/first line\n\n    highs = []    # reading and processing rest of the file\n    for row in reader:\n        high = int(row[5])  #TMAX index is 5\n        highs.append(high)\nprint(highs)\n\n[62, 58, 70, 70, 67, 59, 58, 62, 66, 59, 56, 63, 65, 58, 56, 59, 64, 60, 60, 61, 65, 65, 63, 59, 64, 65, 68, 66, 64, 67, 65]\n\n\n\nPlotting data in temperature chart\n\nimport csv\nimport matplotlib.pyplot as plt\n\nfilename = \"E:\\machine learning projects\\sitka_weather_07-2018_simple.csv\"\nwith open(filename) as f:\n    reader = csv.reader(f)\n    header_row = next(reader) #reads next/first line\n\n# plotting\nplt.style.use('classic')\nfig, ax = plt.subplots()\nax.plot(highs, c= 'red')\n\n#formatting\nax.set_title(\"Daily high temperatures, 2023\", fontsize = 22)\nax.set_xlabel('', fontsize = 16)\nax.set_ylabel(\"Temperature (F)\", fontsize = 22)\nax.tick_params(axis = 'both', which = 'major', labelsize=16)\nplt.show()\n\n\n\n\n\n### Plotting dates\n\n\nfrom datetime import datetime\nfirst_date = datetime.strptime('2023-10-14', '%Y-%m-%d')\nprint(first_date)                              \n\n2023-10-14 00:00:00\n\n\n\nimport csv\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\n\nfilename = \"E:\\machine learning projects\\sitka_weather_07-2018_simple.csv\"\nwith open(filename) as f:\n    reader = csv.reader(f)\n    header_row = next(reader) #reads next/first line\n    \n    #getting dates and highs\n    dates, highs = [], []\n    for row in reader:\n        current_date = datetime.strptime(row[2], '%Y-%m-%d')\n        high = int(row[5])\n        dates.append(current_date)\n        highs.append(high)\n\n# plotting\nplt.style.use('classic')\nfig, ax = plt.subplots()\nax.plot(dates, highs, c= 'red')\n\n#formatting\nax.set_title(\"Daily high temperatures, 2018\", fontsize = 22)\nax.set_xlabel('', fontsize = 16)\nfig.autofmt_xdate()\nax.set_ylabel(\"Temperature (F)\", fontsize = 22)\nax.tick_params(axis = 'both', which = 'major', labelsize=16)\nplt.show()\n\n\n\n\n\n\nPlotting a second data series\n\nimport csv\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\n\nfilename = \"E:\\machine learning projects\\sitka_weather_07-2018_simple.csv\"\nwith open(filename) as f:\n    reader = csv.reader(f)\n    header_row = next(reader) #reads next/first line\n    \n    #getting dates and highs\n    dates, highs, lows = [], [], []\n    for row in reader:\n        current_date = datetime.strptime(row[2], '%Y-%m-%d')\n        high = int(row[5])\n        low = int(row[6])         #low added\n        dates.append(current_date)\n        highs.append(high)\n        lows.append(low)          #low appended\n\n# plotting\nplt.style.use('classic')\nfig, ax = plt.subplots()\nax.plot(dates, highs, c= 'red')\nax.plot(dates, lows, c = 'blue')\n\n\n#formatting\nax.set_title(\"Daily high and low temperatures, 2018\", fontsize = 24)\nax.set_xlabel('', fontsize = 16)\nfig.autofmt_xdate()\nax.set_ylabel(\"Temperature (F)\", fontsize = 22)\nax.tick_params(axis = 'both', which = 'major', labelsize=16)\nplt.show()\n\n\n\n\n\n\nShading an Area in the chart\n\nalpha controls colors transpirancy (alpha value; 0 to 1 = completely transparent to opaque)\npassing fill_between() the list dates for the x-values and then the two y-value series highs and lows.\n\n\nimport csv\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\n\nfilename = \"E:\\machine learning projects\\sitka_weather_07-2018_simple.csv\"\nwith open(filename) as f:\n    reader = csv.reader(f)\n    header_row = next(reader) #reads next/first line\n    \n    #getting dates and highs\n    dates, highs, lows = [], [], []\n    for row in reader:\n        current_date = datetime.strptime(row[2], '%Y-%m-%d')\n        high = int(row[5])\n        low = int(row[6])         \n        dates.append(current_date)\n        highs.append(high)\n        lows.append(low)          \n\n# plotting\nplt.style.use('classic')\nfig, ax = plt.subplots()\nax.plot(dates, highs, c= 'red', alpha = 0.5)      #alpha for transpirancy added here\nax.plot(dates, lows, c= 'blue', alpha = 0.5)\nax.fill_between (dates, highs, lows, facecolor= 'blue', alpha = 0.1)\n\n\n#formatting\nax.set_title(\"Daily high and low temperatures, 2018\", fontsize = 24)\nax.set_xlabel('', fontsize = 16)\nfig.autofmt_xdate()\nax.set_ylabel(\"Temperature (F)\", fontsize = 22)\nax.tick_params(axis = 'both', which = 'major', labelsize=16)\nplt.show()\n\n\n\n\n\n\n\nError checking\n\nwe’ll provide wrong row nunbers as inputs\n\n\nfilename = \"E:\\machine learning projects\\sitka_weather_07-2018_simple.csv\"\nwith open(filename) as f:\n    reader = csv.reader(f)\n    header_row = next(reader) #reads next/first line\n    \n    #getting dates and highs\n    dates, highs, lows = [], [], []\n    for row in reader:\n        current_date = datetime.strptime(row[2], '%Y-%m-%d')\n        high = int(row[4])\n        low = int(row[5])         \n        dates.append(current_date)\n        highs.append(high)\n        lows.append(low)          \n\n# plotting\nplt.style.use('classic')\nfig, ax = plt.subplots()\nax.plot(dates, highs, c= 'red', alpha = 0.5)      #alpha for transpirancy added here\nax.plot(dates, lows, c= 'blue', alpha = 0.5)\nax.fill_between (dates, highs, lows, facecolor= 'blue', alpha = 0.1)\n\n\n#formatting\nax.set_title(\"Daily high and low temperatures, 2018\", fontsize = 24)\nax.set_xlabel('', fontsize = 16)\nfig.autofmt_xdate()\nax.set_ylabel(\"Temperature (F)\", fontsize = 22)\nax.tick_params(axis = 'both', which = 'major', labelsize=16)\nplt.show()\n\nValueError: invalid literal for int() with base 10: ''\n\n\n\nFixing the above error with try/except/else block\n\nfilename = \"E:\\machine learning projects\\sitka_weather_07-2018_simple.csv\"\nwith open(filename) as f:\n    reader = csv.reader(f)\n    header_row = next(reader) #reads next/first line\n    \n    #getting dates and highs\n    dates, highs, lows = [], [], []\n    for row in reader:\n        current_date = datetime.strptime(row[2], '%Y-%m-%d')\n        try:\n            high = int(row[4])\n            low = int(row[5])\n        except ValueError:\n            print(f\"Missing data for {current_date}\")\n        else:\n            dates.append(current_date)\n            highs.append(high)\n            lows.append(low)        \n\n# plotting\nplt.style.use('classic')\nfig, ax = plt.subplots()\nax.plot(dates, highs, c= 'red', alpha = 0.5)      #alpha for transpirancy added here\nax.plot(dates, lows, c= 'blue', alpha = 0.5)\nax.fill_between (dates, highs, lows, facecolor= 'blue', alpha = 0.1)\n\n\n#formatting\nax.set_title(\"Daily high and low temperatures, 2018 \\nUsing try except else method\", fontsize = 24)\nax.set_xlabel('', fontsize = 16)\nfig.autofmt_xdate()\nax.set_ylabel(\"Temperature (F)\", fontsize = 22)\nax.tick_params(axis = 'both', which = 'major', labelsize=16)\nplt.show()\n\nMissing data for 2018-07-01 00:00:00\nMissing data for 2018-07-02 00:00:00\nMissing data for 2018-07-03 00:00:00\nMissing data for 2018-07-04 00:00:00\nMissing data for 2018-07-05 00:00:00\nMissing data for 2018-07-06 00:00:00\nMissing data for 2018-07-07 00:00:00\nMissing data for 2018-07-08 00:00:00\nMissing data for 2018-07-09 00:00:00\nMissing data for 2018-07-10 00:00:00\nMissing data for 2018-07-11 00:00:00\nMissing data for 2018-07-12 00:00:00\nMissing data for 2018-07-13 00:00:00\nMissing data for 2018-07-14 00:00:00\nMissing data for 2018-07-15 00:00:00\nMissing data for 2018-07-16 00:00:00\nMissing data for 2018-07-17 00:00:00\nMissing data for 2018-07-18 00:00:00\nMissing data for 2018-07-19 00:00:00\nMissing data for 2018-07-20 00:00:00\nMissing data for 2018-07-21 00:00:00\nMissing data for 2018-07-22 00:00:00\nMissing data for 2018-07-23 00:00:00\nMissing data for 2018-07-24 00:00:00\nMissing data for 2018-07-25 00:00:00\nMissing data for 2018-07-26 00:00:00\nMissing data for 2018-07-27 00:00:00\nMissing data for 2018-07-28 00:00:00\nMissing data for 2018-07-29 00:00:00\nMissing data for 2018-07-30 00:00:00\nMissing data for 2018-07-31 00:00:00\n\n\n\n\n\n\n\nDownloading your own data\n\npractice session\nSTEPS (https://www.ncdc.noaa.gov/cdo-web/ &gt; search tool &gt; daily summeries &gt; city- Toronto &gt; click search &gt; add to cart &gt; custom GHCN-Daily CSV &gt; continue &gt; data will be emailed)\n\n\nimport csv\n\nfilename_1 = \"E:\\\\machine learning projects\\\\3495022.csv\"\nwith open(filename_1) as f_1:\n    reader = csv.reader(f_1)\n    header_row = next(reader) #reads next/first line\n    \n    for a, b in enumerate(header_row):\n        print(a, b)\n\n0 STATION\n1 NAME\n2 LATITUDE\n3 LONGITUDE\n4 ELEVATION\n5 DATE\n6 DAPR\n7 DAPR_ATTRIBUTES\n8 MDPR\n9 MDPR_ATTRIBUTES\n10 PRCP\n11 PRCP_ATTRIBUTES\n12 SNOW\n13 SNOW_ATTRIBUTES\n14 SNWD\n15 SNWD_ATTRIBUTES\n16 TAVG\n17 TAVG_ATTRIBUTES\n18 TMAX\n19 TMAX_ATTRIBUTES\n20 TMIN\n21 TMIN_ATTRIBUTES\n22 WDFG\n23 WDFG_ATTRIBUTES\n24 WESD\n25 WESD_ATTRIBUTES\n26 WESF\n27 WESF_ATTRIBUTES\n28 WSFG\n29 WSFG_ATTRIBUTES\n\n\n\nimport csv\nfrom datetime import datetime\n\nfilename_1 = \"E:\\\\machine learning projects\\\\3495022.csv\"\nwith open(filename_1) as f_1:\n    reader = csv.reader(f_1)\n    header_row = next(reader) #reads next/first line\n    \n    #getting dates and highs\n    dates, prcp = [], []\n\n    #processing data inside the 'with' block\n    for row in reader:\n        current_date = datetime.strptime(row[5], '%Y-%m-%d')\n        prcp_value = float(row[10])\n        DATE.append(current_date)\n        PRCP.append(prcp_value)\n        \n        # Check if the value is not an empty string before converting to float\n        if prcp:\n            prcp_value = float(prcp)\n            dates.append(current_date)\n            prcp.append(prcp_value)\n\nValueError: could not convert string to float: ''\n\n\n\n\n\nMapping Global Data sets: JSON Format\n\nimport json\n\n# explore the structure of the data\n\nfilename = \"E:/machine learning projects/1.0_day.geojson\"\nwith open(filename, encoding = 'utf-8') as f:\n    all_eq_data = json.load(f)\n    \nreadable_file = 'E:/machine learning projects/readable_1.0_day.geojson'\nwith open(readable_file, 'w') as f:\n    json.dump(all_eq_data, f, indent=4)\n\n\n# opening the readable file that we just created\n\nreadable_file = 'E:/machine learning projects/readable_1.0_day.geojson'\nwith open(readable_file, 'r', encoding = 'utf-8') as f:\n    content = f.read()\n    print(content)\n\n{\n    \"type\": \"FeatureCollection\",\n    \"metadata\": {\n        \"generated\": 1699552743000,\n        \"url\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/1.0_day.geojson\",\n        \"title\": \"USGS Magnitude 1.0+ Earthquakes, Past Day\",\n        \"status\": 200,\n        \"api\": \"1.10.3\",\n        \"count\": 191\n    },\n    \"features\": [\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.80999994,\n                \"place\": \"7 km S of P\\u0101hala, Hawaii\",\n                \"time\": 1699548293900,\n                \"updated\": 1699548484670,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/hv73642362\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/hv73642362.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 50,\n                \"net\": \"hv\",\n                \"code\": \"73642362\",\n                \"ids\": \",hv73642362,\",\n                \"sources\": \",hv,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 32,\n                \"dmin\": null,\n                \"rms\": 0.140000001,\n                \"gap\": 171,\n                \"magType\": \"md\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.8 - 7 km S of P\\u0101hala, Hawaii\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -155.49299621582,\n                    19.1323337554932,\n                    31.1900005340576\n                ]\n            },\n            \"id\": \"hv73642362\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.3,\n                \"place\": \"47 km NNW of Chickaloon, Alaska\",\n                \"time\": 1699547077244,\n                \"updated\": 1699547194385,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/ak023edwvsb0\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023edwvsb0.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 81,\n                \"net\": \"ak\",\n                \"code\": \"023edwvsb0\",\n                \"ids\": \",ak023edwvsb0,\",\n                \"sources\": \",ak,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": null,\n                \"dmin\": null,\n                \"rms\": 1.02,\n                \"gap\": null,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.3 - 47 km NNW of Chickaloon, Alaska\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -148.6656,\n                    62.21,\n                    26.1\n                ]\n            },\n            \"id\": \"ak023edwvsb0\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 3.3,\n                \"place\": \"62 km SSW of Whites City, New Mexico\",\n                \"time\": 1699546775038,\n                \"updated\": 1699548922019,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vzhg\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vzhg.geojson\",\n                \"felt\": 1,\n                \"cdi\": 1,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 168,\n                \"net\": \"tx\",\n                \"code\": \"2023vzhg\",\n                \"ids\": \",tx2023vzhg,us7000l9wr,\",\n                \"sources\": \",tx,us,\",\n                \"types\": \",dyfi,origin,phase-data,\",\n                \"nst\": 13,\n                \"dmin\": 0.1,\n                \"rms\": 0.1,\n                \"gap\": 80,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 3.3 - 62 km SSW of Whites City, New Mexico\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.593,\n                    31.647,\n                    7.5969\n                ]\n            },\n            \"id\": \"tx2023vzhg\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.5,\n                \"place\": \"15 km E of Seven Trees, CA\",\n                \"time\": 1699546696380,\n                \"updated\": 1699548912144,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960536\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960536.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 35,\n                \"net\": \"nc\",\n                \"code\": \"73960536\",\n                \"ids\": \",nc73960536,\",\n                \"sources\": \",nc,\",\n                \"types\": \",nearby-cities,origin,phase-data,scitech-link,\",\n                \"nst\": 17,\n                \"dmin\": 0.04013,\n                \"rms\": 0.05,\n                \"gap\": 51,\n                \"magType\": \"md\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.5 - 15 km E of Seven Trees, CA\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -121.668335,\n                    37.2989998,\n                    6.19\n                ]\n            },\n            \"id\": \"nc73960536\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.8,\n                \"place\": \"45 km NW of Toyah, Texas\",\n                \"time\": 1699546087245,\n                \"updated\": 1699548100415,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vzgy\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vzgy.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 50,\n                \"net\": \"tx\",\n                \"code\": \"2023vzgy\",\n                \"ids\": \",tx2023vzgy,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 18,\n                \"dmin\": 0,\n                \"rms\": 0.4,\n                \"gap\": 56,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.8 - 45 km NW of Toyah, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.12,\n                    31.62,\n                    4.5431\n                ]\n            },\n            \"id\": \"tx2023vzgy\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.5,\n                \"place\": \"60 km SSW of Whites City, New Mexico\",\n                \"time\": 1699545502173,\n                \"updated\": 1699551505040,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vzgq\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vzgq.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 96,\n                \"net\": \"tx\",\n                \"code\": \"2023vzgq\",\n                \"ids\": \",us7000l9vz,tx2023vzgq,\",\n                \"sources\": \",us,tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 14,\n                \"dmin\": 0,\n                \"rms\": 0.2,\n                \"gap\": 72,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.5 - 60 km SSW of Whites City, New Mexico\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.506,\n                    31.637,\n                    7.0571\n                ]\n            },\n            \"id\": \"tx2023vzgq\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 4.7,\n                \"place\": \"Banda Sea\",\n                \"time\": 1699544209694,\n                \"updated\": 1699549188040,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9uy\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9uy.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 340,\n                \"net\": \"us\",\n                \"code\": \"7000l9uy\",\n                \"ids\": \",us7000l9uy,\",\n                \"sources\": \",us,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 69,\n                \"dmin\": 7.367,\n                \"rms\": 0.53,\n                \"gap\": 57,\n                \"magType\": \"mb\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 4.7 - Banda Sea\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    129.5293,\n                    -6.3194,\n                    10\n                ]\n            },\n            \"id\": \"us7000l9uy\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.09,\n                \"place\": \"7 km WNW of Cobb, CA\",\n                \"time\": 1699543811820,\n                \"updated\": 1699547110873,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960501\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960501.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 18,\n                \"net\": \"nc\",\n                \"code\": \"73960501\",\n                \"ids\": \",nc73960501,\",\n                \"sources\": \",nc,\",\n                \"types\": \",nearby-cities,origin,phase-data,scitech-link,\",\n                \"nst\": 16,\n                \"dmin\": 0.007444,\n                \"rms\": 0.02,\n                \"gap\": 65,\n                \"magType\": \"md\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.1 - 7 km WNW of Cobb, CA\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -122.7994995,\n                    38.8363342,\n                    2.14\n                ]\n            },\n            \"id\": \"nc73960501\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.8,\n                \"place\": \"53 km WNW of Toyah, Texas\",\n                \"time\": 1699543500458,\n                \"updated\": 1699545925286,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vzfl\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vzfl.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 50,\n                \"net\": \"tx\",\n                \"code\": \"2023vzfl\",\n                \"ids\": \",tx2023vzfl,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 17,\n                \"dmin\": 0.1,\n                \"rms\": 0.6,\n                \"gap\": 77,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.8 - 53 km WNW of Toyah, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.272,\n                    31.577,\n                    1.6266\n                ]\n            },\n            \"id\": \"tx2023vzfl\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 4.9,\n                \"place\": \"Kuril Islands\",\n                \"time\": 1699543336287,\n                \"updated\": 1699548616040,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9uv\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9uv.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 369,\n                \"net\": \"us\",\n                \"code\": \"7000l9uv\",\n                \"ids\": \",us7000l9uv,\",\n                \"sources\": \",us,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 125,\n                \"dmin\": 6.248,\n                \"rms\": 0.69,\n                \"gap\": 77,\n                \"magType\": \"mb\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 4.9 - Kuril Islands\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    154.1018,\n                    47.308,\n                    31.08\n                ]\n            },\n            \"id\": \"us7000l9uv\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2,\n                \"place\": \"45 km W of Mentone, Texas\",\n                \"time\": 1699542055337,\n                \"updated\": 1699544719069,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vzer\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vzer.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 62,\n                \"net\": \"tx\",\n                \"code\": \"2023vzer\",\n                \"ids\": \",tx2023vzer,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 14,\n                \"dmin\": 0,\n                \"rms\": 1.3,\n                \"gap\": 217,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.0 - 45 km W of Mentone, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.078,\n                    31.67,\n                    7.8692\n                ]\n            },\n            \"id\": \"tx2023vzer\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.6,\n                \"place\": \"39 km WSW of Mentone, Texas\",\n                \"time\": 1699541612567,\n                \"updated\": 1699544712057,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vzel\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vzel.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 39,\n                \"net\": \"tx\",\n                \"code\": \"2023vzel\",\n                \"ids\": \",tx2023vzel,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 15,\n                \"dmin\": 0,\n                \"rms\": 0.2,\n                \"gap\": 51,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.6 - 39 km WSW of Mentone, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.003,\n                    31.625,\n                    6.1375\n                ]\n            },\n            \"id\": \"tx2023vzel\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.01,\n                \"place\": \"17 km W of Minersville, Utah\",\n                \"time\": 1699541423710,\n                \"updated\": 1699549422720,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/uu60553917\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/uu60553917.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 16,\n                \"net\": \"uu\",\n                \"code\": \"60553917\",\n                \"ids\": \",uu60553917,\",\n                \"sources\": \",uu,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 7,\n                \"dmin\": 0.1413,\n                \"rms\": 0.09,\n                \"gap\": 189,\n                \"magType\": \"md\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.0 - 17 km W of Minersville, Utah\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -113.1221667,\n                    38.207,\n                    10.61\n                ]\n            },\n            \"id\": \"uu60553917\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 3.2,\n                \"place\": \"Alaska Peninsula\",\n                \"time\": 1699541195270,\n                \"updated\": 1699546792040,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/ak023edvtmp3\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023edvtmp3.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 158,\n                \"net\": \"ak\",\n                \"code\": \"023edvtmp3\",\n                \"ids\": \",us7000l9ur,ak023edvtmp3,\",\n                \"sources\": \",us,ak,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": null,\n                \"dmin\": null,\n                \"rms\": 0.51,\n                \"gap\": null,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 3.2 - Alaska Peninsula\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -158.5309,\n                    55.49,\n                    13.9\n                ]\n            },\n            \"id\": \"ak023edvtmp3\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.4,\n                \"place\": \"37 km NW of Valdez, Alaska\",\n                \"time\": 1699539886310,\n                \"updated\": 1699539949938,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/ak023edvoxsn\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023edvoxsn.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 89,\n                \"net\": \"ak\",\n                \"code\": \"023edvoxsn\",\n                \"ids\": \",ak023edvoxsn,\",\n                \"sources\": \",ak,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": null,\n                \"dmin\": null,\n                \"rms\": 0.25,\n                \"gap\": null,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.4 - 37 km NW of Valdez, Alaska\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -146.8141,\n                    61.3851,\n                    36.6\n                ]\n            },\n            \"id\": \"ak023edvoxsn\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 5.1,\n                \"place\": \"37 km NE of Luganville, Vanuatu\",\n                \"time\": 1699539129087,\n                \"updated\": 1699540088040,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9ul\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9ul.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 400,\n                \"net\": \"us\",\n                \"code\": \"7000l9ul\",\n                \"ids\": \",us7000l9ul,\",\n                \"sources\": \",us,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 43,\n                \"dmin\": 6,\n                \"rms\": 0.63,\n                \"gap\": 108,\n                \"magType\": \"mww\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 5.1 - 37 km NE of Luganville, Vanuatu\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    167.4497,\n                    -15.3325,\n                    130.066\n                ]\n            },\n            \"id\": \"us7000l9ul\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.8,\n                \"place\": \"27 km N of Two Rivers, Alaska\",\n                \"time\": 1699539017168,\n                \"updated\": 1699539138451,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/ak023edvlvis\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023edvlvis.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 50,\n                \"net\": \"ak\",\n                \"code\": \"023edvlvis\",\n                \"ids\": \",ak023edvlvis,\",\n                \"sources\": \",ak,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": null,\n                \"dmin\": null,\n                \"rms\": 0.39,\n                \"gap\": null,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.8 - 27 km N of Two Rivers, Alaska\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -147.1284,\n                    65.114,\n                    8\n                ]\n            },\n            \"id\": \"ak023edvlvis\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.7,\n                \"place\": \"35 km NNW of Toyah, Texas\",\n                \"time\": 1699538800671,\n                \"updated\": 1699540345082,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vzcx\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vzcx.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 44,\n                \"net\": \"tx\",\n                \"code\": \"2023vzcx\",\n                \"ids\": \",tx2023vzcx,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 15,\n                \"dmin\": 0,\n                \"rms\": 0.3,\n                \"gap\": 72,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.7 - 35 km NNW of Toyah, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -103.955,\n                    31.601,\n                    4.8633\n                ]\n            },\n            \"id\": \"tx2023vzcx\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.7,\n                \"place\": \"36 km WSW of Mentone, Texas\",\n                \"time\": 1699538479259,\n                \"updated\": 1699540335022,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vzct\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vzct.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 44,\n                \"net\": \"tx\",\n                \"code\": \"2023vzct\",\n                \"ids\": \",tx2023vzct,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 15,\n                \"dmin\": 0,\n                \"rms\": 0.4,\n                \"gap\": 50,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.7 - 36 km WSW of Mentone, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -103.978,\n                    31.638,\n                    4.6081\n                ]\n            },\n            \"id\": \"tx2023vzct\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 0.98,\n                \"place\": \"3 km WSW of Anderson Springs, CA\",\n                \"time\": 1699538180000,\n                \"updated\": 1699547291887,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960466\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960466.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 15,\n                \"net\": \"nc\",\n                \"code\": \"73960466\",\n                \"ids\": \",nc73960466,\",\n                \"sources\": \",nc,\",\n                \"types\": \",nearby-cities,origin,phase-data,scitech-link,\",\n                \"nst\": 7,\n                \"dmin\": 0.008438,\n                \"rms\": 0.02,\n                \"gap\": 107,\n                \"magType\": \"md\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.0 - 3 km WSW of Anderson Springs, CA\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -122.7216644,\n                    38.7696648,\n                    1.14\n                ]\n            },\n            \"id\": \"nc73960466\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.5,\n                \"place\": \"western Texas\",\n                \"time\": 1699538082907,\n                \"updated\": 1699540699388,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vzcn\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vzcn.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 35,\n                \"net\": \"tx\",\n                \"code\": \"2023vzcn\",\n                \"ids\": \",tx2023vzcn,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 8,\n                \"dmin\": 0.1,\n                \"rms\": 0.5,\n                \"gap\": 210,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.5 - western Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.199,\n                    31.7,\n                    2.3485\n                ]\n            },\n            \"id\": \"tx2023vzcn\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.6,\n                \"place\": \"57 km S of Whites City, New Mexico\",\n                \"time\": 1699537644417,\n                \"updated\": 1699538521009,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vzcg\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vzcg.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 39,\n                \"net\": \"tx\",\n                \"code\": \"2023vzcg\",\n                \"ids\": \",tx2023vzcg,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 17,\n                \"dmin\": 0,\n                \"rms\": 0.5,\n                \"gap\": 67,\n                \"magType\": \"mlv\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.6 - 57 km S of Whites City, New Mexico\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.292,\n                    31.664,\n                    3.1803\n                ]\n            },\n            \"id\": \"tx2023vzcg\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.03,\n                \"place\": \"9 km NW of The Geysers, CA\",\n                \"time\": 1699536386920,\n                \"updated\": 1699545673727,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960456\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960456.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 16,\n                \"net\": \"nc\",\n                \"code\": \"73960456\",\n                \"ids\": \",nc73960456,\",\n                \"sources\": \",nc,\",\n                \"types\": \",nearby-cities,origin,phase-data,scitech-link,\",\n                \"nst\": 19,\n                \"dmin\": 0.007705,\n                \"rms\": 0.02,\n                \"gap\": 74,\n                \"magType\": \"md\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.0 - 9 km NW of The Geysers, CA\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -122.8256683,\n                    38.8378334,\n                    1.78\n                ]\n            },\n            \"id\": \"nc73960456\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 4.5,\n                \"place\": \"37 km W of Arica, Chile\",\n                \"time\": 1699535112443,\n                \"updated\": 1699546587040,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9u4\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9u4.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 312,\n                \"net\": \"us\",\n                \"code\": \"7000l9u4\",\n                \"ids\": \",us7000l9u4,\",\n                \"sources\": \",us,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 27,\n                \"dmin\": 0.294,\n                \"rms\": 1.04,\n                \"gap\": 143,\n                \"magType\": \"mb\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 4.5 - 37 km W of Arica, Chile\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -70.6461,\n                    -18.4235,\n                    87.39\n                ]\n            },\n            \"id\": \"us7000l9u4\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.6,\n                \"place\": \"western Texas\",\n                \"time\": 1699535095161,\n                \"updated\": 1699535908041,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vzax\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vzax.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 39,\n                \"net\": \"tx\",\n                \"code\": \"2023vzax\",\n                \"ids\": \",tx2023vzax,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 15,\n                \"dmin\": 0,\n                \"rms\": 0.4,\n                \"gap\": 88,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.6 - western Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -103.987,\n                    31.636,\n                    4.1188\n                ]\n            },\n            \"id\": \"tx2023vzax\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2,\n                \"place\": \"38 km WSW of Mentone, Texas\",\n                \"time\": 1699535045425,\n                \"updated\": 1699535903848,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vzav\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vzav.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 62,\n                \"net\": \"tx\",\n                \"code\": \"2023vzav\",\n                \"ids\": \",tx2023vzav,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 16,\n                \"dmin\": 0,\n                \"rms\": 0.6,\n                \"gap\": 46,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.0 - 38 km WSW of Mentone, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -103.994,\n                    31.627,\n                    8.2171\n                ]\n            },\n            \"id\": \"tx2023vzav\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.5,\n                \"place\": \"western Texas\",\n                \"time\": 1699534586458,\n                \"updated\": 1699538965468,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vzao\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vzao.geojson\",\n                \"felt\": 2,\n                \"cdi\": 2.2,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 35,\n                \"net\": \"tx\",\n                \"code\": \"2023vzao\",\n                \"ids\": \",tx2023vzao,\",\n                \"sources\": \",tx,\",\n                \"types\": \",dyfi,origin,phase-data,\",\n                \"nst\": 7,\n                \"dmin\": 0.1,\n                \"rms\": 0.1,\n                \"gap\": 161,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.5 - western Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.546,\n                    31.59,\n                    3.4441\n                ]\n            },\n            \"id\": \"tx2023vzao\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.4,\n                \"place\": \"California-Nevada border region\",\n                \"time\": 1699534300904,\n                \"updated\": 1699534463161,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/nn00868648\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nn00868648.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 30,\n                \"net\": \"nn\",\n                \"code\": \"00868648\",\n                \"ids\": \",nn00868648,\",\n                \"sources\": \",nn,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 8,\n                \"dmin\": 0.392,\n                \"rms\": 0.1432,\n                \"gap\": 115.92,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.4 - California-Nevada border region\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -118.1693,\n                    38.5426,\n                    6.8\n                ]\n            },\n            \"id\": \"nn00868648\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 0.95,\n                \"place\": \"1 km SSE of The Geysers, CA\",\n                \"time\": 1699534232670,\n                \"updated\": 1699542372415,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960441\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960441.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 14,\n                \"net\": \"nc\",\n                \"code\": \"73960441\",\n                \"ids\": \",nc73960441,\",\n                \"sources\": \",nc,\",\n                \"types\": \",nearby-cities,origin,phase-data,scitech-link,\",\n                \"nst\": 8,\n                \"dmin\": 0.01094,\n                \"rms\": 0.2,\n                \"gap\": 118,\n                \"magType\": \"md\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.0 - 1 km SSE of The Geysers, CA\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -122.7528305,\n                    38.7664986,\n                    -0.84\n                ]\n            },\n            \"id\": \"nc73960441\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.6,\n                \"place\": \"western Texas\",\n                \"time\": 1699534154284,\n                \"updated\": 1699536350178,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vzai\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vzai.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 39,\n                \"net\": \"tx\",\n                \"code\": \"2023vzai\",\n                \"ids\": \",tx2023vzai,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 16,\n                \"dmin\": 0,\n                \"rms\": 0.4,\n                \"gap\": 56,\n                \"magType\": \"mlv\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.6 - western Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.119,\n                    31.619,\n                    4.0234\n                ]\n            },\n            \"id\": \"tx2023vzai\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.61,\n                \"place\": \"0 km E of Howardville, Missouri\",\n                \"time\": 1699533438360,\n                \"updated\": 1699546551790,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/nm60559656\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nm60559656.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 40,\n                \"net\": \"nm\",\n                \"code\": \"60559656\",\n                \"ids\": \",nm60559656,\",\n                \"sources\": \",nm,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 28,\n                \"dmin\": 0.0412,\n                \"rms\": 0.04,\n                \"gap\": 39,\n                \"magType\": \"md\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.6 - 0 km E of Howardville, Missouri\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -89.597,\n                    36.5683333333333,\n                    8.08\n                ]\n            },\n            \"id\": \"nm60559656\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 4.6,\n                \"place\": \"south of the Fiji Islands\",\n                \"time\": 1699533175860,\n                \"updated\": 1699535447040,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9tz\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9tz.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 326,\n                \"net\": \"us\",\n                \"code\": \"7000l9tz\",\n                \"ids\": \",us7000l9tz,\",\n                \"sources\": \",us,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 31,\n                \"dmin\": 3.627,\n                \"rms\": 0.84,\n                \"gap\": 119,\n                \"magType\": \"mb\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 4.6 - south of the Fiji Islands\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -177.1013,\n                    -24.3611,\n                    161.789\n                ]\n            },\n            \"id\": \"us7000l9tz\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.25,\n                \"place\": \"16 km E of Seven Trees, CA\",\n                \"time\": 1699533080730,\n                \"updated\": 1699540755262,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960436\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960436.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 24,\n                \"net\": \"nc\",\n                \"code\": \"73960436\",\n                \"ids\": \",nc73960436,\",\n                \"sources\": \",nc,\",\n                \"types\": \",nearby-cities,origin,phase-data,scitech-link,\",\n                \"nst\": 8,\n                \"dmin\": 0.0253,\n                \"rms\": 0.02,\n                \"gap\": 137,\n                \"magType\": \"md\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.3 - 16 km E of Seven Trees, CA\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -121.6598358,\n                    37.2820015,\n                    1.99\n                ]\n            },\n            \"id\": \"nc73960436\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.26,\n                \"place\": \"11 km SSE of Hemet, CA\",\n                \"time\": 1699532803520,\n                \"updated\": 1699539532170,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/ci39710714\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ci39710714.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 24,\n                \"net\": \"ci\",\n                \"code\": \"39710714\",\n                \"ids\": \",ci39710714,\",\n                \"sources\": \",ci,\",\n                \"types\": \",focal-mechanism,nearby-cities,origin,phase-data,scitech-link,\",\n                \"nst\": 48,\n                \"dmin\": 0.03406,\n                \"rms\": 0.15,\n                \"gap\": 36,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.3 - 11 km SSE of Hemet, CA\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -116.9331667,\n                    33.6538333,\n                    15.92\n                ]\n            },\n            \"id\": \"ci39710714\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.1,\n                \"place\": \"75 km N of Chickaloon, Alaska\",\n                \"time\": 1699532141116,\n                \"updated\": 1699532243503,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/ak023edug4qn\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023edug4qn.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 68,\n                \"net\": \"ak\",\n                \"code\": \"023edug4qn\",\n                \"ids\": \",ak023edug4qn,\",\n                \"sources\": \",ak,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": null,\n                \"dmin\": null,\n                \"rms\": 0.61,\n                \"gap\": null,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.1 - 75 km N of Chickaloon, Alaska\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -148.2058,\n                    62.4626,\n                    43.7\n                ]\n            },\n            \"id\": \"ak023edug4qn\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.1,\n                \"place\": \"63 km ESE of Denali Park, Alaska\",\n                \"time\": 1699531979635,\n                \"updated\": 1699532104007,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/ak023edufj0r\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023edufj0r.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 68,\n                \"net\": \"ak\",\n                \"code\": \"023edufj0r\",\n                \"ids\": \",ak023edufj0r,\",\n                \"sources\": \",ak,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": null,\n                \"dmin\": null,\n                \"rms\": 0.69,\n                \"gap\": null,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.1 - 63 km ESE of Denali Park, Alaska\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -147.7109,\n                    63.5436,\n                    9\n                ]\n            },\n            \"id\": \"ak023edufj0r\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.54,\n                \"place\": \"36 km S of Silver Gate, Montana\",\n                \"time\": 1699531900140,\n                \"updated\": 1699548835050,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/uu60553912\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/uu60553912.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 36,\n                \"net\": \"uu\",\n                \"code\": \"60553912\",\n                \"ids\": \",uu60553912,\",\n                \"sources\": \",uu,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 8,\n                \"dmin\": 0.09852,\n                \"rms\": 0.22,\n                \"gap\": 155,\n                \"magType\": \"md\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.5 - 36 km S of Silver Gate, Montana\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -110.0391667,\n                    44.6796667,\n                    12.6\n                ]\n            },\n            \"id\": \"uu60553912\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 4.4,\n                \"place\": \"69 km NNE of Calama, Chile\",\n                \"time\": 1699531272150,\n                \"updated\": 1699533144040,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9tr\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9tr.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 298,\n                \"net\": \"us\",\n                \"code\": \"7000l9tr\",\n                \"ids\": \",us7000l9tr,\",\n                \"sources\": \",us,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 24,\n                \"dmin\": 0.972,\n                \"rms\": 0.7,\n                \"gap\": 51,\n                \"magType\": \"mb\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 4.4 - 69 km NNE of Calama, Chile\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -68.7276,\n                    -21.8525,\n                    141.668\n                ]\n            },\n            \"id\": \"us7000l9tr\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.33,\n                \"place\": \"9 km NW of Bridgeport, CA\",\n                \"time\": 1699530819620,\n                \"updated\": 1699552461719,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960426\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960426.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 84,\n                \"net\": \"nc\",\n                \"code\": \"73960426\",\n                \"ids\": \",nc73960426,nn00868645,\",\n                \"sources\": \",nc,nn,\",\n                \"types\": \",nearby-cities,origin,phase-data,scitech-link,\",\n                \"nst\": 30,\n                \"dmin\": 0.2225,\n                \"rms\": 0.07,\n                \"gap\": 115,\n                \"magType\": \"md\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.3 - 9 km NW of Bridgeport, CA\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -119.315,\n                    38.3036667,\n                    11.84\n                ]\n            },\n            \"id\": \"nc73960426\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.9,\n                \"place\": \"37 km WSW of Mentone, Texas\",\n                \"time\": 1699530741120,\n                \"updated\": 1699532235014,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyym\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyym.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 56,\n                \"net\": \"tx\",\n                \"code\": \"2023vyym\",\n                \"ids\": \",tx2023vyym,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 27,\n                \"dmin\": 0,\n                \"rms\": 0.1,\n                \"gap\": 51,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.9 - 37 km WSW of Mentone, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -103.98,\n                    31.624,\n                    6.2215\n                ]\n            },\n            \"id\": \"tx2023vyym\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.5,\n                \"place\": \"64 km SSW of Whites City, New Mexico\",\n                \"time\": 1699530260506,\n                \"updated\": 1699531977635,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyyf\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyyf.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 35,\n                \"net\": \"tx\",\n                \"code\": \"2023vyyf\",\n                \"ids\": \",tx2023vyyf,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 17,\n                \"dmin\": 0.1,\n                \"rms\": 0.5,\n                \"gap\": 147,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.5 - 64 km SSW of Whites City, New Mexico\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.513,\n                    31.607,\n                    7.1771\n                ]\n            },\n            \"id\": \"tx2023vyyf\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.9,\n                \"place\": \"62 km S of Whites City, New Mexico\",\n                \"time\": 1699530144721,\n                \"updated\": 1699540622341,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyyd\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyyd.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 56,\n                \"net\": \"tx\",\n                \"code\": \"2023vyyd\",\n                \"ids\": \",tx2023vyyd,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 16,\n                \"dmin\": 0,\n                \"rms\": 0.2,\n                \"gap\": 215,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.9 - 62 km S of Whites City, New Mexico\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.5,\n                    31.62,\n                    7.3656\n                ]\n            },\n            \"id\": \"tx2023vyyd\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.6,\n                \"place\": \"western Texas\",\n                \"time\": 1699529368860,\n                \"updated\": 1699530884898,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyxu\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyxu.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 39,\n                \"net\": \"tx\",\n                \"code\": \"2023vyxu\",\n                \"ids\": \",tx2023vyxu,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 9,\n                \"dmin\": 0.1,\n                \"rms\": 0.1,\n                \"gap\": 127,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.6 - western Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -103.993,\n                    31.628,\n                    4.9879\n                ]\n            },\n            \"id\": \"tx2023vyxu\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.4,\n                \"place\": \"38 km WSW of Mentone, Texas\",\n                \"time\": 1699528901589,\n                \"updated\": 1699543793680,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyxl\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyxl.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 89,\n                \"net\": \"tx\",\n                \"code\": \"2023vyxl\",\n                \"ids\": \",tx2023vyxl,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 12,\n                \"dmin\": 0,\n                \"rms\": 0.1,\n                \"gap\": 44,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.4 - 38 km WSW of Mentone, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -103.997,\n                    31.634,\n                    5.7717\n                ]\n            },\n            \"id\": \"tx2023vyxl\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2,\n                \"place\": \"2 km W of Guayanilla, Puerto Rico\",\n                \"time\": 1699527492410,\n                \"updated\": 1699531205750,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/pr71431118\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/pr71431118.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 62,\n                \"net\": \"pr\",\n                \"code\": \"71431118\",\n                \"ids\": \",pr71431118,\",\n                \"sources\": \",pr,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 9,\n                \"dmin\": 0.07531,\n                \"rms\": 0.13,\n                \"gap\": 151,\n                \"magType\": \"md\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.0 - 2 km W of Guayanilla, Puerto Rico\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -66.8158333333333,\n                    18.0203333333333,\n                    18.28\n                ]\n            },\n            \"id\": \"pr71431118\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.6,\n                \"place\": \"55 km WNW of Toyah, Texas\",\n                \"time\": 1699527449442,\n                \"updated\": 1699529648303,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vywq\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vywq.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 39,\n                \"net\": \"tx\",\n                \"code\": \"2023vywq\",\n                \"ids\": \",tx2023vywq,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 16,\n                \"dmin\": 0.1,\n                \"rms\": 1,\n                \"gap\": 75,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.6 - 55 km WNW of Toyah, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.28,\n                    31.594,\n                    0.6259\n                ]\n            },\n            \"id\": \"tx2023vywq\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.32,\n                \"place\": \"6 km NW of The Geysers, CA\",\n                \"time\": 1699527131000,\n                \"updated\": 1699536010795,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960406\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960406.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 27,\n                \"net\": \"nc\",\n                \"code\": \"73960406\",\n                \"ids\": \",nc73960406,\",\n                \"sources\": \",nc,\",\n                \"types\": \",nearby-cities,origin,phase-data,scitech-link,\",\n                \"nst\": 22,\n                \"dmin\": 0.01265,\n                \"rms\": 0.02,\n                \"gap\": 31,\n                \"magType\": \"md\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.3 - 6 km NW of The Geysers, CA\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -122.7988358,\n                    38.8203316,\n                    0.59\n                ]\n            },\n            \"id\": \"nc73960406\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 4.7,\n                \"place\": \"Solomon Islands\",\n                \"time\": 1699526362419,\n                \"updated\": 1699529391040,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9tf\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9tf.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 340,\n                \"net\": \"us\",\n                \"code\": \"7000l9tf\",\n                \"ids\": \",us7000l9tf,\",\n                \"sources\": \",us,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 30,\n                \"dmin\": 2.947,\n                \"rms\": 0.82,\n                \"gap\": 138,\n                \"magType\": \"mb\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 4.7 - Solomon Islands\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    162.2896,\n                    -11.2877,\n                    10\n                ]\n            },\n            \"id\": \"us7000l9tf\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.8,\n                \"place\": \"8 km NW of Bridgeport, California\",\n                \"time\": 1699525521181,\n                \"updated\": 1699551384417,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/nn00868657\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nn00868657.geojson\",\n                \"felt\": 1,\n                \"cdi\": 2.7,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 121,\n                \"net\": \"nn\",\n                \"code\": \"00868657\",\n                \"ids\": \",nn00868657,\",\n                \"sources\": \",nn,\",\n                \"types\": \",dyfi,origin,phase-data,\",\n                \"nst\": 7,\n                \"dmin\": 0.23,\n                \"rms\": 0.1722,\n                \"gap\": 145.66,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.8 - 8 km NW of Bridgeport, California\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -119.3044,\n                    38.2996,\n                    8.5\n                ]\n            },\n            \"id\": \"nn00868657\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.59,\n                \"place\": \"8 km NW of Bridgeport, CA\",\n                \"time\": 1699525502270,\n                \"updated\": 1699551410825,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960401\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960401.geojson\",\n                \"felt\": 0,\n                \"cdi\": 1,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 103,\n                \"net\": \"nc\",\n                \"code\": \"73960401\",\n                \"ids\": \",nn00868642,nc73960401,\",\n                \"sources\": \",nn,nc,\",\n                \"types\": \",dyfi,focal-mechanism,nearby-cities,origin,phase-data,scitech-link,\",\n                \"nst\": 47,\n                \"dmin\": 0.2278,\n                \"rms\": 0.15,\n                \"gap\": 115,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.6 - 8 km NW of Bridgeport, CA\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -119.31,\n                    38.2996667,\n                    5.57\n                ]\n            },\n            \"id\": \"nc73960401\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2,\n                \"place\": \"40 km W of Mentone, Texas\",\n                \"time\": 1699525233123,\n                \"updated\": 1699526929944,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyvl\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyvl.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 62,\n                \"net\": \"tx\",\n                \"code\": \"2023vyvl\",\n                \"ids\": \",tx2023vyvl,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 20,\n                \"dmin\": 0,\n                \"rms\": 0.2,\n                \"gap\": 51,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.0 - 40 km W of Mentone, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.023,\n                    31.649,\n                    4.8543\n                ]\n            },\n            \"id\": \"tx2023vyvl\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 4.6,\n                \"place\": \"Banda Sea\",\n                \"time\": 1699524776695,\n                \"updated\": 1699529976040,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9tb\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9tb.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 326,\n                \"net\": \"us\",\n                \"code\": \"7000l9tb\",\n                \"ids\": \",us7000l9tb,\",\n                \"sources\": \",us,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 37,\n                \"dmin\": 4.382,\n                \"rms\": 0.91,\n                \"gap\": 92,\n                \"magType\": \"mb\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 4.6 - Banda Sea\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    129.501,\n                    -6.3633,\n                    10\n                ]\n            },\n            \"id\": \"us7000l9tb\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.05,\n                \"place\": \"6 km NW of The Geysers, CA\",\n                \"time\": 1699523783020,\n                \"updated\": 1699525093778,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960391\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960391.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 17,\n                \"net\": \"nc\",\n                \"code\": \"73960391\",\n                \"ids\": \",nc73960391,\",\n                \"sources\": \",nc,\",\n                \"types\": \",nearby-cities,origin,phase-data,scitech-link,\",\n                \"nst\": 24,\n                \"dmin\": 0.004774,\n                \"rms\": 0.02,\n                \"gap\": 37,\n                \"magType\": \"md\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.1 - 6 km NW of The Geysers, CA\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -122.8059998,\n                    38.8198318,\n                    2.63\n                ]\n            },\n            \"id\": \"nc73960391\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2,\n                \"place\": \"61 km ESE of Denali Park, Alaska\",\n                \"time\": 1699522922371,\n                \"updated\": 1699523042897,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/ak023edstj9p\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023edstj9p.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 62,\n                \"net\": \"ak\",\n                \"code\": \"023edstj9p\",\n                \"ids\": \",ak023edstj9p,\",\n                \"sources\": \",ak,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": null,\n                \"dmin\": null,\n                \"rms\": 0.71,\n                \"gap\": null,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.0 - 61 km ESE of Denali Park, Alaska\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -147.7193,\n                    63.5852,\n                    0\n                ]\n            },\n            \"id\": \"ak023edstj9p\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2,\n                \"place\": \"66 km SSW of Whites City, New Mexico\",\n                \"time\": 1699522836410,\n                \"updated\": 1699525123100,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyub\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyub.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 62,\n                \"net\": \"tx\",\n                \"code\": \"2023vyub\",\n                \"ids\": \",tx2023vyub,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 18,\n                \"dmin\": 0.1,\n                \"rms\": 0.4,\n                \"gap\": 162,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.0 - 66 km SSW of Whites City, New Mexico\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.546,\n                    31.597,\n                    3.3162\n                ]\n            },\n            \"id\": \"tx2023vyub\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.68,\n                \"place\": \"1 km ENE of The Geysers, CA\",\n                \"time\": 1699522755010,\n                \"updated\": 1699533490553,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960376\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960376.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 43,\n                \"net\": \"nc\",\n                \"code\": \"73960376\",\n                \"ids\": \",nc73960376,\",\n                \"sources\": \",nc,\",\n                \"types\": \",nearby-cities,origin,phase-data,scitech-link,\",\n                \"nst\": 24,\n                \"dmin\": 0.007337,\n                \"rms\": 0.04,\n                \"gap\": 75,\n                \"magType\": \"md\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.7 - 1 km ENE of The Geysers, CA\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -122.7456665,\n                    38.7809982,\n                    0.29\n                ]\n            },\n            \"id\": \"nc73960376\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.21,\n                \"place\": \"14 km NW of Pawnee, Oklahoma\",\n                \"time\": 1699522553619,\n                \"updated\": 1699536831132,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/ok2023vyty\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ok2023vyty.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 23,\n                \"net\": \"ok\",\n                \"code\": \"2023vyty\",\n                \"ids\": \",ok2023vyty,\",\n                \"sources\": \",ok,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 55,\n                \"dmin\": 0.02699470272,\n                \"rms\": 0.27,\n                \"gap\": 63,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.2 - 14 km NW of Pawnee, Oklahoma\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -96.92633333,\n                    36.42883333,\n                    5.82\n                ]\n            },\n            \"id\": \"ok2023vyty\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.22,\n                \"place\": \"12 km ENE of Healdsburg, CA\",\n                \"time\": 1699521519650,\n                \"updated\": 1699531810411,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960371\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960371.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 23,\n                \"net\": \"nc\",\n                \"code\": \"73960371\",\n                \"ids\": \",nc73960371,\",\n                \"sources\": \",nc,\",\n                \"types\": \",nearby-cities,origin,phase-data,scitech-link,\",\n                \"nst\": 8,\n                \"dmin\": 0.0666,\n                \"rms\": 0.02,\n                \"gap\": 170,\n                \"magType\": \"md\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.2 - 12 km ENE of Healdsburg, CA\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -122.7360001,\n                    38.6418343,\n                    7.61\n                ]\n            },\n            \"id\": \"nc73960371\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.9,\n                \"place\": \"39 km W of Mentone, Texas\",\n                \"time\": 1699521298476,\n                \"updated\": 1699522590145,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vytg\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vytg.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 56,\n                \"net\": \"tx\",\n                \"code\": \"2023vytg\",\n                \"ids\": \",tx2023vytg,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 14,\n                \"dmin\": 0,\n                \"rms\": 0.2,\n                \"gap\": 81,\n                \"magType\": \"mlv\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.9 - 39 km W of Mentone, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.012,\n                    31.638,\n                    5.6419\n                ]\n            },\n            \"id\": \"tx2023vytg\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 5.1,\n                \"place\": \"282 km SSE of Amahai, Indonesia\",\n                \"time\": 1699521257574,\n                \"updated\": 1699522340040,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9t5\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9t5.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 400,\n                \"net\": \"us\",\n                \"code\": \"7000l9t5\",\n                \"ids\": \",us7000l9t5,\",\n                \"sources\": \",us,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 65,\n                \"dmin\": 6.889,\n                \"rms\": 0.69,\n                \"gap\": 46,\n                \"magType\": \"mww\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 5.1 - 282 km SSE of Amahai, Indonesia\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    130.1774,\n                    -5.5615,\n                    10\n                ]\n            },\n            \"id\": \"us7000l9t5\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.2,\n                \"place\": \"52 km NW of Toyah, Texas\",\n                \"time\": 1699520728701,\n                \"updated\": 1699522678069,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vysy\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vysy.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 74,\n                \"net\": \"tx\",\n                \"code\": \"2023vysy\",\n                \"ids\": \",tx2023vysy,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 16,\n                \"dmin\": 0.1,\n                \"rms\": 0.2,\n                \"gap\": 77,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.2 - 52 km NW of Toyah, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.25,\n                    31.581,\n                    4.1076\n                ]\n            },\n            \"id\": \"tx2023vysy\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.18,\n                \"place\": \"31 km N of Beaver, Utah\",\n                \"time\": 1699520490720,\n                \"updated\": 1699540478360,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/uu60033944\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/uu60033944.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 21,\n                \"net\": \"uu\",\n                \"code\": \"60033944\",\n                \"ids\": \",uu60033944,\",\n                \"sources\": \",uu,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 10,\n                \"dmin\": 0.09396,\n                \"rms\": 0.05,\n                \"gap\": 155,\n                \"magType\": \"md\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.2 - 31 km N of Beaver, Utah\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -112.6983333,\n                    38.5526667,\n                    7.3\n                ]\n            },\n            \"id\": \"uu60033944\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.54,\n                \"place\": \"Utah\",\n                \"time\": 1699520478260,\n                \"updated\": 1699540290880,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/uu60553892\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/uu60553892.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 36,\n                \"net\": \"uu\",\n                \"code\": \"60553892\",\n                \"ids\": \",uu60553892,\",\n                \"sources\": \",uu,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 22,\n                \"dmin\": 0.1057,\n                \"rms\": 0.16,\n                \"gap\": 64,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.5 - Utah\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -112.6993333,\n                    38.5698333,\n                    8.82\n                ]\n            },\n            \"id\": \"uu60553892\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.5,\n                \"place\": \"62 km S of Whites City, New Mexico\",\n                \"time\": 1699520210892,\n                \"updated\": 1699534433082,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vysp\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vysp.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 96,\n                \"net\": \"tx\",\n                \"code\": \"2023vysp\",\n                \"ids\": \",tx2023vysp,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 30,\n                \"dmin\": 0,\n                \"rms\": 0.2,\n                \"gap\": 75,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.5 - 62 km S of Whites City, New Mexico\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.497,\n                    31.621,\n                    7.3141\n                ]\n            },\n            \"id\": \"tx2023vysp\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.9,\n                \"place\": \"Southern Alaska\",\n                \"time\": 1699520073785,\n                \"updated\": 1699520200659,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/ak023edsaqoi\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023edsaqoi.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 56,\n                \"net\": \"ak\",\n                \"code\": \"023edsaqoi\",\n                \"ids\": \",ak023edsaqoi,\",\n                \"sources\": \",ak,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": null,\n                \"dmin\": null,\n                \"rms\": 0.25,\n                \"gap\": null,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.9 - Southern Alaska\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -150.6645,\n                    61.4013,\n                    52.4\n                ]\n            },\n            \"id\": \"ak023edsaqoi\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.07,\n                \"place\": \"6 km NW of The Geysers, CA\",\n                \"time\": 1699519613460,\n                \"updated\": 1699521974485,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960361\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960361.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 18,\n                \"net\": \"nc\",\n                \"code\": \"73960361\",\n                \"ids\": \",nc73960361,\",\n                \"sources\": \",nc,\",\n                \"types\": \",nearby-cities,origin,phase-data,scitech-link,\",\n                \"nst\": 22,\n                \"dmin\": 0.0044,\n                \"rms\": 0.02,\n                \"gap\": 39,\n                \"magType\": \"md\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.1 - 6 km NW of The Geysers, CA\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -122.8046646,\n                    38.8221664,\n                    3.17\n                ]\n            },\n            \"id\": \"nc73960361\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.6,\n                \"place\": \"western Texas\",\n                \"time\": 1699519220905,\n                \"updated\": 1699520370661,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vysd\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vysd.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 39,\n                \"net\": \"tx\",\n                \"code\": \"2023vysd\",\n                \"ids\": \",tx2023vysd,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 13,\n                \"dmin\": 0,\n                \"rms\": 0.6,\n                \"gap\": 51,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.6 - western Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.007,\n                    31.638,\n                    7.7441\n                ]\n            },\n            \"id\": \"tx2023vysd\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 4.5,\n                \"place\": \"Banda Sea\",\n                \"time\": 1699519007517,\n                \"updated\": 1699524755040,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9sy\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9sy.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 312,\n                \"net\": \"us\",\n                \"code\": \"7000l9sy\",\n                \"ids\": \",us7000l9sy,\",\n                \"sources\": \",us,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 17,\n                \"dmin\": 9.275,\n                \"rms\": 0.61,\n                \"gap\": 68,\n                \"magType\": \"mb\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 4.5 - Banda Sea\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    129.8777,\n                    -6.4094,\n                    10\n                ]\n            },\n            \"id\": \"us7000l9sy\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.6,\n                \"place\": \"40 km W of Mentone, Texas\",\n                \"time\": 1699518754200,\n                \"updated\": 1699520362560,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyrw\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyrw.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 39,\n                \"net\": \"tx\",\n                \"code\": \"2023vyrw\",\n                \"ids\": \",tx2023vyrw,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 19,\n                \"dmin\": 0,\n                \"rms\": 0.5,\n                \"gap\": 43,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.6 - 40 km W of Mentone, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.015,\n                    31.643,\n                    4.7886\n                ]\n            },\n            \"id\": \"tx2023vyrw\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.6,\n                \"place\": \"39 km W of Mentone, Texas\",\n                \"time\": 1699518647476,\n                \"updated\": 1699535240046,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyrv\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyrv.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 104,\n                \"net\": \"tx\",\n                \"code\": \"2023vyrv\",\n                \"ids\": \",tx2023vyrv,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 15,\n                \"dmin\": 0,\n                \"rms\": 0.1,\n                \"gap\": 42,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.6 - 39 km W of Mentone, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.01,\n                    31.638,\n                    5.1418\n                ]\n            },\n            \"id\": \"tx2023vyrv\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 4.4,\n                \"place\": \"4 km SE of Chicacao, Guatemala\",\n                \"time\": 1699518223388,\n                \"updated\": 1699523766040,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9sw\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9sw.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 298,\n                \"net\": \"us\",\n                \"code\": \"7000l9sw\",\n                \"ids\": \",us7000l9sw,\",\n                \"sources\": \",us,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 65,\n                \"dmin\": 1.591,\n                \"rms\": 0.67,\n                \"gap\": 181,\n                \"magType\": \"mb\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 4.4 - 4 km SE of Chicacao, Guatemala\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -91.2969,\n                    14.5122,\n                    11.921\n                ]\n            },\n            \"id\": \"us7000l9sw\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.6,\n                \"place\": \"45 km NW of Toyah, Texas\",\n                \"time\": 1699517106377,\n                \"updated\": 1699520350684,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyqz\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyqz.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 39,\n                \"net\": \"tx\",\n                \"code\": \"2023vyqz\",\n                \"ids\": \",tx2023vyqz,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 19,\n                \"dmin\": 0,\n                \"rms\": 0.4,\n                \"gap\": 57,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.6 - 45 km NW of Toyah, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.115,\n                    31.614,\n                    3.939\n                ]\n            },\n            \"id\": \"tx2023vyqz\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.6,\n                \"place\": \"western Texas\",\n                \"time\": 1699516779628,\n                \"updated\": 1699542955423,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyqu\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyqu.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 104,\n                \"net\": \"tx\",\n                \"code\": \"2023vyqu\",\n                \"ids\": \",tx2023vyqu,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 15,\n                \"dmin\": 0,\n                \"rms\": 0.1,\n                \"gap\": 43,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.6 - western Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.013,\n                    31.65,\n                    4.4863\n                ]\n            },\n            \"id\": \"tx2023vyqu\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2,\n                \"place\": \"37 km WSW of Mentone, Texas\",\n                \"time\": 1699516703855,\n                \"updated\": 1699518389934,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyqt\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyqt.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 62,\n                \"net\": \"tx\",\n                \"code\": \"2023vyqt\",\n                \"ids\": \",tx2023vyqt,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 16,\n                \"dmin\": 0,\n                \"rms\": 0.2,\n                \"gap\": 46,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.0 - 37 km WSW of Mentone, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -103.989,\n                    31.634,\n                    5.7471\n                ]\n            },\n            \"id\": \"tx2023vyqt\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.2,\n                \"place\": \"37 km WSW of Mentone, Texas\",\n                \"time\": 1699516572955,\n                \"updated\": 1699535443164,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyqr\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyqr.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 74,\n                \"net\": \"tx\",\n                \"code\": \"2023vyqr\",\n                \"ids\": \",tx2023vyqr,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 15,\n                \"dmin\": 0,\n                \"rms\": 0.1,\n                \"gap\": 46,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.2 - 37 km WSW of Mentone, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -103.99,\n                    31.633,\n                    5.0775\n                ]\n            },\n            \"id\": \"tx2023vyqr\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.9,\n                \"place\": \"44 km WNW of Ninilchik, Alaska\",\n                \"time\": 1699516148030,\n                \"updated\": 1699516302655,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/ak023edro6ib\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023edro6ib.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 56,\n                \"net\": \"ak\",\n                \"code\": \"023edro6ib\",\n                \"ids\": \",ak023edro6ib,\",\n                \"sources\": \",ak,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": null,\n                \"dmin\": null,\n                \"rms\": 0.23,\n                \"gap\": null,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.9 - 44 km WNW of Ninilchik, Alaska\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -152.4429,\n                    60.1347,\n                    88.9\n                ]\n            },\n            \"id\": \"ak023edro6ib\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 3,\n                \"place\": \"41 km W of Mentone, Texas\",\n                \"time\": 1699516015994,\n                \"updated\": 1699517219040,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyqk\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyqk.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 138,\n                \"net\": \"tx\",\n                \"code\": \"2023vyqk\",\n                \"ids\": \",us7000l9sp,tx2023vyqk,\",\n                \"sources\": \",us,tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 15,\n                \"dmin\": 0,\n                \"rms\": 0.2,\n                \"gap\": 80,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 3.0 - 41 km W of Mentone, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.026,\n                    31.641,\n                    6.1316\n                ]\n            },\n            \"id\": \"tx2023vyqk\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.8,\n                \"place\": \"37 km W of Mentone, Texas\",\n                \"time\": 1699515848845,\n                \"updated\": 1699517967298,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyqj\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyqj.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 50,\n                \"net\": \"tx\",\n                \"code\": \"2023vyqj\",\n                \"ids\": \",tx2023vyqj,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 20,\n                \"dmin\": 0,\n                \"rms\": 0.3,\n                \"gap\": 49,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.8 - 37 km W of Mentone, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -103.986,\n                    31.64,\n                    4.4907\n                ]\n            },\n            \"id\": \"tx2023vyqj\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.6,\n                \"place\": \"38 km W of Mentone, Texas\",\n                \"time\": 1699515640399,\n                \"updated\": 1699518370548,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyqe\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyqe.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 39,\n                \"net\": \"tx\",\n                \"code\": \"2023vyqe\",\n                \"ids\": \",tx2023vyqe,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 14,\n                \"dmin\": 0,\n                \"rms\": 0.8,\n                \"gap\": 88,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.6 - 38 km W of Mentone, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104,\n                    31.642,\n                    8.2456\n                ]\n            },\n            \"id\": \"tx2023vyqe\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 4.3,\n                \"place\": \"Banda Sea\",\n                \"time\": 1699515368452,\n                \"updated\": 1699521555040,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9sz\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9sz.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 284,\n                \"net\": \"us\",\n                \"code\": \"7000l9sz\",\n                \"ids\": \",us7000l9sz,\",\n                \"sources\": \",us,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 18,\n                \"dmin\": 1.923,\n                \"rms\": 0.71,\n                \"gap\": 86,\n                \"magType\": \"mb\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 4.3 - Banda Sea\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    129.5507,\n                    -6.4253,\n                    10\n                ]\n            },\n            \"id\": \"us7000l9sz\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.5,\n                \"place\": \"western Texas\",\n                \"time\": 1699514872100,\n                \"updated\": 1699517801250,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vypu\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vypu.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 35,\n                \"net\": \"tx\",\n                \"code\": \"2023vypu\",\n                \"ids\": \",tx2023vypu,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 11,\n                \"dmin\": 0.1,\n                \"rms\": 0.2,\n                \"gap\": 67,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.5 - western Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -103.98,\n                    31.593,\n                    4.7695\n                ]\n            },\n            \"id\": \"tx2023vypu\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.26,\n                \"place\": \"2 km NNE of Bear Dance, Montana\",\n                \"time\": 1699513945500,\n                \"updated\": 1699539858990,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/mb90032413\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/mb90032413.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 24,\n                \"net\": \"mb\",\n                \"code\": \"90032413\",\n                \"ids\": \",mb90032413,\",\n                \"sources\": \",mb,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 12,\n                \"dmin\": 0.2601,\n                \"rms\": 0.07,\n                \"gap\": 198,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.3 - 2 km NNE of Bear Dance, Montana\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -114.021666666667,\n                    47.9383333333333,\n                    5.06\n                ]\n            },\n            \"id\": \"mb90032413\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.81,\n                \"place\": \"17 km ENE of Thousand Palms, CA\",\n                \"time\": 1699513927790,\n                \"updated\": 1699539408190,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/ci39710666\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ci39710666.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 50,\n                \"net\": \"ci\",\n                \"code\": \"39710666\",\n                \"ids\": \",ci39710666,\",\n                \"sources\": \",ci,\",\n                \"types\": \",focal-mechanism,nearby-cities,origin,phase-data,scitech-link,\",\n                \"nst\": 65,\n                \"dmin\": 0.0582,\n                \"rms\": 0.13,\n                \"gap\": 48,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.8 - 17 km ENE of Thousand Palms, CA\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -116.2318333,\n                    33.8933333,\n                    6.9\n                ]\n            },\n            \"id\": \"ci39710666\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.6,\n                \"place\": \"Southern Alaska\",\n                \"time\": 1699513811313,\n                \"updated\": 1699514060386,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/ak023edrftu7\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023edrftu7.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 104,\n                \"net\": \"ak\",\n                \"code\": \"023edrftu7\",\n                \"ids\": \",ak023edrftu7,\",\n                \"sources\": \",ak,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": null,\n                \"dmin\": null,\n                \"rms\": 0.34,\n                \"gap\": null,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.6 - Southern Alaska\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -153.4325,\n                    60.0018,\n                    140.2\n                ]\n            },\n            \"id\": \"ak023edrftu7\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.7,\n                \"place\": \"western Texas\",\n                \"time\": 1699513613102,\n                \"updated\": 1699514936916,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vypc\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vypc.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 44,\n                \"net\": \"tx\",\n                \"code\": \"2023vypc\",\n                \"ids\": \",tx2023vypc,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 14,\n                \"dmin\": 0.1,\n                \"rms\": 0.2,\n                \"gap\": 187,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.7 - western Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.579,\n                    31.654,\n                    3.4025\n                ]\n            },\n            \"id\": \"tx2023vypc\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.42,\n                \"place\": \"2 km SW of P\\u0101hala, Hawaii\",\n                \"time\": 1699512491770,\n                \"updated\": 1699513874040,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/hv73642132\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/hv73642132.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 90,\n                \"net\": \"hv\",\n                \"code\": \"73642132\",\n                \"ids\": \",us7000l9si,hv73642132,\",\n                \"sources\": \",us,hv,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 50,\n                \"dmin\": null,\n                \"rms\": 0.109999999,\n                \"gap\": 131,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.4 - 2 km SW of P\\u0101hala, Hawaii\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -155.501998901367,\n                    19.1879997253418,\n                    32.7000007629395\n                ]\n            },\n            \"id\": \"hv73642132\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 4.4,\n                \"place\": \"143 km E of Miyako, Japan\",\n                \"time\": 1699510438937,\n                \"updated\": 1699513220040,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9sh\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9sh.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 298,\n                \"net\": \"us\",\n                \"code\": \"7000l9sh\",\n                \"ids\": \",us7000l9sh,\",\n                \"sources\": \",us,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 30,\n                \"dmin\": 2.195,\n                \"rms\": 0.49,\n                \"gap\": 115,\n                \"magType\": \"mb\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 4.4 - 143 km E of Miyako, Japan\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    143.6088,\n                    39.7733,\n                    10\n                ]\n            },\n            \"id\": \"us7000l9sh\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.6,\n                \"place\": \"western Texas\",\n                \"time\": 1699509936731,\n                \"updated\": 1699537102707,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyna\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyna.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 39,\n                \"net\": \"tx\",\n                \"code\": \"2023vyna\",\n                \"ids\": \",tx2023vyna,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 12,\n                \"dmin\": 0,\n                \"rms\": 0.1,\n                \"gap\": 65,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.6 - western Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -103.993,\n                    31.634,\n                    5.6174\n                ]\n            },\n            \"id\": \"tx2023vyna\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.5,\n                \"place\": \"20 km ESE of Schurz, Nevada\",\n                \"time\": 1699509657132,\n                \"updated\": 1699549079466,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/nn00868639\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nn00868639.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 35,\n                \"net\": \"nn\",\n                \"code\": \"00868639\",\n                \"ids\": \",nn00868639,\",\n                \"sources\": \",nn,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 11,\n                \"dmin\": 0.411,\n                \"rms\": 0.1416,\n                \"gap\": 65.14,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.5 - 20 km ESE of Schurz, Nevada\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -118.5879,\n                    38.8893,\n                    7.3\n                ]\n            },\n            \"id\": \"nn00868639\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.1,\n                \"place\": \"19 km ESE of Schurz, Nevada\",\n                \"time\": 1699508884925,\n                \"updated\": 1699549273208,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/nn00868636\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nn00868636.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 68,\n                \"net\": \"nn\",\n                \"code\": \"00868636\",\n                \"ids\": \",nn00868636,\",\n                \"sources\": \",nn,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 14,\n                \"dmin\": 0.412,\n                \"rms\": 0.1079,\n                \"gap\": 57.62,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.1 - 19 km ESE of Schurz, Nevada\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -118.5934,\n                    38.8967,\n                    9.5\n                ]\n            },\n            \"id\": \"nn00868636\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2,\n                \"place\": \"western Texas\",\n                \"time\": 1699508572919,\n                \"updated\": 1699510921373,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyme\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyme.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 62,\n                \"net\": \"tx\",\n                \"code\": \"2023vyme\",\n                \"ids\": \",tx2023vyme,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 12,\n                \"dmin\": 0,\n                \"rms\": 0.4,\n                \"gap\": 97,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.0 - western Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.001,\n                    31.621,\n                    3.8232\n                ]\n            },\n            \"id\": \"tx2023vyme\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.7,\n                \"place\": \"53 km NW of Toyah, Texas\",\n                \"time\": 1699508529502,\n                \"updated\": 1699531918203,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vymg\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vymg.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 112,\n                \"net\": \"tx\",\n                \"code\": \"2023vymg\",\n                \"ids\": \",us7000l9sb,tx2023vymg,\",\n                \"sources\": \",us,tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 30,\n                \"dmin\": 0.1,\n                \"rms\": 0.1,\n                \"gap\": 67,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.7 - 53 km NW of Toyah, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.184,\n                    31.668,\n                    6.4015\n                ]\n            },\n            \"id\": \"tx2023vymg\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.2,\n                \"place\": \"56 km S of Whites City, New Mexico\",\n                \"time\": 1699508438306,\n                \"updated\": 1699533641499,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vymf\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vymf.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 74,\n                \"net\": \"tx\",\n                \"code\": \"2023vymf\",\n                \"ids\": \",tx2023vymf,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 15,\n                \"dmin\": 0,\n                \"rms\": 0.4,\n                \"gap\": 74,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.2 - 56 km S of Whites City, New Mexico\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.424,\n                    31.667,\n                    8.6252\n                ]\n            },\n            \"id\": \"tx2023vymf\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.5,\n                \"place\": \"41 km W of Mentone, Texas\",\n                \"time\": 1699508104952,\n                \"updated\": 1699510917532,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyma\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyma.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 35,\n                \"net\": \"tx\",\n                \"code\": \"2023vyma\",\n                \"ids\": \",tx2023vyma,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 14,\n                \"dmin\": 0,\n                \"rms\": 0.2,\n                \"gap\": 74,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.5 - 41 km W of Mentone, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.025,\n                    31.634,\n                    4.5579\n                ]\n            },\n            \"id\": \"tx2023vyma\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.1,\n                \"place\": \"Kodiak Island region, Alaska\",\n                \"time\": 1699507036979,\n                \"updated\": 1699507174812,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/ak023edqah2p\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023edqah2p.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 68,\n                \"net\": \"ak\",\n                \"code\": \"023edqah2p\",\n                \"ids\": \",ak023edqah2p,\",\n                \"sources\": \",ak,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": null,\n                \"dmin\": null,\n                \"rms\": 0.15,\n                \"gap\": null,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.1 - Kodiak Island region, Alaska\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -153.979,\n                    58.6447,\n                    95.5\n                ]\n            },\n            \"id\": \"ak023edqah2p\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.84,\n                \"place\": \"6 km SSW of Perryville, Arkansas\",\n                \"time\": 1699506444610,\n                \"updated\": 1699538456680,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/nm60501283\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nm60501283.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 52,\n                \"net\": \"nm\",\n                \"code\": \"60501283\",\n                \"ids\": \",nm60501283,\",\n                \"sources\": \",nm,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 10,\n                \"dmin\": 0.4476,\n                \"rms\": 0.27,\n                \"gap\": 90,\n                \"magType\": \"md\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.8 - 6 km SSW of Perryville, Arkansas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -92.8431666666667,\n                    34.952,\n                    8.02\n                ]\n            },\n            \"id\": \"nm60501283\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.1,\n                \"place\": \"54 km NW of Toyah, Texas\",\n                \"time\": 1699506225157,\n                \"updated\": 1699508490429,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vylc\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vylc.geojson\",\n                \"felt\": 1,\n                \"cdi\": 2.7,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 68,\n                \"net\": \"tx\",\n                \"code\": \"2023vylc\",\n                \"ids\": \",tx2023vylc,\",\n                \"sources\": \",tx,\",\n                \"types\": \",dyfi,origin,phase-data,\",\n                \"nst\": 18,\n                \"dmin\": 0.1,\n                \"rms\": 0.2,\n                \"gap\": 69,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.1 - 54 km NW of Toyah, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.191,\n                    31.663,\n                    4.4283\n                ]\n            },\n            \"id\": \"tx2023vylc\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.7,\n                \"place\": \"54 km NW of Toyah, Texas\",\n                \"time\": 1699505900866,\n                \"updated\": 1699523729091,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyku\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyku.geojson\",\n                \"felt\": 1,\n                \"cdi\": 4.2,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 113,\n                \"net\": \"tx\",\n                \"code\": \"2023vyku\",\n                \"ids\": \",us7000l9s5,tx2023vyku,\",\n                \"sources\": \",us,tx,\",\n                \"types\": \",dyfi,origin,phase-data,\",\n                \"nst\": 19,\n                \"dmin\": 0.1,\n                \"rms\": 0.2,\n                \"gap\": 58,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.7 - 54 km NW of Toyah, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.184,\n                    31.669,\n                    6.1316\n                ]\n            },\n            \"id\": \"tx2023vyku\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.39,\n                \"place\": \"14 km SSE of Gu\\u00e1nica, Puerto Rico\",\n                \"time\": 1699505814970,\n                \"updated\": 1699506316610,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/pr71431113\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/pr71431113.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 88,\n                \"net\": \"pr\",\n                \"code\": \"71431113\",\n                \"ids\": \",pr71431113,\",\n                \"sources\": \",pr,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 7,\n                \"dmin\": 0.1231,\n                \"rms\": 0.08,\n                \"gap\": 251,\n                \"magType\": \"md\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.4 - 14 km SSE of Gu\\u00e1nica, Puerto Rico\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -66.853,\n                    17.8538333333333,\n                    7.52\n                ]\n            },\n            \"id\": \"pr71431113\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.38,\n                \"place\": \"3 km NNW of Cholame, CA\",\n                \"time\": 1699505013500,\n                \"updated\": 1699514591800,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960286\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960286.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 29,\n                \"net\": \"nc\",\n                \"code\": \"73960286\",\n                \"ids\": \",nc73960286,\",\n                \"sources\": \",nc,\",\n                \"types\": \",nearby-cities,origin,phase-data,scitech-link,\",\n                \"nst\": 18,\n                \"dmin\": 0.04815,\n                \"rms\": 0.06,\n                \"gap\": 143,\n                \"magType\": \"md\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.4 - 3 km NNW of Cholame, CA\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -120.3041687,\n                    35.7498322,\n                    1.75\n                ]\n            },\n            \"id\": \"nc73960286\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.1,\n                \"place\": \"59 km W of Nanwalek, Alaska\",\n                \"time\": 1699504730584,\n                \"updated\": 1699504847928,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/ak023edptm3m\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023edptm3m.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 68,\n                \"net\": \"ak\",\n                \"code\": \"023edptm3m\",\n                \"ids\": \",ak023edptm3m,\",\n                \"sources\": \",ak,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": null,\n                \"dmin\": null,\n                \"rms\": 0.13,\n                \"gap\": null,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.1 - 59 km W of Nanwalek, Alaska\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -152.9681,\n                    59.3643,\n                    76.7\n                ]\n            },\n            \"id\": \"ak023edptm3m\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.9,\n                \"place\": \"4 km NNW of Meadow Lakes, Alaska\",\n                \"time\": 1699503804711,\n                \"updated\": 1699503913105,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/ak023edpqcd3\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023edpqcd3.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 56,\n                \"net\": \"ak\",\n                \"code\": \"023edpqcd3\",\n                \"ids\": \",ak023edpqcd3,\",\n                \"sources\": \",ak,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": null,\n                \"dmin\": null,\n                \"rms\": 0.68,\n                \"gap\": null,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.9 - 4 km NNW of Meadow Lakes, Alaska\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -149.6195,\n                    61.6654,\n                    35.7\n                ]\n            },\n            \"id\": \"ak023edpqcd3\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 4.3,\n                \"place\": \"90 km W of San Antonio de los Cobres, Argentina\",\n                \"time\": 1699503203126,\n                \"updated\": 1699504680040,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9rl\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9rl.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 284,\n                \"net\": \"us\",\n                \"code\": \"7000l9rl\",\n                \"ids\": \",us7000l9rl,\",\n                \"sources\": \",us,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 33,\n                \"dmin\": 1.645,\n                \"rms\": 0.74,\n                \"gap\": 70,\n                \"magType\": \"mb\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 4.3 - 90 km W of San Antonio de los Cobres, Argentina\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -67.2001,\n                    -24.3375,\n                    183.697\n                ]\n            },\n            \"id\": \"us7000l9rl\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.1,\n                \"place\": \"39 km W of Mentone, Texas\",\n                \"time\": 1699503150555,\n                \"updated\": 1699532610825,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyji\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyji.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 68,\n                \"net\": \"tx\",\n                \"code\": \"2023vyji\",\n                \"ids\": \",tx2023vyji,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 20,\n                \"dmin\": 0,\n                \"rms\": 0.2,\n                \"gap\": 58,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.1 - 39 km W of Mentone, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.01,\n                    31.636,\n                    6.0159\n                ]\n            },\n            \"id\": \"tx2023vyji\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.7,\n                \"place\": \"38 km WSW of Mentone, Texas\",\n                \"time\": 1699501293390,\n                \"updated\": 1699504404061,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyil\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyil.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 44,\n                \"net\": \"tx\",\n                \"code\": \"2023vyil\",\n                \"ids\": \",tx2023vyil,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 15,\n                \"dmin\": 0,\n                \"rms\": 0.2,\n                \"gap\": 82,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.7 - 38 km WSW of Mentone, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -103.996,\n                    31.629,\n                    5.9821\n                ]\n            },\n            \"id\": \"tx2023vyil\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.4,\n                \"place\": \"37 km WNW of Beluga, Alaska\",\n                \"time\": 1699500850632,\n                \"updated\": 1699500988356,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/ak023edp78jr\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023edp78jr.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 30,\n                \"net\": \"ak\",\n                \"code\": \"023edp78jr\",\n                \"ids\": \",ak023edp78jr,\",\n                \"sources\": \",ak,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": null,\n                \"dmin\": null,\n                \"rms\": 0.66,\n                \"gap\": null,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.4 - 37 km WNW of Beluga, Alaska\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -151.7181,\n                    61.2871,\n                    74.4\n                ]\n            },\n            \"id\": \"ak023edp78jr\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.19,\n                \"place\": \"28 km SSW of Gustine, CA\",\n                \"time\": 1699500656950,\n                \"updated\": 1699544477609,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960281\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960281.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 22,\n                \"net\": \"nc\",\n                \"code\": \"73960281\",\n                \"ids\": \",nc73960281,\",\n                \"sources\": \",nc,\",\n                \"types\": \",nearby-cities,origin,phase-data,scitech-link,\",\n                \"nst\": 34,\n                \"dmin\": 0.05094,\n                \"rms\": 0.13,\n                \"gap\": 75,\n                \"magType\": \"md\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.2 - 28 km SSW of Gustine, CA\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -121.1673333,\n                    37.0475,\n                    4.65\n                ]\n            },\n            \"id\": \"nc73960281\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.8,\n                \"place\": \"western Texas\",\n                \"time\": 1699500649254,\n                \"updated\": 1699502370637,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyib\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyib.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 50,\n                \"net\": \"tx\",\n                \"code\": \"2023vyib\",\n                \"ids\": \",tx2023vyib,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 20,\n                \"dmin\": 0,\n                \"rms\": 0.2,\n                \"gap\": 47,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.8 - western Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -103.999,\n                    31.638,\n                    4.9107\n                ]\n            },\n            \"id\": \"tx2023vyib\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.6,\n                \"place\": \"western Texas\",\n                \"time\": 1699500311429,\n                \"updated\": 1699501975529,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyhw\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyhw.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 39,\n                \"net\": \"tx\",\n                \"code\": \"2023vyhw\",\n                \"ids\": \",tx2023vyhw,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 15,\n                \"dmin\": 0,\n                \"rms\": 0.4,\n                \"gap\": 79,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.6 - western Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -103.975,\n                    31.619,\n                    5.3082\n                ]\n            },\n            \"id\": \"tx2023vyhw\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.7,\n                \"place\": \"western Texas\",\n                \"time\": 1699500055395,\n                \"updated\": 1699549945404,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyhs\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyhs.geojson\",\n                \"felt\": 1,\n                \"cdi\": 2.9,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 45,\n                \"net\": \"tx\",\n                \"code\": \"2023vyhs\",\n                \"ids\": \",tx2023vyhs,\",\n                \"sources\": \",tx,\",\n                \"types\": \",dyfi,origin,phase-data,\",\n                \"nst\": 16,\n                \"dmin\": 0,\n                \"rms\": 0.4,\n                \"gap\": 45,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.7 - western Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104,\n                    31.642,\n                    4.9693\n                ]\n            },\n            \"id\": \"tx2023vyhs\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.6,\n                \"place\": \"53 km NW of Toyah, Texas\",\n                \"time\": 1699499571990,\n                \"updated\": 1699501278847,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyhl\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyhl.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 39,\n                \"net\": \"tx\",\n                \"code\": \"2023vyhl\",\n                \"ids\": \",tx2023vyhl,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 14,\n                \"dmin\": 0.1,\n                \"rms\": 0.3,\n                \"gap\": 73,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.6 - 53 km NW of Toyah, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.194,\n                    31.648,\n                    4.5506\n                ]\n            },\n            \"id\": \"tx2023vyhl\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.17,\n                \"place\": \"7 km WSW of Hurricane, Utah\",\n                \"time\": 1699499235480,\n                \"updated\": 1699548678370,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/uu60553882\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/uu60553882.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 21,\n                \"net\": \"uu\",\n                \"code\": \"60553882\",\n                \"ids\": \",uu60553882,\",\n                \"sources\": \",uu,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 10,\n                \"dmin\": 0.1127,\n                \"rms\": 0.13,\n                \"gap\": 114,\n                \"magType\": \"md\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.2 - 7 km WSW of Hurricane, Utah\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -113.3703333,\n                    37.1505,\n                    15.4\n                ]\n            },\n            \"id\": \"uu60553882\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.49,\n                \"place\": \"5 km NE of Westmorland, CA\",\n                \"time\": 1699498781070,\n                \"updated\": 1699540892280,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/ci39710602\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ci39710602.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 34,\n                \"net\": \"ci\",\n                \"code\": \"39710602\",\n                \"ids\": \",ci39710602,\",\n                \"sources\": \",ci,\",\n                \"types\": \",nearby-cities,origin,phase-data,scitech-link,\",\n                \"nst\": 22,\n                \"dmin\": 0.02511,\n                \"rms\": 0.16,\n                \"gap\": 58,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.5 - 5 km NE of Westmorland, CA\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -115.5763333,\n                    33.063,\n                    7.37\n                ]\n            },\n            \"id\": \"ci39710602\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.5,\n                \"place\": \"51 km SSE of Whites City, New Mexico\",\n                \"time\": 1699498688732,\n                \"updated\": 1699500184504,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vygv\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vygv.geojson\",\n                \"felt\": 1,\n                \"cdi\": 0,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 35,\n                \"net\": \"tx\",\n                \"code\": \"2023vygv\",\n                \"ids\": \",tx2023vygv,\",\n                \"sources\": \",tx,\",\n                \"types\": \",dyfi,origin,phase-data,\",\n                \"nst\": 7,\n                \"dmin\": 0.1,\n                \"rms\": 0.7,\n                \"gap\": 105,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.5 - 51 km SSE of Whites City, New Mexico\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.228,\n                    31.725,\n                    19.2719\n                ]\n            },\n            \"id\": \"tx2023vygv\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.1,\n                \"place\": \"24 km ESE of Susitna North, Alaska\",\n                \"time\": 1699498679104,\n                \"updated\": 1699498759198,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/ak023edoqu5i\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023edoqu5i.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 68,\n                \"net\": \"ak\",\n                \"code\": \"023edoqu5i\",\n                \"ids\": \",ak023edoqu5i,\",\n                \"sources\": \",ak,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": null,\n                \"dmin\": null,\n                \"rms\": 0.23,\n                \"gap\": null,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.1 - 24 km ESE of Susitna North, Alaska\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -149.4329,\n                    62.0699,\n                    38\n                ]\n            },\n            \"id\": \"ak023edoqu5i\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 4.5,\n                \"place\": \"4 km NNE of Vogar, Iceland\",\n                \"time\": 1699498622894,\n                \"updated\": 1699503193040,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9ri\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9ri.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 312,\n                \"net\": \"us\",\n                \"code\": \"7000l9ri\",\n                \"ids\": \",us7000l9ri,\",\n                \"sources\": \",us,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 21,\n                \"dmin\": 0.856,\n                \"rms\": 1.26,\n                \"gap\": 101,\n                \"magType\": \"mb\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 4.5 - 4 km NNE of Vogar, Iceland\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -22.3655,\n                    64.0233,\n                    10\n                ]\n            },\n            \"id\": \"us7000l9ri\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.51,\n                \"place\": \"5 km NE of Westmorland, CA\",\n                \"time\": 1699498602190,\n                \"updated\": 1699540032082,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/ci39710594\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ci39710594.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 35,\n                \"net\": \"ci\",\n                \"code\": \"39710594\",\n                \"ids\": \",ci39710594,\",\n                \"sources\": \",ci,\",\n                \"types\": \",nearby-cities,origin,phase-data,scitech-link,\",\n                \"nst\": 21,\n                \"dmin\": 0.0281,\n                \"rms\": 0.19,\n                \"gap\": 58,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.5 - 5 km NE of Westmorland, CA\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -115.575,\n                    33.0658333,\n                    10.06\n                ]\n            },\n            \"id\": \"ci39710594\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 4.3,\n                \"place\": \"7 km E of Vogar, Iceland\",\n                \"time\": 1699498596530,\n                \"updated\": 1699504749040,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9rj\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9rj.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 284,\n                \"net\": \"us\",\n                \"code\": \"7000l9rj\",\n                \"ids\": \",us7000l9rj,\",\n                \"sources\": \",us,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 15,\n                \"dmin\": 0.867,\n                \"rms\": 0.98,\n                \"gap\": 121,\n                \"magType\": \"mb\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 4.3 - 7 km E of Vogar, Iceland\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -22.2277,\n                    63.9775,\n                    11.878\n                ]\n            },\n            \"id\": \"us7000l9rj\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.4,\n                \"place\": \"56 km S of Whites City, New Mexico\",\n                \"time\": 1699497204689,\n                \"updated\": 1699535727681,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vygb\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vygb.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 89,\n                \"net\": \"tx\",\n                \"code\": \"2023vygb\",\n                \"ids\": \",tx2023vygb,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 17,\n                \"dmin\": 0,\n                \"rms\": 0.2,\n                \"gap\": 67,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.4 - 56 km S of Whites City, New Mexico\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.31,\n                    31.67,\n                    6.1316\n                ]\n            },\n            \"id\": \"tx2023vygb\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 4.6,\n                \"place\": \"Turkey-Syria border region\",\n                \"time\": 1699497134623,\n                \"updated\": 1699539447281,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9r5\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9r5.geojson\",\n                \"felt\": 8,\n                \"cdi\": 6.6,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 331,\n                \"net\": \"us\",\n                \"code\": \"7000l9r5\",\n                \"ids\": \",us7000l9r5,\",\n                \"sources\": \",us,\",\n                \"types\": \",dyfi,moment-tensor,origin,phase-data,\",\n                \"nst\": 93,\n                \"dmin\": 0.836,\n                \"rms\": 0.98,\n                \"gap\": 51,\n                \"magType\": \"mwr\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 4.6 - Turkey-Syria border region\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    36.4369,\n                    36.6095,\n                    10\n                ]\n            },\n            \"id\": \"us7000l9r5\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.8,\n                \"place\": \"37 km WSW of Mentone, Texas\",\n                \"time\": 1699496816186,\n                \"updated\": 1699499146641,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyfu\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyfu.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 50,\n                \"net\": \"tx\",\n                \"code\": \"2023vyfu\",\n                \"ids\": \",tx2023vyfu,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 15,\n                \"dmin\": 0,\n                \"rms\": 0.2,\n                \"gap\": 61,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.8 - 37 km WSW of Mentone, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -103.978,\n                    31.619,\n                    5.7893\n                ]\n            },\n            \"id\": \"tx2023vyfu\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.1,\n                \"place\": \"western Texas\",\n                \"time\": 1699496702580,\n                \"updated\": 1699499084885,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyfw\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyfw.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 68,\n                \"net\": \"tx\",\n                \"code\": \"2023vyfw\",\n                \"ids\": \",tx2023vyfw,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 12,\n                \"dmin\": 0,\n                \"rms\": 0.3,\n                \"gap\": 119,\n                \"magType\": \"mlv\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.1 - western Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -103.988,\n                    31.647,\n                    4.7845\n                ]\n            },\n            \"id\": \"tx2023vyfw\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.6,\n                \"place\": \"39 km W of Mentone, Texas\",\n                \"time\": 1699496398739,\n                \"updated\": 1699498087945,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyfo\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyfo.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 39,\n                \"net\": \"tx\",\n                \"code\": \"2023vyfo\",\n                \"ids\": \",tx2023vyfo,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 21,\n                \"dmin\": 0,\n                \"rms\": 0.4,\n                \"gap\": 41,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.6 - 39 km W of Mentone, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.012,\n                    31.64,\n                    4.7964\n                ]\n            },\n            \"id\": \"tx2023vyfo\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.8,\n                \"place\": \"36 km W of Mentone, Texas\",\n                \"time\": 1699494609928,\n                \"updated\": 1699497012834,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyer\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyer.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 50,\n                \"net\": \"tx\",\n                \"code\": \"2023vyer\",\n                \"ids\": \",tx2023vyer,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 11,\n                \"dmin\": 0,\n                \"rms\": 0.5,\n                \"gap\": 100,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.8 - 36 km W of Mentone, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -103.981,\n                    31.642,\n                    7.9125\n                ]\n            },\n            \"id\": \"tx2023vyer\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.7,\n                \"place\": \"52 km NW of Toyah, Texas\",\n                \"time\": 1699493942031,\n                \"updated\": 1699496119585,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyef\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyef.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 44,\n                \"net\": \"tx\",\n                \"code\": \"2023vyef\",\n                \"ids\": \",tx2023vyef,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 18,\n                \"dmin\": 0.1,\n                \"rms\": 0.2,\n                \"gap\": 72,\n                \"magType\": \"mlv\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.7 - 52 km NW of Toyah, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.249,\n                    31.588,\n                    4.0227\n                ]\n            },\n            \"id\": \"tx2023vyef\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.64,\n                \"place\": \"10 km NNE of Brooktrails, CA\",\n                \"time\": 1699493898830,\n                \"updated\": 1699548311978,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960246\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960246.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 41,\n                \"net\": \"nc\",\n                \"code\": \"73960246\",\n                \"ids\": \",nc73960246,\",\n                \"sources\": \",nc,\",\n                \"types\": \",nearby-cities,origin,phase-data,scitech-link,\",\n                \"nst\": 21,\n                \"dmin\": 0.05343,\n                \"rms\": 0.09,\n                \"gap\": 55,\n                \"magType\": \"md\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.6 - 10 km NNE of Brooktrails, CA\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -123.3298333,\n                    39.5221667,\n                    9.07\n                ]\n            },\n            \"id\": \"nc73960246\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 4.5,\n                \"place\": \"5 km NNE of Vogar, Iceland\",\n                \"time\": 1699493053880,\n                \"updated\": 1699495393897,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9r3\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9r3.geojson\",\n                \"felt\": 2,\n                \"cdi\": 3.4,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 312,\n                \"net\": \"us\",\n                \"code\": \"7000l9r3\",\n                \"ids\": \",us7000l9r3,\",\n                \"sources\": \",us,\",\n                \"types\": \",dyfi,origin,phase-data,\",\n                \"nst\": 41,\n                \"dmin\": 0.845,\n                \"rms\": 0.87,\n                \"gap\": 121,\n                \"magType\": \"mb\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 4.5 - 5 km NNE of Vogar, Iceland\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -22.3378,\n                    64.0286,\n                    4.486\n                ]\n            },\n            \"id\": \"us7000l9r3\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 4.9,\n                \"place\": \"147 km E of Hihifo, Tonga\",\n                \"time\": 1699491143396,\n                \"updated\": 1699492562040,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9r2\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9r2.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 369,\n                \"net\": \"us\",\n                \"code\": \"7000l9r2\",\n                \"ids\": \",us7000l9r2,\",\n                \"sources\": \",us,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 45,\n                \"dmin\": 2.359,\n                \"rms\": 1.31,\n                \"gap\": 84,\n                \"magType\": \"mb\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 4.9 - 147 km E of Hihifo, Tonga\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -172.4349,\n                    -15.7715,\n                    10\n                ]\n            },\n            \"id\": \"us7000l9r2\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 4.6,\n                \"place\": \"7 km SE of Vogar, Iceland\",\n                \"time\": 1699490768244,\n                \"updated\": 1699499264426,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9r1\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9r1.geojson\",\n                \"felt\": 7,\n                \"cdi\": 3.8,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 328,\n                \"net\": \"us\",\n                \"code\": \"7000l9r1\",\n                \"ids\": \",us7000l9r1,\",\n                \"sources\": \",us,\",\n                \"types\": \",dyfi,origin,phase-data,\",\n                \"nst\": 41,\n                \"dmin\": 0.915,\n                \"rms\": 0.79,\n                \"gap\": 116,\n                \"magType\": \"mb\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 4.6 - 7 km SE of Vogar, Iceland\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -22.2736,\n                    63.9337,\n                    9.355\n                ]\n            },\n            \"id\": \"us7000l9r1\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.07,\n                \"place\": \"13 km SE of Ocotillo, CA\",\n                \"time\": 1699490748990,\n                \"updated\": 1699492821590,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/ci39710570\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ci39710570.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 66,\n                \"net\": \"ci\",\n                \"code\": \"39710570\",\n                \"ids\": \",ci39710570,\",\n                \"sources\": \",ci,\",\n                \"types\": \",focal-mechanism,nearby-cities,origin,phase-data,scitech-link,\",\n                \"nst\": 43,\n                \"dmin\": 0.0268,\n                \"rms\": 0.17,\n                \"gap\": 45,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.1 - 13 km SE of Ocotillo, CA\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -115.8911667,\n                    32.653,\n                    10.2\n                ]\n            },\n            \"id\": \"ci39710570\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.65,\n                \"place\": \"21 km SW of Stella, Puerto Rico\",\n                \"time\": 1699489974180,\n                \"updated\": 1699490637760,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/pr71431108\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/pr71431108.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 108,\n                \"net\": \"pr\",\n                \"code\": \"71431108\",\n                \"ids\": \",pr71431108,\",\n                \"sources\": \",pr,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 8,\n                \"dmin\": 0.2283,\n                \"rms\": 0.25,\n                \"gap\": 218,\n                \"magType\": \"md\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.7 - 21 km SW of Stella, Puerto Rico\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -67.384,\n                    18.1736666666667,\n                    23.64\n                ]\n            },\n            \"id\": \"pr71431108\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.27,\n                \"place\": \"12 km ESE of Middletown, CA\",\n                \"time\": 1699489809270,\n                \"updated\": 1699505353021,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960236\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960236.geojson\",\n                \"felt\": 2,\n                \"cdi\": 2.2,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 80,\n                \"net\": \"nc\",\n                \"code\": \"73960236\",\n                \"ids\": \",nc73960236,\",\n                \"sources\": \",nc,\",\n                \"types\": \",dyfi,focal-mechanism,nearby-cities,origin,phase-data,scitech-link,\",\n                \"nst\": 33,\n                \"dmin\": 0.0954,\n                \"rms\": 0.1,\n                \"gap\": 57,\n                \"magType\": \"md\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.3 - 12 km ESE of Middletown, CA\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -122.4948349,\n                    38.6911659,\n                    7.02\n                ]\n            },\n            \"id\": \"nc73960236\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.5,\n                \"place\": \"5 km NNW of Boron, CA\",\n                \"time\": 1699488961290,\n                \"updated\": 1699492315884,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/ci39710554\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ci39710554.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 35,\n                \"net\": \"ci\",\n                \"code\": \"39710554\",\n                \"ids\": \",ci39710554,\",\n                \"sources\": \",ci,\",\n                \"types\": \",nearby-cities,origin,phase-data,scitech-link,\",\n                \"nst\": 15,\n                \"dmin\": 0.1103,\n                \"rms\": 0.2,\n                \"gap\": 114,\n                \"magType\": \"mh\",\n                \"type\": \"quarry blast\",\n                \"title\": \"M 1.5 Quarry Blast - 5 km NNW of Boron, CA\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -117.6615,\n                    35.0411667,\n                    -0.83\n                ]\n            },\n            \"id\": \"ci39710554\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 4.3,\n                \"place\": \"11 km E of Vogar, Iceland\",\n                \"time\": 1699488795375,\n                \"updated\": 1699491823617,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9qw\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9qw.geojson\",\n                \"felt\": 0,\n                \"cdi\": 1,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 284,\n                \"net\": \"us\",\n                \"code\": \"7000l9qw\",\n                \"ids\": \",us7000l9qw,\",\n                \"sources\": \",us,\",\n                \"types\": \",dyfi,origin,phase-data,\",\n                \"nst\": 23,\n                \"dmin\": 0.847,\n                \"rms\": 0.52,\n                \"gap\": 121,\n                \"magType\": \"mb\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 4.3 - 11 km E of Vogar, Iceland\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -22.1579,\n                    63.9843,\n                    10.229\n                ]\n            },\n            \"id\": \"us7000l9qw\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.7,\n                \"place\": \"44 km NW of Toyah, Texas\",\n                \"time\": 1699487419075,\n                \"updated\": 1699489336152,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyaq\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyaq.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 44,\n                \"net\": \"tx\",\n                \"code\": \"2023vyaq\",\n                \"ids\": \",tx2023vyaq,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 18,\n                \"dmin\": 0,\n                \"rms\": 0.3,\n                \"gap\": 72,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.7 - 44 km NW of Toyah, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.1,\n                    31.615,\n                    4.4847\n                ]\n            },\n            \"id\": \"tx2023vyaq\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 4.7,\n                \"place\": \"26 km SSE of San Pedro de Atacama, Chile\",\n                \"time\": 1699487150246,\n                \"updated\": 1699487860040,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9qs\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9qs.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 340,\n                \"net\": \"us\",\n                \"code\": \"7000l9qs\",\n                \"ids\": \",us7000l9qs,\",\n                \"sources\": \",us,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 39,\n                \"dmin\": 0.192,\n                \"rms\": 0.66,\n                \"gap\": 83,\n                \"magType\": \"mb\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 4.7 - 26 km SSE of San Pedro de Atacama, Chile\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -68.1174,\n                    -23.1364,\n                    134.8\n                ]\n            },\n            \"id\": \"us7000l9qs\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.3599999,\n                \"place\": \"31 km SW of Laup\\u0101hoehoe, Hawaii\",\n                \"time\": 1699487085830,\n                \"updated\": 1699487286490,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/hv73641997\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/hv73641997.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 86,\n                \"net\": \"hv\",\n                \"code\": \"73641997\",\n                \"ids\": \",hv73641997,\",\n                \"sources\": \",hv,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 41,\n                \"dmin\": null,\n                \"rms\": 0.140000001,\n                \"gap\": 139,\n                \"magType\": \"md\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.4 - 31 km SW of Laup\\u0101hoehoe, Hawaii\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -155.408996582031,\n                    19.7481670379639,\n                    21.3500003814697\n                ]\n            },\n            \"id\": \"hv73641997\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.7,\n                \"place\": \"39 km W of Mentone, Texas\",\n                \"time\": 1699486993997,\n                \"updated\": 1699489469028,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyal\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyal.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 44,\n                \"net\": \"tx\",\n                \"code\": \"2023vyal\",\n                \"ids\": \",tx2023vyal,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 8,\n                \"dmin\": 0,\n                \"rms\": 0.1,\n                \"gap\": 103,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.7 - 39 km W of Mentone, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.008,\n                    31.647,\n                    4.9476\n                ]\n            },\n            \"id\": \"tx2023vyal\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.1,\n                \"place\": \"western Texas\",\n                \"time\": 1699486945136,\n                \"updated\": 1699489316045,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyaj\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyaj.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 68,\n                \"net\": \"tx\",\n                \"code\": \"2023vyaj\",\n                \"ids\": \",tx2023vyaj,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 17,\n                \"dmin\": 0,\n                \"rms\": 0.5,\n                \"gap\": 58,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.1 - western Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.111,\n                    31.619,\n                    4.3029\n                ]\n            },\n            \"id\": \"tx2023vyaj\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.15,\n                \"place\": \"33 km NW of Stanley, Idaho\",\n                \"time\": 1699486598400,\n                \"updated\": 1699540372660,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/mb90032408\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/mb90032408.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 71,\n                \"net\": \"mb\",\n                \"code\": \"90032408\",\n                \"ids\": \",mb90032408,\",\n                \"sources\": \",mb,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 12,\n                \"dmin\": 0.4305,\n                \"rms\": 0.09,\n                \"gap\": 73,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.2 - 33 km NW of Stanley, Idaho\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -115.204666666667,\n                    44.4515,\n                    5.38\n                ]\n            },\n            \"id\": \"mb90032408\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.8,\n                \"place\": \"74 km NNW of Ambler, Alaska\",\n                \"time\": 1699486079389,\n                \"updated\": 1699487156040,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9qh\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9qh.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 121,\n                \"net\": \"us\",\n                \"code\": \"7000l9qh\",\n                \"ids\": \",us7000l9qh,\",\n                \"sources\": \",us,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 25,\n                \"dmin\": 0.54,\n                \"rms\": 0.59,\n                \"gap\": 60,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.8 - 74 km NNW of Ambler, Alaska\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -158.4688,\n                    67.7142,\n                    5.507\n                ]\n            },\n            \"id\": \"us7000l9qh\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.65,\n                \"place\": \"4 km ESE of Adjuntas, Puerto Rico\",\n                \"time\": 1699486011760,\n                \"updated\": 1699487127280,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/pr71431093\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/pr71431093.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 108,\n                \"net\": \"pr\",\n                \"code\": \"71431093\",\n                \"ids\": \",pr71431093,\",\n                \"sources\": \",pr,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 21,\n                \"dmin\": 0.1076,\n                \"rms\": 0.14,\n                \"gap\": 82,\n                \"magType\": \"md\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.7 - 4 km ESE of Adjuntas, Puerto Rico\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -66.6828333333333,\n                    18.1506666666667,\n                    18.7\n                ]\n            },\n            \"id\": \"pr71431093\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.55,\n                \"place\": \"13 km SSW of Searles Valley, CA\",\n                \"time\": 1699484386020,\n                \"updated\": 1699492148544,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/ci39710546\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ci39710546.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 37,\n                \"net\": \"ci\",\n                \"code\": \"39710546\",\n                \"ids\": \",ci39710546,\",\n                \"sources\": \",ci,\",\n                \"types\": \",nearby-cities,origin,phase-data,scitech-link,\",\n                \"nst\": 33,\n                \"dmin\": 0.01922,\n                \"rms\": 0.15,\n                \"gap\": 67,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.6 - 13 km SSW of Searles Valley, CA\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -117.4393333,\n                    35.6506667,\n                    9.52\n                ]\n            },\n            \"id\": \"ci39710546\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 4.4,\n                \"place\": \"93 km WNW of Yomitan, Japan\",\n                \"time\": 1699484033548,\n                \"updated\": 1699485685040,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9qc\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9qc.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 298,\n                \"net\": \"us\",\n                \"code\": \"7000l9qc\",\n                \"ids\": \",us7000l9qc,\",\n                \"sources\": \",us,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 31,\n                \"dmin\": 1.31,\n                \"rms\": 1.19,\n                \"gap\": 127,\n                \"magType\": \"mb\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 4.4 - 93 km WNW of Yomitan, Japan\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    126.8343,\n                    26.586,\n                    132.919\n                ]\n            },\n            \"id\": \"us7000l9qc\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.7,\n                \"place\": \"30 km ESE of Susitna North, Alaska\",\n                \"time\": 1699483826551,\n                \"updated\": 1699483962656,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/ak023ecd2l8f\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023ecd2l8f.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 44,\n                \"net\": \"ak\",\n                \"code\": \"023ecd2l8f\",\n                \"ids\": \",ak023ecd2l8f,\",\n                \"sources\": \",ak,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": null,\n                \"dmin\": null,\n                \"rms\": 1.07,\n                \"gap\": null,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.7 - 30 km ESE of Susitna North, Alaska\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -149.2955,\n                    62.0755,\n                    19.2\n                ]\n            },\n            \"id\": \"ak023ecd2l8f\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.38,\n                \"place\": \"12 km N of Kenefic, Oklahoma\",\n                \"time\": 1699483416219,\n                \"updated\": 1699537885699,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/ok2023vxyl\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ok2023vxyl.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 29,\n                \"net\": \"ok\",\n                \"code\": \"2023vxyl\",\n                \"ids\": \",ok2023vxyl,\",\n                \"sources\": \",ok,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 46,\n                \"dmin\": 0.6541716292,\n                \"rms\": 0.43,\n                \"gap\": 173,\n                \"magType\": \"ml\",\n                \"type\": \"quarry blast\",\n                \"title\": \"M 1.4 Quarry Blast - 12 km N of Kenefic, Oklahoma\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -96.38816667,\n                    34.262,\n                    0\n                ]\n            },\n            \"id\": \"ok2023vxyl\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.4,\n                \"place\": \"54 km NW of Toyah, Texas\",\n                \"time\": 1699483288391,\n                \"updated\": 1699526957966,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vxyk\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vxyk.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 89,\n                \"net\": \"tx\",\n                \"code\": \"2023vxyk\",\n                \"ids\": \",tx2023vxyk,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 48,\n                \"dmin\": 0.1,\n                \"rms\": 0.2,\n                \"gap\": 61,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.4 - 54 km NW of Toyah, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.247,\n                    31.608,\n                    7.4684\n                ]\n            },\n            \"id\": \"tx2023vxyk\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.12,\n                \"place\": \"8 km N of Yucca Valley, CA\",\n                \"time\": 1699482561120,\n                \"updated\": 1699491570627,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/ci39710522\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ci39710522.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 19,\n                \"net\": \"ci\",\n                \"code\": \"39710522\",\n                \"ids\": \",ci39710522,\",\n                \"sources\": \",ci,\",\n                \"types\": \",nearby-cities,origin,phase-data,scitech-link,\",\n                \"nst\": 14,\n                \"dmin\": 0.03184,\n                \"rms\": 0.09,\n                \"gap\": 64,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.1 - 8 km N of Yucca Valley, CA\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -116.4395,\n                    34.1815,\n                    10.39\n                ]\n            },\n            \"id\": \"ci39710522\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 0.95,\n                \"place\": \"11 km N of Borrego Springs, CA\",\n                \"time\": 1699481849860,\n                \"updated\": 1699490908954,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/ci39710514\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ci39710514.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 14,\n                \"net\": \"ci\",\n                \"code\": \"39710514\",\n                \"ids\": \",ci39710514,\",\n                \"sources\": \",ci,\",\n                \"types\": \",nearby-cities,origin,phase-data,scitech-link,\",\n                \"nst\": 37,\n                \"dmin\": 0.0266,\n                \"rms\": 0.18,\n                \"gap\": 125,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.0 - 11 km N of Borrego Springs, CA\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -116.3616667,\n                    33.353,\n                    13.8\n                ]\n            },\n            \"id\": \"ci39710514\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.6500000000000001,\n                \"place\": \"14 km S of Princeton, Canada\",\n                \"time\": 1699481611520,\n                \"updated\": 1699508363230,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/uw61971376\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/uw61971376.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 42,\n                \"net\": \"uw\",\n                \"code\": \"61971376\",\n                \"ids\": \",uw61971376,\",\n                \"sources\": \",uw,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 5,\n                \"dmin\": 0.5619,\n                \"rms\": 0.28,\n                \"gap\": 208,\n                \"magType\": \"ml\",\n                \"type\": \"explosion\",\n                \"title\": \"M 1.7 Explosion - 14 km S of Princeton, Canada\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -120.48416666666667,\n                    49.324333333333335,\n                    -0.62\n                ]\n            },\n            \"id\": \"uw61971376\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.45,\n                \"place\": \"5 km S of Salinas, Puerto Rico\",\n                \"time\": 1699481361280,\n                \"updated\": 1699482655290,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/pr71431078\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/pr71431078.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 92,\n                \"net\": \"pr\",\n                \"code\": \"71431078\",\n                \"ids\": \",pr71431078,\",\n                \"sources\": \",pr,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 17,\n                \"dmin\": 0.1121,\n                \"rms\": 0.23,\n                \"gap\": 196,\n                \"magType\": \"md\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.5 - 5 km S of Salinas, Puerto Rico\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -66.2923333333333,\n                    17.9306666666667,\n                    9.21\n                ]\n            },\n            \"id\": \"pr71431078\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.07,\n                \"place\": \"8 km WNW of Cobb, CA\",\n                \"time\": 1699480981860,\n                \"updated\": 1699482733799,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960196\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960196.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 18,\n                \"net\": \"nc\",\n                \"code\": \"73960196\",\n                \"ids\": \",nc73960196,\",\n                \"sources\": \",nc,\",\n                \"types\": \",nearby-cities,origin,phase-data,scitech-link,\",\n                \"nst\": 19,\n                \"dmin\": 0.01152,\n                \"rms\": 0.03,\n                \"gap\": 55,\n                \"magType\": \"md\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.1 - 8 km WNW of Cobb, CA\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -122.8146667,\n                    38.8453331,\n                    1.42\n                ]\n            },\n            \"id\": \"nc73960196\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.08,\n                \"place\": \"8 km NW of The Geysers, CA\",\n                \"time\": 1699480666110,\n                \"updated\": 1699482014744,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960191\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960191.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 18,\n                \"net\": \"nc\",\n                \"code\": \"73960191\",\n                \"ids\": \",nc73960191,\",\n                \"sources\": \",nc,\",\n                \"types\": \",nearby-cities,origin,phase-data,scitech-link,\",\n                \"nst\": 22,\n                \"dmin\": 0.009205,\n                \"rms\": 0.02,\n                \"gap\": 49,\n                \"magType\": \"md\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.1 - 8 km NW of The Geysers, CA\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -122.8178329,\n                    38.8303337,\n                    1.9\n                ]\n            },\n            \"id\": \"nc73960191\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.1,\n                \"place\": \"11 km NE of Burney, CA\",\n                \"time\": 1699480568450,\n                \"updated\": 1699492751783,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960186\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960186.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 68,\n                \"net\": \"nc\",\n                \"code\": \"73960186\",\n                \"ids\": \",nc73960186,\",\n                \"sources\": \",nc,\",\n                \"types\": \",nearby-cities,origin,phase-data,scitech-link,\",\n                \"nst\": 13,\n                \"dmin\": 0.1634,\n                \"rms\": 0.16,\n                \"gap\": 79,\n                \"magType\": \"md\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.1 - 11 km NE of Burney, CA\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -121.5709991,\n                    40.9568329,\n                    3.69\n                ]\n            },\n            \"id\": \"nc73960186\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.52,\n                \"place\": \"3 km WSW of Fuig, Puerto Rico\",\n                \"time\": 1699480468220,\n                \"updated\": 1699481189740,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/pr71431058\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/pr71431058.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 98,\n                \"net\": \"pr\",\n                \"code\": \"71431058\",\n                \"ids\": \",pr71431058,\",\n                \"sources\": \",pr,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 7,\n                \"dmin\": 0.06623,\n                \"rms\": 0.1,\n                \"gap\": 180,\n                \"magType\": \"md\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.5 - 3 km WSW of Fuig, Puerto Rico\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -66.9488333333333,\n                    17.974,\n                    12.09\n                ]\n            },\n            \"id\": \"pr71431058\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.9,\n                \"place\": \"44 km NW of Toyah, Texas\",\n                \"time\": 1699479495795,\n                \"updated\": 1699481251658,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vxwi\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vxwi.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 56,\n                \"net\": \"tx\",\n                \"code\": \"2023vxwi\",\n                \"ids\": \",tx2023vxwi,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 20,\n                \"dmin\": 0,\n                \"rms\": 0.4,\n                \"gap\": 56,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.9 - 44 km NW of Toyah, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.104,\n                    31.613,\n                    9.9878\n                ]\n            },\n            \"id\": \"tx2023vxwi\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.9,\n                \"place\": \"35 km W of Mentone, Texas\",\n                \"time\": 1699479472800,\n                \"updated\": 1699480404097,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vxwh\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vxwh.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 56,\n                \"net\": \"tx\",\n                \"code\": \"2023vxwh\",\n                \"ids\": \",tx2023vxwh,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 12,\n                \"dmin\": 0,\n                \"rms\": 0.7,\n                \"gap\": 91,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.9 - 35 km W of Mentone, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -103.971,\n                    31.656,\n                    6.9929\n                ]\n            },\n            \"id\": \"tx2023vxwh\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 3.4,\n                \"place\": \"60 km WNW of Nanwalek, Alaska\",\n                \"time\": 1699478935143,\n                \"updated\": 1699488353682,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/ak023eccchzy\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023eccchzy.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 178,\n                \"net\": \"ak\",\n                \"code\": \"023eccchzy\",\n                \"ids\": \",us7000l9q2,ak023eccchzy,\",\n                \"sources\": \",us,ak,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": null,\n                \"dmin\": null,\n                \"rms\": 0.53,\n                \"gap\": null,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 3.4 - 60 km WNW of Nanwalek, Alaska\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -152.9379,\n                    59.5141,\n                    99.8\n                ]\n            },\n            \"id\": \"ak023eccchzy\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.01,\n                \"place\": \"6 km ENE of Coalinga, CA\",\n                \"time\": 1699478411810,\n                \"updated\": 1699481235844,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960171\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960171.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 16,\n                \"net\": \"nc\",\n                \"code\": \"73960171\",\n                \"ids\": \",nc73960171,\",\n                \"sources\": \",nc,\",\n                \"types\": \",nearby-cities,origin,phase-data,scitech-link,\",\n                \"nst\": 9,\n                \"dmin\": 0.08826,\n                \"rms\": 0.06,\n                \"gap\": 195,\n                \"magType\": \"md\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.0 - 6 km ENE of Coalinga, CA\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -120.3001633,\n                    36.1626663,\n                    12.33\n                ]\n            },\n            \"id\": \"nc73960171\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.7,\n                \"place\": \"4 km NE of Coalinga, CA\",\n                \"time\": 1699478227310,\n                \"updated\": 1699491853696,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960161\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960161.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 44,\n                \"net\": \"nc\",\n                \"code\": \"73960161\",\n                \"ids\": \",nc73960161,\",\n                \"sources\": \",nc,\",\n                \"types\": \",nearby-cities,origin,phase-data,scitech-link,\",\n                \"nst\": 14,\n                \"dmin\": 0.06788,\n                \"rms\": 0.04,\n                \"gap\": 129,\n                \"magType\": \"md\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.7 - 4 km NE of Coalinga, CA\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -120.3261642,\n                    36.1641655,\n                    13.24\n                ]\n            },\n            \"id\": \"nc73960161\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.11,\n                \"place\": \"4 km WSW of Cobb, CA\",\n                \"time\": 1699478193630,\n                \"updated\": 1699490293538,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960156\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960156.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 19,\n                \"net\": \"nc\",\n                \"code\": \"73960156\",\n                \"ids\": \",nc73960156,\",\n                \"sources\": \",nc,\",\n                \"types\": \",nearby-cities,origin,phase-data,scitech-link,\",\n                \"nst\": 15,\n                \"dmin\": 0.01402,\n                \"rms\": 0.03,\n                \"gap\": 109,\n                \"magType\": \"md\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.1 - 4 km WSW of Cobb, CA\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -122.7628326,\n                    38.8146667,\n                    2.13\n                ]\n            },\n            \"id\": \"nc73960156\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.2,\n                \"place\": \"39 km WSW of Mentone, Texas\",\n                \"time\": 1699478006666,\n                \"updated\": 1699537285978,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vxvn\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vxvn.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 74,\n                \"net\": \"tx\",\n                \"code\": \"2023vxvn\",\n                \"ids\": \",tx2023vxvn,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 15,\n                \"dmin\": 0,\n                \"rms\": 0.1,\n                \"gap\": 50,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.2 - 39 km WSW of Mentone, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.011,\n                    31.628,\n                    6.3886\n                ]\n            },\n            \"id\": \"tx2023vxvn\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.8,\n                \"place\": \"40 km W of Mentone, Texas\",\n                \"time\": 1699477502793,\n                \"updated\": 1699479283950,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vxvf\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vxvf.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 50,\n                \"net\": \"tx\",\n                \"code\": \"2023vxvf\",\n                \"ids\": \",tx2023vxvf,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 15,\n                \"dmin\": 0,\n                \"rms\": 0.2,\n                \"gap\": 50,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.8 - 40 km W of Mentone, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.021,\n                    31.666,\n                    4.8365\n                ]\n            },\n            \"id\": \"tx2023vxvf\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.5,\n                \"place\": \"western Texas\",\n                \"time\": 1699477394574,\n                \"updated\": 1699480179782,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vxvd\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vxvd.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 35,\n                \"net\": \"tx\",\n                \"code\": \"2023vxvd\",\n                \"ids\": \",tx2023vxvd,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 7,\n                \"dmin\": 0,\n                \"rms\": 1,\n                \"gap\": 137,\n                \"magType\": \"mlv\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.5 - western Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.279,\n                    31.719,\n                    1.8806\n                ]\n            },\n            \"id\": \"tx2023vxvd\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.97,\n                \"place\": \"11 km ENE of Borrego Springs, CA\",\n                \"time\": 1699477059310,\n                \"updated\": 1699490515130,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/ci39710466\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ci39710466.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 60,\n                \"net\": \"ci\",\n                \"code\": \"39710466\",\n                \"ids\": \",ci39710466,\",\n                \"sources\": \",ci,\",\n                \"types\": \",focal-mechanism,nearby-cities,origin,phase-data,scitech-link,\",\n                \"nst\": 75,\n                \"dmin\": 0.1019,\n                \"rms\": 0.21,\n                \"gap\": 25,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.0 - 11 km ENE of Borrego Springs, CA\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -116.2613333,\n                    33.2763333,\n                    7.04\n                ]\n            },\n            \"id\": \"ci39710466\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.8,\n                \"place\": \"western Texas\",\n                \"time\": 1699477005522,\n                \"updated\": 1699478302630,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vxux\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vxux.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 50,\n                \"net\": \"tx\",\n                \"code\": \"2023vxux\",\n                \"ids\": \",tx2023vxux,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 19,\n                \"dmin\": 0,\n                \"rms\": 0.4,\n                \"gap\": 43,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.8 - western Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.007,\n                    31.645,\n                    5.2482\n                ]\n            },\n            \"id\": \"tx2023vxux\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.5,\n                \"place\": \"38 km W of Mentone, Texas\",\n                \"time\": 1699476657721,\n                \"updated\": 1699478295537,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vxut\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vxut.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 35,\n                \"net\": \"tx\",\n                \"code\": \"2023vxut\",\n                \"ids\": \",tx2023vxut,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 18,\n                \"dmin\": 0,\n                \"rms\": 0.6,\n                \"gap\": 41,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.5 - 38 km W of Mentone, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -103.998,\n                    31.643,\n                    5.5481\n                ]\n            },\n            \"id\": \"tx2023vxut\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.22,\n                \"place\": \"3 km SE of Home Gardens, CA\",\n                \"time\": 1699476367290,\n                \"updated\": 1699482798541,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/ci39710450\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ci39710450.geojson\",\n                \"felt\": 1,\n                \"cdi\": 0,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 23,\n                \"net\": \"ci\",\n                \"code\": \"39710450\",\n                \"ids\": \",ci39710450,\",\n                \"sources\": \",ci,\",\n                \"types\": \",dyfi,nearby-cities,origin,phase-data,scitech-link,\",\n                \"nst\": 55,\n                \"dmin\": 0.05302,\n                \"rms\": 0.25,\n                \"gap\": 41,\n                \"magType\": \"ml\",\n                \"type\": \"quarry blast\",\n                \"title\": \"M 1.2 Quarry Blast - 3 km SE of Home Gardens, CA\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -117.4996667,\n                    33.8615,\n                    -0.45\n                ]\n            },\n            \"id\": \"ci39710450\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.1,\n                \"place\": \"15 km W of Womens Bay, Alaska\",\n                \"time\": 1699476326541,\n                \"updated\": 1699476463940,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/ak023ecbunfy\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023ecbunfy.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 68,\n                \"net\": \"ak\",\n                \"code\": \"023ecbunfy\",\n                \"ids\": \",ak023ecbunfy,\",\n                \"sources\": \",ak,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": null,\n                \"dmin\": null,\n                \"rms\": 0.4,\n                \"gap\": null,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.1 - 15 km W of Womens Bay, Alaska\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -152.9199,\n                    57.6656,\n                    14.9\n                ]\n            },\n            \"id\": \"ak023ecbunfy\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.2,\n                \"place\": \"58 km SW of Karluk, Alaska\",\n                \"time\": 1699476170823,\n                \"updated\": 1699476317467,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/ak023ecbu262\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023ecbu262.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 74,\n                \"net\": \"ak\",\n                \"code\": \"023ecbu262\",\n                \"ids\": \",ak023ecbu262,\",\n                \"sources\": \",ak,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": null,\n                \"dmin\": null,\n                \"rms\": 0.72,\n                \"gap\": null,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.2 - 58 km SW of Karluk, Alaska\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -155.2603,\n                    57.2798,\n                    35.8\n                ]\n            },\n            \"id\": \"ak023ecbu262\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.4,\n                \"place\": \"34 km NNW of Petersville, Alaska\",\n                \"time\": 1699475539704,\n                \"updated\": 1699475680842,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/ak023ecbrulu\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023ecbrulu.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 89,\n                \"net\": \"ak\",\n                \"code\": \"023ecbrulu\",\n                \"ids\": \",ak023ecbrulu,\",\n                \"sources\": \",ak,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": null,\n                \"dmin\": null,\n                \"rms\": 0.58,\n                \"gap\": null,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.4 - 34 km NNW of Petersville, Alaska\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -150.9323,\n                    62.7946,\n                    95\n                ]\n            },\n            \"id\": \"ak023ecbrulu\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.79999995,\n                \"place\": \"4 km SSW of P\\u0101hala, Hawaii\",\n                \"time\": 1699473165090,\n                \"updated\": 1699473355830,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/hv73641817\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/hv73641817.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 50,\n                \"net\": \"hv\",\n                \"code\": \"73641817\",\n                \"ids\": \",hv73641817,\",\n                \"sources\": \",hv,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 27,\n                \"dmin\": null,\n                \"rms\": 0.0900000036,\n                \"gap\": 209,\n                \"magType\": \"md\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.8 - 4 km SSW of P\\u0101hala, Hawaii\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -155.496170043945,\n                    19.1704998016357,\n                    34.1199989318848\n                ]\n            },\n            \"id\": \"hv73641817\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2,\n                \"place\": \"28 km E of Skwentna, Alaska\",\n                \"time\": 1699472705907,\n                \"updated\": 1699472841629,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/ak023ecb968u\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023ecb968u.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 62,\n                \"net\": \"ak\",\n                \"code\": \"023ecb968u\",\n                \"ids\": \",ak023ecb968u,\",\n                \"sources\": \",ak,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": null,\n                \"dmin\": null,\n                \"rms\": 0.31,\n                \"gap\": null,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.0 - 28 km E of Skwentna, Alaska\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -150.8539,\n                    61.9622,\n                    67.5\n                ]\n            },\n            \"id\": \"ak023ecb968u\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 0.98,\n                \"place\": \"5 km ENE of Coalinga, CA\",\n                \"time\": 1699472461040,\n                \"updated\": 1699475413148,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960136\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960136.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 15,\n                \"net\": \"nc\",\n                \"code\": \"73960136\",\n                \"ids\": \",nc73960136,\",\n                \"sources\": \",nc,\",\n                \"types\": \",nearby-cities,origin,phase-data,scitech-link,\",\n                \"nst\": 8,\n                \"dmin\": 0.1449,\n                \"rms\": 0.02,\n                \"gap\": 190,\n                \"magType\": \"md\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.0 - 5 km ENE of Coalinga, CA\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -120.309166,\n                    36.1604996,\n                    10.2\n                ]\n            },\n            \"id\": \"nc73960136\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.16,\n                \"place\": \"4 km NW of Cobb, CA\",\n                \"time\": 1699472279460,\n                \"updated\": 1699487053232,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960131\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960131.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 21,\n                \"net\": \"nc\",\n                \"code\": \"73960131\",\n                \"ids\": \",nc73960131,\",\n                \"sources\": \",nc,\",\n                \"types\": \",nearby-cities,origin,phase-data,scitech-link,\",\n                \"nst\": 7,\n                \"dmin\": 0.01223,\n                \"rms\": 0.03,\n                \"gap\": 193,\n                \"magType\": \"md\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.2 - 4 km NW of Cobb, CA\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -122.7623367,\n                    38.8479996,\n                    0.94\n                ]\n            },\n            \"id\": \"nc73960131\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.8,\n                \"place\": \"40 km W of Mentone, Texas\",\n                \"time\": 1699472011250,\n                \"updated\": 1699473500324,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vxse\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vxse.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 50,\n                \"net\": \"tx\",\n                \"code\": \"2023vxse\",\n                \"ids\": \",tx2023vxse,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 20,\n                \"dmin\": 0,\n                \"rms\": 0.4,\n                \"gap\": 47,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.8 - 40 km W of Mentone, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.018,\n                    31.653,\n                    5.2881\n                ]\n            },\n            \"id\": \"tx2023vxse\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.1,\n                \"place\": \"western Texas\",\n                \"time\": 1699471106424,\n                \"updated\": 1699473808806,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vxrs\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vxrs.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 68,\n                \"net\": \"tx\",\n                \"code\": \"2023vxrs\",\n                \"ids\": \",tx2023vxrs,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 18,\n                \"dmin\": 0.1,\n                \"rms\": 0.3,\n                \"gap\": 159,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.1 - western Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.541,\n                    31.585,\n                    2.8322\n                ]\n            },\n            \"id\": \"tx2023vxrs\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.56,\n                \"place\": \"3 km NE of The Geysers, CA\",\n                \"time\": 1699470623950,\n                \"updated\": 1699484474981,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960126\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960126.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 37,\n                \"net\": \"nc\",\n                \"code\": \"73960126\",\n                \"ids\": \",nc73960126,\",\n                \"sources\": \",nc,\",\n                \"types\": \",nearby-cities,origin,phase-data,scitech-link,\",\n                \"nst\": 27,\n                \"dmin\": 0.01246,\n                \"rms\": 0.03,\n                \"gap\": 98,\n                \"magType\": \"md\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.6 - 3 km NE of The Geysers, CA\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -122.730835,\n                    38.7948341,\n                    1.2\n                ]\n            },\n            \"id\": \"nc73960126\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.08,\n                \"place\": \"3 km NE of The Geysers, CA\",\n                \"time\": 1699470610690,\n                \"updated\": 1699472713895,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960121\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960121.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 18,\n                \"net\": \"nc\",\n                \"code\": \"73960121\",\n                \"ids\": \",nc73960121,\",\n                \"sources\": \",nc,\",\n                \"types\": \",nearby-cities,origin,phase-data,scitech-link,\",\n                \"nst\": 15,\n                \"dmin\": 0.01288,\n                \"rms\": 0.02,\n                \"gap\": 99,\n                \"magType\": \"md\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.1 - 3 km NE of The Geysers, CA\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -122.7294998,\n                    38.7948341,\n                    1.95\n                ]\n            },\n            \"id\": \"nc73960121\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.9,\n                \"place\": \"38 km WSW of Mentone, Texas\",\n                \"time\": 1699470534832,\n                \"updated\": 1699471481707,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vxrj\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vxrj.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 56,\n                \"net\": \"tx\",\n                \"code\": \"2023vxrj\",\n                \"ids\": \",tx2023vxrj,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 21,\n                \"dmin\": 0,\n                \"rms\": 0.3,\n                \"gap\": 50,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.9 - 38 km WSW of Mentone, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -103.996,\n                    31.638,\n                    4.4323\n                ]\n            },\n            \"id\": \"tx2023vxrj\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.4,\n                \"place\": \"70 km W of Nanwalek, Alaska\",\n                \"time\": 1699470219606,\n                \"updated\": 1699470392871,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/ak023ecb08rs\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023ecb08rs.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 89,\n                \"net\": \"ak\",\n                \"code\": \"023ecb08rs\",\n                \"ids\": \",ak023ecb08rs,\",\n                \"sources\": \",ak,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": null,\n                \"dmin\": null,\n                \"rms\": 0.73,\n                \"gap\": null,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.4 - 70 km W of Nanwalek, Alaska\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -153.1594,\n                    59.355,\n                    88.3\n                ]\n            },\n            \"id\": \"ak023ecb08rs\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.4,\n                \"place\": \"47 km WSW of Anchor Point, Alaska\",\n                \"time\": 1699469276379,\n                \"updated\": 1699469454902,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/ak023ecao9yt\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023ecao9yt.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 89,\n                \"net\": \"ak\",\n                \"code\": \"023ecao9yt\",\n                \"ids\": \",ak023ecao9yt,\",\n                \"sources\": \",ak,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": null,\n                \"dmin\": null,\n                \"rms\": 0.86,\n                \"gap\": null,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.4 - 47 km WSW of Anchor Point, Alaska\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -152.612,\n                    59.6262,\n                    74.5\n                ]\n            },\n            \"id\": \"ak023ecao9yt\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.9,\n                \"place\": \"39 km W of Mentone, Texas\",\n                \"time\": 1699468626083,\n                \"updated\": 1699470921482,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vxqh\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vxqh.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 56,\n                \"net\": \"tx\",\n                \"code\": \"2023vxqh\",\n                \"ids\": \",tx2023vxqh,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 16,\n                \"dmin\": 0,\n                \"rms\": 0.3,\n                \"gap\": 45,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.9 - 39 km W of Mentone, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.015,\n                    31.647,\n                    4.1758\n                ]\n            },\n            \"id\": \"tx2023vxqh\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.4,\n                \"place\": \"40 km W of Mentone, Texas\",\n                \"time\": 1699468165641,\n                \"updated\": 1699506715268,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vxqa\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vxqa.geojson\",\n                \"felt\": 6,\n                \"cdi\": 3.8,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 91,\n                \"net\": \"tx\",\n                \"code\": \"2023vxqa\",\n                \"ids\": \",tx2023vxqa,\",\n                \"sources\": \",tx,\",\n                \"types\": \",dyfi,origin,phase-data,\",\n                \"nst\": 19,\n                \"dmin\": 0,\n                \"rms\": 0.2,\n                \"gap\": 47,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.4 - 40 km W of Mentone, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.018,\n                    31.651,\n                    5.6817\n                ]\n            },\n            \"id\": \"tx2023vxqa\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.1,\n                \"place\": \"1 km WNW of Point Possession, Alaska\",\n                \"time\": 1699468152567,\n                \"updated\": 1699468278477,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/ak023ecakbp4\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023ecakbp4.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 68,\n                \"net\": \"ak\",\n                \"code\": \"023ecakbp4\",\n                \"ids\": \",ak023ecakbp4,\",\n                \"sources\": \",ak,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": null,\n                \"dmin\": null,\n                \"rms\": 0.77,\n                \"gap\": null,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.1 - 1 km WNW of Point Possession, Alaska\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -150.7113,\n                    60.9274,\n                    34.6\n                ]\n            },\n            \"id\": \"ak023ecakbp4\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 4.6,\n                \"place\": \"298 km WSW of Tual, Indonesia\",\n                \"time\": 1699467981946,\n                \"updated\": 1699471139040,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9p4\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9p4.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 326,\n                \"net\": \"us\",\n                \"code\": \"7000l9p4\",\n                \"ids\": \",us7000l9p4,\",\n                \"sources\": \",us,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 44,\n                \"dmin\": 1.737,\n                \"rms\": 0.93,\n                \"gap\": 56,\n                \"magType\": \"mb\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 4.6 - 298 km WSW of Tual, Indonesia\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    130.133,\n                    -6.2563,\n                    10\n                ]\n            },\n            \"id\": \"us7000l9p4\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2,\n                \"place\": \"40 km W of Mentone, Texas\",\n                \"time\": 1699467671508,\n                \"updated\": 1699470264466,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vxpu\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vxpu.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 62,\n                \"net\": \"tx\",\n                \"code\": \"2023vxpu\",\n                \"ids\": \",tx2023vxpu,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 33,\n                \"dmin\": 0,\n                \"rms\": 0.2,\n                \"gap\": 37,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.0 - 40 km W of Mentone, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.02,\n                    31.654,\n                    6.3758\n                ]\n            },\n            \"id\": \"tx2023vxpu\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.9,\n                \"place\": \"37 km WSW of Mentone, Texas\",\n                \"time\": 1699467600466,\n                \"updated\": 1699534956449,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vxpt\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vxpt.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 56,\n                \"net\": \"tx\",\n                \"code\": \"2023vxpt\",\n                \"ids\": \",tx2023vxpt,\",\n                \"sources\": \",tx,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 22,\n                \"dmin\": 0,\n                \"rms\": 0.1,\n                \"gap\": 47,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.9 - 37 km WSW of Mentone, Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -103.988,\n                    31.633,\n                    5.8616\n                ]\n            },\n            \"id\": \"tx2023vxpt\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.61,\n                \"place\": \"11 km ESE of Ashford, Washington\",\n                \"time\": 1699467325410,\n                \"updated\": 1699486036250,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/uw61971251\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/uw61971251.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 40,\n                \"net\": \"uw\",\n                \"code\": \"61971251\",\n                \"ids\": \",uw61971251,\",\n                \"sources\": \",uw,\",\n                \"types\": \",origin,phase-data,\",\n                \"nst\": 50,\n                \"dmin\": 0.04012,\n                \"rms\": 0.14,\n                \"gap\": 69,\n                \"magType\": \"ml\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.6 - 11 km ESE of Ashford, Washington\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -121.90316666666666,\n                    46.701166666666666,\n                    7.25\n                ]\n            },\n            \"id\": \"uw61971251\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 1.64,\n                \"place\": \"3 km SE of Guerneville, CA\",\n                \"time\": 1699467192620,\n                \"updated\": 1699477813346,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/nc73957865\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73957865.geojson\",\n                \"felt\": null,\n                \"cdi\": null,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"automatic\",\n                \"tsunami\": 0,\n                \"sig\": 41,\n                \"net\": \"nc\",\n                \"code\": \"73957865\",\n                \"ids\": \",nc73957865,\",\n                \"sources\": \",nc,\",\n                \"types\": \",nearby-cities,origin,phase-data,scitech-link,\",\n                \"nst\": 7,\n                \"dmin\": 0.249,\n                \"rms\": 0.14,\n                \"gap\": 215,\n                \"magType\": \"md\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 1.6 - 3 km SE of Guerneville, CA\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -122.9683304,\n                    38.4850006,\n                    24.01\n                ]\n            },\n            \"id\": \"nc73957865\"\n        },\n        {\n            \"type\": \"Feature\",\n            \"properties\": {\n                \"mag\": 2.5,\n                \"place\": \"western Texas\",\n                \"time\": 1699466411777,\n                \"updated\": 1699502416780,\n                \"tz\": null,\n                \"url\": \"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vxpc\",\n                \"detail\": \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vxpc.geojson\",\n                \"felt\": 4,\n                \"cdi\": 2.7,\n                \"mmi\": null,\n                \"alert\": null,\n                \"status\": \"reviewed\",\n                \"tsunami\": 0,\n                \"sig\": 97,\n                \"net\": \"tx\",\n                \"code\": \"2023vxpc\",\n                \"ids\": \",tx2023vxpc,\",\n                \"sources\": \",tx,\",\n                \"types\": \",dyfi,origin,phase-data,\",\n                \"nst\": 28,\n                \"dmin\": 0,\n                \"rms\": 0.1,\n                \"gap\": 41,\n                \"magType\": \"ml(texnet)\",\n                \"type\": \"earthquake\",\n                \"title\": \"M 2.5 - western Texas\"\n            },\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    -104.011,\n                    31.638,\n                    5.6046\n                ]\n            },\n            \"id\": \"tx2023vxpc\"\n        }\n    ],\n    \"bbox\": [\n        -177.1013,\n        -24.3611,\n        -0.84,\n        167.4497,\n        67.7142,\n        183.697\n    ]\n}\n\n\n\n# created from \n\nreadable_file = 'E:/machine learning projects/1.0_day.geojson'\nwith open(readable_file, 'r', encoding = 'utf-8') as f:\n    content = f.read()\n    print(content)\n\n{\"type\":\"FeatureCollection\",\"metadata\":{\"generated\":1699552743000,\"url\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/1.0_day.geojson\",\"title\":\"USGS Magnitude 1.0+ Earthquakes, Past Day\",\"status\":200,\"api\":\"1.10.3\",\"count\":191},\"features\":[{\"type\":\"Feature\",\"properties\":{\"mag\":1.80999994,\"place\":\"7 km S of Pāhala, Hawaii\",\"time\":1699548293900,\"updated\":1699548484670,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/hv73642362\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/hv73642362.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":50,\"net\":\"hv\",\"code\":\"73642362\",\"ids\":\",hv73642362,\",\"sources\":\",hv,\",\"types\":\",origin,phase-data,\",\"nst\":32,\"dmin\":null,\"rms\":0.140000001,\"gap\":171,\"magType\":\"md\",\"type\":\"earthquake\",\"title\":\"M 1.8 - 7 km S of Pāhala, Hawaii\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-155.49299621582,19.1323337554932,31.1900005340576]},\"id\":\"hv73642362\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.3,\"place\":\"47 km NNW of Chickaloon, Alaska\",\"time\":1699547077244,\"updated\":1699547194385,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/ak023edwvsb0\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023edwvsb0.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":81,\"net\":\"ak\",\"code\":\"023edwvsb0\",\"ids\":\",ak023edwvsb0,\",\"sources\":\",ak,\",\"types\":\",origin,phase-data,\",\"nst\":null,\"dmin\":null,\"rms\":1.02,\"gap\":null,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 2.3 - 47 km NNW of Chickaloon, Alaska\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-148.6656,62.21,26.1]},\"id\":\"ak023edwvsb0\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":3.3,\"place\":\"62 km SSW of Whites City, New Mexico\",\"time\":1699546775038,\"updated\":1699548922019,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vzhg\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vzhg.geojson\",\"felt\":1,\"cdi\":1,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":168,\"net\":\"tx\",\"code\":\"2023vzhg\",\"ids\":\",tx2023vzhg,us7000l9wr,\",\"sources\":\",tx,us,\",\"types\":\",dyfi,origin,phase-data,\",\"nst\":13,\"dmin\":0.1,\"rms\":0.1,\"gap\":80,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 3.3 - 62 km SSW of Whites City, New Mexico\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.593,31.647,7.5969]},\"id\":\"tx2023vzhg\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.5,\"place\":\"15 km E of Seven Trees, CA\",\"time\":1699546696380,\"updated\":1699548912144,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960536\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960536.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":35,\"net\":\"nc\",\"code\":\"73960536\",\"ids\":\",nc73960536,\",\"sources\":\",nc,\",\"types\":\",nearby-cities,origin,phase-data,scitech-link,\",\"nst\":17,\"dmin\":0.04013,\"rms\":0.05,\"gap\":51,\"magType\":\"md\",\"type\":\"earthquake\",\"title\":\"M 1.5 - 15 km E of Seven Trees, CA\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-121.668335,37.2989998,6.19]},\"id\":\"nc73960536\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.8,\"place\":\"45 km NW of Toyah, Texas\",\"time\":1699546087245,\"updated\":1699548100415,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vzgy\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vzgy.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":50,\"net\":\"tx\",\"code\":\"2023vzgy\",\"ids\":\",tx2023vzgy,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":18,\"dmin\":0,\"rms\":0.4,\"gap\":56,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 1.8 - 45 km NW of Toyah, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.12,31.62,4.5431]},\"id\":\"tx2023vzgy\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.5,\"place\":\"60 km SSW of Whites City, New Mexico\",\"time\":1699545502173,\"updated\":1699551505040,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vzgq\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vzgq.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":96,\"net\":\"tx\",\"code\":\"2023vzgq\",\"ids\":\",us7000l9vz,tx2023vzgq,\",\"sources\":\",us,tx,\",\"types\":\",origin,phase-data,\",\"nst\":14,\"dmin\":0,\"rms\":0.2,\"gap\":72,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 2.5 - 60 km SSW of Whites City, New Mexico\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.506,31.637,7.0571]},\"id\":\"tx2023vzgq\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":4.7,\"place\":\"Banda Sea\",\"time\":1699544209694,\"updated\":1699549188040,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9uy\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9uy.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":340,\"net\":\"us\",\"code\":\"7000l9uy\",\"ids\":\",us7000l9uy,\",\"sources\":\",us,\",\"types\":\",origin,phase-data,\",\"nst\":69,\"dmin\":7.367,\"rms\":0.53,\"gap\":57,\"magType\":\"mb\",\"type\":\"earthquake\",\"title\":\"M 4.7 - Banda Sea\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[129.5293,-6.3194,10]},\"id\":\"us7000l9uy\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.09,\"place\":\"7 km WNW of Cobb, CA\",\"time\":1699543811820,\"updated\":1699547110873,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960501\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960501.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":18,\"net\":\"nc\",\"code\":\"73960501\",\"ids\":\",nc73960501,\",\"sources\":\",nc,\",\"types\":\",nearby-cities,origin,phase-data,scitech-link,\",\"nst\":16,\"dmin\":0.007444,\"rms\":0.02,\"gap\":65,\"magType\":\"md\",\"type\":\"earthquake\",\"title\":\"M 1.1 - 7 km WNW of Cobb, CA\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-122.7994995,38.8363342,2.14]},\"id\":\"nc73960501\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.8,\"place\":\"53 km WNW of Toyah, Texas\",\"time\":1699543500458,\"updated\":1699545925286,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vzfl\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vzfl.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":50,\"net\":\"tx\",\"code\":\"2023vzfl\",\"ids\":\",tx2023vzfl,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":17,\"dmin\":0.1,\"rms\":0.6,\"gap\":77,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 1.8 - 53 km WNW of Toyah, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.272,31.577,1.6266]},\"id\":\"tx2023vzfl\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":4.9,\"place\":\"Kuril Islands\",\"time\":1699543336287,\"updated\":1699548616040,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9uv\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9uv.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":369,\"net\":\"us\",\"code\":\"7000l9uv\",\"ids\":\",us7000l9uv,\",\"sources\":\",us,\",\"types\":\",origin,phase-data,\",\"nst\":125,\"dmin\":6.248,\"rms\":0.69,\"gap\":77,\"magType\":\"mb\",\"type\":\"earthquake\",\"title\":\"M 4.9 - Kuril Islands\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[154.1018,47.308,31.08]},\"id\":\"us7000l9uv\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2,\"place\":\"45 km W of Mentone, Texas\",\"time\":1699542055337,\"updated\":1699544719069,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vzer\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vzer.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":62,\"net\":\"tx\",\"code\":\"2023vzer\",\"ids\":\",tx2023vzer,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":14,\"dmin\":0,\"rms\":1.3,\"gap\":217,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 2.0 - 45 km W of Mentone, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.078,31.67,7.8692]},\"id\":\"tx2023vzer\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.6,\"place\":\"39 km WSW of Mentone, Texas\",\"time\":1699541612567,\"updated\":1699544712057,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vzel\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vzel.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":39,\"net\":\"tx\",\"code\":\"2023vzel\",\"ids\":\",tx2023vzel,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":15,\"dmin\":0,\"rms\":0.2,\"gap\":51,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 1.6 - 39 km WSW of Mentone, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.003,31.625,6.1375]},\"id\":\"tx2023vzel\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.01,\"place\":\"17 km W of Minersville, Utah\",\"time\":1699541423710,\"updated\":1699549422720,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/uu60553917\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/uu60553917.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":16,\"net\":\"uu\",\"code\":\"60553917\",\"ids\":\",uu60553917,\",\"sources\":\",uu,\",\"types\":\",origin,phase-data,\",\"nst\":7,\"dmin\":0.1413,\"rms\":0.09,\"gap\":189,\"magType\":\"md\",\"type\":\"earthquake\",\"title\":\"M 1.0 - 17 km W of Minersville, Utah\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-113.1221667,38.207,10.61]},\"id\":\"uu60553917\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":3.2,\"place\":\"Alaska Peninsula\",\"time\":1699541195270,\"updated\":1699546792040,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/ak023edvtmp3\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023edvtmp3.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":158,\"net\":\"ak\",\"code\":\"023edvtmp3\",\"ids\":\",us7000l9ur,ak023edvtmp3,\",\"sources\":\",us,ak,\",\"types\":\",origin,phase-data,\",\"nst\":null,\"dmin\":null,\"rms\":0.51,\"gap\":null,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 3.2 - Alaska Peninsula\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-158.5309,55.49,13.9]},\"id\":\"ak023edvtmp3\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.4,\"place\":\"37 km NW of Valdez, Alaska\",\"time\":1699539886310,\"updated\":1699539949938,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/ak023edvoxsn\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023edvoxsn.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":89,\"net\":\"ak\",\"code\":\"023edvoxsn\",\"ids\":\",ak023edvoxsn,\",\"sources\":\",ak,\",\"types\":\",origin,phase-data,\",\"nst\":null,\"dmin\":null,\"rms\":0.25,\"gap\":null,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 2.4 - 37 km NW of Valdez, Alaska\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-146.8141,61.3851,36.6]},\"id\":\"ak023edvoxsn\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":5.1,\"place\":\"37 km NE of Luganville, Vanuatu\",\"time\":1699539129087,\"updated\":1699540088040,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9ul\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9ul.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":400,\"net\":\"us\",\"code\":\"7000l9ul\",\"ids\":\",us7000l9ul,\",\"sources\":\",us,\",\"types\":\",origin,phase-data,\",\"nst\":43,\"dmin\":6,\"rms\":0.63,\"gap\":108,\"magType\":\"mww\",\"type\":\"earthquake\",\"title\":\"M 5.1 - 37 km NE of Luganville, Vanuatu\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[167.4497,-15.3325,130.066]},\"id\":\"us7000l9ul\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.8,\"place\":\"27 km N of Two Rivers, Alaska\",\"time\":1699539017168,\"updated\":1699539138451,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/ak023edvlvis\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023edvlvis.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":50,\"net\":\"ak\",\"code\":\"023edvlvis\",\"ids\":\",ak023edvlvis,\",\"sources\":\",ak,\",\"types\":\",origin,phase-data,\",\"nst\":null,\"dmin\":null,\"rms\":0.39,\"gap\":null,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 1.8 - 27 km N of Two Rivers, Alaska\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-147.1284,65.114,8]},\"id\":\"ak023edvlvis\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.7,\"place\":\"35 km NNW of Toyah, Texas\",\"time\":1699538800671,\"updated\":1699540345082,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vzcx\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vzcx.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":44,\"net\":\"tx\",\"code\":\"2023vzcx\",\"ids\":\",tx2023vzcx,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":15,\"dmin\":0,\"rms\":0.3,\"gap\":72,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 1.7 - 35 km NNW of Toyah, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-103.955,31.601,4.8633]},\"id\":\"tx2023vzcx\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.7,\"place\":\"36 km WSW of Mentone, Texas\",\"time\":1699538479259,\"updated\":1699540335022,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vzct\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vzct.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":44,\"net\":\"tx\",\"code\":\"2023vzct\",\"ids\":\",tx2023vzct,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":15,\"dmin\":0,\"rms\":0.4,\"gap\":50,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 1.7 - 36 km WSW of Mentone, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-103.978,31.638,4.6081]},\"id\":\"tx2023vzct\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":0.98,\"place\":\"3 km WSW of Anderson Springs, CA\",\"time\":1699538180000,\"updated\":1699547291887,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960466\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960466.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":15,\"net\":\"nc\",\"code\":\"73960466\",\"ids\":\",nc73960466,\",\"sources\":\",nc,\",\"types\":\",nearby-cities,origin,phase-data,scitech-link,\",\"nst\":7,\"dmin\":0.008438,\"rms\":0.02,\"gap\":107,\"magType\":\"md\",\"type\":\"earthquake\",\"title\":\"M 1.0 - 3 km WSW of Anderson Springs, CA\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-122.7216644,38.7696648,1.14]},\"id\":\"nc73960466\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.5,\"place\":\"western Texas\",\"time\":1699538082907,\"updated\":1699540699388,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vzcn\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vzcn.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":35,\"net\":\"tx\",\"code\":\"2023vzcn\",\"ids\":\",tx2023vzcn,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":8,\"dmin\":0.1,\"rms\":0.5,\"gap\":210,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 1.5 - western Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.199,31.7,2.3485]},\"id\":\"tx2023vzcn\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.6,\"place\":\"57 km S of Whites City, New Mexico\",\"time\":1699537644417,\"updated\":1699538521009,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vzcg\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vzcg.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":39,\"net\":\"tx\",\"code\":\"2023vzcg\",\"ids\":\",tx2023vzcg,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":17,\"dmin\":0,\"rms\":0.5,\"gap\":67,\"magType\":\"mlv\",\"type\":\"earthquake\",\"title\":\"M 1.6 - 57 km S of Whites City, New Mexico\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.292,31.664,3.1803]},\"id\":\"tx2023vzcg\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.03,\"place\":\"9 km NW of The Geysers, CA\",\"time\":1699536386920,\"updated\":1699545673727,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960456\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960456.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":16,\"net\":\"nc\",\"code\":\"73960456\",\"ids\":\",nc73960456,\",\"sources\":\",nc,\",\"types\":\",nearby-cities,origin,phase-data,scitech-link,\",\"nst\":19,\"dmin\":0.007705,\"rms\":0.02,\"gap\":74,\"magType\":\"md\",\"type\":\"earthquake\",\"title\":\"M 1.0 - 9 km NW of The Geysers, CA\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-122.8256683,38.8378334,1.78]},\"id\":\"nc73960456\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":4.5,\"place\":\"37 km W of Arica, Chile\",\"time\":1699535112443,\"updated\":1699546587040,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9u4\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9u4.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":312,\"net\":\"us\",\"code\":\"7000l9u4\",\"ids\":\",us7000l9u4,\",\"sources\":\",us,\",\"types\":\",origin,phase-data,\",\"nst\":27,\"dmin\":0.294,\"rms\":1.04,\"gap\":143,\"magType\":\"mb\",\"type\":\"earthquake\",\"title\":\"M 4.5 - 37 km W of Arica, Chile\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-70.6461,-18.4235,87.39]},\"id\":\"us7000l9u4\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.6,\"place\":\"western Texas\",\"time\":1699535095161,\"updated\":1699535908041,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vzax\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vzax.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":39,\"net\":\"tx\",\"code\":\"2023vzax\",\"ids\":\",tx2023vzax,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":15,\"dmin\":0,\"rms\":0.4,\"gap\":88,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 1.6 - western Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-103.987,31.636,4.1188]},\"id\":\"tx2023vzax\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2,\"place\":\"38 km WSW of Mentone, Texas\",\"time\":1699535045425,\"updated\":1699535903848,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vzav\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vzav.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":62,\"net\":\"tx\",\"code\":\"2023vzav\",\"ids\":\",tx2023vzav,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":16,\"dmin\":0,\"rms\":0.6,\"gap\":46,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 2.0 - 38 km WSW of Mentone, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-103.994,31.627,8.2171]},\"id\":\"tx2023vzav\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.5,\"place\":\"western Texas\",\"time\":1699534586458,\"updated\":1699538965468,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vzao\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vzao.geojson\",\"felt\":2,\"cdi\":2.2,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":35,\"net\":\"tx\",\"code\":\"2023vzao\",\"ids\":\",tx2023vzao,\",\"sources\":\",tx,\",\"types\":\",dyfi,origin,phase-data,\",\"nst\":7,\"dmin\":0.1,\"rms\":0.1,\"gap\":161,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 1.5 - western Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.546,31.59,3.4441]},\"id\":\"tx2023vzao\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.4,\"place\":\"California-Nevada border region\",\"time\":1699534300904,\"updated\":1699534463161,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/nn00868648\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nn00868648.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":30,\"net\":\"nn\",\"code\":\"00868648\",\"ids\":\",nn00868648,\",\"sources\":\",nn,\",\"types\":\",origin,phase-data,\",\"nst\":8,\"dmin\":0.392,\"rms\":0.1432,\"gap\":115.92,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 1.4 - California-Nevada border region\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-118.1693,38.5426,6.8]},\"id\":\"nn00868648\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":0.95,\"place\":\"1 km SSE of The Geysers, CA\",\"time\":1699534232670,\"updated\":1699542372415,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960441\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960441.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":14,\"net\":\"nc\",\"code\":\"73960441\",\"ids\":\",nc73960441,\",\"sources\":\",nc,\",\"types\":\",nearby-cities,origin,phase-data,scitech-link,\",\"nst\":8,\"dmin\":0.01094,\"rms\":0.2,\"gap\":118,\"magType\":\"md\",\"type\":\"earthquake\",\"title\":\"M 1.0 - 1 km SSE of The Geysers, CA\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-122.7528305,38.7664986,-0.84]},\"id\":\"nc73960441\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.6,\"place\":\"western Texas\",\"time\":1699534154284,\"updated\":1699536350178,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vzai\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vzai.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":39,\"net\":\"tx\",\"code\":\"2023vzai\",\"ids\":\",tx2023vzai,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":16,\"dmin\":0,\"rms\":0.4,\"gap\":56,\"magType\":\"mlv\",\"type\":\"earthquake\",\"title\":\"M 1.6 - western Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.119,31.619,4.0234]},\"id\":\"tx2023vzai\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.61,\"place\":\"0 km E of Howardville, Missouri\",\"time\":1699533438360,\"updated\":1699546551790,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/nm60559656\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nm60559656.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":40,\"net\":\"nm\",\"code\":\"60559656\",\"ids\":\",nm60559656,\",\"sources\":\",nm,\",\"types\":\",origin,phase-data,\",\"nst\":28,\"dmin\":0.0412,\"rms\":0.04,\"gap\":39,\"magType\":\"md\",\"type\":\"earthquake\",\"title\":\"M 1.6 - 0 km E of Howardville, Missouri\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-89.597,36.5683333333333,8.08]},\"id\":\"nm60559656\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":4.6,\"place\":\"south of the Fiji Islands\",\"time\":1699533175860,\"updated\":1699535447040,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9tz\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9tz.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":326,\"net\":\"us\",\"code\":\"7000l9tz\",\"ids\":\",us7000l9tz,\",\"sources\":\",us,\",\"types\":\",origin,phase-data,\",\"nst\":31,\"dmin\":3.627,\"rms\":0.84,\"gap\":119,\"magType\":\"mb\",\"type\":\"earthquake\",\"title\":\"M 4.6 - south of the Fiji Islands\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-177.1013,-24.3611,161.789]},\"id\":\"us7000l9tz\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.25,\"place\":\"16 km E of Seven Trees, CA\",\"time\":1699533080730,\"updated\":1699540755262,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960436\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960436.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":24,\"net\":\"nc\",\"code\":\"73960436\",\"ids\":\",nc73960436,\",\"sources\":\",nc,\",\"types\":\",nearby-cities,origin,phase-data,scitech-link,\",\"nst\":8,\"dmin\":0.0253,\"rms\":0.02,\"gap\":137,\"magType\":\"md\",\"type\":\"earthquake\",\"title\":\"M 1.3 - 16 km E of Seven Trees, CA\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-121.6598358,37.2820015,1.99]},\"id\":\"nc73960436\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.26,\"place\":\"11 km SSE of Hemet, CA\",\"time\":1699532803520,\"updated\":1699539532170,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/ci39710714\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ci39710714.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":24,\"net\":\"ci\",\"code\":\"39710714\",\"ids\":\",ci39710714,\",\"sources\":\",ci,\",\"types\":\",focal-mechanism,nearby-cities,origin,phase-data,scitech-link,\",\"nst\":48,\"dmin\":0.03406,\"rms\":0.15,\"gap\":36,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 1.3 - 11 km SSE of Hemet, CA\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-116.9331667,33.6538333,15.92]},\"id\":\"ci39710714\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.1,\"place\":\"75 km N of Chickaloon, Alaska\",\"time\":1699532141116,\"updated\":1699532243503,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/ak023edug4qn\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023edug4qn.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":68,\"net\":\"ak\",\"code\":\"023edug4qn\",\"ids\":\",ak023edug4qn,\",\"sources\":\",ak,\",\"types\":\",origin,phase-data,\",\"nst\":null,\"dmin\":null,\"rms\":0.61,\"gap\":null,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 2.1 - 75 km N of Chickaloon, Alaska\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-148.2058,62.4626,43.7]},\"id\":\"ak023edug4qn\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.1,\"place\":\"63 km ESE of Denali Park, Alaska\",\"time\":1699531979635,\"updated\":1699532104007,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/ak023edufj0r\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023edufj0r.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":68,\"net\":\"ak\",\"code\":\"023edufj0r\",\"ids\":\",ak023edufj0r,\",\"sources\":\",ak,\",\"types\":\",origin,phase-data,\",\"nst\":null,\"dmin\":null,\"rms\":0.69,\"gap\":null,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 2.1 - 63 km ESE of Denali Park, Alaska\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-147.7109,63.5436,9]},\"id\":\"ak023edufj0r\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.54,\"place\":\"36 km S of Silver Gate, Montana\",\"time\":1699531900140,\"updated\":1699548835050,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/uu60553912\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/uu60553912.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":36,\"net\":\"uu\",\"code\":\"60553912\",\"ids\":\",uu60553912,\",\"sources\":\",uu,\",\"types\":\",origin,phase-data,\",\"nst\":8,\"dmin\":0.09852,\"rms\":0.22,\"gap\":155,\"magType\":\"md\",\"type\":\"earthquake\",\"title\":\"M 1.5 - 36 km S of Silver Gate, Montana\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-110.0391667,44.6796667,12.6]},\"id\":\"uu60553912\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":4.4,\"place\":\"69 km NNE of Calama, Chile\",\"time\":1699531272150,\"updated\":1699533144040,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9tr\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9tr.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":298,\"net\":\"us\",\"code\":\"7000l9tr\",\"ids\":\",us7000l9tr,\",\"sources\":\",us,\",\"types\":\",origin,phase-data,\",\"nst\":24,\"dmin\":0.972,\"rms\":0.7,\"gap\":51,\"magType\":\"mb\",\"type\":\"earthquake\",\"title\":\"M 4.4 - 69 km NNE of Calama, Chile\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-68.7276,-21.8525,141.668]},\"id\":\"us7000l9tr\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.33,\"place\":\"9 km NW of Bridgeport, CA\",\"time\":1699530819620,\"updated\":1699552461719,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960426\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960426.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":84,\"net\":\"nc\",\"code\":\"73960426\",\"ids\":\",nc73960426,nn00868645,\",\"sources\":\",nc,nn,\",\"types\":\",nearby-cities,origin,phase-data,scitech-link,\",\"nst\":30,\"dmin\":0.2225,\"rms\":0.07,\"gap\":115,\"magType\":\"md\",\"type\":\"earthquake\",\"title\":\"M 2.3 - 9 km NW of Bridgeport, CA\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-119.315,38.3036667,11.84]},\"id\":\"nc73960426\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.9,\"place\":\"37 km WSW of Mentone, Texas\",\"time\":1699530741120,\"updated\":1699532235014,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyym\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyym.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":56,\"net\":\"tx\",\"code\":\"2023vyym\",\"ids\":\",tx2023vyym,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":27,\"dmin\":0,\"rms\":0.1,\"gap\":51,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 1.9 - 37 km WSW of Mentone, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-103.98,31.624,6.2215]},\"id\":\"tx2023vyym\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.5,\"place\":\"64 km SSW of Whites City, New Mexico\",\"time\":1699530260506,\"updated\":1699531977635,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyyf\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyyf.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":35,\"net\":\"tx\",\"code\":\"2023vyyf\",\"ids\":\",tx2023vyyf,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":17,\"dmin\":0.1,\"rms\":0.5,\"gap\":147,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 1.5 - 64 km SSW of Whites City, New Mexico\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.513,31.607,7.1771]},\"id\":\"tx2023vyyf\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.9,\"place\":\"62 km S of Whites City, New Mexico\",\"time\":1699530144721,\"updated\":1699540622341,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyyd\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyyd.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":56,\"net\":\"tx\",\"code\":\"2023vyyd\",\"ids\":\",tx2023vyyd,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":16,\"dmin\":0,\"rms\":0.2,\"gap\":215,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 1.9 - 62 km S of Whites City, New Mexico\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.5,31.62,7.3656]},\"id\":\"tx2023vyyd\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.6,\"place\":\"western Texas\",\"time\":1699529368860,\"updated\":1699530884898,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyxu\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyxu.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":39,\"net\":\"tx\",\"code\":\"2023vyxu\",\"ids\":\",tx2023vyxu,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":9,\"dmin\":0.1,\"rms\":0.1,\"gap\":127,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 1.6 - western Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-103.993,31.628,4.9879]},\"id\":\"tx2023vyxu\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.4,\"place\":\"38 km WSW of Mentone, Texas\",\"time\":1699528901589,\"updated\":1699543793680,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyxl\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyxl.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":89,\"net\":\"tx\",\"code\":\"2023vyxl\",\"ids\":\",tx2023vyxl,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":12,\"dmin\":0,\"rms\":0.1,\"gap\":44,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 2.4 - 38 km WSW of Mentone, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-103.997,31.634,5.7717]},\"id\":\"tx2023vyxl\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2,\"place\":\"2 km W of Guayanilla, Puerto Rico\",\"time\":1699527492410,\"updated\":1699531205750,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/pr71431118\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/pr71431118.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":62,\"net\":\"pr\",\"code\":\"71431118\",\"ids\":\",pr71431118,\",\"sources\":\",pr,\",\"types\":\",origin,phase-data,\",\"nst\":9,\"dmin\":0.07531,\"rms\":0.13,\"gap\":151,\"magType\":\"md\",\"type\":\"earthquake\",\"title\":\"M 2.0 - 2 km W of Guayanilla, Puerto Rico\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-66.8158333333333,18.0203333333333,18.28]},\"id\":\"pr71431118\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.6,\"place\":\"55 km WNW of Toyah, Texas\",\"time\":1699527449442,\"updated\":1699529648303,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vywq\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vywq.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":39,\"net\":\"tx\",\"code\":\"2023vywq\",\"ids\":\",tx2023vywq,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":16,\"dmin\":0.1,\"rms\":1,\"gap\":75,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 1.6 - 55 km WNW of Toyah, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.28,31.594,0.6259]},\"id\":\"tx2023vywq\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.32,\"place\":\"6 km NW of The Geysers, CA\",\"time\":1699527131000,\"updated\":1699536010795,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960406\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960406.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":27,\"net\":\"nc\",\"code\":\"73960406\",\"ids\":\",nc73960406,\",\"sources\":\",nc,\",\"types\":\",nearby-cities,origin,phase-data,scitech-link,\",\"nst\":22,\"dmin\":0.01265,\"rms\":0.02,\"gap\":31,\"magType\":\"md\",\"type\":\"earthquake\",\"title\":\"M 1.3 - 6 km NW of The Geysers, CA\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-122.7988358,38.8203316,0.59]},\"id\":\"nc73960406\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":4.7,\"place\":\"Solomon Islands\",\"time\":1699526362419,\"updated\":1699529391040,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9tf\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9tf.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":340,\"net\":\"us\",\"code\":\"7000l9tf\",\"ids\":\",us7000l9tf,\",\"sources\":\",us,\",\"types\":\",origin,phase-data,\",\"nst\":30,\"dmin\":2.947,\"rms\":0.82,\"gap\":138,\"magType\":\"mb\",\"type\":\"earthquake\",\"title\":\"M 4.7 - Solomon Islands\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[162.2896,-11.2877,10]},\"id\":\"us7000l9tf\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.8,\"place\":\"8 km NW of Bridgeport, California\",\"time\":1699525521181,\"updated\":1699551384417,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/nn00868657\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nn00868657.geojson\",\"felt\":1,\"cdi\":2.7,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":121,\"net\":\"nn\",\"code\":\"00868657\",\"ids\":\",nn00868657,\",\"sources\":\",nn,\",\"types\":\",dyfi,origin,phase-data,\",\"nst\":7,\"dmin\":0.23,\"rms\":0.1722,\"gap\":145.66,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 2.8 - 8 km NW of Bridgeport, California\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-119.3044,38.2996,8.5]},\"id\":\"nn00868657\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.59,\"place\":\"8 km NW of Bridgeport, CA\",\"time\":1699525502270,\"updated\":1699551410825,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960401\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960401.geojson\",\"felt\":0,\"cdi\":1,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":103,\"net\":\"nc\",\"code\":\"73960401\",\"ids\":\",nn00868642,nc73960401,\",\"sources\":\",nn,nc,\",\"types\":\",dyfi,focal-mechanism,nearby-cities,origin,phase-data,scitech-link,\",\"nst\":47,\"dmin\":0.2278,\"rms\":0.15,\"gap\":115,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 2.6 - 8 km NW of Bridgeport, CA\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-119.31,38.2996667,5.57]},\"id\":\"nc73960401\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2,\"place\":\"40 km W of Mentone, Texas\",\"time\":1699525233123,\"updated\":1699526929944,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyvl\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyvl.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":62,\"net\":\"tx\",\"code\":\"2023vyvl\",\"ids\":\",tx2023vyvl,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":20,\"dmin\":0,\"rms\":0.2,\"gap\":51,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 2.0 - 40 km W of Mentone, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.023,31.649,4.8543]},\"id\":\"tx2023vyvl\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":4.6,\"place\":\"Banda Sea\",\"time\":1699524776695,\"updated\":1699529976040,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9tb\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9tb.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":326,\"net\":\"us\",\"code\":\"7000l9tb\",\"ids\":\",us7000l9tb,\",\"sources\":\",us,\",\"types\":\",origin,phase-data,\",\"nst\":37,\"dmin\":4.382,\"rms\":0.91,\"gap\":92,\"magType\":\"mb\",\"type\":\"earthquake\",\"title\":\"M 4.6 - Banda Sea\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[129.501,-6.3633,10]},\"id\":\"us7000l9tb\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.05,\"place\":\"6 km NW of The Geysers, CA\",\"time\":1699523783020,\"updated\":1699525093778,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960391\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960391.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":17,\"net\":\"nc\",\"code\":\"73960391\",\"ids\":\",nc73960391,\",\"sources\":\",nc,\",\"types\":\",nearby-cities,origin,phase-data,scitech-link,\",\"nst\":24,\"dmin\":0.004774,\"rms\":0.02,\"gap\":37,\"magType\":\"md\",\"type\":\"earthquake\",\"title\":\"M 1.1 - 6 km NW of The Geysers, CA\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-122.8059998,38.8198318,2.63]},\"id\":\"nc73960391\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2,\"place\":\"61 km ESE of Denali Park, Alaska\",\"time\":1699522922371,\"updated\":1699523042897,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/ak023edstj9p\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023edstj9p.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":62,\"net\":\"ak\",\"code\":\"023edstj9p\",\"ids\":\",ak023edstj9p,\",\"sources\":\",ak,\",\"types\":\",origin,phase-data,\",\"nst\":null,\"dmin\":null,\"rms\":0.71,\"gap\":null,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 2.0 - 61 km ESE of Denali Park, Alaska\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-147.7193,63.5852,0]},\"id\":\"ak023edstj9p\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2,\"place\":\"66 km SSW of Whites City, New Mexico\",\"time\":1699522836410,\"updated\":1699525123100,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyub\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyub.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":62,\"net\":\"tx\",\"code\":\"2023vyub\",\"ids\":\",tx2023vyub,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":18,\"dmin\":0.1,\"rms\":0.4,\"gap\":162,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 2.0 - 66 km SSW of Whites City, New Mexico\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.546,31.597,3.3162]},\"id\":\"tx2023vyub\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.68,\"place\":\"1 km ENE of The Geysers, CA\",\"time\":1699522755010,\"updated\":1699533490553,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960376\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960376.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":43,\"net\":\"nc\",\"code\":\"73960376\",\"ids\":\",nc73960376,\",\"sources\":\",nc,\",\"types\":\",nearby-cities,origin,phase-data,scitech-link,\",\"nst\":24,\"dmin\":0.007337,\"rms\":0.04,\"gap\":75,\"magType\":\"md\",\"type\":\"earthquake\",\"title\":\"M 1.7 - 1 km ENE of The Geysers, CA\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-122.7456665,38.7809982,0.29]},\"id\":\"nc73960376\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.21,\"place\":\"14 km NW of Pawnee, Oklahoma\",\"time\":1699522553619,\"updated\":1699536831132,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/ok2023vyty\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ok2023vyty.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":23,\"net\":\"ok\",\"code\":\"2023vyty\",\"ids\":\",ok2023vyty,\",\"sources\":\",ok,\",\"types\":\",origin,phase-data,\",\"nst\":55,\"dmin\":0.02699470272,\"rms\":0.27,\"gap\":63,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 1.2 - 14 km NW of Pawnee, Oklahoma\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-96.92633333,36.42883333,5.82]},\"id\":\"ok2023vyty\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.22,\"place\":\"12 km ENE of Healdsburg, CA\",\"time\":1699521519650,\"updated\":1699531810411,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960371\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960371.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":23,\"net\":\"nc\",\"code\":\"73960371\",\"ids\":\",nc73960371,\",\"sources\":\",nc,\",\"types\":\",nearby-cities,origin,phase-data,scitech-link,\",\"nst\":8,\"dmin\":0.0666,\"rms\":0.02,\"gap\":170,\"magType\":\"md\",\"type\":\"earthquake\",\"title\":\"M 1.2 - 12 km ENE of Healdsburg, CA\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-122.7360001,38.6418343,7.61]},\"id\":\"nc73960371\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.9,\"place\":\"39 km W of Mentone, Texas\",\"time\":1699521298476,\"updated\":1699522590145,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vytg\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vytg.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":56,\"net\":\"tx\",\"code\":\"2023vytg\",\"ids\":\",tx2023vytg,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":14,\"dmin\":0,\"rms\":0.2,\"gap\":81,\"magType\":\"mlv\",\"type\":\"earthquake\",\"title\":\"M 1.9 - 39 km W of Mentone, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.012,31.638,5.6419]},\"id\":\"tx2023vytg\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":5.1,\"place\":\"282 km SSE of Amahai, Indonesia\",\"time\":1699521257574,\"updated\":1699522340040,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9t5\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9t5.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":400,\"net\":\"us\",\"code\":\"7000l9t5\",\"ids\":\",us7000l9t5,\",\"sources\":\",us,\",\"types\":\",origin,phase-data,\",\"nst\":65,\"dmin\":6.889,\"rms\":0.69,\"gap\":46,\"magType\":\"mww\",\"type\":\"earthquake\",\"title\":\"M 5.1 - 282 km SSE of Amahai, Indonesia\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[130.1774,-5.5615,10]},\"id\":\"us7000l9t5\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.2,\"place\":\"52 km NW of Toyah, Texas\",\"time\":1699520728701,\"updated\":1699522678069,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vysy\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vysy.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":74,\"net\":\"tx\",\"code\":\"2023vysy\",\"ids\":\",tx2023vysy,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":16,\"dmin\":0.1,\"rms\":0.2,\"gap\":77,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 2.2 - 52 km NW of Toyah, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.25,31.581,4.1076]},\"id\":\"tx2023vysy\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.18,\"place\":\"31 km N of Beaver, Utah\",\"time\":1699520490720,\"updated\":1699540478360,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/uu60033944\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/uu60033944.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":21,\"net\":\"uu\",\"code\":\"60033944\",\"ids\":\",uu60033944,\",\"sources\":\",uu,\",\"types\":\",origin,phase-data,\",\"nst\":10,\"dmin\":0.09396,\"rms\":0.05,\"gap\":155,\"magType\":\"md\",\"type\":\"earthquake\",\"title\":\"M 1.2 - 31 km N of Beaver, Utah\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-112.6983333,38.5526667,7.3]},\"id\":\"uu60033944\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.54,\"place\":\"Utah\",\"time\":1699520478260,\"updated\":1699540290880,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/uu60553892\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/uu60553892.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":36,\"net\":\"uu\",\"code\":\"60553892\",\"ids\":\",uu60553892,\",\"sources\":\",uu,\",\"types\":\",origin,phase-data,\",\"nst\":22,\"dmin\":0.1057,\"rms\":0.16,\"gap\":64,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 1.5 - Utah\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-112.6993333,38.5698333,8.82]},\"id\":\"uu60553892\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.5,\"place\":\"62 km S of Whites City, New Mexico\",\"time\":1699520210892,\"updated\":1699534433082,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vysp\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vysp.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":96,\"net\":\"tx\",\"code\":\"2023vysp\",\"ids\":\",tx2023vysp,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":30,\"dmin\":0,\"rms\":0.2,\"gap\":75,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 2.5 - 62 km S of Whites City, New Mexico\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.497,31.621,7.3141]},\"id\":\"tx2023vysp\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.9,\"place\":\"Southern Alaska\",\"time\":1699520073785,\"updated\":1699520200659,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/ak023edsaqoi\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023edsaqoi.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":56,\"net\":\"ak\",\"code\":\"023edsaqoi\",\"ids\":\",ak023edsaqoi,\",\"sources\":\",ak,\",\"types\":\",origin,phase-data,\",\"nst\":null,\"dmin\":null,\"rms\":0.25,\"gap\":null,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 1.9 - Southern Alaska\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-150.6645,61.4013,52.4]},\"id\":\"ak023edsaqoi\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.07,\"place\":\"6 km NW of The Geysers, CA\",\"time\":1699519613460,\"updated\":1699521974485,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960361\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960361.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":18,\"net\":\"nc\",\"code\":\"73960361\",\"ids\":\",nc73960361,\",\"sources\":\",nc,\",\"types\":\",nearby-cities,origin,phase-data,scitech-link,\",\"nst\":22,\"dmin\":0.0044,\"rms\":0.02,\"gap\":39,\"magType\":\"md\",\"type\":\"earthquake\",\"title\":\"M 1.1 - 6 km NW of The Geysers, CA\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-122.8046646,38.8221664,3.17]},\"id\":\"nc73960361\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.6,\"place\":\"western Texas\",\"time\":1699519220905,\"updated\":1699520370661,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vysd\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vysd.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":39,\"net\":\"tx\",\"code\":\"2023vysd\",\"ids\":\",tx2023vysd,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":13,\"dmin\":0,\"rms\":0.6,\"gap\":51,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 1.6 - western Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.007,31.638,7.7441]},\"id\":\"tx2023vysd\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":4.5,\"place\":\"Banda Sea\",\"time\":1699519007517,\"updated\":1699524755040,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9sy\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9sy.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":312,\"net\":\"us\",\"code\":\"7000l9sy\",\"ids\":\",us7000l9sy,\",\"sources\":\",us,\",\"types\":\",origin,phase-data,\",\"nst\":17,\"dmin\":9.275,\"rms\":0.61,\"gap\":68,\"magType\":\"mb\",\"type\":\"earthquake\",\"title\":\"M 4.5 - Banda Sea\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[129.8777,-6.4094,10]},\"id\":\"us7000l9sy\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.6,\"place\":\"40 km W of Mentone, Texas\",\"time\":1699518754200,\"updated\":1699520362560,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyrw\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyrw.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":39,\"net\":\"tx\",\"code\":\"2023vyrw\",\"ids\":\",tx2023vyrw,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":19,\"dmin\":0,\"rms\":0.5,\"gap\":43,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 1.6 - 40 km W of Mentone, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.015,31.643,4.7886]},\"id\":\"tx2023vyrw\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.6,\"place\":\"39 km W of Mentone, Texas\",\"time\":1699518647476,\"updated\":1699535240046,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyrv\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyrv.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":104,\"net\":\"tx\",\"code\":\"2023vyrv\",\"ids\":\",tx2023vyrv,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":15,\"dmin\":0,\"rms\":0.1,\"gap\":42,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 2.6 - 39 km W of Mentone, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.01,31.638,5.1418]},\"id\":\"tx2023vyrv\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":4.4,\"place\":\"4 km SE of Chicacao, Guatemala\",\"time\":1699518223388,\"updated\":1699523766040,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9sw\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9sw.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":298,\"net\":\"us\",\"code\":\"7000l9sw\",\"ids\":\",us7000l9sw,\",\"sources\":\",us,\",\"types\":\",origin,phase-data,\",\"nst\":65,\"dmin\":1.591,\"rms\":0.67,\"gap\":181,\"magType\":\"mb\",\"type\":\"earthquake\",\"title\":\"M 4.4 - 4 km SE of Chicacao, Guatemala\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-91.2969,14.5122,11.921]},\"id\":\"us7000l9sw\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.6,\"place\":\"45 km NW of Toyah, Texas\",\"time\":1699517106377,\"updated\":1699520350684,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyqz\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyqz.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":39,\"net\":\"tx\",\"code\":\"2023vyqz\",\"ids\":\",tx2023vyqz,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":19,\"dmin\":0,\"rms\":0.4,\"gap\":57,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 1.6 - 45 km NW of Toyah, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.115,31.614,3.939]},\"id\":\"tx2023vyqz\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.6,\"place\":\"western Texas\",\"time\":1699516779628,\"updated\":1699542955423,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyqu\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyqu.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":104,\"net\":\"tx\",\"code\":\"2023vyqu\",\"ids\":\",tx2023vyqu,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":15,\"dmin\":0,\"rms\":0.1,\"gap\":43,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 2.6 - western Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.013,31.65,4.4863]},\"id\":\"tx2023vyqu\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2,\"place\":\"37 km WSW of Mentone, Texas\",\"time\":1699516703855,\"updated\":1699518389934,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyqt\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyqt.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":62,\"net\":\"tx\",\"code\":\"2023vyqt\",\"ids\":\",tx2023vyqt,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":16,\"dmin\":0,\"rms\":0.2,\"gap\":46,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 2.0 - 37 km WSW of Mentone, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-103.989,31.634,5.7471]},\"id\":\"tx2023vyqt\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.2,\"place\":\"37 km WSW of Mentone, Texas\",\"time\":1699516572955,\"updated\":1699535443164,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyqr\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyqr.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":74,\"net\":\"tx\",\"code\":\"2023vyqr\",\"ids\":\",tx2023vyqr,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":15,\"dmin\":0,\"rms\":0.1,\"gap\":46,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 2.2 - 37 km WSW of Mentone, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-103.99,31.633,5.0775]},\"id\":\"tx2023vyqr\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.9,\"place\":\"44 km WNW of Ninilchik, Alaska\",\"time\":1699516148030,\"updated\":1699516302655,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/ak023edro6ib\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023edro6ib.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":56,\"net\":\"ak\",\"code\":\"023edro6ib\",\"ids\":\",ak023edro6ib,\",\"sources\":\",ak,\",\"types\":\",origin,phase-data,\",\"nst\":null,\"dmin\":null,\"rms\":0.23,\"gap\":null,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 1.9 - 44 km WNW of Ninilchik, Alaska\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-152.4429,60.1347,88.9]},\"id\":\"ak023edro6ib\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":3,\"place\":\"41 km W of Mentone, Texas\",\"time\":1699516015994,\"updated\":1699517219040,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyqk\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyqk.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":138,\"net\":\"tx\",\"code\":\"2023vyqk\",\"ids\":\",us7000l9sp,tx2023vyqk,\",\"sources\":\",us,tx,\",\"types\":\",origin,phase-data,\",\"nst\":15,\"dmin\":0,\"rms\":0.2,\"gap\":80,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 3.0 - 41 km W of Mentone, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.026,31.641,6.1316]},\"id\":\"tx2023vyqk\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.8,\"place\":\"37 km W of Mentone, Texas\",\"time\":1699515848845,\"updated\":1699517967298,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyqj\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyqj.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":50,\"net\":\"tx\",\"code\":\"2023vyqj\",\"ids\":\",tx2023vyqj,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":20,\"dmin\":0,\"rms\":0.3,\"gap\":49,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 1.8 - 37 km W of Mentone, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-103.986,31.64,4.4907]},\"id\":\"tx2023vyqj\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.6,\"place\":\"38 km W of Mentone, Texas\",\"time\":1699515640399,\"updated\":1699518370548,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyqe\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyqe.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":39,\"net\":\"tx\",\"code\":\"2023vyqe\",\"ids\":\",tx2023vyqe,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":14,\"dmin\":0,\"rms\":0.8,\"gap\":88,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 1.6 - 38 km W of Mentone, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104,31.642,8.2456]},\"id\":\"tx2023vyqe\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":4.3,\"place\":\"Banda Sea\",\"time\":1699515368452,\"updated\":1699521555040,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9sz\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9sz.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":284,\"net\":\"us\",\"code\":\"7000l9sz\",\"ids\":\",us7000l9sz,\",\"sources\":\",us,\",\"types\":\",origin,phase-data,\",\"nst\":18,\"dmin\":1.923,\"rms\":0.71,\"gap\":86,\"magType\":\"mb\",\"type\":\"earthquake\",\"title\":\"M 4.3 - Banda Sea\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[129.5507,-6.4253,10]},\"id\":\"us7000l9sz\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.5,\"place\":\"western Texas\",\"time\":1699514872100,\"updated\":1699517801250,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vypu\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vypu.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":35,\"net\":\"tx\",\"code\":\"2023vypu\",\"ids\":\",tx2023vypu,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":11,\"dmin\":0.1,\"rms\":0.2,\"gap\":67,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 1.5 - western Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-103.98,31.593,4.7695]},\"id\":\"tx2023vypu\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.26,\"place\":\"2 km NNE of Bear Dance, Montana\",\"time\":1699513945500,\"updated\":1699539858990,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/mb90032413\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/mb90032413.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":24,\"net\":\"mb\",\"code\":\"90032413\",\"ids\":\",mb90032413,\",\"sources\":\",mb,\",\"types\":\",origin,phase-data,\",\"nst\":12,\"dmin\":0.2601,\"rms\":0.07,\"gap\":198,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 1.3 - 2 km NNE of Bear Dance, Montana\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-114.021666666667,47.9383333333333,5.06]},\"id\":\"mb90032413\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.81,\"place\":\"17 km ENE of Thousand Palms, CA\",\"time\":1699513927790,\"updated\":1699539408190,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/ci39710666\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ci39710666.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":50,\"net\":\"ci\",\"code\":\"39710666\",\"ids\":\",ci39710666,\",\"sources\":\",ci,\",\"types\":\",focal-mechanism,nearby-cities,origin,phase-data,scitech-link,\",\"nst\":65,\"dmin\":0.0582,\"rms\":0.13,\"gap\":48,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 1.8 - 17 km ENE of Thousand Palms, CA\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-116.2318333,33.8933333,6.9]},\"id\":\"ci39710666\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.6,\"place\":\"Southern Alaska\",\"time\":1699513811313,\"updated\":1699514060386,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/ak023edrftu7\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023edrftu7.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":104,\"net\":\"ak\",\"code\":\"023edrftu7\",\"ids\":\",ak023edrftu7,\",\"sources\":\",ak,\",\"types\":\",origin,phase-data,\",\"nst\":null,\"dmin\":null,\"rms\":0.34,\"gap\":null,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 2.6 - Southern Alaska\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-153.4325,60.0018,140.2]},\"id\":\"ak023edrftu7\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.7,\"place\":\"western Texas\",\"time\":1699513613102,\"updated\":1699514936916,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vypc\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vypc.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":44,\"net\":\"tx\",\"code\":\"2023vypc\",\"ids\":\",tx2023vypc,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":14,\"dmin\":0.1,\"rms\":0.2,\"gap\":187,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 1.7 - western Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.579,31.654,3.4025]},\"id\":\"tx2023vypc\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.42,\"place\":\"2 km SW of Pāhala, Hawaii\",\"time\":1699512491770,\"updated\":1699513874040,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/hv73642132\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/hv73642132.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":90,\"net\":\"hv\",\"code\":\"73642132\",\"ids\":\",us7000l9si,hv73642132,\",\"sources\":\",us,hv,\",\"types\":\",origin,phase-data,\",\"nst\":50,\"dmin\":null,\"rms\":0.109999999,\"gap\":131,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 2.4 - 2 km SW of Pāhala, Hawaii\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-155.501998901367,19.1879997253418,32.7000007629395]},\"id\":\"hv73642132\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":4.4,\"place\":\"143 km E of Miyako, Japan\",\"time\":1699510438937,\"updated\":1699513220040,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9sh\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9sh.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":298,\"net\":\"us\",\"code\":\"7000l9sh\",\"ids\":\",us7000l9sh,\",\"sources\":\",us,\",\"types\":\",origin,phase-data,\",\"nst\":30,\"dmin\":2.195,\"rms\":0.49,\"gap\":115,\"magType\":\"mb\",\"type\":\"earthquake\",\"title\":\"M 4.4 - 143 km E of Miyako, Japan\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[143.6088,39.7733,10]},\"id\":\"us7000l9sh\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.6,\"place\":\"western Texas\",\"time\":1699509936731,\"updated\":1699537102707,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyna\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyna.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":39,\"net\":\"tx\",\"code\":\"2023vyna\",\"ids\":\",tx2023vyna,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":12,\"dmin\":0,\"rms\":0.1,\"gap\":65,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 1.6 - western Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-103.993,31.634,5.6174]},\"id\":\"tx2023vyna\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.5,\"place\":\"20 km ESE of Schurz, Nevada\",\"time\":1699509657132,\"updated\":1699549079466,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/nn00868639\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nn00868639.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":35,\"net\":\"nn\",\"code\":\"00868639\",\"ids\":\",nn00868639,\",\"sources\":\",nn,\",\"types\":\",origin,phase-data,\",\"nst\":11,\"dmin\":0.411,\"rms\":0.1416,\"gap\":65.14,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 1.5 - 20 km ESE of Schurz, Nevada\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-118.5879,38.8893,7.3]},\"id\":\"nn00868639\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.1,\"place\":\"19 km ESE of Schurz, Nevada\",\"time\":1699508884925,\"updated\":1699549273208,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/nn00868636\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nn00868636.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":68,\"net\":\"nn\",\"code\":\"00868636\",\"ids\":\",nn00868636,\",\"sources\":\",nn,\",\"types\":\",origin,phase-data,\",\"nst\":14,\"dmin\":0.412,\"rms\":0.1079,\"gap\":57.62,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 2.1 - 19 km ESE of Schurz, Nevada\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-118.5934,38.8967,9.5]},\"id\":\"nn00868636\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2,\"place\":\"western Texas\",\"time\":1699508572919,\"updated\":1699510921373,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyme\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyme.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":62,\"net\":\"tx\",\"code\":\"2023vyme\",\"ids\":\",tx2023vyme,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":12,\"dmin\":0,\"rms\":0.4,\"gap\":97,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 2.0 - western Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.001,31.621,3.8232]},\"id\":\"tx2023vyme\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.7,\"place\":\"53 km NW of Toyah, Texas\",\"time\":1699508529502,\"updated\":1699531918203,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vymg\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vymg.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":112,\"net\":\"tx\",\"code\":\"2023vymg\",\"ids\":\",us7000l9sb,tx2023vymg,\",\"sources\":\",us,tx,\",\"types\":\",origin,phase-data,\",\"nst\":30,\"dmin\":0.1,\"rms\":0.1,\"gap\":67,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 2.7 - 53 km NW of Toyah, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.184,31.668,6.4015]},\"id\":\"tx2023vymg\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.2,\"place\":\"56 km S of Whites City, New Mexico\",\"time\":1699508438306,\"updated\":1699533641499,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vymf\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vymf.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":74,\"net\":\"tx\",\"code\":\"2023vymf\",\"ids\":\",tx2023vymf,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":15,\"dmin\":0,\"rms\":0.4,\"gap\":74,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 2.2 - 56 km S of Whites City, New Mexico\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.424,31.667,8.6252]},\"id\":\"tx2023vymf\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.5,\"place\":\"41 km W of Mentone, Texas\",\"time\":1699508104952,\"updated\":1699510917532,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyma\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyma.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":35,\"net\":\"tx\",\"code\":\"2023vyma\",\"ids\":\",tx2023vyma,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":14,\"dmin\":0,\"rms\":0.2,\"gap\":74,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 1.5 - 41 km W of Mentone, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.025,31.634,4.5579]},\"id\":\"tx2023vyma\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.1,\"place\":\"Kodiak Island region, Alaska\",\"time\":1699507036979,\"updated\":1699507174812,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/ak023edqah2p\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023edqah2p.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":68,\"net\":\"ak\",\"code\":\"023edqah2p\",\"ids\":\",ak023edqah2p,\",\"sources\":\",ak,\",\"types\":\",origin,phase-data,\",\"nst\":null,\"dmin\":null,\"rms\":0.15,\"gap\":null,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 2.1 - Kodiak Island region, Alaska\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-153.979,58.6447,95.5]},\"id\":\"ak023edqah2p\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.84,\"place\":\"6 km SSW of Perryville, Arkansas\",\"time\":1699506444610,\"updated\":1699538456680,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/nm60501283\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nm60501283.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":52,\"net\":\"nm\",\"code\":\"60501283\",\"ids\":\",nm60501283,\",\"sources\":\",nm,\",\"types\":\",origin,phase-data,\",\"nst\":10,\"dmin\":0.4476,\"rms\":0.27,\"gap\":90,\"magType\":\"md\",\"type\":\"earthquake\",\"title\":\"M 1.8 - 6 km SSW of Perryville, Arkansas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-92.8431666666667,34.952,8.02]},\"id\":\"nm60501283\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.1,\"place\":\"54 km NW of Toyah, Texas\",\"time\":1699506225157,\"updated\":1699508490429,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vylc\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vylc.geojson\",\"felt\":1,\"cdi\":2.7,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":68,\"net\":\"tx\",\"code\":\"2023vylc\",\"ids\":\",tx2023vylc,\",\"sources\":\",tx,\",\"types\":\",dyfi,origin,phase-data,\",\"nst\":18,\"dmin\":0.1,\"rms\":0.2,\"gap\":69,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 2.1 - 54 km NW of Toyah, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.191,31.663,4.4283]},\"id\":\"tx2023vylc\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.7,\"place\":\"54 km NW of Toyah, Texas\",\"time\":1699505900866,\"updated\":1699523729091,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyku\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyku.geojson\",\"felt\":1,\"cdi\":4.2,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":113,\"net\":\"tx\",\"code\":\"2023vyku\",\"ids\":\",us7000l9s5,tx2023vyku,\",\"sources\":\",us,tx,\",\"types\":\",dyfi,origin,phase-data,\",\"nst\":19,\"dmin\":0.1,\"rms\":0.2,\"gap\":58,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 2.7 - 54 km NW of Toyah, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.184,31.669,6.1316]},\"id\":\"tx2023vyku\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.39,\"place\":\"14 km SSE of Guánica, Puerto Rico\",\"time\":1699505814970,\"updated\":1699506316610,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/pr71431113\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/pr71431113.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":88,\"net\":\"pr\",\"code\":\"71431113\",\"ids\":\",pr71431113,\",\"sources\":\",pr,\",\"types\":\",origin,phase-data,\",\"nst\":7,\"dmin\":0.1231,\"rms\":0.08,\"gap\":251,\"magType\":\"md\",\"type\":\"earthquake\",\"title\":\"M 2.4 - 14 km SSE of Guánica, Puerto Rico\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-66.853,17.8538333333333,7.52]},\"id\":\"pr71431113\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.38,\"place\":\"3 km NNW of Cholame, CA\",\"time\":1699505013500,\"updated\":1699514591800,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960286\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960286.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":29,\"net\":\"nc\",\"code\":\"73960286\",\"ids\":\",nc73960286,\",\"sources\":\",nc,\",\"types\":\",nearby-cities,origin,phase-data,scitech-link,\",\"nst\":18,\"dmin\":0.04815,\"rms\":0.06,\"gap\":143,\"magType\":\"md\",\"type\":\"earthquake\",\"title\":\"M 1.4 - 3 km NNW of Cholame, CA\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-120.3041687,35.7498322,1.75]},\"id\":\"nc73960286\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.1,\"place\":\"59 km W of Nanwalek, Alaska\",\"time\":1699504730584,\"updated\":1699504847928,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/ak023edptm3m\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023edptm3m.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":68,\"net\":\"ak\",\"code\":\"023edptm3m\",\"ids\":\",ak023edptm3m,\",\"sources\":\",ak,\",\"types\":\",origin,phase-data,\",\"nst\":null,\"dmin\":null,\"rms\":0.13,\"gap\":null,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 2.1 - 59 km W of Nanwalek, Alaska\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-152.9681,59.3643,76.7]},\"id\":\"ak023edptm3m\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.9,\"place\":\"4 km NNW of Meadow Lakes, Alaska\",\"time\":1699503804711,\"updated\":1699503913105,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/ak023edpqcd3\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023edpqcd3.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":56,\"net\":\"ak\",\"code\":\"023edpqcd3\",\"ids\":\",ak023edpqcd3,\",\"sources\":\",ak,\",\"types\":\",origin,phase-data,\",\"nst\":null,\"dmin\":null,\"rms\":0.68,\"gap\":null,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 1.9 - 4 km NNW of Meadow Lakes, Alaska\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-149.6195,61.6654,35.7]},\"id\":\"ak023edpqcd3\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":4.3,\"place\":\"90 km W of San Antonio de los Cobres, Argentina\",\"time\":1699503203126,\"updated\":1699504680040,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9rl\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9rl.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":284,\"net\":\"us\",\"code\":\"7000l9rl\",\"ids\":\",us7000l9rl,\",\"sources\":\",us,\",\"types\":\",origin,phase-data,\",\"nst\":33,\"dmin\":1.645,\"rms\":0.74,\"gap\":70,\"magType\":\"mb\",\"type\":\"earthquake\",\"title\":\"M 4.3 - 90 km W of San Antonio de los Cobres, Argentina\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-67.2001,-24.3375,183.697]},\"id\":\"us7000l9rl\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.1,\"place\":\"39 km W of Mentone, Texas\",\"time\":1699503150555,\"updated\":1699532610825,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyji\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyji.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":68,\"net\":\"tx\",\"code\":\"2023vyji\",\"ids\":\",tx2023vyji,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":20,\"dmin\":0,\"rms\":0.2,\"gap\":58,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 2.1 - 39 km W of Mentone, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.01,31.636,6.0159]},\"id\":\"tx2023vyji\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.7,\"place\":\"38 km WSW of Mentone, Texas\",\"time\":1699501293390,\"updated\":1699504404061,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyil\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyil.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":44,\"net\":\"tx\",\"code\":\"2023vyil\",\"ids\":\",tx2023vyil,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":15,\"dmin\":0,\"rms\":0.2,\"gap\":82,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 1.7 - 38 km WSW of Mentone, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-103.996,31.629,5.9821]},\"id\":\"tx2023vyil\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.4,\"place\":\"37 km WNW of Beluga, Alaska\",\"time\":1699500850632,\"updated\":1699500988356,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/ak023edp78jr\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023edp78jr.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":30,\"net\":\"ak\",\"code\":\"023edp78jr\",\"ids\":\",ak023edp78jr,\",\"sources\":\",ak,\",\"types\":\",origin,phase-data,\",\"nst\":null,\"dmin\":null,\"rms\":0.66,\"gap\":null,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 1.4 - 37 km WNW of Beluga, Alaska\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-151.7181,61.2871,74.4]},\"id\":\"ak023edp78jr\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.19,\"place\":\"28 km SSW of Gustine, CA\",\"time\":1699500656950,\"updated\":1699544477609,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960281\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960281.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":22,\"net\":\"nc\",\"code\":\"73960281\",\"ids\":\",nc73960281,\",\"sources\":\",nc,\",\"types\":\",nearby-cities,origin,phase-data,scitech-link,\",\"nst\":34,\"dmin\":0.05094,\"rms\":0.13,\"gap\":75,\"magType\":\"md\",\"type\":\"earthquake\",\"title\":\"M 1.2 - 28 km SSW of Gustine, CA\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-121.1673333,37.0475,4.65]},\"id\":\"nc73960281\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.8,\"place\":\"western Texas\",\"time\":1699500649254,\"updated\":1699502370637,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyib\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyib.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":50,\"net\":\"tx\",\"code\":\"2023vyib\",\"ids\":\",tx2023vyib,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":20,\"dmin\":0,\"rms\":0.2,\"gap\":47,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 1.8 - western Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-103.999,31.638,4.9107]},\"id\":\"tx2023vyib\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.6,\"place\":\"western Texas\",\"time\":1699500311429,\"updated\":1699501975529,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyhw\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyhw.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":39,\"net\":\"tx\",\"code\":\"2023vyhw\",\"ids\":\",tx2023vyhw,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":15,\"dmin\":0,\"rms\":0.4,\"gap\":79,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 1.6 - western Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-103.975,31.619,5.3082]},\"id\":\"tx2023vyhw\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.7,\"place\":\"western Texas\",\"time\":1699500055395,\"updated\":1699549945404,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyhs\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyhs.geojson\",\"felt\":1,\"cdi\":2.9,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":45,\"net\":\"tx\",\"code\":\"2023vyhs\",\"ids\":\",tx2023vyhs,\",\"sources\":\",tx,\",\"types\":\",dyfi,origin,phase-data,\",\"nst\":16,\"dmin\":0,\"rms\":0.4,\"gap\":45,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 1.7 - western Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104,31.642,4.9693]},\"id\":\"tx2023vyhs\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.6,\"place\":\"53 km NW of Toyah, Texas\",\"time\":1699499571990,\"updated\":1699501278847,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyhl\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyhl.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":39,\"net\":\"tx\",\"code\":\"2023vyhl\",\"ids\":\",tx2023vyhl,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":14,\"dmin\":0.1,\"rms\":0.3,\"gap\":73,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 1.6 - 53 km NW of Toyah, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.194,31.648,4.5506]},\"id\":\"tx2023vyhl\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.17,\"place\":\"7 km WSW of Hurricane, Utah\",\"time\":1699499235480,\"updated\":1699548678370,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/uu60553882\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/uu60553882.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":21,\"net\":\"uu\",\"code\":\"60553882\",\"ids\":\",uu60553882,\",\"sources\":\",uu,\",\"types\":\",origin,phase-data,\",\"nst\":10,\"dmin\":0.1127,\"rms\":0.13,\"gap\":114,\"magType\":\"md\",\"type\":\"earthquake\",\"title\":\"M 1.2 - 7 km WSW of Hurricane, Utah\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-113.3703333,37.1505,15.4]},\"id\":\"uu60553882\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.49,\"place\":\"5 km NE of Westmorland, CA\",\"time\":1699498781070,\"updated\":1699540892280,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/ci39710602\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ci39710602.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":34,\"net\":\"ci\",\"code\":\"39710602\",\"ids\":\",ci39710602,\",\"sources\":\",ci,\",\"types\":\",nearby-cities,origin,phase-data,scitech-link,\",\"nst\":22,\"dmin\":0.02511,\"rms\":0.16,\"gap\":58,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 1.5 - 5 km NE of Westmorland, CA\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-115.5763333,33.063,7.37]},\"id\":\"ci39710602\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.5,\"place\":\"51 km SSE of Whites City, New Mexico\",\"time\":1699498688732,\"updated\":1699500184504,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vygv\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vygv.geojson\",\"felt\":1,\"cdi\":0,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":35,\"net\":\"tx\",\"code\":\"2023vygv\",\"ids\":\",tx2023vygv,\",\"sources\":\",tx,\",\"types\":\",dyfi,origin,phase-data,\",\"nst\":7,\"dmin\":0.1,\"rms\":0.7,\"gap\":105,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 1.5 - 51 km SSE of Whites City, New Mexico\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.228,31.725,19.2719]},\"id\":\"tx2023vygv\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.1,\"place\":\"24 km ESE of Susitna North, Alaska\",\"time\":1699498679104,\"updated\":1699498759198,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/ak023edoqu5i\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023edoqu5i.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":68,\"net\":\"ak\",\"code\":\"023edoqu5i\",\"ids\":\",ak023edoqu5i,\",\"sources\":\",ak,\",\"types\":\",origin,phase-data,\",\"nst\":null,\"dmin\":null,\"rms\":0.23,\"gap\":null,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 2.1 - 24 km ESE of Susitna North, Alaska\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-149.4329,62.0699,38]},\"id\":\"ak023edoqu5i\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":4.5,\"place\":\"4 km NNE of Vogar, Iceland\",\"time\":1699498622894,\"updated\":1699503193040,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9ri\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9ri.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":312,\"net\":\"us\",\"code\":\"7000l9ri\",\"ids\":\",us7000l9ri,\",\"sources\":\",us,\",\"types\":\",origin,phase-data,\",\"nst\":21,\"dmin\":0.856,\"rms\":1.26,\"gap\":101,\"magType\":\"mb\",\"type\":\"earthquake\",\"title\":\"M 4.5 - 4 km NNE of Vogar, Iceland\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-22.3655,64.0233,10]},\"id\":\"us7000l9ri\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.51,\"place\":\"5 km NE of Westmorland, CA\",\"time\":1699498602190,\"updated\":1699540032082,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/ci39710594\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ci39710594.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":35,\"net\":\"ci\",\"code\":\"39710594\",\"ids\":\",ci39710594,\",\"sources\":\",ci,\",\"types\":\",nearby-cities,origin,phase-data,scitech-link,\",\"nst\":21,\"dmin\":0.0281,\"rms\":0.19,\"gap\":58,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 1.5 - 5 km NE of Westmorland, CA\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-115.575,33.0658333,10.06]},\"id\":\"ci39710594\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":4.3,\"place\":\"7 km E of Vogar, Iceland\",\"time\":1699498596530,\"updated\":1699504749040,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9rj\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9rj.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":284,\"net\":\"us\",\"code\":\"7000l9rj\",\"ids\":\",us7000l9rj,\",\"sources\":\",us,\",\"types\":\",origin,phase-data,\",\"nst\":15,\"dmin\":0.867,\"rms\":0.98,\"gap\":121,\"magType\":\"mb\",\"type\":\"earthquake\",\"title\":\"M 4.3 - 7 km E of Vogar, Iceland\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-22.2277,63.9775,11.878]},\"id\":\"us7000l9rj\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.4,\"place\":\"56 km S of Whites City, New Mexico\",\"time\":1699497204689,\"updated\":1699535727681,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vygb\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vygb.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":89,\"net\":\"tx\",\"code\":\"2023vygb\",\"ids\":\",tx2023vygb,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":17,\"dmin\":0,\"rms\":0.2,\"gap\":67,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 2.4 - 56 km S of Whites City, New Mexico\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.31,31.67,6.1316]},\"id\":\"tx2023vygb\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":4.6,\"place\":\"Turkey-Syria border region\",\"time\":1699497134623,\"updated\":1699539447281,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9r5\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9r5.geojson\",\"felt\":8,\"cdi\":6.6,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":331,\"net\":\"us\",\"code\":\"7000l9r5\",\"ids\":\",us7000l9r5,\",\"sources\":\",us,\",\"types\":\",dyfi,moment-tensor,origin,phase-data,\",\"nst\":93,\"dmin\":0.836,\"rms\":0.98,\"gap\":51,\"magType\":\"mwr\",\"type\":\"earthquake\",\"title\":\"M 4.6 - Turkey-Syria border region\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[36.4369,36.6095,10]},\"id\":\"us7000l9r5\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.8,\"place\":\"37 km WSW of Mentone, Texas\",\"time\":1699496816186,\"updated\":1699499146641,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyfu\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyfu.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":50,\"net\":\"tx\",\"code\":\"2023vyfu\",\"ids\":\",tx2023vyfu,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":15,\"dmin\":0,\"rms\":0.2,\"gap\":61,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 1.8 - 37 km WSW of Mentone, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-103.978,31.619,5.7893]},\"id\":\"tx2023vyfu\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.1,\"place\":\"western Texas\",\"time\":1699496702580,\"updated\":1699499084885,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyfw\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyfw.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":68,\"net\":\"tx\",\"code\":\"2023vyfw\",\"ids\":\",tx2023vyfw,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":12,\"dmin\":0,\"rms\":0.3,\"gap\":119,\"magType\":\"mlv\",\"type\":\"earthquake\",\"title\":\"M 2.1 - western Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-103.988,31.647,4.7845]},\"id\":\"tx2023vyfw\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.6,\"place\":\"39 km W of Mentone, Texas\",\"time\":1699496398739,\"updated\":1699498087945,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyfo\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyfo.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":39,\"net\":\"tx\",\"code\":\"2023vyfo\",\"ids\":\",tx2023vyfo,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":21,\"dmin\":0,\"rms\":0.4,\"gap\":41,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 1.6 - 39 km W of Mentone, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.012,31.64,4.7964]},\"id\":\"tx2023vyfo\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.8,\"place\":\"36 km W of Mentone, Texas\",\"time\":1699494609928,\"updated\":1699497012834,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyer\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyer.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":50,\"net\":\"tx\",\"code\":\"2023vyer\",\"ids\":\",tx2023vyer,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":11,\"dmin\":0,\"rms\":0.5,\"gap\":100,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 1.8 - 36 km W of Mentone, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-103.981,31.642,7.9125]},\"id\":\"tx2023vyer\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.7,\"place\":\"52 km NW of Toyah, Texas\",\"time\":1699493942031,\"updated\":1699496119585,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyef\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyef.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":44,\"net\":\"tx\",\"code\":\"2023vyef\",\"ids\":\",tx2023vyef,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":18,\"dmin\":0.1,\"rms\":0.2,\"gap\":72,\"magType\":\"mlv\",\"type\":\"earthquake\",\"title\":\"M 1.7 - 52 km NW of Toyah, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.249,31.588,4.0227]},\"id\":\"tx2023vyef\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.64,\"place\":\"10 km NNE of Brooktrails, CA\",\"time\":1699493898830,\"updated\":1699548311978,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960246\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960246.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":41,\"net\":\"nc\",\"code\":\"73960246\",\"ids\":\",nc73960246,\",\"sources\":\",nc,\",\"types\":\",nearby-cities,origin,phase-data,scitech-link,\",\"nst\":21,\"dmin\":0.05343,\"rms\":0.09,\"gap\":55,\"magType\":\"md\",\"type\":\"earthquake\",\"title\":\"M 1.6 - 10 km NNE of Brooktrails, CA\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-123.3298333,39.5221667,9.07]},\"id\":\"nc73960246\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":4.5,\"place\":\"5 km NNE of Vogar, Iceland\",\"time\":1699493053880,\"updated\":1699495393897,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9r3\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9r3.geojson\",\"felt\":2,\"cdi\":3.4,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":312,\"net\":\"us\",\"code\":\"7000l9r3\",\"ids\":\",us7000l9r3,\",\"sources\":\",us,\",\"types\":\",dyfi,origin,phase-data,\",\"nst\":41,\"dmin\":0.845,\"rms\":0.87,\"gap\":121,\"magType\":\"mb\",\"type\":\"earthquake\",\"title\":\"M 4.5 - 5 km NNE of Vogar, Iceland\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-22.3378,64.0286,4.486]},\"id\":\"us7000l9r3\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":4.9,\"place\":\"147 km E of Hihifo, Tonga\",\"time\":1699491143396,\"updated\":1699492562040,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9r2\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9r2.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":369,\"net\":\"us\",\"code\":\"7000l9r2\",\"ids\":\",us7000l9r2,\",\"sources\":\",us,\",\"types\":\",origin,phase-data,\",\"nst\":45,\"dmin\":2.359,\"rms\":1.31,\"gap\":84,\"magType\":\"mb\",\"type\":\"earthquake\",\"title\":\"M 4.9 - 147 km E of Hihifo, Tonga\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-172.4349,-15.7715,10]},\"id\":\"us7000l9r2\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":4.6,\"place\":\"7 km SE of Vogar, Iceland\",\"time\":1699490768244,\"updated\":1699499264426,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9r1\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9r1.geojson\",\"felt\":7,\"cdi\":3.8,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":328,\"net\":\"us\",\"code\":\"7000l9r1\",\"ids\":\",us7000l9r1,\",\"sources\":\",us,\",\"types\":\",dyfi,origin,phase-data,\",\"nst\":41,\"dmin\":0.915,\"rms\":0.79,\"gap\":116,\"magType\":\"mb\",\"type\":\"earthquake\",\"title\":\"M 4.6 - 7 km SE of Vogar, Iceland\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-22.2736,63.9337,9.355]},\"id\":\"us7000l9r1\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.07,\"place\":\"13 km SE of Ocotillo, CA\",\"time\":1699490748990,\"updated\":1699492821590,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/ci39710570\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ci39710570.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":66,\"net\":\"ci\",\"code\":\"39710570\",\"ids\":\",ci39710570,\",\"sources\":\",ci,\",\"types\":\",focal-mechanism,nearby-cities,origin,phase-data,scitech-link,\",\"nst\":43,\"dmin\":0.0268,\"rms\":0.17,\"gap\":45,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 2.1 - 13 km SE of Ocotillo, CA\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-115.8911667,32.653,10.2]},\"id\":\"ci39710570\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.65,\"place\":\"21 km SW of Stella, Puerto Rico\",\"time\":1699489974180,\"updated\":1699490637760,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/pr71431108\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/pr71431108.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":108,\"net\":\"pr\",\"code\":\"71431108\",\"ids\":\",pr71431108,\",\"sources\":\",pr,\",\"types\":\",origin,phase-data,\",\"nst\":8,\"dmin\":0.2283,\"rms\":0.25,\"gap\":218,\"magType\":\"md\",\"type\":\"earthquake\",\"title\":\"M 2.7 - 21 km SW of Stella, Puerto Rico\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-67.384,18.1736666666667,23.64]},\"id\":\"pr71431108\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.27,\"place\":\"12 km ESE of Middletown, CA\",\"time\":1699489809270,\"updated\":1699505353021,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960236\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960236.geojson\",\"felt\":2,\"cdi\":2.2,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":80,\"net\":\"nc\",\"code\":\"73960236\",\"ids\":\",nc73960236,\",\"sources\":\",nc,\",\"types\":\",dyfi,focal-mechanism,nearby-cities,origin,phase-data,scitech-link,\",\"nst\":33,\"dmin\":0.0954,\"rms\":0.1,\"gap\":57,\"magType\":\"md\",\"type\":\"earthquake\",\"title\":\"M 2.3 - 12 km ESE of Middletown, CA\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-122.4948349,38.6911659,7.02]},\"id\":\"nc73960236\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.5,\"place\":\"5 km NNW of Boron, CA\",\"time\":1699488961290,\"updated\":1699492315884,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/ci39710554\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ci39710554.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":35,\"net\":\"ci\",\"code\":\"39710554\",\"ids\":\",ci39710554,\",\"sources\":\",ci,\",\"types\":\",nearby-cities,origin,phase-data,scitech-link,\",\"nst\":15,\"dmin\":0.1103,\"rms\":0.2,\"gap\":114,\"magType\":\"mh\",\"type\":\"quarry blast\",\"title\":\"M 1.5 Quarry Blast - 5 km NNW of Boron, CA\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-117.6615,35.0411667,-0.83]},\"id\":\"ci39710554\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":4.3,\"place\":\"11 km E of Vogar, Iceland\",\"time\":1699488795375,\"updated\":1699491823617,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9qw\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9qw.geojson\",\"felt\":0,\"cdi\":1,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":284,\"net\":\"us\",\"code\":\"7000l9qw\",\"ids\":\",us7000l9qw,\",\"sources\":\",us,\",\"types\":\",dyfi,origin,phase-data,\",\"nst\":23,\"dmin\":0.847,\"rms\":0.52,\"gap\":121,\"magType\":\"mb\",\"type\":\"earthquake\",\"title\":\"M 4.3 - 11 km E of Vogar, Iceland\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-22.1579,63.9843,10.229]},\"id\":\"us7000l9qw\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.7,\"place\":\"44 km NW of Toyah, Texas\",\"time\":1699487419075,\"updated\":1699489336152,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyaq\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyaq.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":44,\"net\":\"tx\",\"code\":\"2023vyaq\",\"ids\":\",tx2023vyaq,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":18,\"dmin\":0,\"rms\":0.3,\"gap\":72,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 1.7 - 44 km NW of Toyah, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.1,31.615,4.4847]},\"id\":\"tx2023vyaq\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":4.7,\"place\":\"26 km SSE of San Pedro de Atacama, Chile\",\"time\":1699487150246,\"updated\":1699487860040,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9qs\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9qs.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":340,\"net\":\"us\",\"code\":\"7000l9qs\",\"ids\":\",us7000l9qs,\",\"sources\":\",us,\",\"types\":\",origin,phase-data,\",\"nst\":39,\"dmin\":0.192,\"rms\":0.66,\"gap\":83,\"magType\":\"mb\",\"type\":\"earthquake\",\"title\":\"M 4.7 - 26 km SSE of San Pedro de Atacama, Chile\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-68.1174,-23.1364,134.8]},\"id\":\"us7000l9qs\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.3599999,\"place\":\"31 km SW of Laupāhoehoe, Hawaii\",\"time\":1699487085830,\"updated\":1699487286490,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/hv73641997\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/hv73641997.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":86,\"net\":\"hv\",\"code\":\"73641997\",\"ids\":\",hv73641997,\",\"sources\":\",hv,\",\"types\":\",origin,phase-data,\",\"nst\":41,\"dmin\":null,\"rms\":0.140000001,\"gap\":139,\"magType\":\"md\",\"type\":\"earthquake\",\"title\":\"M 2.4 - 31 km SW of Laupāhoehoe, Hawaii\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-155.408996582031,19.7481670379639,21.3500003814697]},\"id\":\"hv73641997\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.7,\"place\":\"39 km W of Mentone, Texas\",\"time\":1699486993997,\"updated\":1699489469028,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyal\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyal.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":44,\"net\":\"tx\",\"code\":\"2023vyal\",\"ids\":\",tx2023vyal,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":8,\"dmin\":0,\"rms\":0.1,\"gap\":103,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 1.7 - 39 km W of Mentone, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.008,31.647,4.9476]},\"id\":\"tx2023vyal\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.1,\"place\":\"western Texas\",\"time\":1699486945136,\"updated\":1699489316045,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vyaj\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vyaj.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":68,\"net\":\"tx\",\"code\":\"2023vyaj\",\"ids\":\",tx2023vyaj,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":17,\"dmin\":0,\"rms\":0.5,\"gap\":58,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 2.1 - western Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.111,31.619,4.3029]},\"id\":\"tx2023vyaj\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.15,\"place\":\"33 km NW of Stanley, Idaho\",\"time\":1699486598400,\"updated\":1699540372660,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/mb90032408\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/mb90032408.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":71,\"net\":\"mb\",\"code\":\"90032408\",\"ids\":\",mb90032408,\",\"sources\":\",mb,\",\"types\":\",origin,phase-data,\",\"nst\":12,\"dmin\":0.4305,\"rms\":0.09,\"gap\":73,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 2.2 - 33 km NW of Stanley, Idaho\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-115.204666666667,44.4515,5.38]},\"id\":\"mb90032408\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.8,\"place\":\"74 km NNW of Ambler, Alaska\",\"time\":1699486079389,\"updated\":1699487156040,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9qh\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9qh.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":121,\"net\":\"us\",\"code\":\"7000l9qh\",\"ids\":\",us7000l9qh,\",\"sources\":\",us,\",\"types\":\",origin,phase-data,\",\"nst\":25,\"dmin\":0.54,\"rms\":0.59,\"gap\":60,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 2.8 - 74 km NNW of Ambler, Alaska\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-158.4688,67.7142,5.507]},\"id\":\"us7000l9qh\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.65,\"place\":\"4 km ESE of Adjuntas, Puerto Rico\",\"time\":1699486011760,\"updated\":1699487127280,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/pr71431093\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/pr71431093.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":108,\"net\":\"pr\",\"code\":\"71431093\",\"ids\":\",pr71431093,\",\"sources\":\",pr,\",\"types\":\",origin,phase-data,\",\"nst\":21,\"dmin\":0.1076,\"rms\":0.14,\"gap\":82,\"magType\":\"md\",\"type\":\"earthquake\",\"title\":\"M 2.7 - 4 km ESE of Adjuntas, Puerto Rico\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-66.6828333333333,18.1506666666667,18.7]},\"id\":\"pr71431093\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.55,\"place\":\"13 km SSW of Searles Valley, CA\",\"time\":1699484386020,\"updated\":1699492148544,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/ci39710546\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ci39710546.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":37,\"net\":\"ci\",\"code\":\"39710546\",\"ids\":\",ci39710546,\",\"sources\":\",ci,\",\"types\":\",nearby-cities,origin,phase-data,scitech-link,\",\"nst\":33,\"dmin\":0.01922,\"rms\":0.15,\"gap\":67,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 1.6 - 13 km SSW of Searles Valley, CA\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-117.4393333,35.6506667,9.52]},\"id\":\"ci39710546\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":4.4,\"place\":\"93 km WNW of Yomitan, Japan\",\"time\":1699484033548,\"updated\":1699485685040,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9qc\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9qc.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":298,\"net\":\"us\",\"code\":\"7000l9qc\",\"ids\":\",us7000l9qc,\",\"sources\":\",us,\",\"types\":\",origin,phase-data,\",\"nst\":31,\"dmin\":1.31,\"rms\":1.19,\"gap\":127,\"magType\":\"mb\",\"type\":\"earthquake\",\"title\":\"M 4.4 - 93 km WNW of Yomitan, Japan\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[126.8343,26.586,132.919]},\"id\":\"us7000l9qc\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.7,\"place\":\"30 km ESE of Susitna North, Alaska\",\"time\":1699483826551,\"updated\":1699483962656,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/ak023ecd2l8f\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023ecd2l8f.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":44,\"net\":\"ak\",\"code\":\"023ecd2l8f\",\"ids\":\",ak023ecd2l8f,\",\"sources\":\",ak,\",\"types\":\",origin,phase-data,\",\"nst\":null,\"dmin\":null,\"rms\":1.07,\"gap\":null,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 1.7 - 30 km ESE of Susitna North, Alaska\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-149.2955,62.0755,19.2]},\"id\":\"ak023ecd2l8f\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.38,\"place\":\"12 km N of Kenefic, Oklahoma\",\"time\":1699483416219,\"updated\":1699537885699,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/ok2023vxyl\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ok2023vxyl.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":29,\"net\":\"ok\",\"code\":\"2023vxyl\",\"ids\":\",ok2023vxyl,\",\"sources\":\",ok,\",\"types\":\",origin,phase-data,\",\"nst\":46,\"dmin\":0.6541716292,\"rms\":0.43,\"gap\":173,\"magType\":\"ml\",\"type\":\"quarry blast\",\"title\":\"M 1.4 Quarry Blast - 12 km N of Kenefic, Oklahoma\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-96.38816667,34.262,0]},\"id\":\"ok2023vxyl\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.4,\"place\":\"54 km NW of Toyah, Texas\",\"time\":1699483288391,\"updated\":1699526957966,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vxyk\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vxyk.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":89,\"net\":\"tx\",\"code\":\"2023vxyk\",\"ids\":\",tx2023vxyk,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":48,\"dmin\":0.1,\"rms\":0.2,\"gap\":61,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 2.4 - 54 km NW of Toyah, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.247,31.608,7.4684]},\"id\":\"tx2023vxyk\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.12,\"place\":\"8 km N of Yucca Valley, CA\",\"time\":1699482561120,\"updated\":1699491570627,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/ci39710522\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ci39710522.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":19,\"net\":\"ci\",\"code\":\"39710522\",\"ids\":\",ci39710522,\",\"sources\":\",ci,\",\"types\":\",nearby-cities,origin,phase-data,scitech-link,\",\"nst\":14,\"dmin\":0.03184,\"rms\":0.09,\"gap\":64,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 1.1 - 8 km N of Yucca Valley, CA\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-116.4395,34.1815,10.39]},\"id\":\"ci39710522\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":0.95,\"place\":\"11 km N of Borrego Springs, CA\",\"time\":1699481849860,\"updated\":1699490908954,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/ci39710514\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ci39710514.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":14,\"net\":\"ci\",\"code\":\"39710514\",\"ids\":\",ci39710514,\",\"sources\":\",ci,\",\"types\":\",nearby-cities,origin,phase-data,scitech-link,\",\"nst\":37,\"dmin\":0.0266,\"rms\":0.18,\"gap\":125,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 1.0 - 11 km N of Borrego Springs, CA\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-116.3616667,33.353,13.8]},\"id\":\"ci39710514\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.6500000000000001,\"place\":\"14 km S of Princeton, Canada\",\"time\":1699481611520,\"updated\":1699508363230,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/uw61971376\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/uw61971376.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":42,\"net\":\"uw\",\"code\":\"61971376\",\"ids\":\",uw61971376,\",\"sources\":\",uw,\",\"types\":\",origin,phase-data,\",\"nst\":5,\"dmin\":0.5619,\"rms\":0.28,\"gap\":208,\"magType\":\"ml\",\"type\":\"explosion\",\"title\":\"M 1.7 Explosion - 14 km S of Princeton, Canada\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-120.48416666666667,49.324333333333335,-0.62]},\"id\":\"uw61971376\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.45,\"place\":\"5 km S of Salinas, Puerto Rico\",\"time\":1699481361280,\"updated\":1699482655290,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/pr71431078\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/pr71431078.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":92,\"net\":\"pr\",\"code\":\"71431078\",\"ids\":\",pr71431078,\",\"sources\":\",pr,\",\"types\":\",origin,phase-data,\",\"nst\":17,\"dmin\":0.1121,\"rms\":0.23,\"gap\":196,\"magType\":\"md\",\"type\":\"earthquake\",\"title\":\"M 2.5 - 5 km S of Salinas, Puerto Rico\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-66.2923333333333,17.9306666666667,9.21]},\"id\":\"pr71431078\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.07,\"place\":\"8 km WNW of Cobb, CA\",\"time\":1699480981860,\"updated\":1699482733799,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960196\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960196.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":18,\"net\":\"nc\",\"code\":\"73960196\",\"ids\":\",nc73960196,\",\"sources\":\",nc,\",\"types\":\",nearby-cities,origin,phase-data,scitech-link,\",\"nst\":19,\"dmin\":0.01152,\"rms\":0.03,\"gap\":55,\"magType\":\"md\",\"type\":\"earthquake\",\"title\":\"M 1.1 - 8 km WNW of Cobb, CA\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-122.8146667,38.8453331,1.42]},\"id\":\"nc73960196\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.08,\"place\":\"8 km NW of The Geysers, CA\",\"time\":1699480666110,\"updated\":1699482014744,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960191\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960191.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":18,\"net\":\"nc\",\"code\":\"73960191\",\"ids\":\",nc73960191,\",\"sources\":\",nc,\",\"types\":\",nearby-cities,origin,phase-data,scitech-link,\",\"nst\":22,\"dmin\":0.009205,\"rms\":0.02,\"gap\":49,\"magType\":\"md\",\"type\":\"earthquake\",\"title\":\"M 1.1 - 8 km NW of The Geysers, CA\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-122.8178329,38.8303337,1.9]},\"id\":\"nc73960191\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.1,\"place\":\"11 km NE of Burney, CA\",\"time\":1699480568450,\"updated\":1699492751783,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960186\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960186.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":68,\"net\":\"nc\",\"code\":\"73960186\",\"ids\":\",nc73960186,\",\"sources\":\",nc,\",\"types\":\",nearby-cities,origin,phase-data,scitech-link,\",\"nst\":13,\"dmin\":0.1634,\"rms\":0.16,\"gap\":79,\"magType\":\"md\",\"type\":\"earthquake\",\"title\":\"M 2.1 - 11 km NE of Burney, CA\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-121.5709991,40.9568329,3.69]},\"id\":\"nc73960186\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.52,\"place\":\"3 km WSW of Fuig, Puerto Rico\",\"time\":1699480468220,\"updated\":1699481189740,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/pr71431058\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/pr71431058.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":98,\"net\":\"pr\",\"code\":\"71431058\",\"ids\":\",pr71431058,\",\"sources\":\",pr,\",\"types\":\",origin,phase-data,\",\"nst\":7,\"dmin\":0.06623,\"rms\":0.1,\"gap\":180,\"magType\":\"md\",\"type\":\"earthquake\",\"title\":\"M 2.5 - 3 km WSW of Fuig, Puerto Rico\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-66.9488333333333,17.974,12.09]},\"id\":\"pr71431058\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.9,\"place\":\"44 km NW of Toyah, Texas\",\"time\":1699479495795,\"updated\":1699481251658,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vxwi\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vxwi.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":56,\"net\":\"tx\",\"code\":\"2023vxwi\",\"ids\":\",tx2023vxwi,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":20,\"dmin\":0,\"rms\":0.4,\"gap\":56,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 1.9 - 44 km NW of Toyah, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.104,31.613,9.9878]},\"id\":\"tx2023vxwi\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.9,\"place\":\"35 km W of Mentone, Texas\",\"time\":1699479472800,\"updated\":1699480404097,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vxwh\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vxwh.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":56,\"net\":\"tx\",\"code\":\"2023vxwh\",\"ids\":\",tx2023vxwh,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":12,\"dmin\":0,\"rms\":0.7,\"gap\":91,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 1.9 - 35 km W of Mentone, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-103.971,31.656,6.9929]},\"id\":\"tx2023vxwh\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":3.4,\"place\":\"60 km WNW of Nanwalek, Alaska\",\"time\":1699478935143,\"updated\":1699488353682,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/ak023eccchzy\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023eccchzy.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":178,\"net\":\"ak\",\"code\":\"023eccchzy\",\"ids\":\",us7000l9q2,ak023eccchzy,\",\"sources\":\",us,ak,\",\"types\":\",origin,phase-data,\",\"nst\":null,\"dmin\":null,\"rms\":0.53,\"gap\":null,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 3.4 - 60 km WNW of Nanwalek, Alaska\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-152.9379,59.5141,99.8]},\"id\":\"ak023eccchzy\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.01,\"place\":\"6 km ENE of Coalinga, CA\",\"time\":1699478411810,\"updated\":1699481235844,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960171\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960171.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":16,\"net\":\"nc\",\"code\":\"73960171\",\"ids\":\",nc73960171,\",\"sources\":\",nc,\",\"types\":\",nearby-cities,origin,phase-data,scitech-link,\",\"nst\":9,\"dmin\":0.08826,\"rms\":0.06,\"gap\":195,\"magType\":\"md\",\"type\":\"earthquake\",\"title\":\"M 1.0 - 6 km ENE of Coalinga, CA\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-120.3001633,36.1626663,12.33]},\"id\":\"nc73960171\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.7,\"place\":\"4 km NE of Coalinga, CA\",\"time\":1699478227310,\"updated\":1699491853696,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960161\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960161.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":44,\"net\":\"nc\",\"code\":\"73960161\",\"ids\":\",nc73960161,\",\"sources\":\",nc,\",\"types\":\",nearby-cities,origin,phase-data,scitech-link,\",\"nst\":14,\"dmin\":0.06788,\"rms\":0.04,\"gap\":129,\"magType\":\"md\",\"type\":\"earthquake\",\"title\":\"M 1.7 - 4 km NE of Coalinga, CA\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-120.3261642,36.1641655,13.24]},\"id\":\"nc73960161\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.11,\"place\":\"4 km WSW of Cobb, CA\",\"time\":1699478193630,\"updated\":1699490293538,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960156\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960156.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":19,\"net\":\"nc\",\"code\":\"73960156\",\"ids\":\",nc73960156,\",\"sources\":\",nc,\",\"types\":\",nearby-cities,origin,phase-data,scitech-link,\",\"nst\":15,\"dmin\":0.01402,\"rms\":0.03,\"gap\":109,\"magType\":\"md\",\"type\":\"earthquake\",\"title\":\"M 1.1 - 4 km WSW of Cobb, CA\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-122.7628326,38.8146667,2.13]},\"id\":\"nc73960156\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.2,\"place\":\"39 km WSW of Mentone, Texas\",\"time\":1699478006666,\"updated\":1699537285978,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vxvn\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vxvn.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":74,\"net\":\"tx\",\"code\":\"2023vxvn\",\"ids\":\",tx2023vxvn,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":15,\"dmin\":0,\"rms\":0.1,\"gap\":50,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 2.2 - 39 km WSW of Mentone, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.011,31.628,6.3886]},\"id\":\"tx2023vxvn\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.8,\"place\":\"40 km W of Mentone, Texas\",\"time\":1699477502793,\"updated\":1699479283950,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vxvf\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vxvf.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":50,\"net\":\"tx\",\"code\":\"2023vxvf\",\"ids\":\",tx2023vxvf,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":15,\"dmin\":0,\"rms\":0.2,\"gap\":50,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 1.8 - 40 km W of Mentone, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.021,31.666,4.8365]},\"id\":\"tx2023vxvf\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.5,\"place\":\"western Texas\",\"time\":1699477394574,\"updated\":1699480179782,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vxvd\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vxvd.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":35,\"net\":\"tx\",\"code\":\"2023vxvd\",\"ids\":\",tx2023vxvd,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":7,\"dmin\":0,\"rms\":1,\"gap\":137,\"magType\":\"mlv\",\"type\":\"earthquake\",\"title\":\"M 1.5 - western Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.279,31.719,1.8806]},\"id\":\"tx2023vxvd\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.97,\"place\":\"11 km ENE of Borrego Springs, CA\",\"time\":1699477059310,\"updated\":1699490515130,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/ci39710466\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ci39710466.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":60,\"net\":\"ci\",\"code\":\"39710466\",\"ids\":\",ci39710466,\",\"sources\":\",ci,\",\"types\":\",focal-mechanism,nearby-cities,origin,phase-data,scitech-link,\",\"nst\":75,\"dmin\":0.1019,\"rms\":0.21,\"gap\":25,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 2.0 - 11 km ENE of Borrego Springs, CA\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-116.2613333,33.2763333,7.04]},\"id\":\"ci39710466\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.8,\"place\":\"western Texas\",\"time\":1699477005522,\"updated\":1699478302630,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vxux\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vxux.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":50,\"net\":\"tx\",\"code\":\"2023vxux\",\"ids\":\",tx2023vxux,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":19,\"dmin\":0,\"rms\":0.4,\"gap\":43,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 1.8 - western Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.007,31.645,5.2482]},\"id\":\"tx2023vxux\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.5,\"place\":\"38 km W of Mentone, Texas\",\"time\":1699476657721,\"updated\":1699478295537,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vxut\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vxut.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":35,\"net\":\"tx\",\"code\":\"2023vxut\",\"ids\":\",tx2023vxut,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":18,\"dmin\":0,\"rms\":0.6,\"gap\":41,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 1.5 - 38 km W of Mentone, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-103.998,31.643,5.5481]},\"id\":\"tx2023vxut\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.22,\"place\":\"3 km SE of Home Gardens, CA\",\"time\":1699476367290,\"updated\":1699482798541,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/ci39710450\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ci39710450.geojson\",\"felt\":1,\"cdi\":0,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":23,\"net\":\"ci\",\"code\":\"39710450\",\"ids\":\",ci39710450,\",\"sources\":\",ci,\",\"types\":\",dyfi,nearby-cities,origin,phase-data,scitech-link,\",\"nst\":55,\"dmin\":0.05302,\"rms\":0.25,\"gap\":41,\"magType\":\"ml\",\"type\":\"quarry blast\",\"title\":\"M 1.2 Quarry Blast - 3 km SE of Home Gardens, CA\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-117.4996667,33.8615,-0.45]},\"id\":\"ci39710450\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.1,\"place\":\"15 km W of Womens Bay, Alaska\",\"time\":1699476326541,\"updated\":1699476463940,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/ak023ecbunfy\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023ecbunfy.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":68,\"net\":\"ak\",\"code\":\"023ecbunfy\",\"ids\":\",ak023ecbunfy,\",\"sources\":\",ak,\",\"types\":\",origin,phase-data,\",\"nst\":null,\"dmin\":null,\"rms\":0.4,\"gap\":null,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 2.1 - 15 km W of Womens Bay, Alaska\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-152.9199,57.6656,14.9]},\"id\":\"ak023ecbunfy\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.2,\"place\":\"58 km SW of Karluk, Alaska\",\"time\":1699476170823,\"updated\":1699476317467,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/ak023ecbu262\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023ecbu262.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":74,\"net\":\"ak\",\"code\":\"023ecbu262\",\"ids\":\",ak023ecbu262,\",\"sources\":\",ak,\",\"types\":\",origin,phase-data,\",\"nst\":null,\"dmin\":null,\"rms\":0.72,\"gap\":null,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 2.2 - 58 km SW of Karluk, Alaska\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-155.2603,57.2798,35.8]},\"id\":\"ak023ecbu262\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.4,\"place\":\"34 km NNW of Petersville, Alaska\",\"time\":1699475539704,\"updated\":1699475680842,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/ak023ecbrulu\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023ecbrulu.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":89,\"net\":\"ak\",\"code\":\"023ecbrulu\",\"ids\":\",ak023ecbrulu,\",\"sources\":\",ak,\",\"types\":\",origin,phase-data,\",\"nst\":null,\"dmin\":null,\"rms\":0.58,\"gap\":null,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 2.4 - 34 km NNW of Petersville, Alaska\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-150.9323,62.7946,95]},\"id\":\"ak023ecbrulu\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.79999995,\"place\":\"4 km SSW of Pāhala, Hawaii\",\"time\":1699473165090,\"updated\":1699473355830,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/hv73641817\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/hv73641817.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":50,\"net\":\"hv\",\"code\":\"73641817\",\"ids\":\",hv73641817,\",\"sources\":\",hv,\",\"types\":\",origin,phase-data,\",\"nst\":27,\"dmin\":null,\"rms\":0.0900000036,\"gap\":209,\"magType\":\"md\",\"type\":\"earthquake\",\"title\":\"M 1.8 - 4 km SSW of Pāhala, Hawaii\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-155.496170043945,19.1704998016357,34.1199989318848]},\"id\":\"hv73641817\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2,\"place\":\"28 km E of Skwentna, Alaska\",\"time\":1699472705907,\"updated\":1699472841629,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/ak023ecb968u\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023ecb968u.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":62,\"net\":\"ak\",\"code\":\"023ecb968u\",\"ids\":\",ak023ecb968u,\",\"sources\":\",ak,\",\"types\":\",origin,phase-data,\",\"nst\":null,\"dmin\":null,\"rms\":0.31,\"gap\":null,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 2.0 - 28 km E of Skwentna, Alaska\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-150.8539,61.9622,67.5]},\"id\":\"ak023ecb968u\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":0.98,\"place\":\"5 km ENE of Coalinga, CA\",\"time\":1699472461040,\"updated\":1699475413148,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960136\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960136.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":15,\"net\":\"nc\",\"code\":\"73960136\",\"ids\":\",nc73960136,\",\"sources\":\",nc,\",\"types\":\",nearby-cities,origin,phase-data,scitech-link,\",\"nst\":8,\"dmin\":0.1449,\"rms\":0.02,\"gap\":190,\"magType\":\"md\",\"type\":\"earthquake\",\"title\":\"M 1.0 - 5 km ENE of Coalinga, CA\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-120.309166,36.1604996,10.2]},\"id\":\"nc73960136\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.16,\"place\":\"4 km NW of Cobb, CA\",\"time\":1699472279460,\"updated\":1699487053232,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960131\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960131.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":21,\"net\":\"nc\",\"code\":\"73960131\",\"ids\":\",nc73960131,\",\"sources\":\",nc,\",\"types\":\",nearby-cities,origin,phase-data,scitech-link,\",\"nst\":7,\"dmin\":0.01223,\"rms\":0.03,\"gap\":193,\"magType\":\"md\",\"type\":\"earthquake\",\"title\":\"M 1.2 - 4 km NW of Cobb, CA\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-122.7623367,38.8479996,0.94]},\"id\":\"nc73960131\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.8,\"place\":\"40 km W of Mentone, Texas\",\"time\":1699472011250,\"updated\":1699473500324,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vxse\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vxse.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":50,\"net\":\"tx\",\"code\":\"2023vxse\",\"ids\":\",tx2023vxse,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":20,\"dmin\":0,\"rms\":0.4,\"gap\":47,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 1.8 - 40 km W of Mentone, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.018,31.653,5.2881]},\"id\":\"tx2023vxse\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.1,\"place\":\"western Texas\",\"time\":1699471106424,\"updated\":1699473808806,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vxrs\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vxrs.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":68,\"net\":\"tx\",\"code\":\"2023vxrs\",\"ids\":\",tx2023vxrs,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":18,\"dmin\":0.1,\"rms\":0.3,\"gap\":159,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 2.1 - western Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.541,31.585,2.8322]},\"id\":\"tx2023vxrs\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.56,\"place\":\"3 km NE of The Geysers, CA\",\"time\":1699470623950,\"updated\":1699484474981,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960126\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960126.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":37,\"net\":\"nc\",\"code\":\"73960126\",\"ids\":\",nc73960126,\",\"sources\":\",nc,\",\"types\":\",nearby-cities,origin,phase-data,scitech-link,\",\"nst\":27,\"dmin\":0.01246,\"rms\":0.03,\"gap\":98,\"magType\":\"md\",\"type\":\"earthquake\",\"title\":\"M 1.6 - 3 km NE of The Geysers, CA\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-122.730835,38.7948341,1.2]},\"id\":\"nc73960126\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.08,\"place\":\"3 km NE of The Geysers, CA\",\"time\":1699470610690,\"updated\":1699472713895,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/nc73960121\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73960121.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":18,\"net\":\"nc\",\"code\":\"73960121\",\"ids\":\",nc73960121,\",\"sources\":\",nc,\",\"types\":\",nearby-cities,origin,phase-data,scitech-link,\",\"nst\":15,\"dmin\":0.01288,\"rms\":0.02,\"gap\":99,\"magType\":\"md\",\"type\":\"earthquake\",\"title\":\"M 1.1 - 3 km NE of The Geysers, CA\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-122.7294998,38.7948341,1.95]},\"id\":\"nc73960121\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.9,\"place\":\"38 km WSW of Mentone, Texas\",\"time\":1699470534832,\"updated\":1699471481707,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vxrj\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vxrj.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":56,\"net\":\"tx\",\"code\":\"2023vxrj\",\"ids\":\",tx2023vxrj,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":21,\"dmin\":0,\"rms\":0.3,\"gap\":50,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 1.9 - 38 km WSW of Mentone, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-103.996,31.638,4.4323]},\"id\":\"tx2023vxrj\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.4,\"place\":\"70 km W of Nanwalek, Alaska\",\"time\":1699470219606,\"updated\":1699470392871,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/ak023ecb08rs\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023ecb08rs.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":89,\"net\":\"ak\",\"code\":\"023ecb08rs\",\"ids\":\",ak023ecb08rs,\",\"sources\":\",ak,\",\"types\":\",origin,phase-data,\",\"nst\":null,\"dmin\":null,\"rms\":0.73,\"gap\":null,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 2.4 - 70 km W of Nanwalek, Alaska\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-153.1594,59.355,88.3]},\"id\":\"ak023ecb08rs\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.4,\"place\":\"47 km WSW of Anchor Point, Alaska\",\"time\":1699469276379,\"updated\":1699469454902,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/ak023ecao9yt\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023ecao9yt.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":89,\"net\":\"ak\",\"code\":\"023ecao9yt\",\"ids\":\",ak023ecao9yt,\",\"sources\":\",ak,\",\"types\":\",origin,phase-data,\",\"nst\":null,\"dmin\":null,\"rms\":0.86,\"gap\":null,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 2.4 - 47 km WSW of Anchor Point, Alaska\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-152.612,59.6262,74.5]},\"id\":\"ak023ecao9yt\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.9,\"place\":\"39 km W of Mentone, Texas\",\"time\":1699468626083,\"updated\":1699470921482,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vxqh\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vxqh.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":56,\"net\":\"tx\",\"code\":\"2023vxqh\",\"ids\":\",tx2023vxqh,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":16,\"dmin\":0,\"rms\":0.3,\"gap\":45,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 1.9 - 39 km W of Mentone, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.015,31.647,4.1758]},\"id\":\"tx2023vxqh\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.4,\"place\":\"40 km W of Mentone, Texas\",\"time\":1699468165641,\"updated\":1699506715268,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vxqa\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vxqa.geojson\",\"felt\":6,\"cdi\":3.8,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":91,\"net\":\"tx\",\"code\":\"2023vxqa\",\"ids\":\",tx2023vxqa,\",\"sources\":\",tx,\",\"types\":\",dyfi,origin,phase-data,\",\"nst\":19,\"dmin\":0,\"rms\":0.2,\"gap\":47,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 2.4 - 40 km W of Mentone, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.018,31.651,5.6817]},\"id\":\"tx2023vxqa\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.1,\"place\":\"1 km WNW of Point Possession, Alaska\",\"time\":1699468152567,\"updated\":1699468278477,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/ak023ecakbp4\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/ak023ecakbp4.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":68,\"net\":\"ak\",\"code\":\"023ecakbp4\",\"ids\":\",ak023ecakbp4,\",\"sources\":\",ak,\",\"types\":\",origin,phase-data,\",\"nst\":null,\"dmin\":null,\"rms\":0.77,\"gap\":null,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 2.1 - 1 km WNW of Point Possession, Alaska\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-150.7113,60.9274,34.6]},\"id\":\"ak023ecakbp4\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":4.6,\"place\":\"298 km WSW of Tual, Indonesia\",\"time\":1699467981946,\"updated\":1699471139040,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/us7000l9p4\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/us7000l9p4.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":326,\"net\":\"us\",\"code\":\"7000l9p4\",\"ids\":\",us7000l9p4,\",\"sources\":\",us,\",\"types\":\",origin,phase-data,\",\"nst\":44,\"dmin\":1.737,\"rms\":0.93,\"gap\":56,\"magType\":\"mb\",\"type\":\"earthquake\",\"title\":\"M 4.6 - 298 km WSW of Tual, Indonesia\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[130.133,-6.2563,10]},\"id\":\"us7000l9p4\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2,\"place\":\"40 km W of Mentone, Texas\",\"time\":1699467671508,\"updated\":1699470264466,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vxpu\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vxpu.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":62,\"net\":\"tx\",\"code\":\"2023vxpu\",\"ids\":\",tx2023vxpu,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":33,\"dmin\":0,\"rms\":0.2,\"gap\":37,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 2.0 - 40 km W of Mentone, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.02,31.654,6.3758]},\"id\":\"tx2023vxpu\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.9,\"place\":\"37 km WSW of Mentone, Texas\",\"time\":1699467600466,\"updated\":1699534956449,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vxpt\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vxpt.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":56,\"net\":\"tx\",\"code\":\"2023vxpt\",\"ids\":\",tx2023vxpt,\",\"sources\":\",tx,\",\"types\":\",origin,phase-data,\",\"nst\":22,\"dmin\":0,\"rms\":0.1,\"gap\":47,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 1.9 - 37 km WSW of Mentone, Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-103.988,31.633,5.8616]},\"id\":\"tx2023vxpt\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.61,\"place\":\"11 km ESE of Ashford, Washington\",\"time\":1699467325410,\"updated\":1699486036250,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/uw61971251\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/uw61971251.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":40,\"net\":\"uw\",\"code\":\"61971251\",\"ids\":\",uw61971251,\",\"sources\":\",uw,\",\"types\":\",origin,phase-data,\",\"nst\":50,\"dmin\":0.04012,\"rms\":0.14,\"gap\":69,\"magType\":\"ml\",\"type\":\"earthquake\",\"title\":\"M 1.6 - 11 km ESE of Ashford, Washington\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-121.90316666666666,46.701166666666666,7.25]},\"id\":\"uw61971251\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":1.64,\"place\":\"3 km SE of Guerneville, CA\",\"time\":1699467192620,\"updated\":1699477813346,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/nc73957865\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/nc73957865.geojson\",\"felt\":null,\"cdi\":null,\"mmi\":null,\"alert\":null,\"status\":\"automatic\",\"tsunami\":0,\"sig\":41,\"net\":\"nc\",\"code\":\"73957865\",\"ids\":\",nc73957865,\",\"sources\":\",nc,\",\"types\":\",nearby-cities,origin,phase-data,scitech-link,\",\"nst\":7,\"dmin\":0.249,\"rms\":0.14,\"gap\":215,\"magType\":\"md\",\"type\":\"earthquake\",\"title\":\"M 1.6 - 3 km SE of Guerneville, CA\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-122.9683304,38.4850006,24.01]},\"id\":\"nc73957865\"},\n{\"type\":\"Feature\",\"properties\":{\"mag\":2.5,\"place\":\"western Texas\",\"time\":1699466411777,\"updated\":1699502416780,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/tx2023vxpc\",\"detail\":\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/detail/tx2023vxpc.geojson\",\"felt\":4,\"cdi\":2.7,\"mmi\":null,\"alert\":null,\"status\":\"reviewed\",\"tsunami\":0,\"sig\":97,\"net\":\"tx\",\"code\":\"2023vxpc\",\"ids\":\",tx2023vxpc,\",\"sources\":\",tx,\",\"types\":\",dyfi,origin,phase-data,\",\"nst\":28,\"dmin\":0,\"rms\":0.1,\"gap\":41,\"magType\":\"ml(texnet)\",\"type\":\"earthquake\",\"title\":\"M 2.5 - western Texas\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[-104.011,31.638,5.6046]},\"id\":\"tx2023vxpc\"}],\"bbox\":[-177.1013,-24.3611,-0.84,167.4497,67.7142,183.697]}\n\n\n\nInference-\nWe clearly see the readability of both the files.\n\n\n\nMaking a list of all earthquakes\n\nall_eq_dicts = all_eq_data['features']\nprint(len(all_eq_dicts))\n\n191\n\n\n\n\nExtracting magnitues\n\nmags = []\nfor eq_dict in all_eq_dicts:\n    mag = eq_dict['properties']['mag']\n    mags.append(mag)\n    \nprint(mags[:20])\n\n[1.80999994, 2.3, 3.3, 1.5, 1.8, 2.5, 4.7, 1.09, 1.8, 4.9, 2, 1.6, 1.01, 3.2, 2.4, 5.1, 1.8, 1.7, 1.7, 0.98]\n\n\n\n\nExtracting location data\n\nmags, lons, lats = [], [], []\nfor eq_dict in all_eq_dicts:\n    mag = eq_dict['properties'] ['mag']\n    lon = eq_dict['geometry'] ['coordinates'][0]\n    lat = eq_dict['geometry'] ['coordinates'][1]\n    mags.append(mag)\n    lons.append(lon)\n    lats.append(lat)\n    \nprint(lons[:5])\nprint(lats[:5])\n\n[-155.49299621582, -148.6656, -104.593, -121.668335, -104.12]\n[19.1323337554932, 62.21, 31.647, 37.2989998, 31.62]\n\n\n\n\nBuilding a World map\n\nimport json\n\nfrom plotly.graph_objs import Scattergeo, Layout\nfrom plotly import offline\n\n# map the earthquakes\ndata = [Scattergeo(lon = lons, lat = lats)]\n\nmy_layout = Layout(title = 'Global Earthquakes')\n\nfig = {'data': data, 'layout' : my_layout}\noffline.plot(fig, filename = 'global_earthquakes.html')\n\n'global_earthquakes.html'\n\n\n\n\nDifferent way of specifying Chart data\n\ndata_2 = [Scattergeo(lon = lons, lat = lats)]\n\nmy_layout_2 = Layout (title = 'Global_earthquakes_2')\n\nfig = {'data' : data_2, 'layout' : my_layout_2}\noffline.plot (fig, filename = 'global_earthquakes.html')\n\n'global_earthquakes.html'\n\n\n\n\nCustomize marker size\n\ndata_3 = [{\n    'type' : 'scattergeo',\n    'lon': lons, \n    'lat': lats, \n    'marker': {\n        'size': [5* mag for mag in mags], \n    },\n}]\n\nmy_layout_3 = Layout(title = 'Global Earthquakes')\n\nfig = {'data' : data_3, 'layout' : my_layout_3}\noffline.plot (fig, filename = 'global_earthquakes_3.html')\n\n'global_earthquakes_3.html'\n\n\n\n\nCustomize marker colors\n\n\ndata_4 = [{\n    'type' : 'scattergeo',\n    'lon': lons, \n    'lat': lats, \n    'marker': {\n        'size': [5* mag for mag in mags],\n        'color' : mags, \n        'colorscale': 'Viridis',\n        'reversescale': True, \n        'colorbar': {'title': 'Magnitude'},\n    },\n}]\n\nmy_layout_4 = Layout(title = 'Global Earthquakes')\n\nfig = {'data' : data_4, 'layout' : my_layout_4}\noffline.plot (fig, filename = 'global_earthquakes_4.html')\n\n'global_earthquakes_4.html'\n\n\n\n\nOther colorscales\n\nfrom plotly import colors\n\nfor key in colors.PLOTLY_SCALES.keys():\n    print(key)\n\nGreys\nYlGnBu\nGreens\nYlOrRd\nBluered\nRdBu\nReds\nBlues\nPicnic\nRainbow\nPortland\nJet\nHot\nBlackbody\nEarth\nElectric\nViridis\nCividis\n\n\n\n\nAdding hover text\n\nmags, lons, lats, hover_texts = [], [], [], []\nfor eq_dict in all_eq_dicts:\n    mag = eq_dict['properties'] ['mag']\n    lon = eq_dict['geometry'] ['coordinates'][0]\n    lat = eq_dict['geometry'] ['coordinates'][1]\n    title = eq_dict['properties'] ['title'] [2]\n    mags.append(mag)\n    lons.append(lon)\n    lats.append(lat)\n    hover_texts.append(title)    #added\n\n# map the earthquakes\ndata_5 = [{\n    'type' : 'scattergeo',\n    'lon': lons, \n    'lat': lats, \n    'text' : hover_texts,\n    'marker': {\n        'size': [5* mag for mag in mags],\n        'color' : mags, \n        'colorscale': 'Viridis',\n        'reversescale': True, \n        'colorbar': {'title': 'Magnitude'},\n            },\n}]\n\nmy_layout_5 = Layout(title = 'Global Earthquakes')\n\nfig = {'data' : data_5, 'layout' : my_layout_5}\noffline.plot (fig, filename = 'global_earthquakes_5.html')\n\n'global_earthquakes_5.html'"
  },
  {
    "objectID": "posts/python/p_3.html",
    "href": "posts/python/p_3.html",
    "title": "Dictionaries",
    "section": "",
    "text": "A dictionary in Python is a collection of key-value pairs. 1. looping through a dictionary 2. work with information stored in a dictionary 3. access and modify individual elements in a dictionary 4. nest multiple dictionaries in a list, nest lists in a dictionary, and nest a dictionary inside a dictionary\n\n\n\nd_0= {'colour' : 'verte', 'point' : 5}\n\naccess = d_0['point'] #square brackets to access values\nprint(f\"You have earned {access} points!\")\n\nYou have earned 5 points!\n\n\n\n\n\n\nd_0['tout va bien'] = 0\nd_0['oui, ça va'] = 1\nprint(d_0)\n\n{'tout va bien': 0, 'oui, ça va': 1}\n\n\n\n\n\n\nd_0 = {'colour' : 'green'}    #curly brackets\nprint(f\"The new colour is {d_0['colour']}.\")\n\nThe new colour is green.\n\n\n\nd_0 = {'colour' : 'marron'}\nprint(f\"The changed colour is {d_0['colour']}.\")  #square brackets for accessing specific elements\n\nThe changed colour is marron.\n\n\n\n\n\n\n\nprint(d_0)\n\n{'tout va bien': 0, 'oui, ça va': 1}\n\n\n\nprint(d_0)\n\n{'tout va bien': 0, 'oui, ça va': 1}\n\n\n\n\n\n\nfavorite_language = {\n    'kunal': 'français',\n    'ritika' : 'espagneol',\n    'kartik' : 'russe',\n    'vaibhav' : 'almande'\n}\n\n\nlanguage = favorite_language['kunal'].title()\nprint(f\"Kunal's favorite lanuage is {language}.\")\n\nKunal's favorite lanuage is Français.\n\n\n\n\n\n\n\n\n\nuser_0 = {'dob' : 'nov 1995',\n         'birth_place' : 'gugaron', \n         'education' : 'masters',\n         'children' : '2', \n         }\n\nfor key, value in user_0.items():\n    print(f\"\\nKey: {key}\")\n    \n    print(f\"Value: {value}\")\n\n\nKey: dob\nValue: nov 1995\n\nKey: birth_place\nValue: gugaron\n\nKey: education\nValue: masters\n\nKey: children\nValue: 2\n\n\n\nprint(d_0)\n\n{'tout va bien': 0, 'oui, ça va': 1}\n\n\n\n\n\nfavorite_language\nfor name in favorite_language.keys():\n    print(name.title())\n\nKunal\nRitika\nKartik\nVaibhav\n\n\n\nfor language in favorite_language.values():\n    print(language.title())\n\nFrançais\nEspagneol\nRusse\nAlmande\n\n\n\n\n\n\n\nif 'raghav' not in favorite_language.keys():\n    print(\"Raghav, please take your poll!\")\n\nRaghav, please take your poll!\n\n\n\n\n\n\nfor name in sorted(favorite_language.keys()):\n    print(f\"{name.title()}, thank you for taking the poll!\")\n\nKartik, thank you for taking the poll!\nKunal, thank you for taking the poll!\nRitika, thank you for taking the poll!\nVaibhav, thank you for taking the poll!\n\n\n\n\n\n\nfavorite_language.update({'vaisahli' : 'français'})\nfavorite_language\n\n{'kunal': 'français',\n 'ritika': 'espagneol',\n 'kartik': 'russe',\n 'vaibhav': 'almande',\n 'vaisahli': 'français'}\n\n\n\n\n\n\nfor language in set(favorite_language.values()):\n    print(language.title())\n\nRusse\nAlmande\nEspagneol\nFrançais\n\n\n\n\n\n\nprint(user_0)\nprint(d_0)\nprint(favorite_language)\n\n{'dob': 'nov 1995', 'birth_place': 'gugaron', 'education': 'masters', 'children': '2'}\n{'tout va bien': 0, 'oui, ça va': 1}\n{'kunal': 'français', 'ritika': 'espagneol', 'kartik': 'russe', 'vaibhav': 'almande', 'vaisahli': 'français'}\n\n\n\n\n\n\ncombined = [user_0, d_0, favorite_language]\n\nfor tout in combined:\n    print(tout)\n\n{'dob': 'nov 1995', 'birth_place': 'gugaron', 'education': 'masters', 'children': '2'}\n{'tout va bien': 0, 'oui, ça va': 1}\n{'kunal': 'français', 'ritika': 'espagneol', 'kartik': 'russe', 'vaibhav': 'almande', 'vaisahli': 'français'}\n\n\n\n\n\n\nprogramming_languages = {'kunal' : ['python', 'latex', 'html', 'java'],\n                        'john' : ['html', 'c', 'C++'],\n                        'gofi' : ['java', 'python', 'R', 'html']}\n\nfor name, languages in programming_languages.items():  #use items to iterate thorough dictionary items\n    print(f\"\\n{name.title()}'s favorite languages are:\")\n    for language in languages:\n        print(f\"\\t{language.title()}\")\n\n\nKunal's favorite languages are:\n    Python\n    Latex\n    Html\n    Java\n\nJohn's favorite languages are:\n    Html\n    C\n    C++\n\nGofi's favorite languages are:\n    Java\n    Python\n    R\n    Html\n\n\n\n\n\n\nusers = {\n    'kkhurana' : {\n        'first' : 'kunal',\n        'last' : 'khurana', \n        'location': 'montréal',\n        },\n    \n        'asharma' : {\n            'first': 'anita',\n            'last': 'sharma',\n            'location': 'sudbery'\n        },\n    \n    'jarora' : {\n        'first' : 'jatin',\n        'last' : 'arora',\n        'location' : 'perth'\n    },\n    }\n\n\nusers\n\n{'kkhurana': {'first': 'kunal', 'last': 'khurana', 'location': 'montréal'},\n 'asharma': {'first': 'anita', 'last': 'sharma', 'location': 'sudbery'},\n 'jarora': {'first': 'jatin', 'last': 'arora', 'location': 'perth'}}\n\n\n\nfor username, user_info in users.items():\n    print(f\"\\nUsername: {username}\")\n    full_name = f\"{user_info['first']} {user_info['last']}\"\n    location = user_info['location']\n    \n    print(f\"\\tFull name: {full_name.title()}\")\n    print(f\"\\tLocation: {location.title()}\")\n\n\nUsername: kkhurana\n    Full name: Kunal Khurana\n    Location: Montréal\n\nUsername: asharma\n    Full name: Anita Sharma\n    Location: Sudbery\n\nUsername: jarora\n    Full name: Jatin Arora\n    Location: Perth"
  },
  {
    "objectID": "posts/python/p_3.html#learning-outcomes",
    "href": "posts/python/p_3.html#learning-outcomes",
    "title": "Dictionaries",
    "section": "",
    "text": "A dictionary in Python is a collection of key-value pairs. 1. looping through a dictionary 2. work with information stored in a dictionary 3. access and modify individual elements in a dictionary 4. nest multiple dictionaries in a list, nest lists in a dictionary, and nest a dictionary inside a dictionary\n\n\n\nd_0= {'colour' : 'verte', 'point' : 5}\n\naccess = d_0['point'] #square brackets to access values\nprint(f\"You have earned {access} points!\")\n\nYou have earned 5 points!\n\n\n\n\n\n\nd_0['tout va bien'] = 0\nd_0['oui, ça va'] = 1\nprint(d_0)\n\n{'tout va bien': 0, 'oui, ça va': 1}\n\n\n\n\n\n\nd_0 = {'colour' : 'green'}    #curly brackets\nprint(f\"The new colour is {d_0['colour']}.\")\n\nThe new colour is green.\n\n\n\nd_0 = {'colour' : 'marron'}\nprint(f\"The changed colour is {d_0['colour']}.\")  #square brackets for accessing specific elements\n\nThe changed colour is marron.\n\n\n\n\n\n\n\nprint(d_0)\n\n{'tout va bien': 0, 'oui, ça va': 1}\n\n\n\nprint(d_0)\n\n{'tout va bien': 0, 'oui, ça va': 1}\n\n\n\n\n\n\nfavorite_language = {\n    'kunal': 'français',\n    'ritika' : 'espagneol',\n    'kartik' : 'russe',\n    'vaibhav' : 'almande'\n}\n\n\nlanguage = favorite_language['kunal'].title()\nprint(f\"Kunal's favorite lanuage is {language}.\")\n\nKunal's favorite lanuage is Français.\n\n\n\n\n\n\n\n\n\nuser_0 = {'dob' : 'nov 1995',\n         'birth_place' : 'gugaron', \n         'education' : 'masters',\n         'children' : '2', \n         }\n\nfor key, value in user_0.items():\n    print(f\"\\nKey: {key}\")\n    \n    print(f\"Value: {value}\")\n\n\nKey: dob\nValue: nov 1995\n\nKey: birth_place\nValue: gugaron\n\nKey: education\nValue: masters\n\nKey: children\nValue: 2\n\n\n\nprint(d_0)\n\n{'tout va bien': 0, 'oui, ça va': 1}\n\n\n\n\n\nfavorite_language\nfor name in favorite_language.keys():\n    print(name.title())\n\nKunal\nRitika\nKartik\nVaibhav\n\n\n\nfor language in favorite_language.values():\n    print(language.title())\n\nFrançais\nEspagneol\nRusse\nAlmande\n\n\n\n\n\n\n\nif 'raghav' not in favorite_language.keys():\n    print(\"Raghav, please take your poll!\")\n\nRaghav, please take your poll!\n\n\n\n\n\n\nfor name in sorted(favorite_language.keys()):\n    print(f\"{name.title()}, thank you for taking the poll!\")\n\nKartik, thank you for taking the poll!\nKunal, thank you for taking the poll!\nRitika, thank you for taking the poll!\nVaibhav, thank you for taking the poll!\n\n\n\n\n\n\nfavorite_language.update({'vaisahli' : 'français'})\nfavorite_language\n\n{'kunal': 'français',\n 'ritika': 'espagneol',\n 'kartik': 'russe',\n 'vaibhav': 'almande',\n 'vaisahli': 'français'}\n\n\n\n\n\n\nfor language in set(favorite_language.values()):\n    print(language.title())\n\nRusse\nAlmande\nEspagneol\nFrançais\n\n\n\n\n\n\nprint(user_0)\nprint(d_0)\nprint(favorite_language)\n\n{'dob': 'nov 1995', 'birth_place': 'gugaron', 'education': 'masters', 'children': '2'}\n{'tout va bien': 0, 'oui, ça va': 1}\n{'kunal': 'français', 'ritika': 'espagneol', 'kartik': 'russe', 'vaibhav': 'almande', 'vaisahli': 'français'}\n\n\n\n\n\n\ncombined = [user_0, d_0, favorite_language]\n\nfor tout in combined:\n    print(tout)\n\n{'dob': 'nov 1995', 'birth_place': 'gugaron', 'education': 'masters', 'children': '2'}\n{'tout va bien': 0, 'oui, ça va': 1}\n{'kunal': 'français', 'ritika': 'espagneol', 'kartik': 'russe', 'vaibhav': 'almande', 'vaisahli': 'français'}\n\n\n\n\n\n\nprogramming_languages = {'kunal' : ['python', 'latex', 'html', 'java'],\n                        'john' : ['html', 'c', 'C++'],\n                        'gofi' : ['java', 'python', 'R', 'html']}\n\nfor name, languages in programming_languages.items():  #use items to iterate thorough dictionary items\n    print(f\"\\n{name.title()}'s favorite languages are:\")\n    for language in languages:\n        print(f\"\\t{language.title()}\")\n\n\nKunal's favorite languages are:\n    Python\n    Latex\n    Html\n    Java\n\nJohn's favorite languages are:\n    Html\n    C\n    C++\n\nGofi's favorite languages are:\n    Java\n    Python\n    R\n    Html\n\n\n\n\n\n\nusers = {\n    'kkhurana' : {\n        'first' : 'kunal',\n        'last' : 'khurana', \n        'location': 'montréal',\n        },\n    \n        'asharma' : {\n            'first': 'anita',\n            'last': 'sharma',\n            'location': 'sudbery'\n        },\n    \n    'jarora' : {\n        'first' : 'jatin',\n        'last' : 'arora',\n        'location' : 'perth'\n    },\n    }\n\n\nusers\n\n{'kkhurana': {'first': 'kunal', 'last': 'khurana', 'location': 'montréal'},\n 'asharma': {'first': 'anita', 'last': 'sharma', 'location': 'sudbery'},\n 'jarora': {'first': 'jatin', 'last': 'arora', 'location': 'perth'}}\n\n\n\nfor username, user_info in users.items():\n    print(f\"\\nUsername: {username}\")\n    full_name = f\"{user_info['first']} {user_info['last']}\"\n    location = user_info['location']\n    \n    print(f\"\\tFull name: {full_name.title()}\")\n    print(f\"\\tLocation: {location.title()}\")\n\n\nUsername: kkhurana\n    Full name: Kunal Khurana\n    Location: Montréal\n\nUsername: asharma\n    Full name: Anita Sharma\n    Location: Sudbery\n\nUsername: jarora\n    Full name: Jatin Arora\n    Location: Perth"
  },
  {
    "objectID": "posts/python/p_5.html",
    "href": "posts/python/p_5.html",
    "title": "Functions",
    "section": "",
    "text": "write functions and pass arguments\nhow to use positional and keyword arguments, and how to use arbitrary number of arguments\nusing functions with lists, dictionaries, if statements, and while loops.\nstoring functions in seperate files called modules\nstyling the functions so as they become structured and comprehensible\n\n\n\n\nIf you’re writing a function and notice the function is doing too many different tasks, try to split the code into two functions.\n\n\n\n\n\n\n# greet_user\n\ndef greet_user():\n    print(\"Hello!\")\n\n#calling\ngreet_user()\n\nHello!\n\n\n\n# passing information to the function\n\ndef greet_user(username):    #username would accept the value that i provide\n    print(f\"Hello, {username.title()}!\")\n    \n# calling with username \ngreet_user('kunal')  #don't forget to call the function as string\n\nHello, Kunal!\n\n\n\n# practice\ndef display_message(content):\n    print(f\"Hope you are feeling energized this monday, and you enjoyed your weekend well. I'm curious about {content. title()}!\")\n    \n# calling\ndisplay_message(\"your prepration for the maths test!!\")\n\n\nHope you are feeling energized this monday, and you enjoyed your weekend well. I'm curious about Your Prepration For The Maths Test!!!\n\n\n\n# pets\ndef describe_pet(name, breed):        #no inverted commas here\n    print(f\"I've a {name.title()} at home!\")\n    print(f\"{name.title()} belongs to {breed.title()} breed.\")\n    \n# calling\ndescribe_pet('bruno', 'german shephard')\n\nI've a Bruno at home!\nBruno belongs to German Shephard breed.\n\n\n\ndescribe_pet('ravi', 'husky')\n\nI've a Ravi at home!\nRavi belongs to Husky breed.\n\n\n\n### Default values for parameters\n# pets\ndef describe_pet(name, breed = 'husky'):        \n    print(f\"I've a {name.title()} at home!\")\n    print(f\"{name.title()} belongs to {breed.title()} breed.\")\n    \n# calling\ndescribe_pet('bruno')  #by default the breed is husky!\n\nI've a Bruno at home!\nBruno belongs to Husky breed.\n\n\n\n\n\n\ndef get_formatted_name(first_name, middle_name, last_name):\n    \"Return a full name, neatly formatted.\"\n    full_name = f\"{first_name} {middle_name} {last_name}\"\n    return full_name.title()\ntitre = get_formatted_name ('hardy', 'singh' , 'sandhu')\nprint(titre)\n\nHardy Singh Sandhu\n\n\n\n\n\n\ndef build_person(first_name, last_name, age = None):\n    person = {'first' : first_name, 'last' : last_name}\n    if age:                             # assigns a particular age to the group\n        person['age'] = age\n    return person\n\ntitre = build_person('jimi', 'hendrix', age = 27)\nprint(titre)\n\n{'first': 'jimi', 'last': 'hendrix', 'age': 27}\n\n\n\n\n\n\ndef get_formatted_name(first_name, last_name):\n    \"Return a full name, neatly formatted.\"\n    full_name = f\"{first_name} {last_name}\"\n    return full_name.title()\n\n# infinite loop\nwhile True:\n    print(\"\\nPlease tell me your name:\")\n    f_name = input(\"First name: \")\n    l_name = input(\"Last name: \")\n    \n    formatted_name = get_formatted_name(f_name, l_name)\n    print(f\"\\nHello, {formatted_name}!\")\n\n\ndef get_formatted_name(first_name, last_name):\n    \"Return a full name, neatly formatted.\"\n    full_name = f\"{first_name} {last_name}\"\n    return full_name.title()\n\n# adding break to stop the loop\nwhile True:\n    print(\"\\nPlease tell me your name:\")\n    print(\"(enter 'q' at any time to qutit)\")\n    \n    f_name = input(\"First name: \")\n    if f_name == 'q':\n        break\n        \n    l_name = input(\"Last name: \")\n    if l_name == 'q':\n        break\n    \n    formatted_name = get_formatted_name(f_name, l_name)\n    print(f\"\\nHello, {formatted_name}!\")\n\n\nPlease tell me your name:\n(enter 'q' at any time to qutit)\nFirst name: Kunal\nLast name: Khurana\n\nHello, Kunal Khurana!\n\nPlease tell me your name:\n(enter 'q' at any time to qutit)\nFirst name: q\n\n\n\n# capital, country\n\ndef get_capital_country (capital, country):\n    cap_c = f\"{capital} {country}\"\n    return cap_C.title()\n\nwhile True:\n    print(\"\\nPlease print capital name and country name)\n    print(\"enter q anytime to quit\")\n          \n    capital = input(\"Capital name : \")\n    if capital == 'q':\n          break\n    \n    country = input(\"Country name : \")\n    if country == 'q':\n          break\n\n\n# city_country\ndef city_country (city, country):\n    formatted_string = f\"{city.title()}, {country.title()}\"\n    return formatted_string\n# INFINITE LOOPL; HOW TO STOP\n\ncount = 0\nwhile count &lt; 3:\n    print(\"\\nThe name of the city and capital are-\")\n    print(city_country('newyork', 'united states'))\n    print(city_country('delhi', 'india'))\n    count += 1 #increment the count\n\nprint (\"loop has ended\")\n\n\nThe name of the city and capital are-\nNewyork, United States\nDelhi, India\n\nThe name of the city and capital are-\nNewyork, United States\nDelhi, India\n\nThe name of the city and capital are-\nNewyork, United States\nDelhi, India\nloop has ended\n\n\n\n\n\n\ndef print_models(unprinted_designs, completed_models):  #defining with two parameters\n    while unprinted_designs:\n        current_design = unprinted_designs.pop()\n        print(f\"Printing model: {current_design}\")\n        completed_models.append(current_design)\n        \ndef show_completed_models(completed_models):           #defining with one parameter\n    print(\"\\nThe following models have been printed:\")\n    for completed_model in completed_models:\n        print(completed_model)\n        \nunprinted_designs = ['ferrari', 'lancer', 'accord']\ncompleted_models = []\n\nprint_models(unprinted_designs, completed_models)     #don't forget to print\nshow_completed_models(completed_models)\n\nPrinting model: accord\nPrinting model: lancer\nPrinting model: ferrari\n\nThe following models have been printed:\naccord\nlancer\nferrari\n\n\n\n\nprint_models(unprinted_designs[:], completed_models)     #[:] will save a copy to the old list\nshow_completed_models(completed_models)\n\n\nThe following models have been printed:\naccord\nlancer\nferrari\n\n\n\n\n\n\ndef make_pizza(*toppings):\n    print(toppings)#prints requested toppings\n    \nmake_pizza('pepperoni')\nmake_pizza('mushrooms', 'green peppers', 'extra cheese')\n\n\n('pepperoni',)\n('mushrooms', 'green peppers', 'extra cheese')\n\n\nThe asterisk in the parameter name *toppings tells Python to make an empty tuple called toppings and pack whatever values it receives into this tuple.\n\ndef make_pizza(*toppings):\n    print(\"\\nMake the pizza with following toppings-\")\n    for topping in toppings:\n        print(f\"-{topping}\")\n    \nmake_pizza('pepperoni')\nmake_pizza('mushrooms', 'green peppers', 'extra cheese')\n\n\nMake the pizza with following toppings-\n-pepperoni\n\nMake the pizza with following toppings-\n-mushrooms\n-green peppers\n-extra cheese\n\n\n\n### adding an additional paramter 'size'\ndef make_pizza(size, *toppings):\n    print(f\"\\nMake a {size}-inch pizza with the following toppings-\")\n    for topping in toppings:\n        print(f\"-{topping}\")\n    \nmake_pizza(12, 'pepperoni')\nmake_pizza(16, 'mushrooms', 'green peppers', 'extra cheese')\n\n\n\nMake a 12-inch pizza with the following toppings-\n-pepperoni\n\nMake a 16-inch pizza with the following toppings-\n-mushrooms\n-green peppers\n-extra cheese\n\n\n\n\n\n\n(**) before user_info allows the function to accept any number of keyword arguments and store them as key-value pairs in a dictionary called user_info.\n\n\ndef user_profile(first, last, **user_info):   \n    user_info['first_name'] = first   \n    user_info['last_name'] = last\n    return user_info\n\nmore_info = user_profile('Kunal', 'Khurana', \n                         location= 'montréal', \n                         field= 'agirculture')\n \nprint(more_info)\n\n{'location': 'montréal', 'field': 'agirculture', 'first_name': 'Kunal', 'last_name': 'Khurana'}\n\n\n\n\n\n\nmodule import- writing a code and storing it somewhere; simply importing it and using it. e.g.- from maths import stats\ngenerally written as module_name.function_name()\nmay be used to import as many funcitons as needed, simply seperating them by comma. e.g.- from module_name import function_0, function_1, fucntion_2\nan alias can be provided to make the process simpler e.g.- from pizza import make_pizza as mp. This would save us some time while writing the code\nwe may also import all the functions from modules also with asterik() e.g.- from pizza import"
  },
  {
    "objectID": "posts/python/p_5.html#learning-outcomes",
    "href": "posts/python/p_5.html#learning-outcomes",
    "title": "Functions",
    "section": "",
    "text": "write functions and pass arguments\nhow to use positional and keyword arguments, and how to use arbitrary number of arguments\nusing functions with lists, dictionaries, if statements, and while loops.\nstoring functions in seperate files called modules\nstyling the functions so as they become structured and comprehensible\n\n\n\n\nIf you’re writing a function and notice the function is doing too many different tasks, try to split the code into two functions.\n\n\n\n\n\n\n# greet_user\n\ndef greet_user():\n    print(\"Hello!\")\n\n#calling\ngreet_user()\n\nHello!\n\n\n\n# passing information to the function\n\ndef greet_user(username):    #username would accept the value that i provide\n    print(f\"Hello, {username.title()}!\")\n    \n# calling with username \ngreet_user('kunal')  #don't forget to call the function as string\n\nHello, Kunal!\n\n\n\n# practice\ndef display_message(content):\n    print(f\"Hope you are feeling energized this monday, and you enjoyed your weekend well. I'm curious about {content. title()}!\")\n    \n# calling\ndisplay_message(\"your prepration for the maths test!!\")\n\n\nHope you are feeling energized this monday, and you enjoyed your weekend well. I'm curious about Your Prepration For The Maths Test!!!\n\n\n\n# pets\ndef describe_pet(name, breed):        #no inverted commas here\n    print(f\"I've a {name.title()} at home!\")\n    print(f\"{name.title()} belongs to {breed.title()} breed.\")\n    \n# calling\ndescribe_pet('bruno', 'german shephard')\n\nI've a Bruno at home!\nBruno belongs to German Shephard breed.\n\n\n\ndescribe_pet('ravi', 'husky')\n\nI've a Ravi at home!\nRavi belongs to Husky breed.\n\n\n\n### Default values for parameters\n# pets\ndef describe_pet(name, breed = 'husky'):        \n    print(f\"I've a {name.title()} at home!\")\n    print(f\"{name.title()} belongs to {breed.title()} breed.\")\n    \n# calling\ndescribe_pet('bruno')  #by default the breed is husky!\n\nI've a Bruno at home!\nBruno belongs to Husky breed.\n\n\n\n\n\n\ndef get_formatted_name(first_name, middle_name, last_name):\n    \"Return a full name, neatly formatted.\"\n    full_name = f\"{first_name} {middle_name} {last_name}\"\n    return full_name.title()\ntitre = get_formatted_name ('hardy', 'singh' , 'sandhu')\nprint(titre)\n\nHardy Singh Sandhu\n\n\n\n\n\n\ndef build_person(first_name, last_name, age = None):\n    person = {'first' : first_name, 'last' : last_name}\n    if age:                             # assigns a particular age to the group\n        person['age'] = age\n    return person\n\ntitre = build_person('jimi', 'hendrix', age = 27)\nprint(titre)\n\n{'first': 'jimi', 'last': 'hendrix', 'age': 27}\n\n\n\n\n\n\ndef get_formatted_name(first_name, last_name):\n    \"Return a full name, neatly formatted.\"\n    full_name = f\"{first_name} {last_name}\"\n    return full_name.title()\n\n# infinite loop\nwhile True:\n    print(\"\\nPlease tell me your name:\")\n    f_name = input(\"First name: \")\n    l_name = input(\"Last name: \")\n    \n    formatted_name = get_formatted_name(f_name, l_name)\n    print(f\"\\nHello, {formatted_name}!\")\n\n\ndef get_formatted_name(first_name, last_name):\n    \"Return a full name, neatly formatted.\"\n    full_name = f\"{first_name} {last_name}\"\n    return full_name.title()\n\n# adding break to stop the loop\nwhile True:\n    print(\"\\nPlease tell me your name:\")\n    print(\"(enter 'q' at any time to qutit)\")\n    \n    f_name = input(\"First name: \")\n    if f_name == 'q':\n        break\n        \n    l_name = input(\"Last name: \")\n    if l_name == 'q':\n        break\n    \n    formatted_name = get_formatted_name(f_name, l_name)\n    print(f\"\\nHello, {formatted_name}!\")\n\n\nPlease tell me your name:\n(enter 'q' at any time to qutit)\nFirst name: Kunal\nLast name: Khurana\n\nHello, Kunal Khurana!\n\nPlease tell me your name:\n(enter 'q' at any time to qutit)\nFirst name: q\n\n\n\n# capital, country\n\ndef get_capital_country (capital, country):\n    cap_c = f\"{capital} {country}\"\n    return cap_C.title()\n\nwhile True:\n    print(\"\\nPlease print capital name and country name)\n    print(\"enter q anytime to quit\")\n          \n    capital = input(\"Capital name : \")\n    if capital == 'q':\n          break\n    \n    country = input(\"Country name : \")\n    if country == 'q':\n          break\n\n\n# city_country\ndef city_country (city, country):\n    formatted_string = f\"{city.title()}, {country.title()}\"\n    return formatted_string\n# INFINITE LOOPL; HOW TO STOP\n\ncount = 0\nwhile count &lt; 3:\n    print(\"\\nThe name of the city and capital are-\")\n    print(city_country('newyork', 'united states'))\n    print(city_country('delhi', 'india'))\n    count += 1 #increment the count\n\nprint (\"loop has ended\")\n\n\nThe name of the city and capital are-\nNewyork, United States\nDelhi, India\n\nThe name of the city and capital are-\nNewyork, United States\nDelhi, India\n\nThe name of the city and capital are-\nNewyork, United States\nDelhi, India\nloop has ended\n\n\n\n\n\n\ndef print_models(unprinted_designs, completed_models):  #defining with two parameters\n    while unprinted_designs:\n        current_design = unprinted_designs.pop()\n        print(f\"Printing model: {current_design}\")\n        completed_models.append(current_design)\n        \ndef show_completed_models(completed_models):           #defining with one parameter\n    print(\"\\nThe following models have been printed:\")\n    for completed_model in completed_models:\n        print(completed_model)\n        \nunprinted_designs = ['ferrari', 'lancer', 'accord']\ncompleted_models = []\n\nprint_models(unprinted_designs, completed_models)     #don't forget to print\nshow_completed_models(completed_models)\n\nPrinting model: accord\nPrinting model: lancer\nPrinting model: ferrari\n\nThe following models have been printed:\naccord\nlancer\nferrari\n\n\n\n\nprint_models(unprinted_designs[:], completed_models)     #[:] will save a copy to the old list\nshow_completed_models(completed_models)\n\n\nThe following models have been printed:\naccord\nlancer\nferrari\n\n\n\n\n\n\ndef make_pizza(*toppings):\n    print(toppings)#prints requested toppings\n    \nmake_pizza('pepperoni')\nmake_pizza('mushrooms', 'green peppers', 'extra cheese')\n\n\n('pepperoni',)\n('mushrooms', 'green peppers', 'extra cheese')\n\n\nThe asterisk in the parameter name *toppings tells Python to make an empty tuple called toppings and pack whatever values it receives into this tuple.\n\ndef make_pizza(*toppings):\n    print(\"\\nMake the pizza with following toppings-\")\n    for topping in toppings:\n        print(f\"-{topping}\")\n    \nmake_pizza('pepperoni')\nmake_pizza('mushrooms', 'green peppers', 'extra cheese')\n\n\nMake the pizza with following toppings-\n-pepperoni\n\nMake the pizza with following toppings-\n-mushrooms\n-green peppers\n-extra cheese\n\n\n\n### adding an additional paramter 'size'\ndef make_pizza(size, *toppings):\n    print(f\"\\nMake a {size}-inch pizza with the following toppings-\")\n    for topping in toppings:\n        print(f\"-{topping}\")\n    \nmake_pizza(12, 'pepperoni')\nmake_pizza(16, 'mushrooms', 'green peppers', 'extra cheese')\n\n\n\nMake a 12-inch pizza with the following toppings-\n-pepperoni\n\nMake a 16-inch pizza with the following toppings-\n-mushrooms\n-green peppers\n-extra cheese\n\n\n\n\n\n\n(**) before user_info allows the function to accept any number of keyword arguments and store them as key-value pairs in a dictionary called user_info.\n\n\ndef user_profile(first, last, **user_info):   \n    user_info['first_name'] = first   \n    user_info['last_name'] = last\n    return user_info\n\nmore_info = user_profile('Kunal', 'Khurana', \n                         location= 'montréal', \n                         field= 'agirculture')\n \nprint(more_info)\n\n{'location': 'montréal', 'field': 'agirculture', 'first_name': 'Kunal', 'last_name': 'Khurana'}\n\n\n\n\n\n\nmodule import- writing a code and storing it somewhere; simply importing it and using it. e.g.- from maths import stats\ngenerally written as module_name.function_name()\nmay be used to import as many funcitons as needed, simply seperating them by comma. e.g.- from module_name import function_0, function_1, fucntion_2\nan alias can be provided to make the process simpler e.g.- from pizza import make_pizza as mp. This would save us some time while writing the code\nwe may also import all the functions from modules also with asterik() e.g.- from pizza import"
  },
  {
    "objectID": "posts/python/p_7.html",
    "href": "posts/python/p_7.html",
    "title": "Files and exceptions",
    "section": "",
    "text": "working with files (reading entire files or its contents)\nwrite a file and append text into the file\nexceptions and handling those\nPython data structures\n‘json’ module- saves the data when program stops running"
  },
  {
    "objectID": "posts/python/p_7.html#reading-a-file",
    "href": "posts/python/p_7.html#reading-a-file",
    "title": "Files and exceptions",
    "section": "Reading a file",
    "text": "Reading a file\n\nfile_path = \"E:\\\\machine learning projects\\\\python.txt\"\nwith open (file_path) as file_object:  \n    for line in file_object:\n        print(line)\n\nI love programming.\n\nI love creating new games.\n\nI also love finding meaning in the large datasets.\n\nI want to create an application with python.\n\n\n\n\n# stripping extra lines\nfile_path = \"E:\\\\machine learning projects\\\\python.txt\"\nwith open (file_path) as file_object:  \n    for line in file_object:\n        print(line.rstrip())\n\nI love programming.\n\n\n\nmaking a list of lines from a file\n\nwith open (file_path) as file_object:  \n    lines = file_object.readlines()    #we used readlines method\n    \nfor line in lines:\n    print(line.rstrip())\n        \n\nThis file contains text that will be viewed in python's crash_course#7!\n\nthere is some white space in between!\n\n\n\n\nworking with file’s contents\n\npi_string = ''\nfor line in lines:\n    pi_string += line.rstrip() \n    \nprint(pi_string)\nprint(len(pi_string))\n\nThis file contains text that will be viewed in python's crash_course#7!there is some white space in between!\n108\n\n\n\npi_string = ''\nfor line in lines:\n    pi_string += line.rstrip() + ' '  #adding a space in between lines\n    \nprint(pi_string)\nprint(len(pi_string))\n\nThis file contains text that will be viewed in python's crash_course#7!  there is some white space in between! \n111\n\n\n\nany_string = ''\nfor line in lines:\n    any_string += line.rstrip() + '\\n'  #similarly, adding a line in between lines\n    \nprint(any_string)\nprint(len(any_string))\n\nThis file contains text that will be viewed in python's crash_course#7!\n\nthere is some white space in between!\n\n111\n\n\n\nany_string = ''\nfor line in lines:\n    any_string += line.rstrip() + '\\n'  #similarly, adding a line in between lines\n    \nprint(f\"{any_string[:52]}...\")         #prints first 52 characters\nprint(len(any_string))\n\nThis file contains text that will be viewed in pytho...\n111"
  },
  {
    "objectID": "posts/python/p_7.html#writing-into-a-file",
    "href": "posts/python/p_7.html#writing-into-a-file",
    "title": "Files and exceptions",
    "section": "Writing into a File",
    "text": "Writing into a File\n\n\nwith open ('python.txt', 'w') as file_object:\n    file_object.write(\"I love programming.\\n\")\n    file_object.write(\"I love creating new games.\\n\")"
  },
  {
    "objectID": "posts/python/p_7.html#appending-to-a-file",
    "href": "posts/python/p_7.html#appending-to-a-file",
    "title": "Files and exceptions",
    "section": "Appending to a File",
    "text": "Appending to a File\n\nwith open (\"python.txt\", 'a') as file_object:  # adding content\n    file_object.write(\"I also love finding meaning in the large datasets.\\n\")\n    file_object.write(\"I want to create an application with python.\\n\")\n\n\nSome examples\n\n# Prompt the user for their name\nname = input(\"Please enter your name: \")\n\n# Open the file in write mode and write the name to it\nwith open(\"guest.txt\", \"w\") as file:\n    file.write(name)\n\nprint(\"Thank you! Your name has been written to guest.txt.\")\n\nPlease enter your name: Rahul\nThank you! Your name has been written to guest.txt.\n\n\n\n# reading the above file\nfile_path = \"E:\\\\machine learning projects\\\\guest.txt\"\nwith open (file_path) as file_object:  \n    for line in file_object:\n        print(line)\n\nRahulKunal\n\nRahul\n\nKriti\n\nMegha\n\nSoumiksha\n\nRoshni\n\nSridevi\n\njatin\n\nsakshi\n\n\n\n\n# using while and break to store more names\nwhile True:\n    # Prompt the user for their name\n    name = input(\"Please enter your name (or 'q' to quit): \")\n\n    if name.lower() == 'q':\n        break\n\n    # Open the file in append mode and write the name to it\n    with open(\"guest.txt\", \"a\") as file:\n        file.write(name + '\\n')\n\nprint(\"Thank you! Your names have been added to guest.txt.\")\n\nPlease enter your name (or 'q' to quit): jatin\nPlease enter your name (or 'q' to quit): sakshi\nPlease enter your name (or 'q' to quit): q\nThank you! Your names have been added to guest.txt.\n\n\n\nUsing while and break loop to ask people why they like programming\n\n\n# using while and break to store more names\nwhile True:\n    # Prompt the user for their name\n    name = input(\"Please enter your name  \")\n    detail = input(\"why do you like programming? (or 'q' to quit): \")\n\n    if detail.lower() == 'q':\n        break\n\n    # Open the file in append mode and write the name to it\n    with open(\"programming.txt\", \"a\") as file:\n        file.write(name + detail + '\\n')\n\nprint(\"Thank you! Your names have been added to programming.txt.\")\n\nPlease enter your name  hogaya abhi\nwhy do you like programming? (or 'q' to quit): q\nThank you! Your names have been added to programming.txt.\n\n\n\n# reading the above file\nfile_path = \"E:\\\\machine learning projects\\\\programming.txt\"\nwith open (file_path) as file_object:  \n    for line in file_object:\n        print(line)\n\nI want to analyse big data and make an applicaiton\n\nI want to solve complex mathematical problems\n\ni am passionate about learning new things\n\ndata analysis and deep learning\n\nhelp vishal with complex analysis\n\nonline collaborations"
  },
  {
    "objectID": "posts/python/p_7.html#exceptions",
    "href": "posts/python/p_7.html#exceptions",
    "title": "Files and exceptions",
    "section": "Exceptions",
    "text": "Exceptions\n\n5/0\n\nZeroDivisionError: division by zero\n\n\n\n# using try-except block to handle this kind of error\n\n\ntry:\n    print(5/0)           #contains the code that we wish to try\nexcept ZeroDivisionError:\n    print(\"Error: You can't divide by zero!\")\n\nError: You can't divide by zero!\n\n\n\ntry:\n    print(5/0)           #contains the code that we wish to try\nexcept ZeroDivisionError:\n    print(\"Error: You can't divide by zero!\")\nelse:\n    print(answer)\n\nError: You can't divide by zero!\n\n\n\nhandling the FileNotFoundError Exception\n\n\nfilename = 'kunal.txt'\n\nwith open(filename, encoding= 'utf-8') as f:\n    contents = f.read()\n\nFileNotFoundError: [Errno 2] No such file or directory: 'kunal.txt'\n\n\n\n\nTackling the FileNotFoundError\n\nfilename = 'kunal.txt'\n\ntry:\n    with open(filename, encoding= 'utf-8') as f:\n        contents = f.read()\n    \nexcept FileNotFoundError:\n    print(f\"Sorry, the file {filename} doesnot exist.\")\n\nSorry, the file kunal.txt doesnot exist.\n\n\n\n\nAnalyzing text\n\ntitle = 'learning python the easy way'\ntitle.split()\n\n['learning', 'python', 'the', 'easy', 'way']\n\n\n\nfilename = 'kunal.txt'\n\ntry:\n    with open(filename, encoding='utf-8') as f:\n        contents = f.read()\nexcept FileNotFoundError:\n    print(f\"Sorry, the file{filename} doesnot exist.\")\nelse:\n    #count the approxiate words\n    words = contents.split()\n    num_words = len(words)\n    print(f\"The file {filename} has about {num_words} words.\\n\")\n    print(words)\n\nThe file kunal.txt has about 138 words.\n\n['Ce', 'chapitre', 'propose', 'd’utiliser', 'l’extension', 'Git', 'de', 'JupyterLab.', 'Un', 'tutoriel', 'présentant', 'cette', 'extension', 'est', 'disponible', 'ici.', 'Les', 'principaux', 'IDE', 'disponibles', '(Visual', 'Studio,', 'PyCharm,', 'RStudio)', 'présentent', 'des', 'fonctionalités', 'similaires.', 'Il', 'est', 'tout', 'à', 'fait', 'possible', 'd’en', 'utiliser', 'un', 'autre.', 'VisualStudio', 'propose', 'probablement,', 'à', 'l’heure', 'actuelle,', 'l’ensemble', 'le', 'plus', 'complet.', 'Certains', 'passages', 'de', 'ce', 'TD', 'nécessitent', 'd’utiliser', 'la', 'ligne', 'de', 'commande.', 'Il', 'est', 'tout', 'à', 'fait', 'possible', 'de', 'réaliser', 'ce', 'TD', 'entièrement', 'avec', 'celle-ci.', 'Cependant,', 'pour', 'une', 'personne', 'débutante', 'en', 'Git,', 'l’utilisation', 'd’une', 'interface', 'graphique', 'peut', 'constituer', 'un', 'élément', 'important', 'pour', 'la', 'compréhension', 'et', 'l’adoption', 'de', 'Git.', 'Une', 'fois', 'à', 'l’aise', 'avec', 'Git,', 'on', 'peut', 'tout', 'à', 'fait', 'se', 'passer', 'des', 'interfaces', 'graphiques', 'pour', 'les', 'routines', 'quotidiennes', 'et', 'ne', 'les', 'utiliser', 'que', 'pour', 'certaines', 'opérations', 'où', 'elles', 's’avèrent', 'fort', 'pratiques', '(notamment', 'la', 'comparaison', 'de', 'deux', 'fichiers', 'avant', 'de', 'devoir', 'fusionner).']\n\n\n\n\nWorking with multiple files\n\ndef count_words(filename):\n    \n\n    try:\n        with open(filename, encoding='utf-8') as f:\n            contents = f.read()\n    except FileNotFoundError:\n        print(f\"Sorry, the file{filename} doesnot exist.\")\n    else:\n    #count the approxiate words\n        words = contents.split()\n        num_words = len(words)\n        print(f\"The file {filename} has about {num_words} words.\\n\")\n\nfilename = 'kunal.txt'\ncount_words(filename)\n\nThe file kunal.txt has about 138 words.\n\n\n\n\n#missing file car.txt has no effect on program's execution\n\ndef count_words(filename):\n    \n    try:\n        with open(filename, encoding='utf-8') as f:\n            contents = f.read()\n    except FileNotFoundError:\n        print(f\"Sorry, the file {filename} doesnot exist.\")\n    else:\n    #count the approxiate words\n        words = contents.split()\n        num_words = len(words)\n        print(f\"The file {filename} has about {num_words} words.\\n\")\n\nfilenames = ['kunal.txt', 'programming.txt', 'python.txt', 'car.txt']   # car.txt is missing\nfor filename in filenames:\n    count_words(filename)\n\nThe file kunal.txt has about 138 words.\n\nThe file programming.txt has about 36 words.\n\nThe file python.txt has about 25 words.\n\nSorry, the file car.txt doesnot exist.\n\n\n\n\nFailing silently\n\ndef count_words(filename):\n    \n    try:\n        with open(filename, encoding='utf-8') as f:\n            contents = f.read()\n    except FileNotFoundError:\n        pass          #awesome, no need to write a default message\n        \n    else:\n    #count the approxiate words\n        words = contents.split()\n        num_words = len(words)\n        print(f\"The file {filename} has about {num_words} words.\\n\")\n\nfilenames = ['kunal.txt', 'programming.txt', 'python.txt', 'car.txt']   # car.txt is missing\nfor filename in filenames:\n    count_words(filename)\n\nThe file kunal.txt has about 138 words.\n\nThe file programming.txt has about 36 words.\n\nThe file python.txt has about 25 words.\n\n\n\n\n\nDeciding which errors to report\n\nIf users know which texts are supposed to be analyzed, they might appreciate a message informing them why some texts were not analyzed.\nContrary to it, If users expect to see some results but don’t know which texts are supposed to be analyzed, they might not need to know that some texts were unavailable.\n\n\n\nStoring data\n\nThe first program will use json.dump() to store the set of numbers, and the second program will use json.load().\n\n\n#1\nimport json\n\nnumbers = [12,3,31,123,44,23]\n\nfilename = 'numbers.json'\nwith open(filename, 'w') as f:   #'w' for writing\n    json.dump(numbers,f) \n\n\n#2\nimport json\n\nfilename = 'numbers.json'\nwith open(filename) as f:\n    numbers = json.load(f)    #' ' for reading\n    \nprint(numbers)\n\n[12, 3, 31, 123, 44, 23]\n\n\n\n\nSaving and using User-Generated Data\n\nSaving data with json is useful when you’re working with user-generated data, because if you don’t store your user’s information somehow, you’ll lose it when the program stops running.\n\n\nimport json\n\nusername = input(\"what is your name? \")\n\nfilename = \"username.json\"\nwith open(filename, 'w') as f:\n    json.dump(username, f)\n    print(f\"We'll remember your name when you come back, {username_2}!\")\n\nwhat is your name? kk\nWe'll remember your name when you come back, kk!\n\n\n\nimport json\n\nfilename = \"username.json\"\nwith open(filename) as f:\n    username = json.load(f)\n    print(f\"Welcome back, {username}!\")\n\nWelcome back, kk!\n\n\n\n\nRefactoring\n1. means to improve the code by breaking it up into a series of functions for specific jobs.\n\n#1\nimport json\n\ndef greet_user():\n    filename = \"username.json\"\n    try:\n        with open(filename) as f:\n            username = json.load(f)\n    except FileNotFoundError:\n        username = input(\"What is your name? \")\n        with open(filename, 'w') as f:\n            json.dump(username, f)\n            print(f\"We'll remember you when you come back, {username}!\")\n    else:\n        print(f\"Welcome back, {username}!\")\n        \ngreet_user()\n\nWelcome back, kk!\n\n\n\n#2\n# store information and greet the user\ndef get_stored_username():\n    filename = 'username.json'\n    try:\n        with open(filename) as f:\n            username = json.load(f)\n    except FileNotFoundError:\n        return None\n    else:\n        return username\ndef greet_user():\n    username = input(\"What is your name? \")\n    if username:\n        print(f\"Welcome back, {username}!\")\n    else:\n        username = input(\"What is your name? \")\n        filename = 'username.json'\n        with open(filename, 'w') as f:\n            json.dump(username, f)\n            print(f\"We'll remember you when you come back, {username}!\")\n            \n            \ngreet_user()\n\nWhat is your name? Rahul\nWelcome back, Rahul!\n\n\n\n#3\n# writing a code that stores the prompt for a new username\n\nimport json\n\ndef get_stored_username():\n    filename = 'username.json'\n    try:\n        with open(filename) as f:\n            username = json.load(f)\n    except FileNotFoundError:\n        return None\n    else:\n        return username\n\ndef get_new_username():\n    username = input(\"What is your name? \")\n    filename = 'username.json'\n    with open(filename, 'w') as f:\n        json.dump(username, f)\n    return username\n\ndef greet_user():\n    username = get_stored_username()\n    if username:\n        print(f\"Welcome back, {username}!\")\n    else:\n        username = get_new_username()\n        print(f\"We'll remember you when your come back, {username}!\")\n        \ngreet_user()\n\nWelcome back, kk!"
  },
  {
    "objectID": "posts/python/Time_series-checkpoint.html",
    "href": "posts/python/Time_series-checkpoint.html",
    "title": "Time Series",
    "section": "",
    "text": "Converting between Sting and Datetime\n\n\n\n\n\nIndexing, selection, subsetting\nTime series with duplicate indices\n\n\n\n\n\nGenerating Date Ranges\nFrequencies and Date offsets\nShifting (Leading and Lagging) Data\nShifting dates with offsets\n\n\n\n\n\nTime zone Loacalization and Conversion\nOperations with Time Zone-Aware Timestamp Objects\nOperations between different time zones\n\n\n\n\n\nperiod frequency conversion\nQuaterly period frequencies\nConverting timestamps to periods (and back)\nCreating a PeriodIndex from Arrays\n\n\n\n\n\nDonwsampling\nOpen-high-low-close(OHLC) resampling\nUnsampling and Interpolation\nResampling with periods\nGrouped time resampling\n\n\n\n\n\nExponentially weighted functions\nBinary moving window functions\nUser-defined window functions\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom datetime import datetime\n\n\n## Data and Time Data Types and Tools\n\nnow = datetime.now()\n\nnow\n\n\nnow.year, now.month, now.day\n\n\ndelta = datetime(2011, 1, 7) - datetime(2008, 6, 3, 4, 1, 4)\n\ndelta\n\n\ndelta.days\n\n\ndelta.seconds\n\n\n# add or subtract a timedelta to yield a new shifted object\n\nfrom datetime import timedelta\n\nstart = datetime(2024, 2, 28)\n\nstart + timedelta(12)\n\n\nstart - 2 * timedelta(12)\n\n\nhelp(datetime)\n\n\n### Converting between String and DateFrame\n\nstamp = datetime(2024, 1, 30)\n\nstr(stamp)\n\n\nstamp.strftime(\"%Y-%m-%d\")\n\n\nvalue = '2024-2-28'\ndatetime.strptime(value, '%Y-%m-%d')\n\n\ndatestrs = [\"2/23/2024\", \"2/28/2024\"]\n\n[datetime.strptime(x, \"%m/%d/%Y\") for x in datestrs]\n\n\n# using pandas\ndatestrs = [\"2024-2-23 12:00:00\", \"2024-2-28 00:00:00\"]\n\n\npd.to_datetime(datestrs)\n\n\nidx = pd.to_datetime(datestrs+ [None])\n\n\nidx\n\n\nidx[2]\n\n\npd.isna(idx)\n\n\n\n\n\ndates = [datetime(2024,2,23), datetime(2024,2,24),\n        datetime(2024,2,25), datetime(2024,2,26),\n        datetime(2024,2,27), datetime(2024,2,28)]\n\n\nts = pd.Series(np.random.standard_normal(6),\n              index = dates)\n\n\nts\n\n\nts.index\n\n\nts + ts[::2]\n\n\nts.index.dtype\n\n\nstamp = ts.index[0]\n\n\nstamp\n\n\n\n\nstamp = ts.index[2]\n\n\nts[stamp]\n\n\nstamp\n\n\nts[\"2024-02-25\"]\n\n\nlonger_ts = pd.Series(np.random.standard_normal(1000),\n                     index = pd.date_range(\"2000-01-01\", \n                                           periods=1000))\n\n\nlonger_ts\n\n\nlonger_ts[\"2001\"]\n\n\nlonger_ts[\"2001-05\"]\n\n\n# slicing with datatime objects works aswell\n\nts[datetime(2011,1,7):]\n\n\nts.truncate(after=\"2001-01-09\")\n\n\n# for DataFrame\ndates_df = pd.date_range(\"2024-01-01\", \n                         periods=100,\n                        freq= \"W-WED\")\n\n\nlong_df = pd.DataFrame(np.random.standard_normal((6, 4)),\n                      index = dates, \n                      columns = ['Colorado', 'Texas', \n                                'New York', \"Ohio\"])\n\n\n\nlong_df.loc[\"2011-05-01\"]\n\n\n\n\n\ndates = pd.DatetimeIndex([\"2017-12-03\", \"2018-11-06\", \"2019-11-06\",\n                          \"2021-01-28\", \"2022-09-06\"])\n\ndup_ts = pd.Series(np.arange(5), index=dates)\n\n\ndup_ts\n\n\n# checking if the index is unique\ndup_ts.index.is_unique\n\n\n# unique indexes can be grouped\ngrouped = dup_ts.groupby(level=0)\n\n\ngrouped.mean()\n\n\ngrouped.count()\n\n\n\n\n\n\nts\n\n\nresampler = ts.resample(\"D\")\n\n\nresampler\n\n\n\n\nshortcut details\n\n\nindex = pd.date_range(\"2012-04-01\", \"2012-06-01\")\nindex\n\n\npd.date_range(start=\"2024-04-01\", periods=20)\n\n\npd.date_range(end=\"2024-03-24\", periods=20)\n\n\npd.date_range(\"2000-01-01\", '2000-12-01', freq=\"BM\")\n\n\n\n\n\nfrom pandas.tseries.offsets import Hour, Minute\n\n\nhour = Hour()\n\n\nhour\n\n\nfour_hours = Hour(4)\n\n\nfour_hours\n\n\n# combined addition\nHour(2) + Minute(30)\n\n\npd.date_range(\"2023-05-01\", periods = 10, freq='1h30min')\n\n\n# week of month dates\nmonthly_dates = pd.date_range(\"2012-01-01\", \"2012-09-01\",\n                             freq=\"WOM-3FRI\")\nlist(monthly_dates)\n\n\n\n\n\nmoving backward and forward through time\n\n\nts = pd.Series(np.random.standard_normal(4),\n              index=pd.date_range(\"2022-01-01\", periods=4,\n                                 freq=\"M\"))\nts\n\n\nts.shift(2)\n\n\nts.shift(-2)\n\n\nts/ts.shift(1)-1\n\n\nts.shift(2, freq=\"M\")\n\n\n# groupby\nts = pd.Series(np.random.standard_normal(20),\n              index=pd.date_range(\"2024-10-02\",\n                                 periods=20, freq=\"4D\"))\n\n\nts\n\n\nts.groupby(MonthEnd().rollfoward).mean()\n\n\nts.resample(\"M\").mean()\n\n2024-10-31    0.076767\n2024-11-30   -0.177065\n2024-12-31    0.635081\nFreq: M, dtype: float64\n\n\n\n\n\n\n\nimport pytz\n\npytz.common_timezones[-5:]\n\n['US/Eastern', 'US/Hawaii', 'US/Mountain', 'US/Pacific', 'UTC']\n\n\n\n# to get timezones, use pytz.timezone\n\ntz = pytz.timezone(\"America/New_York\")\n\ntz\n\n&lt;DstTzInfo 'America/New_York' LMT-1 day, 19:04:00 STD&gt;\n\n\n\n### Time zone localization and conversion\ndates = pd.date_range(\"2021-11-17 09:30\", periods=6)\n\n\nts = pd.Series(np.random.standard_normal(len(dates)),\n              index = dates)\n\n\nts\n\n2021-11-17 09:30:00    0.808643\n2021-11-18 09:30:00    1.435830\n2021-11-19 09:30:00    0.764818\n2021-11-20 09:30:00    0.345263\n2021-11-21 09:30:00   -0.671032\n2021-11-22 09:30:00    0.694027\nFreq: D, dtype: float64\n\n\n\nprint(ts.index.tz)\n\nNone\n\n\n\n# Dates from timezone set\npd.date_range(\"2023-3-01\", periods=10, tz=\"UTC\")\n\nDatetimeIndex(['2023-03-01 00:00:00+00:00', '2023-03-02 00:00:00+00:00',\n               '2023-03-03 00:00:00+00:00', '2023-03-04 00:00:00+00:00',\n               '2023-03-05 00:00:00+00:00', '2023-03-06 00:00:00+00:00',\n               '2023-03-07 00:00:00+00:00', '2023-03-08 00:00:00+00:00',\n               '2023-03-09 00:00:00+00:00', '2023-03-10 00:00:00+00:00'],\n              dtype='datetime64[ns, UTC]', freq='D')\n\n\n\n\n\n\npd.Series(np.random.standard_normal(6),\n       )\n\n0    0.434382\n1    1.383303\n2    0.975721\n3   -1.377090\n4    0.659404\n5    1.098894\ndtype: float64\n\n\n\nvalues= [\"2001Q3\", \"20022Q2\", '20033Q1']\n\n\nperiods = pd.period_range(\"2000-01-01\", \"2000-06-30\",\n                          freq=\"M\")\n\n\nindex = pd.PeriodIndex(values, freq=\"Q-Dec\")\n\n\np = pd.Period(\"2011\", freq= \"A-Jun\")\n\n\np\n\nPeriod('2011', 'A-JUN')\n\n\n\np.asfreq(\"M\", how='start')\n\nPeriod('2010-07', 'M')\n\n\n\np.asfreq(\"M\", how=\"end\")\n\nPeriod('2011-06', 'M')\n\n\n\n# period index\nperiods = pd.period_range(\"2006\", \"2009\", \n                         freq=\"A-Dec\")\n\n\nts= pd.Series(np.random.standard_normal\n             (len(periods)), index= periods)\n\n\nts\n\n2006    1.079865\n2007   -1.891582\n2008   -0.634198\n2009    0.155782\nFreq: A-DEC, dtype: float64\n\n\n\nts.asfreq(\"M\", how='start')\n\n2006-01    1.079865\n2007-01   -1.891582\n2008-01   -0.634198\n2009-01    0.155782\nFreq: M, dtype: float64\n\n\n\n\n\ndates = pd.date_range(\"2000-01-01\",\n                     periods=3, freq=\"M\")\n\n\nts= pd.Series(np.random.standard_normal(3),\n             index=dates)\n\n\nts\n\n2000-01-31   -1.472182\n2000-02-29    0.042816\n2000-03-31    1.232869\nFreq: M, dtype: float64\n\n\n\npts = ts.to_period()\n\n\npts\n\n2000-01   -1.472182\n2000-02    0.042816\n2000-03    1.232869\nFreq: M, dtype: float64\n\n\n\ndates = pd.date_range(\"2022-01-29\", periods=6)\n\n\nts2 = pd.Series(np.random.standard_normal(6),\n               index= dates)\n\n\nts2\n\n2022-01-29   -0.865524\n2022-01-30    1.518387\n2022-01-31    0.327414\n2022-02-01    0.380410\n2022-02-02   -0.984295\n2022-02-03   -2.798704\nFreq: D, dtype: float64\n\n\n\nts2.to_period\n\n&lt;bound method Series.to_period of 2022-01-29   -0.865524\n2022-01-30    1.518387\n2022-01-31    0.327414\n2022-02-01    0.380410\n2022-02-02   -0.984295\n2022-02-03   -2.798704\nFreq: D, dtype: float64&gt;\n\n\n\npts = ts2.to_period()\n\n\npts\n\n2022-01-29   -0.865524\n2022-01-30    1.518387\n2022-01-31    0.327414\n2022-02-01    0.380410\n2022-02-02   -0.984295\n2022-02-03   -2.798704\nFreq: D, dtype: float64\n\n\n\npts.to_timestamp(how='end')\n\n2022-01-29 23:59:59.999999999   -0.865524\n2022-01-30 23:59:59.999999999    1.518387\n2022-01-31 23:59:59.999999999    0.327414\n2022-02-01 23:59:59.999999999    0.380410\n2022-02-02 23:59:59.999999999   -0.984295\n2022-02-03 23:59:59.999999999   -2.798704\nFreq: D, dtype: float64\n\n\n\n\n\n\n\ndates = pd.date_range(\"2022-12-10\", periods=100)\n\n\nts= pd.Series(np.random.standard_normal(len(dates)),\n             index=dates)\n\n\nts\n\n2022-12-10    0.655061\n2022-12-11   -2.144779\n2022-12-12    0.489381\n2022-12-13   -0.117629\n2022-12-14    0.782097\n                ...   \n2023-03-15   -1.263925\n2023-03-16   -0.317576\n2023-03-17    0.398997\n2023-03-18   -0.237727\n2023-03-19   -1.407480\nFreq: D, Length: 100, dtype: float64\n\n\n\nts.resample(\"M\").mean()\n\n2022-12-31   -0.177024\n2023-01-31    0.218224\n2023-02-28   -0.040488\n2023-03-31   -0.288735\nFreq: M, dtype: float64\n\n\n\nts.resample(\"M\", kind=\"period\").mean()\n\n2022-12   -0.177024\n2023-01    0.218224\n2023-02   -0.040488\n2023-03   -0.288735\nFreq: M, dtype: float64\n\n\n\n\n\ndates = pd.date_range(\"2022-01-01\",\n                     periods=12, freq=\"T\")\n\n\nts = pd.Series(np.arange(len(dates)), index=dates)\n\n\nts\n\n2022-01-01 00:00:00     0\n2022-01-01 00:01:00     1\n2022-01-01 00:02:00     2\n2022-01-01 00:03:00     3\n2022-01-01 00:04:00     4\n2022-01-01 00:05:00     5\n2022-01-01 00:06:00     6\n2022-01-01 00:07:00     7\n2022-01-01 00:08:00     8\n2022-01-01 00:09:00     9\n2022-01-01 00:10:00    10\n2022-01-01 00:11:00    11\nFreq: T, dtype: int32\n\n\n\nts.resample(\"5min\").sum()\n\n2022-01-01 00:00:00    10\n2022-01-01 00:05:00    35\n2022-01-01 00:10:00    21\nFreq: 5T, dtype: int32\n\n\n\nts.resample(\"5min\", closed=\"right\", \n            label=\"right\").sum()\n\n2022-01-01 00:00:00     0\n2022-01-01 00:05:00    15\n2022-01-01 00:10:00    40\n2022-01-01 00:15:00    11\nFreq: 5T, dtype: int32\n\n\n\n\n\n\nts = pd.Series(np.random.permutation\n               (np.arange(len(dates))), index=dates)\n\n\nts.resample(\"5min\").ohlc()\n\n\n\n\n\n\n\n\nopen\nhigh\nlow\nclose\n\n\n\n\n2022-01-01 00:00:00\n1\n11\n1\n5\n\n\n2022-01-01 00:05:00\n3\n7\n0\n0\n\n\n2022-01-01 00:10:00\n8\n10\n8\n10\n\n\n\n\n\n\n\n\n\n\n\nframe = pd.DataFrame(np.random.standard_normal((2,5)),\n                    index=pd.date_range(\"2022-01-01\",\n                                        periods=2, freq=\"W-WED\"),\n                     columns = [\"fdk\", 'sadik', 'golewala','pipli','mudki'])\n\n\nframe\n\n\n\n\n\n\n\n\nfdk\nsadik\ngolewala\npipli\nmudki\n\n\n\n\n2022-01-05\n-0.500335\n-0.286433\n0.805294\n0.557015\n-1.293101\n\n\n2022-01-12\n0.470091\n-0.574010\n-0.817633\n0.197509\n0.189306\n\n\n\n\n\n\n\n\nfor n in range(10):\n    print(n)\n\n\n#list comprehension\n\n[n**2 + 2 for n in range(5)]\n\n[2, 3, 6, 11, 18]\n\n\n\n# for loop\nfor n in [n**2 + 2 for n in range(5)]:\n    print(n)\n\n2\n3\n6\n11\n18\n\n\n\ndf_daily = frame.resample('D').asfreq()\n\n\ndf_daily\n\n\n\n\n\n\n\n\nfdk\nsadik\ngolewala\npipli\nmudki\n\n\n\n\n2022-01-05\n-0.500335\n-0.286433\n0.805294\n0.557015\n-1.293101\n\n\n2022-01-06\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2022-01-07\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2022-01-08\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2022-01-09\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2022-01-10\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2022-01-11\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2022-01-12\n0.470091\n-0.574010\n-0.817633\n0.197509\n0.189306\n\n\n\n\n\n\n\n\n# filling certain number of periods\nframe.resample(\"D\").ffill(limit=2)\n\n\n\n\n\n\n\n\nfdk\nsadik\ngolewala\npipli\nmudki\n\n\n\n\n2022-01-05\n-0.500335\n-0.286433\n0.805294\n0.557015\n-1.293101\n\n\n2022-01-06\n-0.500335\n-0.286433\n0.805294\n0.557015\n-1.293101\n\n\n2022-01-07\n-0.500335\n-0.286433\n0.805294\n0.557015\n-1.293101\n\n\n2022-01-08\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2022-01-09\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2022-01-10\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2022-01-11\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2022-01-12\n0.470091\n-0.574010\n-0.817633\n0.197509\n0.189306\n\n\n\n\n\n\n\n\nframe.resample(\"W-THU\").ffill()\n\n\n\n\n\n\n\n\nfdk\nsadik\ngolewala\npipli\nmudki\n\n\n\n\n2022-01-06\n-0.500335\n-0.286433\n0.805294\n0.557015\n-1.293101\n\n\n2022-01-13\n0.470091\n-0.574010\n-0.817633\n0.197509\n0.189306\n\n\n\n\n\n\n\n\n\n\n\nframe2 = pd.DataFrame(np.random.standard_normal((24,4)),\n                     index = pd.period_range(\"1-2000\",\"12-2001\",\n                                            freq=\"M\"),\n                     columns=['fzp', 'fdk', 'btd', 'mudki'])\n\n\nframe2.head()\n\n\n\n\n\n\n\n\nfzp\nfdk\nbtd\nmudki\n\n\n\n\n2000-01\n-0.438864\n0.122812\n-1.157890\n0.403075\n\n\n2000-02\n-0.688525\n-0.721772\n0.011549\n-0.979600\n\n\n2000-03\n0.607322\n-1.119676\n0.089442\n0.060338\n\n\n2000-04\n0.690526\n-0.095600\n0.212831\n0.410823\n\n\n2000-05\n-1.016129\n-0.430221\n-1.235741\n0.250289\n\n\n\n\n\n\n\n\nannual_frame = frame2.resample(\"A-DEC\").mean()\n\n\nannual_frame\n\n\n\n\n\n\n\n\nfzp\nfdk\nbtd\nmudki\n\n\n\n\n2000\n-0.094852\n-0.436623\n-0.600286\n-0.715234\n\n\n2001\n0.025365\n-0.347140\n0.035530\n-0.521633\n\n\n\n\n\n\n\n\n# Q-DEC:Quarterly, year ending in Dec\nannual_frame.resample(\"Q-DEC\").ffill()\n\n\n\n\n\n\n\n\nfzp\nfdk\nbtd\nmudki\n\n\n\n\n2000Q1\n-0.094852\n-0.436623\n-0.600286\n-0.715234\n\n\n2000Q2\n-0.094852\n-0.436623\n-0.600286\n-0.715234\n\n\n2000Q3\n-0.094852\n-0.436623\n-0.600286\n-0.715234\n\n\n2000Q4\n-0.094852\n-0.436623\n-0.600286\n-0.715234\n\n\n2001Q1\n0.025365\n-0.347140\n0.035530\n-0.521633\n\n\n2001Q2\n0.025365\n-0.347140\n0.035530\n-0.521633\n\n\n2001Q3\n0.025365\n-0.347140\n0.035530\n-0.521633\n\n\n2001Q4\n0.025365\n-0.347140\n0.035530\n-0.521633\n\n\n\n\n\n\n\n\nannual_frame.resample(\"Q-Dec\", convention=\"end\").asfreq()\n\n\n\n\n\n\n\n\nfzp\nfdk\nbtd\nmudki\n\n\n\n\n2000Q4\n-0.094852\n-0.436623\n-0.600286\n-0.715234\n\n\n2001Q1\nNaN\nNaN\nNaN\nNaN\n\n\n2001Q2\nNaN\nNaN\nNaN\nNaN\n\n\n2001Q3\nNaN\nNaN\nNaN\nNaN\n\n\n2001Q4\n0.025365\n-0.347140\n0.035530\n-0.521633\n\n\n\n\n\n\n\n\nannual_frame.resample(\"Q-MAR\").ffill()\n\n\n\n\n\n\n\n\nfzp\nfdk\nbtd\nmudki\n\n\n\n\n2000Q4\n-0.094852\n-0.436623\n-0.600286\n-0.715234\n\n\n2001Q1\n-0.094852\n-0.436623\n-0.600286\n-0.715234\n\n\n2001Q2\n-0.094852\n-0.436623\n-0.600286\n-0.715234\n\n\n2001Q3\n-0.094852\n-0.436623\n-0.600286\n-0.715234\n\n\n2001Q4\n0.025365\n-0.347140\n0.035530\n-0.521633\n\n\n2002Q1\n0.025365\n-0.347140\n0.035530\n-0.521633\n\n\n2002Q2\n0.025365\n-0.347140\n0.035530\n-0.521633\n\n\n2002Q3\n0.025365\n-0.347140\n0.035530\n-0.521633\n\n\n\n\n\n\n\n\n\n\n\nN = 15\ntimes = pd.date_range(\"2024-02-29 00:00\", freq=\"1min\",\n                     periods=N)\ndf = pd.DataFrame({'time':times,\n                  \"value\":np.arange(N)})\n\n\ndf\n\n\n\n\n\n\n\n\ntime\nvalue\n\n\n\n\n0\n2024-02-29 00:00:00\n0\n\n\n1\n2024-02-29 00:01:00\n1\n\n\n2\n2024-02-29 00:02:00\n2\n\n\n3\n2024-02-29 00:03:00\n3\n\n\n4\n2024-02-29 00:04:00\n4\n\n\n5\n2024-02-29 00:05:00\n5\n\n\n6\n2024-02-29 00:06:00\n6\n\n\n7\n2024-02-29 00:07:00\n7\n\n\n8\n2024-02-29 00:08:00\n8\n\n\n9\n2024-02-29 00:09:00\n9\n\n\n10\n2024-02-29 00:10:00\n10\n\n\n11\n2024-02-29 00:11:00\n11\n\n\n12\n2024-02-29 00:12:00\n12\n\n\n13\n2024-02-29 00:13:00\n13\n\n\n14\n2024-02-29 00:14:00\n14\n\n\n\n\n\n\n\n\n# resmpling for 5 min count\ntimes2 = pd.date_range(\"2024-02-29 00:00\", freq=\"5min\",\n                     periods=N)\ndf2 = pd.DataFrame({'time':times2,\n                  \"value\":np.arange(N)})\ndf2\n\n\n\n\n\n\n\n\ntime\nvalue\n\n\n\n\n0\n2024-02-29 00:00:00\n0\n\n\n1\n2024-02-29 00:05:00\n1\n\n\n2\n2024-02-29 00:10:00\n2\n\n\n3\n2024-02-29 00:15:00\n3\n\n\n4\n2024-02-29 00:20:00\n4\n\n\n5\n2024-02-29 00:25:00\n5\n\n\n6\n2024-02-29 00:30:00\n6\n\n\n7\n2024-02-29 00:35:00\n7\n\n\n8\n2024-02-29 00:40:00\n8\n\n\n9\n2024-02-29 00:45:00\n9\n\n\n10\n2024-02-29 00:50:00\n10\n\n\n11\n2024-02-29 00:55:00\n11\n\n\n12\n2024-02-29 01:00:00\n12\n\n\n13\n2024-02-29 01:05:00\n13\n\n\n14\n2024-02-29 01:10:00\n14\n\n\n\n\n\n\n\n\n# simply resampling\ndf.set_index('time').resample(\"5min\").count()\n\n\n\n\n\n\n\n\nvalue\n\n\ntime\n\n\n\n\n\n2024-02-29 00:00:00\n5\n\n\n2024-02-29 00:05:00\n5\n\n\n2024-02-29 00:10:00\n5\n\n\n\n\n\n\n\n\n# DataFrame with multiple time series, \n\ndf3 = pd.DataFrame({\"time\":times.repeat(3),\n                   \"key\":np.tile([\"a\",\"b\",\"c\"],N),\n                   'value':np.arange(N*3.)})\n\n\ndf3.head()\n\n\n\n\n\n\n\n\ntime\nkey\nvalue\n\n\n\n\n0\n2024-02-29 00:00:00\na\n0.0\n\n\n1\n2024-02-29 00:00:00\nb\n1.0\n\n\n2\n2024-02-29 00:00:00\nc\n2.0\n\n\n3\n2024-02-29 00:01:00\na\n3.0\n\n\n4\n2024-02-29 00:01:00\nb\n4.0\n\n\n\n\n\n\n\n\n# pandas grouper object\ntime_key = pd.Grouper(freq=\"5min\")\n\n\nresampled = (df3.set_index(\"time\")\n            .groupby([\"key\", time_key])\n            .sum())\n\n\nresampled\n\n\n\n\n\n\n\n\n\nvalue\n\n\nkey\ntime\n\n\n\n\n\na\n2024-02-29 00:00:00\n30.0\n\n\n2024-02-29 00:05:00\n105.0\n\n\n2024-02-29 00:10:00\n180.0\n\n\nb\n2024-02-29 00:00:00\n35.0\n\n\n2024-02-29 00:05:00\n110.0\n\n\n2024-02-29 00:10:00\n185.0\n\n\nc\n2024-02-29 00:00:00\n40.0\n\n\n2024-02-29 00:05:00\n115.0\n\n\n2024-02-29 00:10:00\n190.0\n\n\n\n\n\n\n\n\nresampled.reset_index()\n\n\n\n\n\n\n\n\nkey\ntime\nvalue\n\n\n\n\n0\na\n2024-02-29 00:00:00\n30.0\n\n\n1\na\n2024-02-29 00:05:00\n105.0\n\n\n2\na\n2024-02-29 00:10:00\n180.0\n\n\n3\nb\n2024-02-29 00:00:00\n35.0\n\n\n4\nb\n2024-02-29 00:05:00\n110.0\n\n\n5\nb\n2024-02-29 00:10:00\n185.0\n\n\n6\nc\n2024-02-29 00:00:00\n40.0\n\n\n7\nc\n2024-02-29 00:05:00\n115.0\n\n\n8\nc\n2024-02-29 00:10:00\n190.0\n\n\n\n\n\n\n\n\n\n\n\n\nused for noisy or gappy data\nexclude automatically missing data\n\n\ndata = pd.read_csv(\"E:\\pythonfordatanalysis\\semainedu26fevrier\\iris.csv\")\n\n\ndata.columns\n\nIndex(['Id', 'Sepal Length (cm)', 'Sepal Width (cm)', 'Petal Length (cm)',\n       'Petal Width (cm)', 'Species'],\n      dtype='object')\n\n\n\ndata['Sepal Width (cm)'].plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\ndata[\"Sepal Width (cm)\"].rolling(5).mean().plot()\n\n&lt;Axes: &gt;"
  },
  {
    "objectID": "posts/python/Time_series-checkpoint.html#date-and-time-data-types-and-tools",
    "href": "posts/python/Time_series-checkpoint.html#date-and-time-data-types-and-tools",
    "title": "Time Series",
    "section": "",
    "text": "Converting between Sting and Datetime"
  },
  {
    "objectID": "posts/python/Time_series-checkpoint.html#time-series-basics",
    "href": "posts/python/Time_series-checkpoint.html#time-series-basics",
    "title": "Time Series",
    "section": "",
    "text": "Indexing, selection, subsetting\nTime series with duplicate indices"
  },
  {
    "objectID": "posts/python/Time_series-checkpoint.html#data-ranges-frequencies-and-shifting",
    "href": "posts/python/Time_series-checkpoint.html#data-ranges-frequencies-and-shifting",
    "title": "Time Series",
    "section": "",
    "text": "Generating Date Ranges\nFrequencies and Date offsets\nShifting (Leading and Lagging) Data\nShifting dates with offsets"
  },
  {
    "objectID": "posts/python/Time_series-checkpoint.html#time-zone-handling",
    "href": "posts/python/Time_series-checkpoint.html#time-zone-handling",
    "title": "Time Series",
    "section": "",
    "text": "Time zone Loacalization and Conversion\nOperations with Time Zone-Aware Timestamp Objects\nOperations between different time zones"
  },
  {
    "objectID": "posts/python/Time_series-checkpoint.html#periods-and-period-arithmetic",
    "href": "posts/python/Time_series-checkpoint.html#periods-and-period-arithmetic",
    "title": "Time Series",
    "section": "",
    "text": "period frequency conversion\nQuaterly period frequencies\nConverting timestamps to periods (and back)\nCreating a PeriodIndex from Arrays"
  },
  {
    "objectID": "posts/python/Time_series-checkpoint.html#resampling-and-frequency-conversion",
    "href": "posts/python/Time_series-checkpoint.html#resampling-and-frequency-conversion",
    "title": "Time Series",
    "section": "",
    "text": "Donwsampling\nOpen-high-low-close(OHLC) resampling\nUnsampling and Interpolation\nResampling with periods\nGrouped time resampling"
  },
  {
    "objectID": "posts/python/Time_series-checkpoint.html#moving-window-functions",
    "href": "posts/python/Time_series-checkpoint.html#moving-window-functions",
    "title": "Time Series",
    "section": "",
    "text": "Exponentially weighted functions\nBinary moving window functions\nUser-defined window functions\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom datetime import datetime\n\n\n## Data and Time Data Types and Tools\n\nnow = datetime.now()\n\nnow\n\n\nnow.year, now.month, now.day\n\n\ndelta = datetime(2011, 1, 7) - datetime(2008, 6, 3, 4, 1, 4)\n\ndelta\n\n\ndelta.days\n\n\ndelta.seconds\n\n\n# add or subtract a timedelta to yield a new shifted object\n\nfrom datetime import timedelta\n\nstart = datetime(2024, 2, 28)\n\nstart + timedelta(12)\n\n\nstart - 2 * timedelta(12)\n\n\nhelp(datetime)\n\n\n### Converting between String and DateFrame\n\nstamp = datetime(2024, 1, 30)\n\nstr(stamp)\n\n\nstamp.strftime(\"%Y-%m-%d\")\n\n\nvalue = '2024-2-28'\ndatetime.strptime(value, '%Y-%m-%d')\n\n\ndatestrs = [\"2/23/2024\", \"2/28/2024\"]\n\n[datetime.strptime(x, \"%m/%d/%Y\") for x in datestrs]\n\n\n# using pandas\ndatestrs = [\"2024-2-23 12:00:00\", \"2024-2-28 00:00:00\"]\n\n\npd.to_datetime(datestrs)\n\n\nidx = pd.to_datetime(datestrs+ [None])\n\n\nidx\n\n\nidx[2]\n\n\npd.isna(idx)"
  },
  {
    "objectID": "posts/python/Time_series-checkpoint.html#time-series-basics-1",
    "href": "posts/python/Time_series-checkpoint.html#time-series-basics-1",
    "title": "Time Series",
    "section": "",
    "text": "dates = [datetime(2024,2,23), datetime(2024,2,24),\n        datetime(2024,2,25), datetime(2024,2,26),\n        datetime(2024,2,27), datetime(2024,2,28)]\n\n\nts = pd.Series(np.random.standard_normal(6),\n              index = dates)\n\n\nts\n\n\nts.index\n\n\nts + ts[::2]\n\n\nts.index.dtype\n\n\nstamp = ts.index[0]\n\n\nstamp\n\n\n\n\nstamp = ts.index[2]\n\n\nts[stamp]\n\n\nstamp\n\n\nts[\"2024-02-25\"]\n\n\nlonger_ts = pd.Series(np.random.standard_normal(1000),\n                     index = pd.date_range(\"2000-01-01\", \n                                           periods=1000))\n\n\nlonger_ts\n\n\nlonger_ts[\"2001\"]\n\n\nlonger_ts[\"2001-05\"]\n\n\n# slicing with datatime objects works aswell\n\nts[datetime(2011,1,7):]\n\n\nts.truncate(after=\"2001-01-09\")\n\n\n# for DataFrame\ndates_df = pd.date_range(\"2024-01-01\", \n                         periods=100,\n                        freq= \"W-WED\")\n\n\nlong_df = pd.DataFrame(np.random.standard_normal((6, 4)),\n                      index = dates, \n                      columns = ['Colorado', 'Texas', \n                                'New York', \"Ohio\"])\n\n\n\nlong_df.loc[\"2011-05-01\"]\n\n\n\n\n\ndates = pd.DatetimeIndex([\"2017-12-03\", \"2018-11-06\", \"2019-11-06\",\n                          \"2021-01-28\", \"2022-09-06\"])\n\ndup_ts = pd.Series(np.arange(5), index=dates)\n\n\ndup_ts\n\n\n# checking if the index is unique\ndup_ts.index.is_unique\n\n\n# unique indexes can be grouped\ngrouped = dup_ts.groupby(level=0)\n\n\ngrouped.mean()\n\n\ngrouped.count()"
  },
  {
    "objectID": "posts/python/Time_series-checkpoint.html#data-ranges-frequencies-and-shifting-1",
    "href": "posts/python/Time_series-checkpoint.html#data-ranges-frequencies-and-shifting-1",
    "title": "Time Series",
    "section": "",
    "text": "ts\n\n\nresampler = ts.resample(\"D\")\n\n\nresampler\n\n\n\n\nshortcut details\n\n\nindex = pd.date_range(\"2012-04-01\", \"2012-06-01\")\nindex\n\n\npd.date_range(start=\"2024-04-01\", periods=20)\n\n\npd.date_range(end=\"2024-03-24\", periods=20)\n\n\npd.date_range(\"2000-01-01\", '2000-12-01', freq=\"BM\")\n\n\n\n\n\nfrom pandas.tseries.offsets import Hour, Minute\n\n\nhour = Hour()\n\n\nhour\n\n\nfour_hours = Hour(4)\n\n\nfour_hours\n\n\n# combined addition\nHour(2) + Minute(30)\n\n\npd.date_range(\"2023-05-01\", periods = 10, freq='1h30min')\n\n\n# week of month dates\nmonthly_dates = pd.date_range(\"2012-01-01\", \"2012-09-01\",\n                             freq=\"WOM-3FRI\")\nlist(monthly_dates)\n\n\n\n\n\nmoving backward and forward through time\n\n\nts = pd.Series(np.random.standard_normal(4),\n              index=pd.date_range(\"2022-01-01\", periods=4,\n                                 freq=\"M\"))\nts\n\n\nts.shift(2)\n\n\nts.shift(-2)\n\n\nts/ts.shift(1)-1\n\n\nts.shift(2, freq=\"M\")\n\n\n# groupby\nts = pd.Series(np.random.standard_normal(20),\n              index=pd.date_range(\"2024-10-02\",\n                                 periods=20, freq=\"4D\"))\n\n\nts\n\n\nts.groupby(MonthEnd().rollfoward).mean()\n\n\nts.resample(\"M\").mean()\n\n2024-10-31    0.076767\n2024-11-30   -0.177065\n2024-12-31    0.635081\nFreq: M, dtype: float64"
  },
  {
    "objectID": "posts/python/Time_series-checkpoint.html#time-zone-handling-1",
    "href": "posts/python/Time_series-checkpoint.html#time-zone-handling-1",
    "title": "Time Series",
    "section": "",
    "text": "import pytz\n\npytz.common_timezones[-5:]\n\n['US/Eastern', 'US/Hawaii', 'US/Mountain', 'US/Pacific', 'UTC']\n\n\n\n# to get timezones, use pytz.timezone\n\ntz = pytz.timezone(\"America/New_York\")\n\ntz\n\n&lt;DstTzInfo 'America/New_York' LMT-1 day, 19:04:00 STD&gt;\n\n\n\n### Time zone localization and conversion\ndates = pd.date_range(\"2021-11-17 09:30\", periods=6)\n\n\nts = pd.Series(np.random.standard_normal(len(dates)),\n              index = dates)\n\n\nts\n\n2021-11-17 09:30:00    0.808643\n2021-11-18 09:30:00    1.435830\n2021-11-19 09:30:00    0.764818\n2021-11-20 09:30:00    0.345263\n2021-11-21 09:30:00   -0.671032\n2021-11-22 09:30:00    0.694027\nFreq: D, dtype: float64\n\n\n\nprint(ts.index.tz)\n\nNone\n\n\n\n# Dates from timezone set\npd.date_range(\"2023-3-01\", periods=10, tz=\"UTC\")\n\nDatetimeIndex(['2023-03-01 00:00:00+00:00', '2023-03-02 00:00:00+00:00',\n               '2023-03-03 00:00:00+00:00', '2023-03-04 00:00:00+00:00',\n               '2023-03-05 00:00:00+00:00', '2023-03-06 00:00:00+00:00',\n               '2023-03-07 00:00:00+00:00', '2023-03-08 00:00:00+00:00',\n               '2023-03-09 00:00:00+00:00', '2023-03-10 00:00:00+00:00'],\n              dtype='datetime64[ns, UTC]', freq='D')"
  },
  {
    "objectID": "posts/python/Time_series-checkpoint.html#periods-and-period-arithmetic-1",
    "href": "posts/python/Time_series-checkpoint.html#periods-and-period-arithmetic-1",
    "title": "Time Series",
    "section": "",
    "text": "pd.Series(np.random.standard_normal(6),\n       )\n\n0    0.434382\n1    1.383303\n2    0.975721\n3   -1.377090\n4    0.659404\n5    1.098894\ndtype: float64\n\n\n\nvalues= [\"2001Q3\", \"20022Q2\", '20033Q1']\n\n\nperiods = pd.period_range(\"2000-01-01\", \"2000-06-30\",\n                          freq=\"M\")\n\n\nindex = pd.PeriodIndex(values, freq=\"Q-Dec\")\n\n\np = pd.Period(\"2011\", freq= \"A-Jun\")\n\n\np\n\nPeriod('2011', 'A-JUN')\n\n\n\np.asfreq(\"M\", how='start')\n\nPeriod('2010-07', 'M')\n\n\n\np.asfreq(\"M\", how=\"end\")\n\nPeriod('2011-06', 'M')\n\n\n\n# period index\nperiods = pd.period_range(\"2006\", \"2009\", \n                         freq=\"A-Dec\")\n\n\nts= pd.Series(np.random.standard_normal\n             (len(periods)), index= periods)\n\n\nts\n\n2006    1.079865\n2007   -1.891582\n2008   -0.634198\n2009    0.155782\nFreq: A-DEC, dtype: float64\n\n\n\nts.asfreq(\"M\", how='start')\n\n2006-01    1.079865\n2007-01   -1.891582\n2008-01   -0.634198\n2009-01    0.155782\nFreq: M, dtype: float64\n\n\n\n\n\ndates = pd.date_range(\"2000-01-01\",\n                     periods=3, freq=\"M\")\n\n\nts= pd.Series(np.random.standard_normal(3),\n             index=dates)\n\n\nts\n\n2000-01-31   -1.472182\n2000-02-29    0.042816\n2000-03-31    1.232869\nFreq: M, dtype: float64\n\n\n\npts = ts.to_period()\n\n\npts\n\n2000-01   -1.472182\n2000-02    0.042816\n2000-03    1.232869\nFreq: M, dtype: float64\n\n\n\ndates = pd.date_range(\"2022-01-29\", periods=6)\n\n\nts2 = pd.Series(np.random.standard_normal(6),\n               index= dates)\n\n\nts2\n\n2022-01-29   -0.865524\n2022-01-30    1.518387\n2022-01-31    0.327414\n2022-02-01    0.380410\n2022-02-02   -0.984295\n2022-02-03   -2.798704\nFreq: D, dtype: float64\n\n\n\nts2.to_period\n\n&lt;bound method Series.to_period of 2022-01-29   -0.865524\n2022-01-30    1.518387\n2022-01-31    0.327414\n2022-02-01    0.380410\n2022-02-02   -0.984295\n2022-02-03   -2.798704\nFreq: D, dtype: float64&gt;\n\n\n\npts = ts2.to_period()\n\n\npts\n\n2022-01-29   -0.865524\n2022-01-30    1.518387\n2022-01-31    0.327414\n2022-02-01    0.380410\n2022-02-02   -0.984295\n2022-02-03   -2.798704\nFreq: D, dtype: float64\n\n\n\npts.to_timestamp(how='end')\n\n2022-01-29 23:59:59.999999999   -0.865524\n2022-01-30 23:59:59.999999999    1.518387\n2022-01-31 23:59:59.999999999    0.327414\n2022-02-01 23:59:59.999999999    0.380410\n2022-02-02 23:59:59.999999999   -0.984295\n2022-02-03 23:59:59.999999999   -2.798704\nFreq: D, dtype: float64"
  },
  {
    "objectID": "posts/python/Time_series-checkpoint.html#reshaping-and-sample-frequency-conversion",
    "href": "posts/python/Time_series-checkpoint.html#reshaping-and-sample-frequency-conversion",
    "title": "Time Series",
    "section": "",
    "text": "dates = pd.date_range(\"2022-12-10\", periods=100)\n\n\nts= pd.Series(np.random.standard_normal(len(dates)),\n             index=dates)\n\n\nts\n\n2022-12-10    0.655061\n2022-12-11   -2.144779\n2022-12-12    0.489381\n2022-12-13   -0.117629\n2022-12-14    0.782097\n                ...   \n2023-03-15   -1.263925\n2023-03-16   -0.317576\n2023-03-17    0.398997\n2023-03-18   -0.237727\n2023-03-19   -1.407480\nFreq: D, Length: 100, dtype: float64\n\n\n\nts.resample(\"M\").mean()\n\n2022-12-31   -0.177024\n2023-01-31    0.218224\n2023-02-28   -0.040488\n2023-03-31   -0.288735\nFreq: M, dtype: float64\n\n\n\nts.resample(\"M\", kind=\"period\").mean()\n\n2022-12   -0.177024\n2023-01    0.218224\n2023-02   -0.040488\n2023-03   -0.288735\nFreq: M, dtype: float64\n\n\n\n\n\ndates = pd.date_range(\"2022-01-01\",\n                     periods=12, freq=\"T\")\n\n\nts = pd.Series(np.arange(len(dates)), index=dates)\n\n\nts\n\n2022-01-01 00:00:00     0\n2022-01-01 00:01:00     1\n2022-01-01 00:02:00     2\n2022-01-01 00:03:00     3\n2022-01-01 00:04:00     4\n2022-01-01 00:05:00     5\n2022-01-01 00:06:00     6\n2022-01-01 00:07:00     7\n2022-01-01 00:08:00     8\n2022-01-01 00:09:00     9\n2022-01-01 00:10:00    10\n2022-01-01 00:11:00    11\nFreq: T, dtype: int32\n\n\n\nts.resample(\"5min\").sum()\n\n2022-01-01 00:00:00    10\n2022-01-01 00:05:00    35\n2022-01-01 00:10:00    21\nFreq: 5T, dtype: int32\n\n\n\nts.resample(\"5min\", closed=\"right\", \n            label=\"right\").sum()\n\n2022-01-01 00:00:00     0\n2022-01-01 00:05:00    15\n2022-01-01 00:10:00    40\n2022-01-01 00:15:00    11\nFreq: 5T, dtype: int32\n\n\n\n\n\n\nts = pd.Series(np.random.permutation\n               (np.arange(len(dates))), index=dates)\n\n\nts.resample(\"5min\").ohlc()\n\n\n\n\n\n\n\n\nopen\nhigh\nlow\nclose\n\n\n\n\n2022-01-01 00:00:00\n1\n11\n1\n5\n\n\n2022-01-01 00:05:00\n3\n7\n0\n0\n\n\n2022-01-01 00:10:00\n8\n10\n8\n10\n\n\n\n\n\n\n\n\n\n\n\nframe = pd.DataFrame(np.random.standard_normal((2,5)),\n                    index=pd.date_range(\"2022-01-01\",\n                                        periods=2, freq=\"W-WED\"),\n                     columns = [\"fdk\", 'sadik', 'golewala','pipli','mudki'])\n\n\nframe\n\n\n\n\n\n\n\n\nfdk\nsadik\ngolewala\npipli\nmudki\n\n\n\n\n2022-01-05\n-0.500335\n-0.286433\n0.805294\n0.557015\n-1.293101\n\n\n2022-01-12\n0.470091\n-0.574010\n-0.817633\n0.197509\n0.189306\n\n\n\n\n\n\n\n\nfor n in range(10):\n    print(n)\n\n\n#list comprehension\n\n[n**2 + 2 for n in range(5)]\n\n[2, 3, 6, 11, 18]\n\n\n\n# for loop\nfor n in [n**2 + 2 for n in range(5)]:\n    print(n)\n\n2\n3\n6\n11\n18\n\n\n\ndf_daily = frame.resample('D').asfreq()\n\n\ndf_daily\n\n\n\n\n\n\n\n\nfdk\nsadik\ngolewala\npipli\nmudki\n\n\n\n\n2022-01-05\n-0.500335\n-0.286433\n0.805294\n0.557015\n-1.293101\n\n\n2022-01-06\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2022-01-07\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2022-01-08\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2022-01-09\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2022-01-10\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2022-01-11\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2022-01-12\n0.470091\n-0.574010\n-0.817633\n0.197509\n0.189306\n\n\n\n\n\n\n\n\n# filling certain number of periods\nframe.resample(\"D\").ffill(limit=2)\n\n\n\n\n\n\n\n\nfdk\nsadik\ngolewala\npipli\nmudki\n\n\n\n\n2022-01-05\n-0.500335\n-0.286433\n0.805294\n0.557015\n-1.293101\n\n\n2022-01-06\n-0.500335\n-0.286433\n0.805294\n0.557015\n-1.293101\n\n\n2022-01-07\n-0.500335\n-0.286433\n0.805294\n0.557015\n-1.293101\n\n\n2022-01-08\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2022-01-09\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2022-01-10\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2022-01-11\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2022-01-12\n0.470091\n-0.574010\n-0.817633\n0.197509\n0.189306\n\n\n\n\n\n\n\n\nframe.resample(\"W-THU\").ffill()\n\n\n\n\n\n\n\n\nfdk\nsadik\ngolewala\npipli\nmudki\n\n\n\n\n2022-01-06\n-0.500335\n-0.286433\n0.805294\n0.557015\n-1.293101\n\n\n2022-01-13\n0.470091\n-0.574010\n-0.817633\n0.197509\n0.189306\n\n\n\n\n\n\n\n\n\n\n\nframe2 = pd.DataFrame(np.random.standard_normal((24,4)),\n                     index = pd.period_range(\"1-2000\",\"12-2001\",\n                                            freq=\"M\"),\n                     columns=['fzp', 'fdk', 'btd', 'mudki'])\n\n\nframe2.head()\n\n\n\n\n\n\n\n\nfzp\nfdk\nbtd\nmudki\n\n\n\n\n2000-01\n-0.438864\n0.122812\n-1.157890\n0.403075\n\n\n2000-02\n-0.688525\n-0.721772\n0.011549\n-0.979600\n\n\n2000-03\n0.607322\n-1.119676\n0.089442\n0.060338\n\n\n2000-04\n0.690526\n-0.095600\n0.212831\n0.410823\n\n\n2000-05\n-1.016129\n-0.430221\n-1.235741\n0.250289\n\n\n\n\n\n\n\n\nannual_frame = frame2.resample(\"A-DEC\").mean()\n\n\nannual_frame\n\n\n\n\n\n\n\n\nfzp\nfdk\nbtd\nmudki\n\n\n\n\n2000\n-0.094852\n-0.436623\n-0.600286\n-0.715234\n\n\n2001\n0.025365\n-0.347140\n0.035530\n-0.521633\n\n\n\n\n\n\n\n\n# Q-DEC:Quarterly, year ending in Dec\nannual_frame.resample(\"Q-DEC\").ffill()\n\n\n\n\n\n\n\n\nfzp\nfdk\nbtd\nmudki\n\n\n\n\n2000Q1\n-0.094852\n-0.436623\n-0.600286\n-0.715234\n\n\n2000Q2\n-0.094852\n-0.436623\n-0.600286\n-0.715234\n\n\n2000Q3\n-0.094852\n-0.436623\n-0.600286\n-0.715234\n\n\n2000Q4\n-0.094852\n-0.436623\n-0.600286\n-0.715234\n\n\n2001Q1\n0.025365\n-0.347140\n0.035530\n-0.521633\n\n\n2001Q2\n0.025365\n-0.347140\n0.035530\n-0.521633\n\n\n2001Q3\n0.025365\n-0.347140\n0.035530\n-0.521633\n\n\n2001Q4\n0.025365\n-0.347140\n0.035530\n-0.521633\n\n\n\n\n\n\n\n\nannual_frame.resample(\"Q-Dec\", convention=\"end\").asfreq()\n\n\n\n\n\n\n\n\nfzp\nfdk\nbtd\nmudki\n\n\n\n\n2000Q4\n-0.094852\n-0.436623\n-0.600286\n-0.715234\n\n\n2001Q1\nNaN\nNaN\nNaN\nNaN\n\n\n2001Q2\nNaN\nNaN\nNaN\nNaN\n\n\n2001Q3\nNaN\nNaN\nNaN\nNaN\n\n\n2001Q4\n0.025365\n-0.347140\n0.035530\n-0.521633\n\n\n\n\n\n\n\n\nannual_frame.resample(\"Q-MAR\").ffill()\n\n\n\n\n\n\n\n\nfzp\nfdk\nbtd\nmudki\n\n\n\n\n2000Q4\n-0.094852\n-0.436623\n-0.600286\n-0.715234\n\n\n2001Q1\n-0.094852\n-0.436623\n-0.600286\n-0.715234\n\n\n2001Q2\n-0.094852\n-0.436623\n-0.600286\n-0.715234\n\n\n2001Q3\n-0.094852\n-0.436623\n-0.600286\n-0.715234\n\n\n2001Q4\n0.025365\n-0.347140\n0.035530\n-0.521633\n\n\n2002Q1\n0.025365\n-0.347140\n0.035530\n-0.521633\n\n\n2002Q2\n0.025365\n-0.347140\n0.035530\n-0.521633\n\n\n2002Q3\n0.025365\n-0.347140\n0.035530\n-0.521633\n\n\n\n\n\n\n\n\n\n\n\nN = 15\ntimes = pd.date_range(\"2024-02-29 00:00\", freq=\"1min\",\n                     periods=N)\ndf = pd.DataFrame({'time':times,\n                  \"value\":np.arange(N)})\n\n\ndf\n\n\n\n\n\n\n\n\ntime\nvalue\n\n\n\n\n0\n2024-02-29 00:00:00\n0\n\n\n1\n2024-02-29 00:01:00\n1\n\n\n2\n2024-02-29 00:02:00\n2\n\n\n3\n2024-02-29 00:03:00\n3\n\n\n4\n2024-02-29 00:04:00\n4\n\n\n5\n2024-02-29 00:05:00\n5\n\n\n6\n2024-02-29 00:06:00\n6\n\n\n7\n2024-02-29 00:07:00\n7\n\n\n8\n2024-02-29 00:08:00\n8\n\n\n9\n2024-02-29 00:09:00\n9\n\n\n10\n2024-02-29 00:10:00\n10\n\n\n11\n2024-02-29 00:11:00\n11\n\n\n12\n2024-02-29 00:12:00\n12\n\n\n13\n2024-02-29 00:13:00\n13\n\n\n14\n2024-02-29 00:14:00\n14\n\n\n\n\n\n\n\n\n# resmpling for 5 min count\ntimes2 = pd.date_range(\"2024-02-29 00:00\", freq=\"5min\",\n                     periods=N)\ndf2 = pd.DataFrame({'time':times2,\n                  \"value\":np.arange(N)})\ndf2\n\n\n\n\n\n\n\n\ntime\nvalue\n\n\n\n\n0\n2024-02-29 00:00:00\n0\n\n\n1\n2024-02-29 00:05:00\n1\n\n\n2\n2024-02-29 00:10:00\n2\n\n\n3\n2024-02-29 00:15:00\n3\n\n\n4\n2024-02-29 00:20:00\n4\n\n\n5\n2024-02-29 00:25:00\n5\n\n\n6\n2024-02-29 00:30:00\n6\n\n\n7\n2024-02-29 00:35:00\n7\n\n\n8\n2024-02-29 00:40:00\n8\n\n\n9\n2024-02-29 00:45:00\n9\n\n\n10\n2024-02-29 00:50:00\n10\n\n\n11\n2024-02-29 00:55:00\n11\n\n\n12\n2024-02-29 01:00:00\n12\n\n\n13\n2024-02-29 01:05:00\n13\n\n\n14\n2024-02-29 01:10:00\n14\n\n\n\n\n\n\n\n\n# simply resampling\ndf.set_index('time').resample(\"5min\").count()\n\n\n\n\n\n\n\n\nvalue\n\n\ntime\n\n\n\n\n\n2024-02-29 00:00:00\n5\n\n\n2024-02-29 00:05:00\n5\n\n\n2024-02-29 00:10:00\n5\n\n\n\n\n\n\n\n\n# DataFrame with multiple time series, \n\ndf3 = pd.DataFrame({\"time\":times.repeat(3),\n                   \"key\":np.tile([\"a\",\"b\",\"c\"],N),\n                   'value':np.arange(N*3.)})\n\n\ndf3.head()\n\n\n\n\n\n\n\n\ntime\nkey\nvalue\n\n\n\n\n0\n2024-02-29 00:00:00\na\n0.0\n\n\n1\n2024-02-29 00:00:00\nb\n1.0\n\n\n2\n2024-02-29 00:00:00\nc\n2.0\n\n\n3\n2024-02-29 00:01:00\na\n3.0\n\n\n4\n2024-02-29 00:01:00\nb\n4.0\n\n\n\n\n\n\n\n\n# pandas grouper object\ntime_key = pd.Grouper(freq=\"5min\")\n\n\nresampled = (df3.set_index(\"time\")\n            .groupby([\"key\", time_key])\n            .sum())\n\n\nresampled\n\n\n\n\n\n\n\n\n\nvalue\n\n\nkey\ntime\n\n\n\n\n\na\n2024-02-29 00:00:00\n30.0\n\n\n2024-02-29 00:05:00\n105.0\n\n\n2024-02-29 00:10:00\n180.0\n\n\nb\n2024-02-29 00:00:00\n35.0\n\n\n2024-02-29 00:05:00\n110.0\n\n\n2024-02-29 00:10:00\n185.0\n\n\nc\n2024-02-29 00:00:00\n40.0\n\n\n2024-02-29 00:05:00\n115.0\n\n\n2024-02-29 00:10:00\n190.0\n\n\n\n\n\n\n\n\nresampled.reset_index()\n\n\n\n\n\n\n\n\nkey\ntime\nvalue\n\n\n\n\n0\na\n2024-02-29 00:00:00\n30.0\n\n\n1\na\n2024-02-29 00:05:00\n105.0\n\n\n2\na\n2024-02-29 00:10:00\n180.0\n\n\n3\nb\n2024-02-29 00:00:00\n35.0\n\n\n4\nb\n2024-02-29 00:05:00\n110.0\n\n\n5\nb\n2024-02-29 00:10:00\n185.0\n\n\n6\nc\n2024-02-29 00:00:00\n40.0\n\n\n7\nc\n2024-02-29 00:05:00\n115.0\n\n\n8\nc\n2024-02-29 00:10:00\n190.0"
  },
  {
    "objectID": "posts/python/Time_series-checkpoint.html#moving-window-functions-1",
    "href": "posts/python/Time_series-checkpoint.html#moving-window-functions-1",
    "title": "Time Series",
    "section": "",
    "text": "used for noisy or gappy data\nexclude automatically missing data\n\n\ndata = pd.read_csv(\"E:\\pythonfordatanalysis\\semainedu26fevrier\\iris.csv\")\n\n\ndata.columns\n\nIndex(['Id', 'Sepal Length (cm)', 'Sepal Width (cm)', 'Petal Length (cm)',\n       'Petal Width (cm)', 'Species'],\n      dtype='object')\n\n\n\ndata['Sepal Width (cm)'].plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\ndata[\"Sepal Width (cm)\"].rolling(5).mean().plot()\n\n&lt;Axes: &gt;"
  },
  {
    "objectID": "posts/qmd_computations/computations.html",
    "href": "posts/qmd_computations/computations.html",
    "title": "Quarto Computations",
    "section": "",
    "text": "Code\nimport numpy as np\na = np.arange(15).reshape(3, 5)\na\n\n\narray([[ 0,  1,  2,  3,  4],\n       [ 5,  6,  7,  8,  9],\n       [10, 11, 12, 13, 14]])"
  },
  {
    "objectID": "posts/qmd_computations/computations.html#numpy",
    "href": "posts/qmd_computations/computations.html#numpy",
    "title": "Quarto Computations",
    "section": "",
    "text": "Code\nimport numpy as np\na = np.arange(15).reshape(3, 5)\na\n\n\narray([[ 0,  1,  2,  3,  4],\n       [ 5,  6,  7,  8,  9],\n       [10, 11, 12, 13, 14]])"
  },
  {
    "objectID": "posts/qmd_computations/computations.html#matplotlib",
    "href": "posts/qmd_computations/computations.html#matplotlib",
    "title": "Quarto Computations",
    "section": "Matplotlib",
    "text": "Matplotlib\n\n\nCode\nimport matplotlib.pyplot as plt\n\nfig = plt.figure()\nx = np.arange(10)\ny = 2.5 * np.sin(x / 20 * np.pi)\nyerr = np.linspace(0.05, 0.2, 10)\n\nplt.errorbar(x, y + 3, yerr=yerr, label='both limits (default)')\nplt.errorbar(x, y + 2, yerr=yerr, uplims=True, label='uplims=True')\nplt.errorbar(x, y + 1, yerr=yerr, uplims=True, lolims=True,\n             label='uplims=True, lolims=True')\n\nupperlimits = [True, False] * 5\nlowerlimits = [False, True] * 5\nplt.errorbar(x, y, yerr=yerr, uplims=upperlimits, lolims=lowerlimits,\n             label='subsets of uplims and lolims')\n\nplt.legend(loc='lower right')\nplt.show(fig)"
  },
  {
    "objectID": "posts/qmd_computations/computations.html#plotly",
    "href": "posts/qmd_computations/computations.html#plotly",
    "title": "Quarto Computations",
    "section": "Plotly",
    "text": "Plotly\n\n\nCode\nimport plotly.express as px\nimport plotly.io as pio\ngapminder = px.data.gapminder()\ngapminder2007 = gapminder.query(\"year == 2007\")\nfig = px.scatter(gapminder2007, \n                 x=\"gdpPercap\", y=\"lifeExp\", color=\"continent\", \n                 size=\"pop\", size_max=60,\n                 hover_name=\"country\")\nfig.show()\n\n\nModuleNotFoundError: No module named 'plotly'"
  },
  {
    "objectID": "posts/sample_ipynb/hello.html",
    "href": "posts/sample_ipynb/hello.html",
    "title": "Quarto Basics",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "posts/sample_ipynb/hello.html#polar-axis",
    "href": "posts/sample_ipynb/hello.html#polar-axis",
    "title": "Quarto Basics",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "posts/semainedu20mai2024/logisticregression.html",
    "href": "posts/semainedu20mai2024/logisticregression.html",
    "title": "logistic_regression",
    "section": "",
    "text": "Probabilities are utilized instead of specific values in this approach which is not the case for linear regression. Instead of mean square error, cross-entropy is employed.\nThe Gradient Descent method is applied for LogisticRegression as well.\nWeight calculation involves subtracting the gradient from the current weight.\nSteps: (i) Training - Initialize weight and bias as zero. (ii) Given a data point - predict result, calculate error, use gradient descent to determine new weight and bias, repeat n times. (iii) Testing - input values into the equation, select label based on probability.\nThe same equation as in linear regression is utilized, integrated into the sigmoid function.\n\n\n\n# creating a class LogisticRegression\nimport numpy as np\n\n# Creating a sigmoid function as we'll be using it\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nclass LogisticRegression:\n    def __init__(self, lr=0.001, n_iters=1000):\n        self.lr = lr\n        self.n_iters = n_iters\n        self.weights = None\n        self.bias = None\n\n\n    # always start by adding fit and predict funciton \n    def fit(self, X, y):\n        # Initializing weights and bias\n        n_samples, n_features = X.shape\n        self.weights = np.zeros(n_features)  # assigning zeros as weights\n        self.bias = 0\n\n        # Gradient Descent\n        for _ in range(self.n_iters):\n            linear_pred = np.dot(X, self.weights) + self.bias\n            predictions = sigmoid(linear_pred)\n            \n            # Gradient calculation\n            dw = (1 / n_samples) * np.dot(X.T, (predictions - y))\n            db = (1 / n_samples) * np.sum(predictions - y)\n\n            # Update weights and bias\n            self.weights -= self.lr * dw\n            self.bias -= self.lr * db\n\n    def predict(self, X):\n        linear_pred = np.dot(X, self.weights) + self.bias\n        y_pred = sigmoid(linear_pred)\n        class_pred = [0 if i &lt;= 0.5 else 1 for i in y_pred]\n        return class_pred\n\n\n\n\n\n# testing how accurate it is with breast_cancer dataset from scikit_learn\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Load data\nbc = datasets.load_breast_cancer()\nX, y = bc.data, bc.target\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n\n# Normalize features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Initialize and fit the logistic regression model\nclf = LogisticRegression(lr=0.01, n_iters=1000)\nclf.fit(X_train, y_train)\n\n# Predict on test data\ny_pred = clf.predict(X_test)\n\n# Accuracy function\ndef accuracy(y_pred, y_test):\n    accuracy = np.sum(y_pred == y_test) / len(y_test)\n    return accuracy\n\n# Calculate accuracy\nacc = accuracy(y_pred, y_test)\nprint(f'Accuracy: {acc:.2f}')\n\nAccuracy: 0.94"
  },
  {
    "objectID": "posts/semainedu20mai2024/logisticregression.html#model-building",
    "href": "posts/semainedu20mai2024/logisticregression.html#model-building",
    "title": "logistic_regression",
    "section": "",
    "text": "# creating a class LogisticRegression\nimport numpy as np\n\n# Creating a sigmoid function as we'll be using it\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nclass LogisticRegression:\n    def __init__(self, lr=0.001, n_iters=1000):\n        self.lr = lr\n        self.n_iters = n_iters\n        self.weights = None\n        self.bias = None\n\n\n    # always start by adding fit and predict funciton \n    def fit(self, X, y):\n        # Initializing weights and bias\n        n_samples, n_features = X.shape\n        self.weights = np.zeros(n_features)  # assigning zeros as weights\n        self.bias = 0\n\n        # Gradient Descent\n        for _ in range(self.n_iters):\n            linear_pred = np.dot(X, self.weights) + self.bias\n            predictions = sigmoid(linear_pred)\n            \n            # Gradient calculation\n            dw = (1 / n_samples) * np.dot(X.T, (predictions - y))\n            db = (1 / n_samples) * np.sum(predictions - y)\n\n            # Update weights and bias\n            self.weights -= self.lr * dw\n            self.bias -= self.lr * db\n\n    def predict(self, X):\n        linear_pred = np.dot(X, self.weights) + self.bias\n        y_pred = sigmoid(linear_pred)\n        class_pred = [0 if i &lt;= 0.5 else 1 for i in y_pred]\n        return class_pred"
  },
  {
    "objectID": "posts/semainedu20mai2024/logisticregression.html#testing",
    "href": "posts/semainedu20mai2024/logisticregression.html#testing",
    "title": "logistic_regression",
    "section": "",
    "text": "# testing how accurate it is with breast_cancer dataset from scikit_learn\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Load data\nbc = datasets.load_breast_cancer()\nX, y = bc.data, bc.target\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n\n# Normalize features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Initialize and fit the logistic regression model\nclf = LogisticRegression(lr=0.01, n_iters=1000)\nclf.fit(X_train, y_train)\n\n# Predict on test data\ny_pred = clf.predict(X_test)\n\n# Accuracy function\ndef accuracy(y_pred, y_test):\n    accuracy = np.sum(y_pred == y_test) / len(y_test)\n    return accuracy\n\n# Calculate accuracy\nacc = accuracy(y_pred, y_test)\nprint(f'Accuracy: {acc:.2f}')\n\nAccuracy: 0.94"
  },
  {
    "objectID": "series-gha.html",
    "href": "series-gha.html",
    "title": "GitHub Actions Series: Making awesome automations",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "series-gha.html#posts-in-english",
    "href": "series-gha.html#posts-in-english",
    "title": "GitHub Actions Series: Making awesome automations",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "series-gha.html#posts-en-français",
    "href": "series-gha.html#posts-en-français",
    "title": "GitHub Actions Series: Making awesome automations",
    "section": "Posts en français",
    "text": "Posts en français\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nNo matching items"
  }
]