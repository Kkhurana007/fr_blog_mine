[
  {
    "objectID": "about-fr.html",
    "href": "about-fr.html",
    "title": "Kunal Khurana",
    "section": "",
    "text": "English  Fran√ßais"
  },
  {
    "objectID": "about-fr.html#education",
    "href": "about-fr.html#education",
    "title": "Kunal Khurana",
    "section": "üìùEducation",
    "text": "üìùEducation\n2022| G√âNIE ET SCIENCE APPLIQU√âES| C√âGEP DU VIEUX MONTR√âAL, CA\n2021| FRANCISATION ET IN√âGRATION AU MILIEU QU√âB√âCOIS| UNIVERSIT√â LAVAL, CA\n2016| Maitrise en Science du Sol| PUNJAB AGRICULTURAL UNIVERSITY, IN\n2013| Baccelaureat en Agriculture | PUNJABI UNIVERSITY PATIALA, IN"
  },
  {
    "objectID": "about-fr.html#exp√©rience-du-travail",
    "href": "about-fr.html#exp√©rience-du-travail",
    "title": "Kunal Khurana",
    "section": "üßë‚Äçüíº Exp√©rience du travail",
    "text": "üßë‚Äçüíº Exp√©rience du travail\nSeptember 2022 ‚Äì March 2023| Assistant de recherche, McGill University\nSEPTEMBER 2020 ‚Äì NOVEMBER 2020| Traductor, CERDA\nJanuary 2019 ‚Äì August 2019| Assistant de recherche, Universit√© Laval\nOctober 2016 ‚Äì March 2017| Assistant de recherche, PUNJAB AGRICULTURAL UNIVERSITY"
  },
  {
    "objectID": "about-fr.html#contacer",
    "href": "about-fr.html#contacer",
    "title": "Kunal Khurana",
    "section": "Contacer!",
    "text": "Contacer!\nL‚Äôune de mes plus grandes motivations pour cr√©er du contenu en ligne est de communiquer avec des personnes fascinantes du monde entier. Si vous souhaitez en discuter, n‚Äôh√©sitez pas √† m‚Äô√©crire. Je serais content d‚Äôavoir de vos nouvelles!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Kunal Khurana",
    "section": "",
    "text": "English  Fran√ßais"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Kunal Khurana",
    "section": "üìùEducation",
    "text": "üìùEducation\n2022| G√âNIE ET SCIENCE APPLIQU√âES| C√©gep du Vieux Montr√©al, CA\n2021| FRANCISATION ET IN√âGRATION AU MILIEU QU√âB√âCOIS| Universit√© Laval, CA\n2016| MASTER‚ÄôS IN SOIL SCIENCE| Punjab Agricultural University, IN\n2013| BACHELOR‚ÄôS IN AGRICULTURE| Punjabi University patiala, IN"
  },
  {
    "objectID": "about.html#work-experience",
    "href": "about.html#work-experience",
    "title": "Kunal Khurana",
    "section": "üßë‚Äçüíº Work Experience",
    "text": "üßë‚Äçüíº Work Experience\nSeptember 2022 ‚Äì March 2023| Resesarch Assistant, McGill University\nSEPTEMBER 2020 ‚Äì NOVEMBER 2020| FREELANCE TRANSLATOR, CERDA\nJanuary 2019 ‚Äì August 2019| Resesarch Assistant, Universit√© Laval\nOctober 2016 ‚Äì March 2017| Resesarch Assistant, Punjab Agriclutural University"
  },
  {
    "objectID": "about.html#lets-connect",
    "href": "about.html#lets-connect",
    "title": "Kunal Khurana",
    "section": "Let‚Äôs Connect!",
    "text": "Let‚Äôs Connect!\nOne of my greatest motivations for creating online content is to engage with fascinating individuals from all over the world. If you‚Äôre interested in having a chat, feel free to reach out to me. I would love to hear from you!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kunal Khurana",
    "section": "",
    "text": "Exploratory analysis\n\n\n3 min\n\n\n\nSoils\n\n\nData Analysis\n\n\n\n\n\n\n\nJun 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n33 min\n\n\n\nGit\n\n\nData Analysis\n\n\n\n\nLino Galiana\n\n\nJun 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n11 min\n\n\n\nGit\n\n\nData Analysis\n\n\n\n\n\n\n\nJun 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n10 min\n\n\n\nNumpy\n\n\nData Analysis\n\n\n\n\nJustin Johnson, Volodymr Kuleshov, and Issac Caswell\n\n\nJun 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n1 min\n\n\n\nMatplotlib\n\n\nData Analysis\n\n\n\n\nKeith Galli\n\n\nJun 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n21 min\n\n\n\nPandas\n\n\nData Analysis\n\n\n\n\n\n\n\nJun 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\npackages\n\n\ndata\n\n\n\n\n\n\n\nMay 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nplot\n\n\ndata\n\n\n\n\n\n\n\nMay 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Visualization\n\n\n7 min\n\n\n\nNumpy\n\n\nPandas\n\n\nData Analysis\n\n\n\n\nNonu Singh\n\n\nMay 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nAI\n\n\nIrrigation\n\n\nMachine learning\n\n\n\n\nKunal Khurana\n\n\nMay 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nC\n\n\nBamboo\n\n\n\n\nKunal Khurana\n\n\nMay 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nMachine learning\n\n\nAI\n\n\nIrrigation\n\n\n\n\nKunal Khurana\n\n\nMay 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nMachine learning\n\n\nSoil Analysis\n\n\nYield prediction\n\n\n\n\nKunal Khurana\n\n\nApr 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nMachine learning\n\n\nIrrigation\n\n\nModelling\n\n\n\n\nKunal Khurana\n\n\nApr 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nDigitalization\n\n\nMachine learning\n\n\nAI\n\n\n\n\nKunal Khurana\n\n\nApr 17, 2023\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n See all posts in English"
  },
  {
    "objectID": "index.html#posts-in-english",
    "href": "index.html#posts-in-english",
    "title": "Kunal Khurana",
    "section": "",
    "text": "Exploratory analysis\n\n\n3 min\n\n\n\nSoils\n\n\nData Analysis\n\n\n\n\n\n\n\nJun 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n33 min\n\n\n\nGit\n\n\nData Analysis\n\n\n\n\nLino Galiana\n\n\nJun 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n11 min\n\n\n\nGit\n\n\nData Analysis\n\n\n\n\n\n\n\nJun 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n10 min\n\n\n\nNumpy\n\n\nData Analysis\n\n\n\n\nJustin Johnson, Volodymr Kuleshov, and Issac Caswell\n\n\nJun 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n1 min\n\n\n\nMatplotlib\n\n\nData Analysis\n\n\n\n\nKeith Galli\n\n\nJun 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n21 min\n\n\n\nPandas\n\n\nData Analysis\n\n\n\n\n\n\n\nJun 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\npackages\n\n\ndata\n\n\n\n\n\n\n\nMay 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nplot\n\n\ndata\n\n\n\n\n\n\n\nMay 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Visualization\n\n\n7 min\n\n\n\nNumpy\n\n\nPandas\n\n\nData Analysis\n\n\n\n\nNonu Singh\n\n\nMay 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nAI\n\n\nIrrigation\n\n\nMachine learning\n\n\n\n\nKunal Khurana\n\n\nMay 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nC\n\n\nBamboo\n\n\n\n\nKunal Khurana\n\n\nMay 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nMachine learning\n\n\nAI\n\n\nIrrigation\n\n\n\n\nKunal Khurana\n\n\nMay 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nMachine learning\n\n\nSoil Analysis\n\n\nYield prediction\n\n\n\n\nKunal Khurana\n\n\nApr 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nMachine learning\n\n\nIrrigation\n\n\nModelling\n\n\n\n\nKunal Khurana\n\n\nApr 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nDigitalization\n\n\nMachine learning\n\n\nAI\n\n\n\n\nKunal Khurana\n\n\nApr 17, 2023\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n See all posts in English"
  },
  {
    "objectID": "index.html#posts-en-fran√ßais",
    "href": "index.html#posts-en-fran√ßais",
    "title": "Kunal Khurana",
    "section": "Posts en fran√ßais",
    "text": "Posts en fran√ßais\n\n\n\n\n\n\n\n\nTools - Git\n\n\nData Analysis\n\n\n11 min\n\n\n\nGit\n\n\nData Analysis\n\n\n\n\n\n\n\nJun 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTools - Git-2\n\n\nData Analysis\n\n\n33 min\n\n\n\nGit\n\n\nData Analysis\n\n\n\n\nLino Galiana\n\n\nJun 8, 2023\n\n\n\n\n\n\n\n\nNo matching items\n\n\n Voir les posts en fran√ßais"
  },
  {
    "objectID": "posts/Ai_watersaving/index.html",
    "href": "posts/Ai_watersaving/index.html",
    "title": "How helpful can AI be in solving the water crisis?",
    "section": "",
    "text": "Wasting water, especially in areas in which it is a scarce resource, is a huge headache (and expense) for farmers and food growers worldwide. This article discusses how AI can help improve water irrigation in agriculture. The article notes that agriculture accounts for approximately 70% of global freshwater usage and that water scarcity is becoming an increasingly pressing issue in many parts of the world. By using AI, farmers can optimize their irrigation practices resulting in reduced water consumption but increased crop yields. The article highlights the use of Machine learning, and in particular deep-learning, algorithms to collect and interpret data from images and identify patterns that spotlight irrigation issues. AI can also be used to create predictive models that help farmers anticipate (real-time) crop water needs to identify areas of a field that need more or less water. By using AI in agriculture, farmers can become more efficient and sustainable in their use of water, ultimately helping to address global water scarcity concerns."
  },
  {
    "objectID": "posts/bamboo_plantation/index.html",
    "href": "posts/bamboo_plantation/index.html",
    "title": "Can bamboo help to solve climate crisis?",
    "section": "",
    "text": "The article discusses how bamboo construction can be a sustainable and affordable solution to housing while also helping mitigate climate change. Bamboo is a fast-growing, renewable resource that can be used to build structurally sound buildings that are earthquake-resistant and can withstand extreme weather conditions. Bamboo also has a lower carbon footprint compared to traditional building materials like concrete and steel. The article highlights several examples of successful bamboo construction projects, including a 22-story bamboo skyscraper in Colombia and a village in China where all the houses are made of bamboo. By using bamboo in construction, we can create more eco-friendly and affordable housing options while also reducing carbon emissions."
  },
  {
    "objectID": "posts/Digitalization_of_agricluture/index.html",
    "href": "posts/Digitalization_of_agricluture/index.html",
    "title": "Digitaliztion of Agriculture",
    "section": "",
    "text": "The article- ‚ÄúThe next wave of agriculture in Saskatchewan is digital and Indigenous-led‚Äù discusses how Indigenous communities in Saskatchewan are utilizing digital technologies to revitalize traditional farming practices.\nWhile also introducing new techniques to increase crop yields and reduce the impact of climate change and with the use of precision agriculture, remote sensing technologies, and data analysis, these communities are working to improve soil health, increase biodiversity, and foster sustainable practices. The article highlights several Indigenous-led initiatives, such as the One House, Many Nations project and the Whitecap Dakota First Nation‚Äôs Regenerative Farming Program, which are helping to promote food sovereignty and economic development in the region. Additionally, these initiatives are providing opportunities for Indigenous youth to learn about their cultural heritage and connect with the land."
  },
  {
    "objectID": "posts/en.html",
    "href": "posts/en.html",
    "title": "All posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\nExploratory analysis\n\n\n3 min\n\n\n\nSoils\n\n\nData Analysis\n\n\n\n\n\n\n\nJun 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n33 min\n\n\n\nGit\n\n\nData Analysis\n\n\n\n\nLino Galiana\n\n\nJun 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n11 min\n\n\n\nGit\n\n\nData Analysis\n\n\n\n\n\n\n\nJun 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n10 min\n\n\n\nNumpy\n\n\nData Analysis\n\n\n\n\nJustin Johnson, Volodymr Kuleshov, and Issac Caswell\n\n\nJun 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n1 min\n\n\n\nMatplotlib\n\n\nData Analysis\n\n\n\n\nKeith Galli\n\n\nJun 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n21 min\n\n\n\nPandas\n\n\nData Analysis\n\n\n\n\n\n\n\nJun 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\npackages\n\n\ndata\n\n\n\n\n\n\n\nMay 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nplot\n\n\ndata\n\n\n\n\n\n\n\nMay 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Visualization\n\n\n7 min\n\n\n\nNumpy\n\n\nPandas\n\n\nData Analysis\n\n\n\n\nNonu Singh\n\n\nMay 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nAI\n\n\nIrrigation\n\n\nMachine learning\n\n\n\n\nKunal Khurana\n\n\nMay 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nC\n\n\nBamboo\n\n\n\n\nKunal Khurana\n\n\nMay 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nMachine learning\n\n\nAI\n\n\nIrrigation\n\n\n\n\nKunal Khurana\n\n\nMay 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nMachine learning\n\n\nSoil Analysis\n\n\nYield prediction\n\n\n\n\nKunal Khurana\n\n\nApr 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nMachine learning\n\n\nIrrigation\n\n\nModelling\n\n\n\n\nKunal Khurana\n\n\nApr 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nDigitalization\n\n\nMachine learning\n\n\nAI\n\n\n\n\nKunal Khurana\n\n\nApr 17, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/en.html#all-posts",
    "href": "posts/en.html#all-posts",
    "title": "All posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\nExploratory analysis\n\n\n3 min\n\n\n\nSoils\n\n\nData Analysis\n\n\n\n\n\n\n\nJun 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n33 min\n\n\n\nGit\n\n\nData Analysis\n\n\n\n\nLino Galiana\n\n\nJun 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n11 min\n\n\n\nGit\n\n\nData Analysis\n\n\n\n\n\n\n\nJun 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n10 min\n\n\n\nNumpy\n\n\nData Analysis\n\n\n\n\nJustin Johnson, Volodymr Kuleshov, and Issac Caswell\n\n\nJun 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n1 min\n\n\n\nMatplotlib\n\n\nData Analysis\n\n\n\n\nKeith Galli\n\n\nJun 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n21 min\n\n\n\nPandas\n\n\nData Analysis\n\n\n\n\n\n\n\nJun 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\npackages\n\n\ndata\n\n\n\n\n\n\n\nMay 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nplot\n\n\ndata\n\n\n\n\n\n\n\nMay 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Visualization\n\n\n7 min\n\n\n\nNumpy\n\n\nPandas\n\n\nData Analysis\n\n\n\n\nNonu Singh\n\n\nMay 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nAI\n\n\nIrrigation\n\n\nMachine learning\n\n\n\n\nKunal Khurana\n\n\nMay 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nC\n\n\nBamboo\n\n\n\n\nKunal Khurana\n\n\nMay 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nMachine learning\n\n\nAI\n\n\nIrrigation\n\n\n\n\nKunal Khurana\n\n\nMay 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nMachine learning\n\n\nSoil Analysis\n\n\nYield prediction\n\n\n\n\nKunal Khurana\n\n\nApr 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nMachine learning\n\n\nIrrigation\n\n\nModelling\n\n\n\n\nKunal Khurana\n\n\nApr 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nDigitalization\n\n\nMachine learning\n\n\nAI\n\n\n\n\nKunal Khurana\n\n\nApr 17, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/fr.html",
    "href": "posts/fr.html",
    "title": "Posts en fran√ßais",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n11 min\n\n\n\nGit\n\n\nData Analysis\n\n\n\n\n\n\n\nJun 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n33 min\n\n\n\nGit\n\n\nData Analysis\n\n\n\n\nLino Galiana\n\n\nJun 8, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/fr.html#posts-en-fran√ßais",
    "href": "posts/fr.html#posts-en-fran√ßais",
    "title": "Posts en fran√ßais",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n11 min\n\n\n\nGit\n\n\nData Analysis\n\n\n\n\n\n\n\nJun 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n33 min\n\n\n\nGit\n\n\nData Analysis\n\n\n\n\nLino Galiana\n\n\nJun 8, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/git/introgit.html",
    "href": "posts/git/introgit.html",
    "title": "Tools - Git",
    "section": "",
    "text": "Cette page reprend des √©l√©ments pr√©sents dans un cours d√©di√© fait avec Romain Avouac"
  },
  {
    "objectID": "posts/git/introgit.html#conserver-et-archiver-du-code",
    "href": "posts/git/introgit.html#conserver-et-archiver-du-code",
    "title": "Tools - Git",
    "section": "Conserver et archiver du code",
    "text": "Conserver et archiver du code\nUne des principales fonctionnalit√©s de la gestion de version est de conserver l‚Äôensemble des fichiers de fa√ßon s√©curis√©e et de proposer un archivage structur√© des codes. Les fichiers sont stock√©s dans un d√©p√¥t, qui constitue le projet.\nTout repose dans la gestion et la pr√©sentation de l‚Äôhistorique des modifications. Chaque modification (ajout, suppression ou changement) sur un ou plusieurs fichiers est identifi√©e par son auteur, sa date et un bref descriptif[1]. Chaque changement est donc unique et ais√©ment identifiable quand les modifications sont class√©es par ordre chronologique. Les groupes de modifications transmis au d√©p√¥t sont appel√©es commit.\n1. [^](#cite_ref-1)\nPlus pr√©cis√©ment, chaque modification est identifi√©e de mani√®re unique par un code SHA auquel est associ√© l‚Äôauteur, l‚Äôhorodatage et des m√©tadonn√©es (par exemple le message descriptif associ√©).\nAvec des outils graphiques, on peut v√©rifier l‚Äô ensemble des √©volutions d‚Äôun fichier (history), ou l‚Äôhistoire d‚Äôun d√©p√¥t. On peut aussi se concentrer sur une modification particuli√®re d‚Äôun fichier ou v√©rifier, pour un fichier, la modification qui a entra√Æn√© l‚Äôapparition de telle ou telle ligne (blame)\nSur son poste de travail, les dizaines (voire centaines) de programmes organis√©s √† la main n‚Äôexistent plus. Tout est regroup√© dans un seul dossier, rassemblant les √©l√©ments du d√©p√¥t. Au sein du d√©p√¥t, tout l‚Äôhistorique est stock√© et accessible rapidement. Si on souhaite travailler sur la derni√®re version des programmes (ou sur une ancienne version sp√©cifique), il n‚Äôy a plus besoin de conserver les autres fichiers car ils sont dans l‚Äôhistorique du projet. Il est alors possible de choisir sur quelle version on veut travailler (la derni√®re commune √† tout le monde, la sienne en train d‚Äô√™tre d√©velopp√©e, celle de l‚Äôann√©e derni√®re, etc.)."
  },
  {
    "objectID": "posts/git/introgit.html#travailler-efficacement-en-√©quipe",
    "href": "posts/git/introgit.html#travailler-efficacement-en-√©quipe",
    "title": "Tools - Git",
    "section": "Travailler efficacement en √©quipe",
    "text": "Travailler efficacement en √©quipe\nLe deuxi√®me avantage de la gestion de version repr√©sente une am√©lioration notable du travail en √©quipe sur des codes en commun.\nLa gestion de version permet de collaborer simplement et avec m√©thode. De fa√ßon organis√©e, elle permet de:\n\ntravailler en parall√®le et fusionner facilement du code\npartager une documentation des programmes gr√¢ce :\n\naux commentaires des modifications\n√† la possibilit√© d‚Äôune documentation commune et collaborative\n\ntrouver rapidement des erreurs et en diffuser rapidement la correction\n\nA ces avantages s‚Äôajoutent les fonctionalit√©s collaboratives des forges qui sont des plateformes o√π peuvent √™tre stock√©s des d√©p√¥ts. N√©anmoins, ces forges proposent aujourd‚Äôhui beaucoup de fonctionalit√©s qui vont au-del√† de l‚Äôarchivage de code: int√©ragir via des issues, faire des suggestions de modifications, ex√©cuter du code dans des environnements normalis√©s, etc. Il faut vraiment les voir comme des r√©seaux sociaux du code. Les principales plateformes dans ce domaine √©tant Github et Gitlab.\nL‚Äôusage individuel, c‚Äôest-√†-dire seul sur son projet, permet aussi de ‚Äútravailler en √©quipe avec soi-m√™me‚Äù car il permet de retrouver des mois plus tard le contenu et le contexte des modifications. Cela est notamment pr√©cieux lors des changements de poste ou des travaux r√©guliers mais espac√©s dans le temps (par exemple, un mois par an chaque ann√©e). M√™me lorsqu‚Äôon travaille tout seul, on collabore avec un moi futur qui peut ne plus se souvenir de la modification des fichiers."
  },
  {
    "objectID": "posts/git/introgit.html#am√©liorer-la-qualit√©-des-codes",
    "href": "posts/git/introgit.html#am√©liorer-la-qualit√©-des-codes",
    "title": "Tools - Git",
    "section": "Am√©liorer la qualit√© des codes",
    "text": "Am√©liorer la qualit√© des codes\nLe fonctionnement de la gestion de version, reposant sur l‚Äôarchivage structur√© des modifications et les commentaires les accompagnant, renforce la qualit√© des programmes informatiques. Ils sont plus document√©s, plus riches et mieux structur√©s. C‚Äôest pour cette raison que le contr√¥le de version ne doit pas √™tre consid√©r√© comme un outil r√©serv√© √† des d√©veloppeurs : toute personne travaillant sur des programmes informatiques, gagne √† utiliser du contr√¥le de version.\nLes services d‚Äôint√©gration continue permettent de faire des tests automatiques de programmes informatiques, notamment de packages, qui renforcent la replicabilit√© des programmes. Mettre en place des m√©thodes de travail fond√©es sur l‚Äôint√©gration continue rend les programmes plus robustes en for√ßant ceux-ci √† tourner sur des machines autres que celles du d√©veloppeur du code."
  },
  {
    "objectID": "posts/git/introgit.html#simplifier-la-communication-autour-dun-projet",
    "href": "posts/git/introgit.html#simplifier-la-communication-autour-dun-projet",
    "title": "Tools - Git",
    "section": "Simplifier la communication autour d‚Äôun projet",
    "text": "Simplifier la communication autour d‚Äôun projet\nLes sites de d√©p√¥ts Github et Gitlab permettent de faire beaucoup plus que seulement archiver des codes. Les fonctionalit√©s de d√©ploiement en continu permettent ainsi de :\n\ncr√©er des sites web pour valoriser des projets (par exemple les sites readthedocs en python)\nd√©ployer de la documentation en continu\nrendre visible la qualit√© d‚Äôun projet avec des services de code coverage, de tests automatiques ou d‚Äôenvironnements int√©gr√©s de travail (binder, etc.) qu‚Äôon rend g√©n√©ralement visible au moyen de badges (exemple ici )"
  },
  {
    "objectID": "posts/git/introgit.html#copies-de-travail-et-d√©p√¥t-collectif",
    "href": "posts/git/introgit.html#copies-de-travail-et-d√©p√¥t-collectif",
    "title": "Tools - Git",
    "section": "Copies de travail et d√©p√¥t collectif",
    "text": "Copies de travail et d√©p√¥t collectif\nGit est un syst√®me d√©centralis√© et asynchrone de gestion de version. Cela signifie que:\n\nChaque membre d‚Äôun projet travail sur une copie locale du d√©p√¥t (syst√®me decentralis√©). Cette copie de travail s‚Äôappelle un clone. Cela signifie qu‚Äôon n‚Äôa pas une coh√©rence en continue de notre version de travail avec le d√©p√¥t ; on peut tr√®s bien ne jamais vouloir les mettre en coh√©rence (par exemple, si on teste une piste qui s‚Äôav√®re infructueuse) ;\nC‚Äôest lorsqu‚Äôon propose la publication de modifications sur le d√©p√¥t collectif qu‚Äôon doit s‚Äôassurer de la coh√©rence avec la version disponible en ligne (syst√®me asynchrone).\n\nLe d√©p√¥t distant est g√©n√©ralement stock√© sur une forge logicielle (Github ou Gitlab) et sert √† centraliser la version collective d‚Äôun projet. Les copies locales sont des copies de travail qu‚Äôon utilise pour faire √©voluer un projet:\n\nIl est tout √† fait possible de faire du contr√¥le de version sans mettre en place de d√©p√¥t distant. Cependant,\n\nc‚Äôest dangereux puisque le d√©p√¥t distant fait office de sauvegarde d‚Äôun projet. Sans d√©p√¥t distant, on peut tout perdre en cas de probl√®me sur la copie locale de travail ;\nc‚Äôest d√©sirer √™tre moins efficace car, comme nous allons le montrer, les fonctionalit√©s des plateformes Github et Gitlab sont √©galement tr√®s b√©n√©fiques lorsqu‚Äôon travaille tout seul."
  },
  {
    "objectID": "posts/git/introgit.html#principe",
    "href": "posts/git/introgit.html#principe",
    "title": "Tools - Git",
    "section": "Principe",
    "text": "Principe\nLes trois manipulations les plus courantes sont les suivantes et repr√©sent√©es sur le diagramme ci-apr√®s :\n\ncommit : je valide les modifications que j‚Äôai faites en local avec un message qui les expliquent\npull : je r√©cup√®re la derni√®re version des codes du d√©p√¥t distant\npush : je transmets mes modifications valid√©es au d√©p√¥t distant\n\n\nLes deux derni√®res manipulations correspondent aux interactions (notamment la mise en coh√©rence) avec le d√©p√¥t commun alors que la premi√®re manipulation commit correspond √† la modification des fichiers faite pour faire √©voluer un projet.\nDe mani√®re plus pr√©cise, il y a trois √©tapes avant d‚Äôenvoyer les modifications valid√©es (commit) au d√©p√¥t. Elles se d√©finissent en fonction des commandes qui permet de les appliquer quand Git est utilis√© en ligne de commandes :\n\ndiff : inspection des modifications. Cela permet de comparer les fichiers modifi√©s et de distinguer les fichiers ajout√©s ou supprim√©s\nstaging area : s√©lection des modifications.\ncommit : validation des modifications s√©lectionn√©es (avec commentaire).\n\n\nLors des √©tapes de push et pull, des conflits peuvent appara√Ætre, par exemple lorsque deux personnes ont modifi√© le m√™me programme simultan√©ment. Le terme conflit peut faire peur mais en fait c‚Äôest l‚Äôun des apports principaux de Git que de faciliter √©norm√©ment la gestion de versions diff√©rentes. Les exercices du chapitre suivant l‚Äôillustreront."
  },
  {
    "objectID": "posts/git/introgit.html#les-branches",
    "href": "posts/git/introgit.html#les-branches",
    "title": "Tools - Git",
    "section": "Les branches",
    "text": "Les branches\nC‚Äôest une des fonctionnalit√©s les plus pratiques de la gestion de version. La cr√©ation de branches dans un projet (qui devient ainsi un arbre) permet de d√©velopper en parall√®le des correctifs ou une nouvelle fonctionnalit√© sans modifier le d√©p√¥t commun.\nCela permet de s√©parer le nouveau d√©veloppement et de faire cohabiter plusieurs versions, pouvant √©voluer s√©par√©ment ou pouvant √™tre facilement rassembl√©es. Git est optimis√© pour le travail sur les branches.\nDans un projet collaboratif, une branche dite master joue le r√¥le du tronc. C‚Äôest autour d‚Äôelle que vont pousser ou se greffer les branches. L‚Äôun des avantages de Git est qu‚Äôon peut toujours revenir en arri√®re. Ce filet de s√©curit√© permet d‚Äôoser des exp√©rimentations, y compris au sein d‚Äôune branche. Il faut √™tre pr√™t √† aller dans la ligne de commande pour cela mais c‚Äôest extr√™mement confortable.\n\nComment nommer les branches ? L√† encore, il y a √©norm√©ment de conventions diff√©rentes. Une fr√©quemment observ√©e est :\n\npour les nouvelles fonctionnalit√©s : feature/nouvelle-fonctionnalite o√π nouvelle-fontionnalite est un nom court r√©sumant la fonctionnalit√©\npour les corrections de bug : issue-num o√π num est le num√©ro de l‚Äôissue\n\nN‚Äôh√©sitez pas √† aller encore plus loin dans la normalisation !"
  },
  {
    "objectID": "posts/git-2/exogit.html",
    "href": "posts/git-2/exogit.html",
    "title": "Tools - Git-2",
    "section": "",
    "text": "Les exercices suivants sont inspir√©s d‚Äôun cours de Git que j‚Äôai construit √† l‚ÄôInsee et dont les ressources sont disponibles ici. L‚Äôid√©e du cadavre exquis, qui m‚Äôa √©t√© inspir√©e par Romain Lesur est inspir√©e de cette ressource et de celle-ci.\nCette partie part du principe que les concepts g√©n√©raux de Git sont ma√Ætris√©s et qu‚Äôun environnement de travail fonctionnel avec Git est disponible. Un exemple de tel environnement est leJupyterLab du SSPCloud o√π une extension Git est pr√©-install√©e:\n\nOutre le chapitre pr√©c√©dent, il existe de nombreuses ressources sur internet sur le sujet. Parmi-celles auquel j‚Äôai contribu√©, vous pourrez trouver un cours de Git orient√© gestion de projet, une version plus l√©g√®re √† partir de slides et des ressources de la documentation collaborative sur R qu‚Äôest utilitR (des √©l√©ments sur la configuration et pratique sur RStudio). Toutes les ressources ne sont donc pas du Python car Git est un outil tranversal qui doit servir quelque soit le langage de pr√©dilection.\nL‚Äôid√©e de ce chapitre est d‚Äôamener, progressivement, √† la mise en oeuvre de pratiques collaboratives devenues standards dans le domaine de l‚Äôopen-source mais √©galement de plus en plus communes dans les administrations et entreprises de la data-science.\nCe chapitre propose d‚Äôutiliser l‚Äôextension Git de JupyterLab. Un tutoriel pr√©sentant cette extension est disponible ici. Les principaux IDE disponibles (Visual Studio, PyCharm, RStudio) pr√©sentent des fonctionalit√©s similaires. Il est tout √† fait possible d‚Äôen utiliser un autre. VisualStudio propose probablement, √† l‚Äôheure actuelle, l‚Äôensemble le plus complet.\nCertains passages de ce TD n√©cessitent d‚Äôutiliser la ligne de commande. Il est tout √† fait possible de r√©aliser ce TD enti√®rement avec celle-ci. Cependant, pour une personne d√©butante en Git, l‚Äôutilisation d‚Äôune interface graphique peut constituer un √©l√©ment important pour la compr√©hension et l‚Äôadoption de Git. Une fois √† l‚Äôaise avec Git, on peut tout √† fait se passer des interfaces graphiques pour les routines quotidiennes et ne les utiliser que pour certaines op√©rations o√π elles s‚Äôav√®rent fort pratiques (notamment la comparaison de deux fichiers avant de devoir fusionner)."
  },
  {
    "objectID": "posts/git-2/exogit.html#rappels-sur-la-notion-de-d√©p√¥t-distant",
    "href": "posts/git-2/exogit.html#rappels-sur-la-notion-de-d√©p√¥t-distant",
    "title": "Tools - Git-2",
    "section": "Rappels sur la notion de d√©p√¥t distant",
    "text": "Rappels sur la notion de d√©p√¥t distant\nPour rappel, comme expliqu√© pr√©c√©demment, il convient de distinguer le d√©p√¥t distant (remote) et la copie ou les copies locales (les clones) d‚Äôun d√©p√¥t. Le d√©p√¥t distant est g√©n√©ralement stock√© sur une forge logicielle (Github ou Gitlab) et sert √† centraliser la version collective d‚Äôun projet. Les copies locales sont des copies de travail qu‚Äôon utilise pour faire √©voluer un projet:\n\nGit est un syst√®me de contr√¥le de version asynchrone c‚Äôest-√†-dire qu‚Äôon n‚Äôinteragit pas en continu avec le d√©p√¥t distant (comme c‚Äôest le cas dans le syst√®me SVN) mais qu‚Äôil est possible d‚Äôavoir une version locale qui se diff√©rencie du d√©p√¥t commun et qu‚Äôon rend coh√©rente de temps en temps.\nBien qu‚Äôil soit possible d‚Äôavoir une utilisation hors-ligne de Git, c‚Äôest-√†-dire un pur contr√¥le de version local sans d√©p√¥t distant, cela est une utilisation rare et qui comporte un int√©r√™t limite. L‚Äôint√©r√™t de Git est d‚Äôoffrir une mani√®re robuste et efficace d‚Äôinteragir avec un d√©p√¥t distant facilitant ainsi la collaboration en √©quipe ou en solitaire.\nPour ces exercices, je propose d‚Äôutiliser Github dont les fonctionalit√©s nous suffiront amplement[1]. Si, dans le futur, les fonctionnalit√©s ne vous conviennent pas (sans l‚Äôapport de fonctionnalit√©s externes, Github propose moins de fonctionalit√©s que Gitlab) ou si vous √™tes mal √† l‚Äôaise concernant le possesseur de Github (Microsoft), vous pourrez utiliser Gitlab , son concurrent. L‚Äôavantage de Github par rapport √† Gitlab est que le premier est plus visible, car mieux index√© par Google et concentre, en partie pour des raisons historiques, plus de d√©veloppeurs Python et R (ce qui est important dans des domaines comme le code o√π les externalit√©s de r√©seau jouent). Le d√©bat Github vs Gitlab n‚Äôa plus beaucoup de sens aujourd‚Äôhui car les fonctionnalit√©s ont converg√© (Github a rattrap√© une partie de son retard sur l‚Äôint√©gration continue) et, de toute mani√®re, on peut tout √† fait connecter des d√©p√¥ts Gitlab et Github.\n1. [^](#cite_ref-1)\nDans sa version en ligne, Github (https://github.com) dispose de plus de visibilit√© que Gitlab (https://gitlab.com). L‚Äôavantage que comportait Gitlab par rapport √† Github √† une √©poque, √† savoir la possibilit√© de disposer gratuitement de ressources pour faire de l‚Äôint√©gration continue, n‚Äôexiste plus depuis que Github a lanc√© son service Github Actions. Cependant, √™tre familiaris√© √† l‚Äôenvironnement Gitlab reste utile car beaucoup de forges logicielles internes reposent sur les fonctionalit√©s open-source (l‚Äôinterface graphique en faisant parti) de Gitlab. Il est donc fort utile de ma√Ætriser les fonctionalit√©s coeur de ces deux interfaces qui sont en fait quasi-identiques."
  },
  {
    "objectID": "posts/git-2/exogit.html#premi√®re-√©tape-cr√©er-un-compte-github",
    "href": "posts/git-2/exogit.html#premi√®re-√©tape-cr√©er-un-compte-github",
    "title": "Tools - Git-2",
    "section": "Premi√®re √©tape: cr√©er un compte Github",
    "text": "Premi√®re √©tape: cr√©er un compte Github\nLes deux premi√®res √©tapes se font sur Github.\n\nExercice 1 : Cr√©er un compte Github\n\nSi vous n‚Äôen avez pas d√©j√† un, cr√©er un compte sur https://github.com\nCr√©er un d√©p√¥t vide. Cr√©ez ce d√©p√¥t priv√©, cela permettra dans l‚Äôexercice 2 d‚Äôactiver notre jeton. Vous pourrez le rendre public apr√®s l‚Äôexercice 2, c‚Äôest comme vous le souhaitez."
  },
  {
    "objectID": "posts/git-2/exogit.html#deuxi√®me-√©tape-cr√©er-un-token-jeton-https",
    "href": "posts/git-2/exogit.html#deuxi√®me-√©tape-cr√©er-un-token-jeton-https",
    "title": "Tools - Git-2",
    "section": "Deuxi√®me √©tape: cr√©er un token (jeton) HTTPS",
    "text": "Deuxi√®me √©tape: cr√©er un token (jeton) HTTPS"
  },
  {
    "objectID": "posts/git-2/exogit.html#principe",
    "href": "posts/git-2/exogit.html#principe",
    "title": "Tools - Git-2",
    "section": "Principe",
    "text": "Principe\nGit est un syst√®me d√©centralis√© de contr√¥le de version : les codes sont modifi√©s par chaque personne sur son poste de travail, puis sont mis en conformit√© avec la version collective disponible sur le d√©p√¥t distant au moment o√π le contributeur le d√©cide.\nIl est donc n√©cessaire que la forge connaisse l‚Äôidentit√© de chacun des contributeurs, afin de d√©terminer qui est l‚Äôauteur d‚Äôune modification apport√©e aux codes stock√©s dans le d√©p√¥t distant. Pour que Github reconnaisse un utilisateur proposant des modifications, il est n√©cessaire de s‚Äôauthentifier (un d√©p√¥t distant, m√™me public, ne peut pas √™tre modifi√© par n‚Äôimporte qui). L‚Äôauthentification consiste ainsi √† fournir un √©l√©ment que seul vous et la forge sont cens√©s conna√Ætre : un mot de passe, une cl√© compliqu√©e, un jeton d‚Äôacc√®s‚Ä¶\nPlus pr√©cis√©ment, il existe deux modalit√©s pour faire conna√Ætre son identit√© √† Github :\n\nune authentification HTTPS (d√©crite ici) : l‚Äôauthentification se fait avec un login et un mot de passe (qu‚Äôil faut renseigner √† chaque interaction avec le d√©p√¥t), ou avec un token (m√©thode √† privil√©gier).\nune authentification SSH : l‚Äôauthentification se fait par une cl√© crypt√©e disponible sur le poste de travail et que GitHub ou GitLab conna√Æt. Une fois configur√©e, cette m√©thode ne n√©cessite plus de faire conna√Ætre son identit√© : l‚Äôempreinte digitale que constitue la cl√© suffit √† reconna√Ætre un utilisateur.\n\nLa documentation collaborative utilitR pr√©sente les raisons pour lesquelles il convient de favoriser la m√©thode HTTPS sur la m√©thode SSH.\nDepuis ao√ªt 2021, Github n‚Äôautorise plus l‚Äôauthentification par mot de passe lorsqu‚Äôon interagit (pull/push) avec un d√©p√¥t distant (raisons ici). Il est n√©cessaire d‚Äôutiliser un token (jeton d‚Äôacc√®s) qui pr√©sente l‚Äôavantage d‚Äô√™tre r√©voquable (on peut √† tout moment supprimer un jeton si, par exemple, on suspecte qu‚Äôil a √©t√© diffus√© par erreur) et √† droits limit√©s (le jeton permet certaines op√©rations standards mais n‚Äôautorise pas certaines op√©rations d√©terminantes comme la suppression d‚Äôun d√©p√¥t).\n\nIl est important de ne jamais stocker un token, et encore moins son mot de passe, dans un projet. Il est possible de stocker un mot de passe ou token de mani√®re s√©curis√©e et durable avec le credential helper de Git. Celui-ci est pr√©sent√© par la suite.\nS‚Äôil n‚Äôest pas possible d‚Äôutiliser le credential helper de Git, un mot de passe ou token peut √™tre stock√© de mani√®re s√©curis√© dans un syst√®me de gestion de mot de passe comme Keepass.\nNe jamais stocker un jeton Github, ou pire un mot de passe, dans un fichier texte non crypt√©. Les logiciels de gestion de mot de passe (comme Keepass, recommand√© par l‚ÄôAnssi) sont simples d‚Äôusage et permettent de ne conserver sur l‚Äôordinateur qu‚Äôune version hash√©e du mot de passe qui ne peut √™tre d√©crypt√©e qu‚Äôavec un mot de passe connu de vous seuls."
  },
  {
    "objectID": "posts/git-2/exogit.html#cr√©er-un-jeton",
    "href": "posts/git-2/exogit.html#cr√©er-un-jeton",
    "title": "Tools - Git-2",
    "section": "Cr√©er un jeton",
    "text": "Cr√©er un jeton\nLa documentation officielle comporte un certain nombre de captures d‚Äô√©cran expliquant comme proc√©der.\nNous allons utiliser le credential helper associ√© √† Git pour stocker ce jeton. Ce credential helper permet de conserver de mani√®re p√©renne un jeton (on peut aussi faire en sorte que le mot de passe soit automatiquement supprim√© de la m√©moire de l‚Äôordinateur au bout, par exemple, d‚Äôune heure). L‚Äôinconv√©nient de cette m√©thode est que Git √©crit en clair le jeton dans un fichier de configuration. C‚Äôest pour cette raison qu‚Äôon utilise des jetons puisque, si ces derniers sont r√©v√©l√©s, on peut toujours les r√©voquer et √©viter les probl√®mes (pour ne pas stocker en clair un jeton il faudrait utiliser une librairie suppl√©mentaire comme libsecrets qui est au-del√† du programme de ce cours).\nMa recommandation, si vous d√©sirez conserver de mani√®re plus durable ou plus s√©curis√©e votre jeton (en ne conservant pas le jeton en clair mais de mani√®re hash√©e), est d‚Äôutiliser un gestionnaire de mot de passe comme Keepass (recommand√© par l‚ÄôAnssi).\n\nExercice 2 : Cr√©er et stocker un token\n:one: Suivre la documentation officielle en ne donnant que les droits repo au jeton (ajouter les droits workflow si vous d√©sirez que votre jeton soit utilisable pour des projets o√π l‚Äôint√©gration continue est n√©cessaire)\nPour r√©sumer les √©tapes devraient √™tre les suivantes:\nSettings &gt; Developers Settings &gt; Personal Access Token &gt; Generate a new token &gt; ‚ÄúMy bash script‚Äù &gt; Expiration ‚Äú30 days‚Äù &gt; cocher juste ‚Äúrepo‚Äù &gt; Generate token &gt; Le copier\n:two: Ouvrir un terminal depuis Jupyter (par exemple File &gt; New &gt; Terminal).\n:three: [Optionnel] Taper dans le terminal la commande qui convient selon votre syst√®me d‚Äôexploitation pour activer le credential helper:\n# Sous mac et linux et le datalab\ngit config --global credential.helper store\n\n# Sous windows\ngit config --global credential.helper manager-core\n:four: R√©cup√©rer, sur la page d‚Äôaccueil de votre d√©p√¥t, l‚Äôurl du d√©p√¥t distant. Il prend la forme suivante\nhttps://github.com/&lt;username&gt;/&lt;reponame&gt;.git\nVous pouvez utiliser l‚Äôicone √† droite pour copier l‚Äôurl.\n:five: Retournez dans le terminal Jupyter. Taper\ngit clone repo_url\no√π repo_url est l‚Äôurl du d√©p√¥t en question (vous pouvez utiliser MAJ+Inser pour coller l‚Äôurl pr√©c√©demment copi√©)\nTapez Entr√©e. Dans le cas d‚Äôun r√©pertoire priv√© et sans credential helper, renseignez ensuite votre identifiant, faites Entr√©e, puis votre personal access token, Entr√©e. Si vous n‚Äôavez pas d‚Äôerreur, cela signifie que l‚Äôauthentification a bien fonctionn√© et donc que tout va bien. Sinon, il vous suffit de r√©√©crire l‚Äôinstruction git clone et de retenter de taper votre personal access token. Normalement, si vous avez cr√©√© un d√©p√¥t vide dans l‚Äôexercice 1, vous avez un message de Git:\n\nwarning: You appear to have cloned an empty repository.\n\nCeci est normal, ce n‚Äôest pas une erreur. Le dossier de votre projet a bien √©t√© cr√©√©.\nSi vous avez une erreur, suivez la consigne pr√©sent√©e ci-apr√®s pour r√©initialiser votre credential helper\n:six: Si vous le d√©sirez, vous pouvez changer la visibilit√© de votre d√©p√¥t en le rendant public.\n3. [^](#cite_ref-3)\nComme le cr√©ateur de Git √©tait un peu paranoiaque, il est normal de ne pas voir le curseur avancer quand on tape des caract√®res pour le mot de passe, si quelqu‚Äôun regarde votre √©cran il ne pourra ainsi pas savoir combien de caract√®res comporte votre mot de passe.\n\n\nSi vous avez fait une faute de frappe dans le mot de passe ou dans le jeton, il est possible de vider la m√©moire de la mani√®re suivante, sous Mac ou Linux :\ngit config --global --unset credential.helper\nSous Windows, si vous avez utilis√© l‚Äôoption manager-core √©voqu√©e ci-dessus, vous pouvez utiliser une interface graphique pour effacer le mot de passe ou jeton erron√©. Pour cela, dans le menu d√©marrer, taper Gestionnaire d'identification (ou Credential Manager si Windows ne trouve pas). Dans l‚Äôinterface graphique qui s‚Äôouvre, il est possible de supprimer le mot de passe ou jeton en question. Apr√®s cela, vous devriez √† nouveau avoir l‚Äôopportunit√© de taper un mot de passe ou jeton lors d‚Äôune authentification HTTPS."
  },
  {
    "objectID": "posts/git-2/exogit.html#envoyer-des-modifications-sur-le-d√©p√¥t-distant-push",
    "href": "posts/git-2/exogit.html#envoyer-des-modifications-sur-le-d√©p√¥t-distant-push",
    "title": "Tools - Git-2",
    "section": "Envoyer des modifications sur le d√©p√¥t distant: push",
    "text": "Envoyer des modifications sur le d√©p√¥t distant: push\n\nExercice 6 : Interagir avec Github\nIl convient maintenant d‚Äôenvoyer les fichiers sur le d√©p√¥t distant.\n\n:one: L‚Äôobjectif est d‚Äôenvoyer vos modifications vers origin. On va passer par la ligne de commande car les boutons push/pull de l‚Äôextension Jupyter ne fonctionnent pas de mani√®re syst√©matique.\nTaper\ngit push origin master\nCela signifie: ‚Äúgit envoie (push) mes modifications sur la branche master (la branche sur laquelle on a travaill√©, on reviendra dessus) vers mon d√©p√¥t (alias origin)‚Äù\nNormalement, si vous avez utilis√© le credential helper, Git ne vous demande pas vos identifiants de connexion. Sinon, il faut taper votre identifiant github et votre mot de passe correspond au personal access token nouvellement cr√©√© !\n:two: Retournez voir le d√©p√¥t sur Github, vous devriez maintenant voir le fichier .gitignore s‚Äôafficher en page d‚Äôaccueil."
  },
  {
    "objectID": "posts/git-2/exogit.html#la-fonctionnalit√©-pull",
    "href": "posts/git-2/exogit.html#la-fonctionnalit√©-pull",
    "title": "Tools - Git-2",
    "section": "La fonctionnalit√© pull",
    "text": "La fonctionnalit√© pull\nLa deuxi√®me mani√®re d‚Äôinteragir avec le d√©p√¥t est de r√©cup√©rer des r√©sultats disponibles en ligne sur sa copie de travail. On appelle cela pull.\nPour le moment, vous √™tes tout seul sur le d√©p√¥t. Il n‚Äôy a donc pas de partenaire pour modifier un fichier dans le d√©p√¥t distant. On va simuler ce cas en utilisant l‚Äôinterface graphique de Github pour modifier des fichiers. On rappatriera les r√©sultats en local dans un deuxi√®me temps.\n\nExercice 7 : Rapatrier des modifs en local\n:one: Se rendre sur votre d√©p√¥t depuis l‚Äôinterface https://github.com. 2 mani√®res de faire √† ce niveau :\n\nCliquer sur Add file &gt; Create new file et appeler le fichier README.md\nCliquer sur le bouton ADD A README qui est affich√© sur la page d‚Äôaccueil. Supprimez tout autre texte si Github vous a sugg√©r√© un contenu pour le README\n\n:two: L‚Äôobjectif est de donner au README.md un titre en ajoutant, au d√©but du document, la ligne suivante :\n# Mon oeuvre d'art surr√©aliste \nSautez une ligne et entrez le texte que vous d√©sirez, sans ponctuation. Par exemple,\nle ch√™ne un jour dit au roseau\n:three: Cliquez sur l‚Äôonglet Preview pour voir le texte mis en forme au format Markdown\n:four: R√©diger un titre et un message compl√©mentaire pour faire le commit. Conserver l‚Äôoption par d√©faut Commit directly to the master branch\n:five: Editer √† nouveau le README en cliquant sur le crayon juste au dessus de l‚Äôaffichage du contenu du README.\nAjouter une deuxi√®me phrase et corrigez la ponctuation de la premi√®re. Ecrire un message de commit et valider.\nLe Ch√™ne un jour dit au roseau :\nVous avez bien sujet d'accuser la Nature\n:six: Au dessus de l‚Äôaborescence des fichiers, vous devriez voir s‚Äôafficher le titre du dernier commit. Vous pouvez cliquer dessus pour voir la modification que vous avez faite.\n:seven: Les r√©sultats sont sur le d√©p√¥t distant mais ne sont pas sur votre dossier de travail dans Jupyter. Il faut re-synchroniser votre copie locale avec le d√©p√¥t distant :\n\nAvec l‚Äôinterface Jupyter, si cela est possible, appuyez tout simplement sur la petite fl√®che vers le bas, qui est celle qui a d√©sormais la pastille orange.\nSi cette fl√®che n‚Äôest pas disponible ou si vous travaillez dans un autre environnement, vous pouvez utiliser la ligne de commande et taper\n\ngit pull origin master\nCela signifie : ‚Äúgit r√©cup√®re (pull) les modifications sur la branche master vers mon d√©p√¥t (alias origin)‚Äù\n:eight: Regarder, sur JupyterLab, l‚Äôonglet History. Cliquez sur le dernier commit et affichez les changements sur le fichier. Vous pouvez remarquer la finesse du contr√¥le de version : Git d√©tecte au sein de la premi√®re ligne de votre texte que vous avez mis des majuscules ou de la ponctuation.\n\nL‚Äôop√©ration pull permet :\n\nA votre syst√®me local de v√©rifier les modifications sur le d√©p√¥t distant que vous n‚Äôauriez pas faites (cette op√©ration s‚Äôappelle fetch)\nDe les fusionner s‚Äôil n‚Äôy a pas de conflit de version ou si les conflits de version sont automatiquement fusionnables (deux modifications d‚Äôun fichier mais qui ne portent pas sur le m√™me emplacement)."
  },
  {
    "objectID": "posts/git-2/exogit.html#le-workflow-adopt√©",
    "href": "posts/git-2/exogit.html#le-workflow-adopt√©",
    "title": "Tools - Git-2",
    "section": "Le workflow adopt√©",
    "text": "Le workflow adopt√©\nNous allons adopter le mode de travail le plus simple, le Github Flow. Il correspond √† cette forme caract√©ristique d‚Äôarbre:\n\nLa branche master constitue le tronc\nLes branches partent de master et divergent\nLorsque les modifications aboutissent, elles sont int√©gr√©es √† master ; la branche en question dispara√Æt:\n\n\nIl existe des workflows plus complexes, notamment le Git Flow que j‚Äôutilise pour d√©velopper ce cours. Ce tutoriel, tr√®s bien fait, illustre avec un graphique la complexit√© accrue de ce flow:\n\nCette fois, une branche interm√©diaire, par exemple une branche development int√®gre des modifications √† tester avant de les int√©grer dans la version officielle (master).\n\nVous pourrez trouvez des dizaines d‚Äôarticles et d‚Äôouvrages sur ce sujet dont chacun pr√©tend avoir trouv√© la meilleure organisation du travail (Git flow, GitHub flow, GitLab flow‚Ä¶). Ne lisez pas trop ces livres et articles sinon vous serez perdus (un peu comme avec les magazines destin√©s aux jeunes parents‚Ä¶).\nLa m√©thode de travail la plus simple est le Github flow qu‚Äôon vous a propos√© d‚Äôadopter. L‚Äôarborescence est reconnaissable: des branches divergent et reviennent syst√©matiquement vers master.\nPour des projets plus complexes dans des √©quipes d√©veloppant des applications, on pourra utiliser d‚Äôautres m√©thodes de travail, notamment le Git flow. Il n‚Äôexiste pas de r√®gles universelles pour d√©terminer la m√©thode de travail ; l‚Äôimportant c‚Äôest, avant tout, de se mettre d‚Äôaccord sur des r√®gles communes de travail avec votre √©quipe."
  },
  {
    "objectID": "posts/git-2/exogit.html#m√©thode-pour-les-merges",
    "href": "posts/git-2/exogit.html#m√©thode-pour-les-merges",
    "title": "Tools - Git-2",
    "section": "M√©thode pour les merges",
    "text": "M√©thode pour les merges\nLes merges vers master doivent imp√©rativement passer par Github (ou Gitlab). Cela permet de garder une trace explicite de ceux-ci (par exemple ici), sans avoir √† chercher dans l‚Äôarborescence, parfois complexe, d‚Äôun projet.\nLa bonne pratique veut qu‚Äôon fasse un squash commit pour √©viter une inflation du nombre de commits dans master: les branches ont vocation √† proposer une multitude de petits commits, les modifications dans master doivent √™tre simples √† tracer d‚Äôo√π le fait de modifier des petits bouts de code.\nComme on l‚Äôa fait dans un exercice pr√©c√©dent, il est tr√®s pratique d‚Äôajouter dans le corps du message close #xx o√π xx est le num√©ro d‚Äôune issue associ√©e √† la pull request. Lorsque la pull request sera fusionn√©e, l‚Äôissue sera automatiquement ferm√©e et un lien sera cr√©√© entre l‚Äôissue et la pull request. Cela vous permettra de comprendre, plusieurs mois ou ann√©es plus tard comment et pourquoi telle ou telle fonctionnalit√© a √©t√© impl√©ment√©e.\nEn revanche, l‚Äôint√©gration des derni√®res modifications de master vers une branche se fait en local. Si votre branche est en conflit, le conflit doit √™tre r√©solu dans la branche et pas dans master. master doit toujours rester propre."
  },
  {
    "objectID": "posts/git-2/exogit.html#mise-en-pratique",
    "href": "posts/git-2/exogit.html#mise-en-pratique",
    "title": "Tools - Git-2",
    "section": "Mise en pratique",
    "text": "Mise en pratique\n\nExercice 9 : Interactions avec le d√©p√¥t distant\nCet exercice se fait par groupe de trois ou quatre. Il y aura deux r√¥les dans ce sc√©nario :\n\nUne personne aura la responsabilit√© d‚Äô√™tre mainteneur\nDeux √† trois personnes seront d√©veloppeurs.\n\n:one: Le mainteneur cr√©e un d√©p√¥t sur Github. Il/Elle donne des droits au(x) d√©veloppeur(s) du projet (Settings &gt; Manage Access &gt; Invite a collaborator).\n:two: Chaque membre du projet, cr√©e une copie locale du projet gr√¢ce √† la commande git clone ou avec le bouton Clone a repository de JupyterLab.\nPour cela, r√©cup√©rer l‚Äôurl HTTPS du d√©p√¥t en copiant l‚Äôurl du d√©p√¥t que vous pouvez trouver, par exemple, dans la page d‚Äôaccueil du d√©p√¥t, en dessous de Quick setup ‚Äî if you‚Äôve done this kind of thing before\nEn ligne de commande, cela donnera:\ngit clone https://github.com/&lt;username&gt;/&lt;reponame&gt;.git\n:three: Chaque membre du projet cr√©e un fichier avec son nom et son pr√©nom, selon cette structure nom-prenom.md en √©vitant les caract√®res sp√©ciaux. Il √©crit dedans trois phrases de son choix sans ponctuation ni majuscules (pour pouvoir effectuer une correction ult√©rieurement). Enfin, il commit sur le projet.\nPour rappel, en ligne de commande cela donnera les commandes suivantes √† modifier\ngit add nom-prenom.md\ngit commit -m \"C'est l'histoire de XXXXX\"\n:four: Chacun essaie d‚Äôenvoyer (push) ses modifications locales sur le d√©p√¥t:\ngit push origin master\n:five: A ce stade, une seule personne (la plus rapide) devrait ne pas avoir rencontr√© de rejet du push. C‚Äôest normal, avant d‚Äôaccepter une modification Git v√©rifie en premier lieu la coh√©rence de la branche avec le d√©p√¥t distant. Le premier ayant fait un push a modifi√© le d√©p√¥t commun ; les autres doivent int√©grer ces modifications dans leur version locale (pull) avant d‚Äôavoir le droit de proposer un changement.\nPour celui/celle/ceux dont le push a √©t√© refus√©, faire\ngit pull origin master\npour ramener les modifications distantes en local.\n:six: Taper git log et regarder la mani√®re dont a √©t√© int√©gr√© la modification de votre camarade ayant pu faire son push\nVous remarquerez que les commits de vos camarades sont int√©gr√©s tels quels √† l‚Äôhistoire du d√©p√¥t.\n:seven: Faire √† nouveau\ngit pull origin master\nLe dernier doit refaire, √† nouveau, les √©tapes 5 √† 7 (dans une √©quipe de quatre il faudra encore le refaire une fois).\n\n\nQuand on fait face √† un rejet du push, on est tent√© de faire passer en force le push malgr√© la mise en garde pr√©c√©dente.\nIl faut imm√©diatement oublier cette solution, elle cr√©e de nombreux probl√®mes et, en fait, ne r√©sout rien. L‚Äôun des risques est de r√©√©crire enti√®rement l‚Äôhistorique rendant les copies locales, et donc les modifications de vos collaborateurs, caduques. Cela vous vaudra, √† raison, des remontrances de vos partenaires qui perdent le b√©n√©fice de leur historique Git qui, s‚Äôils ont des versions sans push depuis longtemps peuvent avoir diverger fortement du d√©p√¥t ma√Ætre.\n\n\nExercice 10 : G√©rer les conflits quand on travaille sur le m√™me fichier\nDans la continuit√© de l‚Äôexercice pr√©c√©dent, chaque personne va travailler sur les fichiers des autres membres de l‚Äô√©quipe.\n:one: Les deux ou trois d√©veloppeurs ajoutent la ponctuation et les majuscules du fichier du premier d√©veloppeur.\n:two: Ils sautent une ligne et ajoutent une phrase (pas tous la m√™me).\n:three: Valider les r√©sultats (git add . et commit) et faire un push\n:four: La personne la plus rapide n‚Äôa, normalement, rencontr√© aucune difficult√© (elle peut s‚Äôarr√™ter temporairement pour regarder ce qui va se passer chez les voisins). Les autres voient leur push refus√© et doivent faire un pull.\n:boom: Il y a conflit, ce qui doit √™tre signal√© par un message du type:\nAuto-merging XXXXXX\nCONFLICT (content): Merge conflict in XXXXXX.md\nAutomatic merge failed; fix conflicts and then commit the result.\n:five: Etudier le r√©sultat de git status\n:six: Si vous ouvrez les fichiers incrimin√©s, vous devriez voir des balises du type\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nthis is some content to mess with\ncontent to append\n=======\ntotally different content to merge later\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; new_branch_to_merge_later\n:seven: Corriger √† la main les fichiers en choisissant, pour chaque ligne, la version qui vous convient et en retirant les balises. Valider en faisant:\ngit add . && git commit -m \"R√©solution du conflit par XXXX\"\nRemplacer XXXX par votre nom. La balise && permet d‚Äôencha√Æner, en une seule ligne de code, les deux commandes.\n:eight: Faire un push. Pour la derni√®re personne, refaire les op√©rations 4 √† 8\n\nGit permet donc de travailler, en m√™me temps, sur le m√™me fichier et de limiter le nombre de gestes manuels n√©cessaires pour faire la fusion. Lorsqu‚Äôon travaille sur des bouts diff√©rents du m√™me fichier, on n‚Äôa m√™me pas besoin de faire de modification manuelle, la fusion peut √™tre automatique.\nGit est un outil tr√®s puissant. Mais, il ne remplace pas une bonne organisation du travail. Vous l‚Äôavez vu, ce mode de travail uniquement sur master peut √™tre p√©nible. Les branches prennent tout leur sens dans ce cas.\n\nExercice 11 : Gestion des branches\n:one: Le mainteneur va contribuer directement dans master et ne cr√©e pas de branche. Chaque d√©veloppeur cr√©e une branche, en local nomm√©e contrib-XXXXX o√π XXXXX est le pr√©nom:\ngit checkout -b contrib-XXXXX\n:two: Chaque membre du groupe cr√©e un fichier README.md o√π il √©crit une phrase sujet-verbe-compl√©ment. Le mainteneur est le seul √† ajouter un titre dans le README (qu‚Äôil commit dans master).\n:three: Chacun push le produit de son subconscient sur le d√©p√¥t.\n:four: Les d√©veloppeurs ouvrent, chacun, une pull request sur Github de leur branche vers master. Ils lui donnent un titre explicite.\n:five: Dans la discussion de chaque pull request, le mainteneur demande au d√©veloppeur d‚Äôint√©grer le titre qu‚Äôil a √©crit.\n:six: Chaque d√©veloppeur, en local, int√®gre cette modification en faisant\n# Pour √™tre s√ªr d'√™tre sur sa propre branche\ngit checkout branche-XXXX\ngit merge master\nR√©gler le conflit et valider (add et commit). Pousser le r√©sultat. Le mainteneur choisit une des pull request et la valide avec l‚Äôoption squash commits. V√©rifier sur la page d‚Äôaccueil le r√©sultat.\n:seven: L‚Äôauteur (si 2 d√©veloppeurs) ou les deux auteurs (si 3 d√©veloppeurs) de la pull request non valid√©e doivent √† nouveau r√©p√©ter l‚Äôop√©ration 6.\n:eight: Une fois le conflit de version r√©gl√© et pouss√©, le mainteneur valide la pull request selon la m√™me proc√©dure que pr√©cedemment.\n:nine: V√©rifier l‚Äôarborescence du d√©p√¥t dans Insights &gt; Network. Votre arbre doit avoir une forme caract√©ristique de ce qu‚Äôon appelle le Github flow:"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Recent posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nTools - Soil Chemistry Dataset\n\n\nExploratory analysis\n\n\n\nJun 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTools - Git-2\n\n\nData Analysis\n\n\n\nJun 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTools - Git\n\n\nData Analysis\n\n\n\nJun 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTools - Numpy\n\n\nData Analysis\n\n\n\nJun 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTools - matplotlib\n\n\nData Analysis\n\n\n\nJun 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTools - pandas\n\n\nData Analysis\n\n\n\nJun 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto Computations\n\n\n\nMay 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto Basics\n\n\n\nMay 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTitanic dataset analysis using Pandas and Numpy\n\n\nData Visualization\n\n\n\nMay 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow helpful can AI be in solving the water crisis?\n\n\n\nMay 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCan bamboo help to solve climate crisis?\n\n\n\nMay 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCan AI decide when to water crops?\n\n\n\nMay 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMachine learning in Agriculture\n\n\n\nApr 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIrrigation scheduling with Machine learning\n\n\n\nApr 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDigitaliztion of Agriculture\n\n\n\nApr 17, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/irrigraiton_scheduling/content.html",
    "href": "posts/irrigraiton_scheduling/content.html",
    "title": "Irrigation scheduling with Machine learning",
    "section": "",
    "text": "The paper ‚ÄúNeural Network soil moisture model for irrigration scheduling‚Äù proposes an intelligent system for predicting soil moisture and optimizing irrigation scheduling using machine learning algorithms.\nThe proposed system uses a wireless sensor network (WSN) to monitor soil moisture levels and collects data from various sensors installed in the soil. The data collected from the WSN is then processed and analyzed using machine learning algorithms to predict soil moisture levels and determine optimal irrigation schedules.\nThe authors used two machine learning algorithms, support vector regression (SVR) and random forest (RF), to develop the predictive models for soil moisture prediction. The models were trained using data collected from the WSN and validated using real-world data.\nThe results of the study showed that the proposed system is effective in predicting soil moisture levels and optimizing irrigation scheduling. The SVR model outperformed the RF model, achieving a mean absolute error of 0.026 for soil moisture prediction.\nHenceforth, the proposed system provides an automated and intelligent solution for soil moisture prediction and irrigation scheduling, which can help farmers optimize their irrigation practices and reduce water usage."
  },
  {
    "objectID": "posts/Machine learning in Agriculture/index.html",
    "href": "posts/Machine learning in Agriculture/index.html",
    "title": "Machine learning in Agriculture",
    "section": "",
    "text": "The article ‚ÄúMachine Learning in Agriculture: A Review‚Äù provides a comprehensive overview of the current state and potential applications of machine learning in agriculture.\nThe authors first introduce the concept of machine learning and its various techniques, including supervised, unsupervised, and reinforcement learning. They then provide an overview of the different areas in agriculture where machine learning can be applied, such as crop yield prediction, plant disease diagnosis, soil analysis, and livestock management.\nThe article further explores the use of different types of sensors and data collection techniques that can be used to gather data for machine learning applications in agriculture. The authors also discuss the challenges associated with implementing machine learning in agriculture, such as data quality, lack of standardization, and limited computational resources.\nThe authors present several case studies and examples of successful machine learning applications in agriculture, including precision farming, crop monitoring, and yield prediction. The article concludes with a discussion on the future directions and potential impact of machine learning in agriculture, including the development of new tools and technologies to support sustainable agriculture practices and enhance food security."
  },
  {
    "objectID": "posts/matplotlib/Matplotlib Tutorial.html",
    "href": "posts/matplotlib/Matplotlib Tutorial.html",
    "title": "Tools - matplotlib",
    "section": "",
    "text": "Load Necessary Libraries\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\n\n\nBasic Graph\n\n\nCode\nx = [0,1,2,3,4]\ny = [0,2,4,6,8]\n\n# Resize your Graph (dpi specifies pixels per inch. When saving probably should use 300 if possible)\nplt.figure(figsize=(8,5), dpi=100)\n\n# Line 1\n\n# Keyword Argument Notation\n#plt.plot(x,y, label='2x', color='red', linewidth=2, marker='.', linestyle='--', markersize=10, markeredgecolor='blue')\n\n# Shorthand notation\n# fmt = '[color][marker][line]'\nplt.plot(x,y, 'b^--', label='2x')\n\n## Line 2\n\n# select interval we want to plot points at\nx2 = np.arange(0,4.5,0.5)\n\n# Plot part of the graph as line\nplt.plot(x2[:6], x2[:6]**2, 'r', label='X^2')\n\n# Plot remainder of graph as a dot\nplt.plot(x2[5:], x2[5:]**2, 'r--')\n\n# Add a title (specify font parameters with fontdict)\nplt.title('Our First Graph!', fontdict={'fontname': 'Comic Sans MS', 'fontsize': 20})\n\n# X and Y labels\nplt.xlabel('X Axis')\nplt.ylabel('Y Axis')\n\n# X, Y axis Tickmarks (scale of your graph)\nplt.xticks([0,1,2,3,4,])\n#plt.yticks([0,2,4,6,8,10])\n\n# Add a legend\nplt.legend()\n\n# Save figure (dpi 300 is good when saving so graph has high resolution)\nplt.savefig('mygraph.png', dpi=300)\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\nBar Chart\n\n\nCode\nlabels = ['A', 'B', 'C']\nvalues = [1,4,2]\n\nplt.figure(figsize=(5,3), dpi=100)\n\nbars = plt.bar(labels, values)\n\npatterns = ['/', 'O', '*']\nfor bar in bars:\n    bar.set_hatch(patterns.pop(0))\n\nplt.savefig('barchart.png', dpi=300)\n\nplt.show()\n\n\n\n\n\n\n\nReal World Examples\nDownload data from his Github (gas_prices.csv & fifa_data.csv)\n\nLine Graph\n\n\nCode\ngas = pd.read_csv('gas_prices.csv')\n\nplt.figure(figsize=(8,5))\n\nplt.title('Gas Prices over Time (in USD)', fontdict={'fontweight':'bold', 'fontsize': 18})\n\nplt.plot(gas.Year, gas.USA, 'b.-', label='United States')\nplt.plot(gas.Year, gas.Canada, 'r.-')\nplt.plot(gas.Year, gas['South Korea'], 'g.-')\nplt.plot(gas.Year, gas.Australia, 'y.-')\n\n# Another Way to plot many values!\n# countries_to_look_at = ['Australia', 'USA', 'Canada', 'South Korea']\n# for country in gas:\n#     if country in countries_to_look_at:\n#         plt.plot(gas.Year, gas[country], marker='.')\n\nplt.xticks(gas.Year[::3].tolist()+[2011])\n\nplt.xlabel('Year')\nplt.ylabel('US Dollars')\n\nplt.legend()\n\nplt.savefig('Gas_price_figure.png', dpi=300)\n\nplt.show()\n\n\n\n\n\n\n\nLoad Fifa Data\n\n\nCode\nfifa = pd.read_csv('fifa_data.csv')\n\nfifa.head(5)\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\n...\nComposure\nMarking\nStandingTackle\nSlidingTackle\nGKDiving\nGKHandling\nGKKicking\nGKPositioning\nGKReflexes\nRelease Clause\n\n\n\n\n0\n0\n158023\nL. Messi\n31\nhttps://cdn.sofifa.org/players/4/19/158023.png\nArgentina\nhttps://cdn.sofifa.org/flags/52.png\n94\n94\nFC Barcelona\n...\n96.0\n33.0\n28.0\n26.0\n6.0\n11.0\n15.0\n14.0\n8.0\n‚Ç¨226.5M\n\n\n1\n1\n20801\nCristiano Ronaldo\n33\nhttps://cdn.sofifa.org/players/4/19/20801.png\nPortugal\nhttps://cdn.sofifa.org/flags/38.png\n94\n94\nJuventus\n...\n95.0\n28.0\n31.0\n23.0\n7.0\n11.0\n15.0\n14.0\n11.0\n‚Ç¨127.1M\n\n\n2\n2\n190871\nNeymar Jr\n26\nhttps://cdn.sofifa.org/players/4/19/190871.png\nBrazil\nhttps://cdn.sofifa.org/flags/54.png\n92\n93\nParis Saint-Germain\n...\n94.0\n27.0\n24.0\n33.0\n9.0\n9.0\n15.0\n15.0\n11.0\n‚Ç¨228.1M\n\n\n3\n3\n193080\nDe Gea\n27\nhttps://cdn.sofifa.org/players/4/19/193080.png\nSpain\nhttps://cdn.sofifa.org/flags/45.png\n91\n93\nManchester United\n...\n68.0\n15.0\n21.0\n13.0\n90.0\n85.0\n87.0\n88.0\n94.0\n‚Ç¨138.6M\n\n\n4\n4\n192985\nK. De Bruyne\n27\nhttps://cdn.sofifa.org/players/4/19/192985.png\nBelgium\nhttps://cdn.sofifa.org/flags/7.png\n91\n92\nManchester City\n...\n88.0\n68.0\n58.0\n51.0\n15.0\n13.0\n5.0\n10.0\n13.0\n‚Ç¨196.4M\n\n\n\n\n5 rows √ó 89 columns\n\n\n\n\n\nHistogram\n\n\nCode\nbins = [40,50,60,70,80,90,100]\n\nplt.figure(figsize=(8,5))\n\nplt.hist(fifa.Overall, bins=bins, color='#abcdef')\n\nplt.xticks(bins)\n\nplt.ylabel('Number of Players')\nplt.xlabel('Skill Level')\nplt.title('Distribution of Player Skills in FIFA 2018')\n\nplt.savefig('histogram.png', dpi=300)\n\nplt.show()\n\n\n\n\n\n\n\nPie Chart\n\n\nCode\nleft = fifa.loc[fifa['Preferred Foot'] == 'Left'].count()[0]\nright = fifa.loc[fifa['Preferred Foot'] == 'Right'].count()[0]\n\nplt.figure(figsize=(8,5))\n\nlabels = ['Left', 'Right']\ncolors = ['#abcdef', '#aabbcc']\n\nplt.pie([left, right], labels = labels, colors=colors, autopct='%.2f %%')\n\nplt.title('Foot Preference of FIFA Players')\n\nplt.show()\n\n\n\n\n\n\n\nPie Chart #2\n\n\nCode\nplt.figure(figsize=(8,5), dpi=100)\n\nplt.style.use('ggplot')\n\nfifa.Weight = [int(x.strip('lbs')) if type(x)==str else x for x in fifa.Weight]\n\nlight = fifa.loc[fifa.Weight &lt; 125].count()[0]\nlight_medium = fifa[(fifa.Weight &gt;= 125) & (fifa.Weight &lt; 150)].count()[0]\nmedium = fifa[(fifa.Weight &gt;= 150) & (fifa.Weight &lt; 175)].count()[0]\nmedium_heavy = fifa[(fifa.Weight &gt;= 175) & (fifa.Weight &lt; 200)].count()[0]\nheavy = fifa[fifa.Weight &gt;= 200].count()[0]\n\nweights = [light,light_medium, medium, medium_heavy, heavy]\nlabel = ['under 125', '125-150', '150-175', '175-200', 'over 200']\nexplode = (.4,.2,0,0,.4)\n\nplt.title('Weight of Professional Soccer Players (lbs)')\n\nplt.pie(weights, labels=label, explode=explode, pctdistance=0.8,autopct='%.2f %%')\nplt.show()\n\n\n\n\n\n\n\nBox and Whiskers Chart\n\n\nCode\nplt.figure(figsize=(5,8), dpi=100)\n\nplt.style.use('default')\n\nbarcelona = fifa.loc[fifa.Club == \"FC Barcelona\"]['Overall']\nmadrid = fifa.loc[fifa.Club == \"Real Madrid\"]['Overall']\nrevs = fifa.loc[fifa.Club == \"New England Revolution\"]['Overall']\n\n#bp = plt.boxplot([barcelona, madrid, revs], labels=['a','b','c'], boxprops=dict(facecolor='red'))\nbp = plt.boxplot([barcelona, madrid, revs], labels=['FC Barcelona','Real Madrid','NE Revolution'], patch_artist=True, medianprops={'linewidth': 2})\n\nplt.title('Professional Soccer Team Comparison')\nplt.ylabel('FIFA Overall Rating')\n\nfor box in bp['boxes']:\n    # change outline color\n    box.set(color='#4286f4', linewidth=2)\n    # change fill color\n    box.set(facecolor = '#e0e0e0' )\n    # change hatch\n    #box.set(hatch = '/')\n    \nplt.show()"
  },
  {
    "objectID": "posts/numpy/Numpy_tutorial.html",
    "href": "posts/numpy/Numpy_tutorial.html",
    "title": "Tools - Numpy",
    "section": "",
    "text": "Python is a great general-purpose programming language on its own, but with the help of a few popular libraries (numpy, scipy, matplotlib) it becomes a powerful environment for scientific computing.\nWe expect that many of you will have some experience with Python and numpy; for the rest of you, this section will serve as a quick crash course both on the Python programming language and on the use of Python for scientific computing.\nSome of you may have previous knowledge in Matlab, in which case we also recommend the numpy for Matlab users page (https://docs.scipy.org/doc/numpy-dev/user/numpy-for-matlab-users.html).\nIn this tutorial, we will cover:\n\nBasic Python: Basic data types (Containers, Lists, Dictionaries, Sets, Tuples), Functions, Classes\nNumpy: Arrays, Array indexing, Datatypes, Array math, Broadcasting\nMatplotlib: Plotting, Subplots, Images\nIPython: Creating notebooks, Typical workflows"
  },
  {
    "objectID": "posts/numpy/Numpy_tutorial.html#introduction",
    "href": "posts/numpy/Numpy_tutorial.html#introduction",
    "title": "Tools - Numpy",
    "section": "",
    "text": "Python is a great general-purpose programming language on its own, but with the help of a few popular libraries (numpy, scipy, matplotlib) it becomes a powerful environment for scientific computing.\nWe expect that many of you will have some experience with Python and numpy; for the rest of you, this section will serve as a quick crash course both on the Python programming language and on the use of Python for scientific computing.\nSome of you may have previous knowledge in Matlab, in which case we also recommend the numpy for Matlab users page (https://docs.scipy.org/doc/numpy-dev/user/numpy-for-matlab-users.html).\nIn this tutorial, we will cover:\n\nBasic Python: Basic data types (Containers, Lists, Dictionaries, Sets, Tuples), Functions, Classes\nNumpy: Arrays, Array indexing, Datatypes, Array math, Broadcasting\nMatplotlib: Plotting, Subplots, Images\nIPython: Creating notebooks, Typical workflows"
  },
  {
    "objectID": "posts/numpy/Numpy_tutorial.html#a-brief-note-on-python-versions",
    "href": "posts/numpy/Numpy_tutorial.html#a-brief-note-on-python-versions",
    "title": "Tools - Numpy",
    "section": "A Brief Note on Python Versions",
    "text": "A Brief Note on Python Versions\nAs of Janurary 1, 2020, Python has officially dropped support for python2. We‚Äôll be using Python 3.7 for this iteration of the course. You can check your Python version at the command line by running python --version. In Colab, we can enforce the Python version by clicking Runtime -&gt; Change Runtime Type and selecting python3. Note that as of April 2020, Colab uses Python 3.6.9 which should run everything without any errors.\n\n!python --version\n\nPython 3.6.9"
  },
  {
    "objectID": "posts/numpy/Numpy_tutorial.html#basics-of-python",
    "href": "posts/numpy/Numpy_tutorial.html#basics-of-python",
    "title": "Tools - Numpy",
    "section": "Basics of Python",
    "text": "Basics of Python\nPython is a high-level, dynamically typed multiparadigm programming language. Python code is often said to be almost like pseudocode, since it allows you to express very powerful ideas in very few lines of code while being very readable. As an example, here is an implementation of the classic quicksort algorithm in Python:\n\ndef quicksort(arr):\n    if len(arr) &lt;= 1:\n        return arr\n    pivot = arr[len(arr) // 2]\n    left = [x for x in arr if x &lt; pivot]\n    middle = [x for x in arr if x == pivot]\n    right = [x for x in arr if x &gt; pivot]\n    return quicksort(left) + middle + quicksort(right)\n\nprint(quicksort([3,6,8,10,1,2,1]))\n\n[1, 1, 2, 3, 6, 8, 10]\n\n\n\nBasic data types\n\nNumbers\nIntegers and floats work as you would expect from other languages:\n\nx = 3\nprint(x, type(x))\n\n3 &lt;class 'int'&gt;\nERROR! Session/line number was not unique in database. History logging moved to new session 60\n\n\n\nprint(x + 1)   # Addition\nprint(x - 1)   # Subtraction\nprint(x * 2)   # Multiplication\nprint(x ** 2)  # Exponentiation\n\n4\n2\n6\n9\n\n\n\nx += 1\nprint(x)\nx *= 2\nprint(x)\n\n9\n18\n\n\n\ny = 2.5\nprint(type(y))\nprint(y, y + 1, y * 2, y ** 2)\n\n&lt;class 'float'&gt;\n2.5 3.5 5.0 6.25\n\n\nNote that unlike many languages, Python does not have unary increment (x++) or decrement (x‚Äì) operators.\nPython also has built-in types for long integers and complex numbers; you can find all of the details in the documentation.\n\n\nBooleans\nPython implements all of the usual operators for Boolean logic, but uses English words rather than symbols (&&, ||, etc.):\n\nt, f = True, False\nprint(type(t))\n\n&lt;class 'bool'&gt;\n\n\nNow we let‚Äôs look at the operations:\n\nprint(t and f) # Logical AND;\nprint(t or f)  # Logical OR;\nprint(not t)   # Logical NOT;\nprint(t != f)  # Logical XOR;\n\nFalse\nTrue\nFalse\nTrue\n\n\n\n\nStrings\n\nhello = 'hello'   # String literals can use single quotes\nworld = \"world\"   # or double quotes; it does not matter\nprint(hello, len(hello))\n\nhello 5\n\n\n\nhw = hello + ' ' + world  # String concatenation\nprint(hw)\n\nhello world\n\n\n\nhw12 = '{} {} {}'.format(hello, world, 12)  # string formatting\nprint(hw12)\n\nhello world 12\n\n\nString objects have a bunch of useful methods; for example:\n\ns = \"hello\"\nprint(s.capitalize())  # Capitalize a string\nprint(s.upper())       # Convert a string to uppercase; prints \"HELLO\"\nprint(s.rjust(7))      # Right-justify a string, padding with spaces\nprint(s.center(7))     # Center a string, padding with spaces\nprint(s.replace('l', '(ell)'))  # Replace all instances of one substring with another\nprint('  world '.strip())  # Strip leading and trailing whitespace\n\nHello\nHELLO\n  hello\n hello \nhe(ell)(ell)o\nworld\n\n\nYou can find a list of all string methods in the documentation.\n\n\n\nContainers\nPython includes several built-in container types: lists, dictionaries, sets, and tuples.\n\nLists\nA list is the Python equivalent of an array, but is resizeable and can contain elements of different types:\n\nxs = [3, 1, 2]   # Create a list\nprint(xs, xs[2])\nprint(xs[-1])     # Negative indices count from the end of the list; prints \"2\"\n\n[3, 1, 2] 2\n2\n\n\n\nxs[2] = 'foo'    # Lists can contain elements of different types\nprint(xs)\n\n[3, 1, 'foo']\n\n\n\nxs.append('bar') # Add a new element to the end of the list\nprint(xs)  \n\n[3, 1, 'foo', 'bar']\n\n\n\nx = xs.pop()     # Remove and return the last element of the list\nprint(x, xs)\n\nbar [3, 1, 'foo']\n\n\nAs usual, you can find all the gory details about lists in the documentation.\n\n\nSlicing\nIn addition to accessing list elements one at a time, Python provides concise syntax to access sublists; this is known as slicing:\n\nnums = list(range(5))    # range is a built-in function that creates a list of integers\nprint(nums)         # Prints \"[0, 1, 2, 3, 4]\"\nprint(nums[2:4])    # Get a slice from index 2 to 4 (exclusive); prints \"[2, 3]\"\nprint(nums[2:])     # Get a slice from index 2 to the end; prints \"[2, 3, 4]\"\nprint(nums[:2])     # Get a slice from the start to index 2 (exclusive); prints \"[0, 1]\"\nprint(nums[:])      # Get a slice of the whole list; prints [\"0, 1, 2, 3, 4]\"\nprint(nums[:-1])    # Slice indices can be negative; prints [\"0, 1, 2, 3]\"\nnums[2:4] = [8, 9] # Assign a new sublist to a slice\nprint(nums)         # Prints \"[0, 1, 8, 9, 4]\"\n\n[0, 1, 2, 3, 4]\n[2, 3]\n[2, 3, 4]\n[0, 1]\n[0, 1, 2, 3, 4]\n[0, 1, 2, 3]\n[0, 1, 8, 9, 4]\n\n\n\n\nLoops\nYou can loop over the elements of a list like this:\n\nanimals = ['cat', 'dog', 'monkey']\nfor animal in animals:\n    print(animal)\n\ncat\ndog\nmonkey\n\n\nIf you want access to the index of each element within the body of a loop, use the built-in enumerate function:\n\nanimals = ['cat', 'dog', 'monkey']\nfor idx, animal in enumerate(animals):\n    print('#{}: {}'.format(idx + 1, animal))\n\n#1: cat\n#2: dog\n#3: monkey\n\n\n\n\nList comprehensions\nWhen programming, frequently we want to transform one type of data into another. As a simple example, consider the following code that computes square numbers:\n\nnums = [0, 1, 2, 3, 4]\nsquares = []\nfor x in nums:\n    squares.append(x ** 2)\nprint(squares)\n\n[0, 1, 4, 9, 16]\n\n\nYou can make this code simpler using a list comprehension:\n\nnums = [0, 1, 2, 3, 4]\nsquares = [x ** 2 for x in nums]\nprint(squares)\n\n[0, 1, 4, 9, 16]\n\n\nList comprehensions can also contain conditions:\n\nnums = [0, 1, 2, 3, 4]\neven_squares = [x ** 2 for x in nums if x % 2 == 0]\nprint(even_squares)\n\n[0, 4, 16]\n\n\n\n\nDictionaries\nA dictionary stores (key, value) pairs, similar to a Map in Java or an object in Javascript. You can use it like this:\n\nd = {'cat': 'cute', 'dog': 'furry'}  # Create a new dictionary with some data\nprint(d['cat'])       # Get an entry from a dictionary; prints \"cute\"\nprint('cat' in d)     # Check if a dictionary has a given key; prints \"True\"\n\ncute\nTrue\n\n\n\nd['fish'] = 'wet'    # Set an entry in a dictionary\nprint(d['fish'])      # Prints \"wet\"\n\nwet\n\n\n\nprint(d['monkey'])  # KeyError: 'monkey' not a key of d\n\nKeyError: ignored\n\n\n\nprint(d.get('monkey', 'N/A'))  # Get an element with a default; prints \"N/A\"\nprint(d.get('fish', 'N/A'))    # Get an element with a default; prints \"wet\"\n\nN/A\nwet\n\n\n\ndel d['fish']        # Remove an element from a dictionary\nprint(d.get('fish', 'N/A')) # \"fish\" is no longer a key; prints \"N/A\"\n\nN/A\n\n\nYou can find all you need to know about dictionaries in the documentation.\nIt is easy to iterate over the keys in a dictionary:\n\nd = {'person': 2, 'cat': 4, 'spider': 8}\nfor animal, legs in d.items():\n    print('A {} has {} legs'.format(animal, legs))\n\nA person has 2 legs\nA cat has 4 legs\nA spider has 8 legs\n\n\nDictionary comprehensions: These are similar to list comprehensions, but allow you to easily construct dictionaries. For example:\n\nnums = [0, 1, 2, 3, 4]\neven_num_to_square = {x: x ** 2 for x in nums if x % 2 == 0}\nprint(even_num_to_square)\n\n{0: 0, 2: 4, 4: 16}\n\n\n\n\nSets\nA set is an unordered collection of distinct elements. As a simple example, consider the following:\n\nanimals = {'cat', 'dog'}\nprint('cat' in animals)   # Check if an element is in a set; prints \"True\"\nprint('fish' in animals)  # prints \"False\"\n\nTrue\nFalse\n\n\n\nanimals.add('fish')      # Add an element to a set\nprint('fish' in animals)\nprint(len(animals))       # Number of elements in a set;\n\nTrue\n3\n\n\n\nanimals.add('cat')       # Adding an element that is already in the set does nothing\nprint(len(animals))       \nanimals.remove('cat')    # Remove an element from a set\nprint(len(animals))       \n\n3\n2\n\n\nLoops: Iterating over a set has the same syntax as iterating over a list; however since sets are unordered, you cannot make assumptions about the order in which you visit the elements of the set:\n\nanimals = {'cat', 'dog', 'fish'}\nfor idx, animal in enumerate(animals):\n    print('#{}: {}'.format(idx + 1, animal))\n\n#1: dog\n#2: cat\n#3: fish\n\n\nSet comprehensions: Like lists and dictionaries, we can easily construct sets using set comprehensions:\n\nfrom math import sqrt\nprint({int(sqrt(x)) for x in range(30)})\n\n{0, 1, 2, 3, 4, 5}\n\n\n\n\nTuples\nA tuple is an (immutable) ordered list of values. A tuple is in many ways similar to a list; one of the most important differences is that tuples can be used as keys in dictionaries and as elements of sets, while lists cannot. Here is a trivial example:\n\nd = {(x, x + 1): x for x in range(10)}  # Create a dictionary with tuple keys\nt = (5, 6)       # Create a tuple\nprint(type(t))\nprint(d[t])       \nprint(d[(1, 2)])\n\n&lt;class 'tuple'&gt;\n5\n1\n\n\n\nt[0] = 1\n\nTypeError: ignored\n\n\n\n\n\nFunctions\nPython functions are defined using the def keyword. For example:\n\ndef sign(x):\n    if x &gt; 0:\n        return 'positive'\n    elif x &lt; 0:\n        return 'negative'\n    else:\n        return 'zero'\n\nfor x in [-1, 0, 1]:\n    print(sign(x))\n\nnegative\nzero\npositive\n\n\nWe will often define functions to take optional keyword arguments, like this:\n\ndef hello(name, loud=False):\n    if loud:\n        print('HELLO, {}'.format(name.upper()))\n    else:\n        print('Hello, {}!'.format(name))\n\nhello('Bob')\nhello('Fred', loud=True)\n\nHello, Bob!\nHELLO, FRED\n\n\n###Classes\nThe syntax for defining classes in Python is straightforward:\n\nclass Greeter:\n\n    # Constructor\n    def __init__(self, name):\n        self.name = name  # Create an instance variable\n\n    # Instance method\n    def greet(self, loud=False):\n        if loud:\n          print('HELLO, {}'.format(self.name.upper()))\n        else:\n          print('Hello, {}!'.format(self.name))\n\ng = Greeter('Fred')  # Construct an instance of the Greeter class\ng.greet()            # Call an instance method; prints \"Hello, Fred\"\ng.greet(loud=True)   # Call an instance method; prints \"HELLO, FRED!\"\n\nHello, Fred!\nHELLO, FRED\n\n\n##Numpy\nNumpy is the core library for scientific computing in Python. It provides a high-performance multidimensional array object, and tools for working with these arrays. If you are already familiar with MATLAB, you might find this tutorial useful to get started with Numpy.\nTo use Numpy, we first need to import the numpy package:\n\nimport numpy as np\n\n###Arrays\nA numpy array is a grid of values, all of the same type, and is indexed by a tuple of nonnegative integers. The number of dimensions is the rank of the array; the shape of an array is a tuple of integers giving the size of the array along each dimension.\nWe can initialize numpy arrays from nested Python lists, and access elements using square brackets:\n\na = np.array([1, 2, 3])  # Create a rank 1 array\nprint(type(a), a.shape, a[0], a[1], a[2])\na[0] = 5                 # Change an element of the array\nprint(a)                  \n\n&lt;class 'numpy.ndarray'&gt; (3,) 1 2 3\n[5 2 3]\n\n\n\nb = np.array([[1,2,3],[4,5,6]])   # Create a rank 2 array\nprint(b)\n\n[[1 2 3]\n [4 5 6]]\n\n\n\nprint(b.shape)\nprint(b[0, 0], b[0, 1], b[1, 0])\n\n(2, 3)\n1 2 4\n\n\nNumpy also provides many functions to create arrays:\n\na = np.zeros((2,2))  # Create an array of all zeros\nprint(a)\n\n[[0. 0.]\n [0. 0.]]\n\n\n\nb = np.ones((1,2))   # Create an array of all ones\nprint(b)\n\n[[1. 1.]]\n\n\n\nc = np.full((2,2), 7) # Create a constant array\nprint(c)\n\n[[7 7]\n [7 7]]\n\n\n\nd = np.eye(2)        # Create a 2x2 identity matrix\nprint(d)\n\n[[1. 0.]\n [0. 1.]]\n\n\n\ne = np.random.random((2,2)) # Create an array filled with random values\nprint(e)\n\n[[0.8690054  0.57244319]\n [0.29647245 0.81464494]]\n\n\n\n\nArray indexing\nNumpy offers several ways to index into arrays.\nSlicing: Similar to Python lists, numpy arrays can be sliced. Since arrays may be multidimensional, you must specify a slice for each dimension of the array:\n\nimport numpy as np\n\n# Create the following rank 2 array with shape (3, 4)\n# [[ 1  2  3  4]\n#  [ 5  6  7  8]\n#  [ 9 10 11 12]]\na = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n\n# Use slicing to pull out the subarray consisting of the first 2 rows\n# and columns 1 and 2; b is the following array of shape (2, 2):\n# [[2 3]\n#  [6 7]]\nb = a[:2, 1:3]\nprint(b)\n\n[[2 3]\n [6 7]]\n\n\nA slice of an array is a view into the same data, so modifying it will modify the original array.\n\nprint(a[0, 1])\nb[0, 0] = 77    # b[0, 0] is the same piece of data as a[0, 1]\nprint(a[0, 1]) \n\n2\n77\n\n\nYou can also mix integer indexing with slice indexing. However, doing so will yield an array of lower rank than the original array. Note that this is quite different from the way that MATLAB handles array slicing:\n\n# Create the following rank 2 array with shape (3, 4)\na = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\nprint(a)\n\n[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]]\n\n\nTwo ways of accessing the data in the middle row of the array. Mixing integer indexing with slices yields an array of lower rank, while using only slices yields an array of the same rank as the original array:\n\nrow_r1 = a[1, :]    # Rank 1 view of the second row of a  \nrow_r2 = a[1:2, :]  # Rank 2 view of the second row of a\nrow_r3 = a[[1], :]  # Rank 2 view of the second row of a\nprint(row_r1, row_r1.shape)\nprint(row_r2, row_r2.shape)\nprint(row_r3, row_r3.shape)\n\n[5 6 7 8] (4,)\n[[5 6 7 8]] (1, 4)\n[[5 6 7 8]] (1, 4)\n\n\n\n# We can make the same distinction when accessing columns of an array:\ncol_r1 = a[:, 1]\ncol_r2 = a[:, 1:2]\nprint(col_r1, col_r1.shape)\nprint()\nprint(col_r2, col_r2.shape)\n\n[ 2  6 10] (3,)\n\n[[ 2]\n [ 6]\n [10]] (3, 1)\n\n\nInteger array indexing: When you index into numpy arrays using slicing, the resulting array view will always be a subarray of the original array. In contrast, integer array indexing allows you to construct arbitrary arrays using the data from another array. Here is an example:\n\na = np.array([[1,2], [3, 4], [5, 6]])\n\n# An example of integer array indexing.\n# The returned array will have shape (3,) and \nprint(a[[0, 1, 2], [0, 1, 0]])\n\n# The above example of integer array indexing is equivalent to this:\nprint(np.array([a[0, 0], a[1, 1], a[2, 0]]))\n\n[1 4 5]\n[1 4 5]\n\n\n\n# When using integer array indexing, you can reuse the same\n# element from the source array:\nprint(a[[0, 0], [1, 1]])\n\n# Equivalent to the previous integer array indexing example\nprint(np.array([a[0, 1], a[0, 1]]))\n\n[2 2]\n[2 2]\n\n\nOne useful trick with integer array indexing is selecting or mutating one element from each row of a matrix:\n\n# Create a new array from which we will select elements\na = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\nprint(a)\n\n[[ 1  2  3]\n [ 4  5  6]\n [ 7  8  9]\n [10 11 12]]\n\n\n\n# Create an array of indices\nb = np.array([0, 2, 0, 1])\n\n# Select one element from each row of a using the indices in b\nprint(a[np.arange(4), b])  # Prints \"[ 1  6  7 11]\"\n\n[ 1  6  7 11]\n\n\n\n# Mutate one element from each row of a using the indices in b\na[np.arange(4), b] += 10\nprint(a)\n\n[[11  2  3]\n [ 4  5 16]\n [17  8  9]\n [10 21 12]]\n\n\nBoolean array indexing: Boolean array indexing lets you pick out arbitrary elements of an array. Frequently this type of indexing is used to select the elements of an array that satisfy some condition. Here is an example:\n\nimport numpy as np\n\na = np.array([[1,2], [3, 4], [5, 6]])\n\nbool_idx = (a &gt; 2)  # Find the elements of a that are bigger than 2;\n                    # this returns a numpy array of Booleans of the same\n                    # shape as a, where each slot of bool_idx tells\n                    # whether that element of a is &gt; 2.\n\nprint(bool_idx)\n\n[[False False]\n [ True  True]\n [ True  True]]\n\n\n\n# We use boolean array indexing to construct a rank 1 array\n# consisting of the elements of a corresponding to the True values\n# of bool_idx\nprint(a[bool_idx])\n\n# We can do all of the above in a single concise statement:\nprint(a[a &gt; 2])\n\n[3 4 5 6]\n[3 4 5 6]\n\n\nFor brevity we have left out a lot of details about numpy array indexing; if you want to know more you should read the documentation.\n###Datatypes\nEvery numpy array is a grid of elements of the same type. Numpy provides a large set of numeric datatypes that you can use to construct arrays. Numpy tries to guess a datatype when you create an array, but functions that construct arrays usually also include an optional argument to explicitly specify the datatype. Here is an example:\n\nx = np.array([1, 2])  # Let numpy choose the datatype\ny = np.array([1.0, 2.0])  # Let numpy choose the datatype\nz = np.array([1, 2], dtype=np.int64)  # Force a particular datatype\n\nprint(x.dtype, y.dtype, z.dtype)\n\nint64 float64 int64\n\n\nYou can read all about numpy datatypes in the documentation.\n\n\nArray math\nBasic mathematical functions operate elementwise on arrays, and are available both as operator overloads and as functions in the numpy module:\n\nx = np.array([[1,2],[3,4]], dtype=np.float64)\ny = np.array([[5,6],[7,8]], dtype=np.float64)\n\n# Elementwise sum; both produce the array\nprint(x + y)\nprint(np.add(x, y))\n\n[[ 6.  8.]\n [10. 12.]]\n[[ 6.  8.]\n [10. 12.]]\n\n\n\n# Elementwise difference; both produce the array\nprint(x - y)\nprint(np.subtract(x, y))\n\n[[-4. -4.]\n [-4. -4.]]\n[[-4. -4.]\n [-4. -4.]]\n\n\n\n# Elementwise product; both produce the array\nprint(x * y)\nprint(np.multiply(x, y))\n\n[[ 5. 12.]\n [21. 32.]]\n[[ 5. 12.]\n [21. 32.]]\n\n\n\n# Elementwise division; both produce the array\n# [[ 0.2         0.33333333]\n#  [ 0.42857143  0.5       ]]\nprint(x / y)\nprint(np.divide(x, y))\n\n[[0.2        0.33333333]\n [0.42857143 0.5       ]]\n[[0.2        0.33333333]\n [0.42857143 0.5       ]]\n\n\n\n# Elementwise square root; produces the array\n# [[ 1.          1.41421356]\n#  [ 1.73205081  2.        ]]\nprint(np.sqrt(x))\n\n[[1.         1.41421356]\n [1.73205081 2.        ]]\n\n\nNote that unlike MATLAB, * is elementwise multiplication, not matrix multiplication. We instead use the dot function to compute inner products of vectors, to multiply a vector by a matrix, and to multiply matrices. dot is available both as a function in the numpy module and as an instance method of array objects:\n\nx = np.array([[1,2],[3,4]])\ny = np.array([[5,6],[7,8]])\n\nv = np.array([9,10])\nw = np.array([11, 12])\n\n# Inner product of vectors; both produce 219\nprint(v.dot(w))\nprint(np.dot(v, w))\n\n219\n219\n\n\nYou can also use the @ operator which is equivalent to numpy‚Äôs dot operator.\n\nprint(v @ w)\n\n219\n\n\n\n# Matrix / vector product; both produce the rank 1 array [29 67]\nprint(x.dot(v))\nprint(np.dot(x, v))\nprint(x @ v)\n\n[29 67]\n[29 67]\n[29 67]\n\n\n\n# Matrix / matrix product; both produce the rank 2 array\n# [[19 22]\n#  [43 50]]\nprint(x.dot(y))\nprint(np.dot(x, y))\nprint(x @ y)\n\n[[19 22]\n [43 50]]\n[[19 22]\n [43 50]]\n[[19 22]\n [43 50]]\n\n\nNumpy provides many useful functions for performing computations on arrays; one of the most useful is sum:\n\nx = np.array([[1,2],[3,4]])\n\nprint(np.sum(x))  # Compute sum of all elements; prints \"10\"\nprint(np.sum(x, axis=0))  # Compute sum of each column; prints \"[4 6]\"\nprint(np.sum(x, axis=1))  # Compute sum of each row; prints \"[3 7]\"\n\n10\n[4 6]\n[3 7]\n\n\nYou can find the full list of mathematical functions provided by numpy in the documentation.\nApart from computing mathematical functions using arrays, we frequently need to reshape or otherwise manipulate data in arrays. The simplest example of this type of operation is transposing a matrix; to transpose a matrix, simply use the T attribute of an array object:\n\nprint(x)\nprint(\"transpose\\n\", x.T)\n\n[[1 2]\n [3 4]]\ntranspose\n [[1 3]\n [2 4]]\n\n\n\nv = np.array([[1,2,3]])\nprint(v )\nprint(\"transpose\\n\", v.T)\n\n[[1 2 3]]\ntranspose\n [[1]\n [2]\n [3]]\n\n\n\n\nBroadcasting\nBroadcasting is a powerful mechanism that allows numpy to work with arrays of different shapes when performing arithmetic operations. Frequently we have a smaller array and a larger array, and we want to use the smaller array multiple times to perform some operation on the larger array.\nFor example, suppose that we want to add a constant vector to each row of a matrix. We could do it like this:\n\n# We will add the vector v to each row of the matrix x,\n# storing the result in the matrix y\nx = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\nv = np.array([1, 0, 1])\ny = np.empty_like(x)   # Create an empty matrix with the same shape as x\n\n# Add the vector v to each row of the matrix x with an explicit loop\nfor i in range(4):\n    y[i, :] = x[i, :] + v\n\nprint(y)\n\n[[ 2  2  4]\n [ 5  5  7]\n [ 8  8 10]\n [11 11 13]]\n\n\nThis works; however when the matrix x is very large, computing an explicit loop in Python could be slow. Note that adding the vector v to each row of the matrix x is equivalent to forming a matrix vv by stacking multiple copies of v vertically, then performing elementwise summation of x and vv. We could implement this approach like this:\n\nvv = np.tile(v, (4, 1))  # Stack 4 copies of v on top of each other\nprint(vv)                # Prints \"[[1 0 1]\n                         #          [1 0 1]\n                         #          [1 0 1]\n                         #          [1 0 1]]\"\n\n[[1 0 1]\n [1 0 1]\n [1 0 1]\n [1 0 1]]\n\n\n\ny = x + vv  # Add x and vv elementwise\nprint(y)\n\n[[ 2  2  4]\n [ 5  5  7]\n [ 8  8 10]\n [11 11 13]]\n\n\nNumpy broadcasting allows us to perform this computation without actually creating multiple copies of v. Consider this version, using broadcasting:\n\nimport numpy as np\n\n# We will add the vector v to each row of the matrix x,\n# storing the result in the matrix y\nx = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\nv = np.array([1, 0, 1])\ny = x + v  # Add v to each row of x using broadcasting\nprint(y)\n\n[[ 2  2  4]\n [ 5  5  7]\n [ 8  8 10]\n [11 11 13]]\n\n\nThe line y = x + v works even though x has shape (4, 3) and v has shape (3,) due to broadcasting; this line works as if v actually had shape (4, 3), where each row was a copy of v, and the sum was performed elementwise.\nBroadcasting two arrays together follows these rules:\n\nIf the arrays do not have the same rank, prepend the shape of the lower rank array with 1s until both shapes have the same length.\nThe two arrays are said to be compatible in a dimension if they have the same size in the dimension, or if one of the arrays has size 1 in that dimension.\nThe arrays can be broadcast together if they are compatible in all dimensions.\nAfter broadcasting, each array behaves as if it had shape equal to the elementwise maximum of shapes of the two input arrays.\nIn any dimension where one array had size 1 and the other array had size greater than 1, the first array behaves as if it were copied along that dimension\n\nIf this explanation does not make sense, try reading the explanation from the documentation or this explanation.\nFunctions that support broadcasting are known as universal functions. You can find the list of all universal functions in the documentation.\nHere are some applications of broadcasting:\n\n# Compute outer product of vectors\nv = np.array([1,2,3])  # v has shape (3,)\nw = np.array([4,5])    # w has shape (2,)\n# To compute an outer product, we first reshape v to be a column\n# vector of shape (3, 1); we can then broadcast it against w to yield\n# an output of shape (3, 2), which is the outer product of v and w:\n\nprint(np.reshape(v, (3, 1)) * w)\n\n[[ 4  5]\n [ 8 10]\n [12 15]]\n\n\n\n# Add a vector to each row of a matrix\nx = np.array([[1,2,3], [4,5,6]])\n# x has shape (2, 3) and v has shape (3,) so they broadcast to (2, 3),\n# giving the following matrix:\n\nprint(x + v)\n\n[[2 4 6]\n [5 7 9]]\n\n\n\n# Add a vector to each column of a matrix\n# x has shape (2, 3) and w has shape (2,).\n# If we transpose x then it has shape (3, 2) and can be broadcast\n# against w to yield a result of shape (3, 2); transposing this result\n# yields the final result of shape (2, 3) which is the matrix x with\n# the vector w added to each column. Gives the following matrix:\n\nprint((x.T + w).T)\n\n[[ 5  6  7]\n [ 9 10 11]]\n\n\n\n# Another solution is to reshape w to be a row vector of shape (2, 1);\n# we can then broadcast it directly against x to produce the same\n# output.\nprint(x + np.reshape(w, (2, 1)))\n\n[[ 5  6  7]\n [ 9 10 11]]\n\n\n\n# Multiply a matrix by a constant:\n# x has shape (2, 3). Numpy treats scalars as arrays of shape ();\n# these can be broadcast together to shape (2, 3), producing the\n# following array:\nprint(x * 2)\n\n[[ 2  4  6]\n [ 8 10 12]]\n\n\nBroadcasting typically makes your code more concise and faster, so you should strive to use it where possible.\nThis brief overview has touched on many of the important things that you need to know about numpy, but is far from complete. Check out the numpy reference to find out much more about numpy."
  },
  {
    "objectID": "posts/numpy/Numpy_tutorial.html#matplotlib",
    "href": "posts/numpy/Numpy_tutorial.html#matplotlib",
    "title": "Tools - Numpy",
    "section": "Matplotlib",
    "text": "Matplotlib\nMatplotlib is a plotting library. In this section give a brief introduction to the matplotlib.pyplot module, which provides a plotting system similar to that of MATLAB.\n\nimport matplotlib.pyplot as plt\n\nBy running this special iPython command, we will be displaying plots inline:\n\n%matplotlib inline\n\n\nPlotting\nThe most important function in matplotlib is plot, which allows you to plot 2D data. Here is a simple example:\n\n# Compute the x and y coordinates for points on a sine curve\nx = np.arange(0, 3 * np.pi, 0.1)\ny = np.sin(x)\n\n# Plot the points using matplotlib\nplt.plot(x, y)\n\n\n\n\nWith just a little bit of extra work we can easily plot multiple lines at once, and add a title, legend, and axis labels:\n\ny_sin = np.sin(x)\ny_cos = np.cos(x)\n\n# Plot the points using matplotlib\nplt.plot(x, y_sin)\nplt.plot(x, y_cos)\nplt.xlabel('x axis label')\nplt.ylabel('y axis label')\nplt.title('Sine and Cosine')\nplt.legend(['Sine', 'Cosine'])\n\n&lt;matplotlib.legend.Legend at 0x7f0f39c04780&gt;\n\n\n\n\n\n\n\nSubplots\nYou can plot different things in the same figure using the subplot function. Here is an example:\n\n# Compute the x and y coordinates for points on sine and cosine curves\nx = np.arange(0, 3 * np.pi, 0.1)\ny_sin = np.sin(x)\ny_cos = np.cos(x)\n\n# Set up a subplot grid that has height 2 and width 1,\n# and set the first such subplot as active.\nplt.subplot(2, 1, 1)\n\n# Make the first plot\nplt.plot(x, y_sin)\nplt.title('Sine')\n\n# Set the second subplot as active, and make the second plot.\nplt.subplot(2, 1, 2)\nplt.plot(x, y_cos)\nplt.title('Cosine')\n\n# Show the figure.\nplt.show()\n\n\n\n\nYou can read much more about the subplot function in the documentation."
  },
  {
    "objectID": "posts/pandas/tools-pandas.html",
    "href": "posts/pandas/tools-pandas.html",
    "title": "Tools - pandas",
    "section": "",
    "text": "Tools - pandas\nThe pandas library provides high-performance, easy-to-use data structures and data analysis tools. The main data structure is the DataFrame, which you can think of as an in-memory 2D table (like a spreadsheet, with column names and row labels). Many features available in Excel are available programmatically, such as creating pivot tables, computing columns based on other columns, plotting graphs, etc. You can also group rows by column value, or join tables much like in SQL. Pandas is also great at handling time series.\nThis notebook follows the fastai style conventions.\nPrerequisites: * NumPy ‚Äì if you are not familiar with NumPy, we recommend that you go through the NumPy tutorial now."
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#creating-a-series",
    "href": "posts/pandas/tools-pandas.html#creating-a-series",
    "title": "Tools - pandas",
    "section": "Creating a Series",
    "text": "Creating a Series\nLet‚Äôs start by creating our first Series object!\n\n\nCode\ns = pd.Series([2,-1,3,5])\ns\n\n\n0    2\n1   -1\n2    3\n3    5\ndtype: int64"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#similar-to-a-1d-ndarray",
    "href": "posts/pandas/tools-pandas.html#similar-to-a-1d-ndarray",
    "title": "Tools - pandas",
    "section": "Similar to a 1D ndarray",
    "text": "Similar to a 1D ndarray\nSeries objects behave much like one-dimensional NumPy ndarrays, and you can often pass them as parameters to NumPy functions:\n\n\nCode\nimport numpy as np\nnp.exp(s)\n\n\n0      7.389056\n1      0.367879\n2     20.085537\n3    148.413159\ndtype: float64\n\n\nArithmetic operations on Series are also possible, and they apply elementwise, just like for ndarrays:\n\n\nCode\ns + [1000,2000,3000,4000]\n\n\n0    1002\n1    1999\n2    3003\n3    4005\ndtype: int64\n\n\nSimilar to NumPy, if you add a single number to a Series, that number is added to all items in the Series. This is called * broadcasting*:\n\n\nCode\ns + 1000\n\n\n0    1002\n1     999\n2    1003\n3    1005\ndtype: int64\n\n\nThe same is true for all binary operations such as * or /, and even conditional operations:\n\n\nCode\ns &lt; 0\n\n\n0    False\n1     True\n2    False\n3    False\ndtype: bool"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#index-labels",
    "href": "posts/pandas/tools-pandas.html#index-labels",
    "title": "Tools - pandas",
    "section": "Index labels",
    "text": "Index labels\nEach item in a Series object has a unique identifier called the index label. By default, it is simply the rank of the item in the Series (starting at 0) but you can also set the index labels manually:\n\n\nCode\ns2 = pd.Series([68, 83, 112, 68], index=[\"alice\", \"bob\", \"charles\", \"darwin\"])\ns2\n\n\nalice       68\nbob         83\ncharles    112\ndarwin      68\ndtype: int64\n\n\nYou can then use the Series just like a dict:\n\n\nCode\ns2[\"bob\"]\n\n\n83\n\n\nYou can still access the items by integer location, like in a regular array:\n\n\nCode\ns2[1]\n\n\n83\n\n\nTo make it clear when you are accessing by label or by integer location, it is recommended to always use the loc attribute when accessing by label, and the iloc attribute when accessing by integer location:\n\n\nCode\ns2.loc[\"bob\"]\n\n\n83\n\n\n\n\nCode\ns2.iloc[1]\n\n\n83\n\n\nSlicing a Series also slices the index labels:\n\n\nCode\ns2.iloc[1:3]\n\n\nbob         83\ncharles    112\ndtype: int64\n\n\nThis can lead to unexpected results when using the default numeric labels, so be careful:\n\n\nCode\nsurprise = pd.Series([1000, 1001, 1002, 1003])\nsurprise\n\n\n0    1000\n1    1001\n2    1002\n3    1003\ndtype: int64\n\n\n\n\nCode\nsurprise_slice = surprise[2:]\nsurprise_slice\n\n\n2    1002\n3    1003\ndtype: int64\n\n\nOh look! The first element has index label 2. The element with index label 0 is absent from the slice:\n\n\nCode\ntry:\n    surprise_slice[0]\nexcept KeyError as e:\n    print(\"Key error:\", e)\n\n\nKey error: 0\n\n\nBut remember that you can access elements by integer location using the iloc attribute. This illustrates another reason why it‚Äôs always better to use loc and iloc to access Series objects:\n\n\nCode\nsurprise_slice.iloc[0]\n\n\n1002"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#init-from-dict",
    "href": "posts/pandas/tools-pandas.html#init-from-dict",
    "title": "Tools - pandas",
    "section": "Init from dict",
    "text": "Init from dict\nYou can create a Series object from a dict. The keys will be used as index labels:\n\n\nCode\nweights = {\"alice\": 68, \"bob\": 83, \"colin\": 86, \"darwin\": 68}\ns3 = pd.Series(weights)\ns3\n\n\nalice     68\nbob       83\ncolin     86\ndarwin    68\ndtype: int64\n\n\nYou can control which elements you want to include in the Series and in what order by explicitly specifying the desired index:\n\n\nCode\ns4 = pd.Series(weights, index = [\"colin\", \"alice\"])\ns4\n\n\ncolin    86\nalice    68\ndtype: int64"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#automatic-alignment",
    "href": "posts/pandas/tools-pandas.html#automatic-alignment",
    "title": "Tools - pandas",
    "section": "Automatic alignment",
    "text": "Automatic alignment\nWhen an operation involves multiple Series objects, pandas automatically aligns items by matching index labels.\n\n\nCode\nprint(s2.keys())\nprint(s3.keys())\n\ns2 + s3\n\n\nIndex(['alice', 'bob', 'charles', 'darwin'], dtype='object')\nIndex(['alice', 'bob', 'colin', 'darwin'], dtype='object')\n\n\nalice      136.0\nbob        166.0\ncharles      NaN\ncolin        NaN\ndarwin     136.0\ndtype: float64\n\n\nThe resulting Series contains the union of index labels from s2 and s3. Since \"colin\" is missing from s2 and \"charles\" is missing from s3, these items have a NaN result value. (ie. Not-a-Number means missing).\nAutomatic alignment is very handy when working with data that may come from various sources with varying structure and missing items. But if you forget to set the right index labels, you can have surprising results:\n\n\nCode\ns5 = pd.Series([1000,1000,1000,1000])\nprint(\"s2 =\", s2.values)\nprint(\"s5 =\", s5.values)\n\ns2 + s5\n\n\ns2 = [ 68  83 112  68]\ns5 = [1000 1000 1000 1000]\n\n\nalice     NaN\nbob       NaN\ncharles   NaN\ndarwin    NaN\n0         NaN\n1         NaN\n2         NaN\n3         NaN\ndtype: float64\n\n\nPandas could not align the Series, since their labels do not match at all, hence the full NaN result."
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#init-with-a-scalar",
    "href": "posts/pandas/tools-pandas.html#init-with-a-scalar",
    "title": "Tools - pandas",
    "section": "Init with a scalar",
    "text": "Init with a scalar\nYou can also initialize a Series object using a scalar and a list of index labels: all items will be set to the scalar.\n\n\nCode\nmeaning = pd.Series(42, [\"life\", \"universe\", \"everything\"])\nmeaning\n\n\nlife          42\nuniverse      42\neverything    42\ndtype: int64"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#series-name",
    "href": "posts/pandas/tools-pandas.html#series-name",
    "title": "Tools - pandas",
    "section": "Series name",
    "text": "Series name\nA Series can have a name:\n\n\nCode\ns6 = pd.Series([83, 68], index=[\"bob\", \"alice\"], name=\"weights\")\ns6\n\n\nbob      83\nalice    68\nName: weights, dtype: int64"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#plotting-a-series",
    "href": "posts/pandas/tools-pandas.html#plotting-a-series",
    "title": "Tools - pandas",
    "section": "Plotting a Series",
    "text": "Plotting a Series\nPandas makes it easy to plot Series data using matplotlib (for more details on matplotlib, check out the matplotlib tutorial). Just import matplotlib and call the plot() method:\n\n\nCode\n%matplotlib inline\nimport matplotlib.pyplot as plt\ntemperatures = [4.4,5.1,6.1,6.2,6.1,6.1,5.7,5.2,4.7,4.1,3.9,3.5]\ns7 = pd.Series(temperatures, name=\"Temperature\")\ns7.plot()\nplt.show()\n\n\n\n\n\nThere are many options for plotting your data. It is not necessary to list them all here: if you need a particular type of plot (histograms, pie charts, etc.), just look for it in the excellent Visualization section of pandas‚Äô documentation, and look at the example code."
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#time-range",
    "href": "posts/pandas/tools-pandas.html#time-range",
    "title": "Tools - pandas",
    "section": "Time range",
    "text": "Time range\nLet‚Äôs start by creating a time series using pd.date_range(). This returns a DatetimeIndex containing one datetime per hour for 12 hours starting on October 29th 2016 at 5:30pm.\n\n\nCode\ndates = pd.date_range('2016/10/29 5:30pm', periods=12, freq='H')\ndates\n\n\nDatetimeIndex(['2016-10-29 17:30:00', '2016-10-29 18:30:00',\n               '2016-10-29 19:30:00', '2016-10-29 20:30:00',\n               '2016-10-29 21:30:00', '2016-10-29 22:30:00',\n               '2016-10-29 23:30:00', '2016-10-30 00:30:00',\n               '2016-10-30 01:30:00', '2016-10-30 02:30:00',\n               '2016-10-30 03:30:00', '2016-10-30 04:30:00'],\n              dtype='datetime64[ns]', freq='H')\n\n\nThis DatetimeIndex may be used as an index in a Series:\n\n\nCode\ntemp_series = pd.Series(temperatures, dates)\ntemp_series\n\n\n2016-10-29 17:30:00    4.4\n2016-10-29 18:30:00    5.1\n2016-10-29 19:30:00    6.1\n2016-10-29 20:30:00    6.2\n2016-10-29 21:30:00    6.1\n2016-10-29 22:30:00    6.1\n2016-10-29 23:30:00    5.7\n2016-10-30 00:30:00    5.2\n2016-10-30 01:30:00    4.7\n2016-10-30 02:30:00    4.1\n2016-10-30 03:30:00    3.9\n2016-10-30 04:30:00    3.5\nFreq: H, dtype: float64\n\n\nLet‚Äôs plot this series:\n\n\nCode\ntemp_series.plot(kind=\"bar\")\n\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#resampling",
    "href": "posts/pandas/tools-pandas.html#resampling",
    "title": "Tools - pandas",
    "section": "Resampling",
    "text": "Resampling\nPandas lets us resample a time series very simply. Just call the resample() method and specify a new frequency:\n\n\nCode\ntemp_series_freq_2H = temp_series.resample(\"2H\")\ntemp_series_freq_2H\n\n\n&lt;pandas.core.resample.DatetimeIndexResampler object at 0x7f69a91f0190&gt;\n\n\nThe resampling operation is actually a deferred operation, which is why we did not get a Series object, but a DatetimeIndexResampler object instead. To actually perform the resampling operation, we can simply call the mean() method: Pandas will compute the mean of every pair of consecutive hours:\n\n\nCode\ntemp_series_freq_2H = temp_series_freq_2H.mean()\n\n\nLet‚Äôs plot the result:\n\n\nCode\ntemp_series_freq_2H.plot(kind=\"bar\")\nplt.show()\n\n\n\n\n\nNote how the values have automatically been aggregated into 2-hour periods. If we look at the 6-8pm period, for example, we had a value of 5.1 at 6:30pm, and 6.1 at 7:30pm. After resampling, we just have one value of 5.6, which is the mean of 5.1 and 6.1. Rather than computing the mean, we could have used any other aggregation function, for example we can decide to keep the minimum value of each period:\n\n\nCode\ntemp_series_freq_2H = temp_series.resample(\"2H\").min()\ntemp_series_freq_2H\n\n\n2016-10-29 16:00:00    4.4\n2016-10-29 18:00:00    5.1\n2016-10-29 20:00:00    6.1\n2016-10-29 22:00:00    5.7\n2016-10-30 00:00:00    4.7\n2016-10-30 02:00:00    3.9\n2016-10-30 04:00:00    3.5\nFreq: 2H, dtype: float64\n\n\nOr, equivalently, we could use the apply() method instead:\n\n\nCode\ntemp_series_freq_2H = temp_series.resample(\"2H\").apply(np.min)\ntemp_series_freq_2H\n\n\n2016-10-29 16:00:00    4.4\n2016-10-29 18:00:00    5.1\n2016-10-29 20:00:00    6.1\n2016-10-29 22:00:00    5.7\n2016-10-30 00:00:00    4.7\n2016-10-30 02:00:00    3.9\n2016-10-30 04:00:00    3.5\nFreq: 2H, dtype: float64"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#upsampling-and-interpolation",
    "href": "posts/pandas/tools-pandas.html#upsampling-and-interpolation",
    "title": "Tools - pandas",
    "section": "Upsampling and interpolation",
    "text": "Upsampling and interpolation\nThis was an example of downsampling. We can also upsample (ie. increase the frequency), but this creates holes in our data:\n\n\nCode\ntemp_series_freq_15min = temp_series.resample(\"15Min\").mean()\ntemp_series_freq_15min.head(n=10) # `head` displays the top n values\n\n\n2016-10-29 17:30:00    4.4\n2016-10-29 17:45:00    NaN\n2016-10-29 18:00:00    NaN\n2016-10-29 18:15:00    NaN\n2016-10-29 18:30:00    5.1\n2016-10-29 18:45:00    NaN\n2016-10-29 19:00:00    NaN\n2016-10-29 19:15:00    NaN\n2016-10-29 19:30:00    6.1\n2016-10-29 19:45:00    NaN\nFreq: 15T, dtype: float64\n\n\nOne solution is to fill the gaps by interpolating. We just call the interpolate() method. The default is to use linear interpolation, but we can also select another method, such as cubic interpolation:\n\n\nCode\ntemp_series_freq_15min = temp_series.resample(\"15Min\").interpolate(method=\"cubic\")\ntemp_series_freq_15min.head(n=10)\n\n\n2016-10-29 17:30:00    4.400000\n2016-10-29 17:45:00    4.452911\n2016-10-29 18:00:00    4.605113\n2016-10-29 18:15:00    4.829758\n2016-10-29 18:30:00    5.100000\n2016-10-29 18:45:00    5.388992\n2016-10-29 19:00:00    5.669887\n2016-10-29 19:15:00    5.915839\n2016-10-29 19:30:00    6.100000\n2016-10-29 19:45:00    6.203621\nFreq: 15T, dtype: float64\n\n\n\n\nCode\ntemp_series.plot(label=\"Period: 1 hour\")\ntemp_series_freq_15min.plot(label=\"Period: 15 minutes\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#timezones",
    "href": "posts/pandas/tools-pandas.html#timezones",
    "title": "Tools - pandas",
    "section": "Timezones",
    "text": "Timezones\nBy default datetimes are naive: they are not aware of timezones, so 2016-10-30 02:30 might mean October 30th 2016 at 2:30am in Paris or in New York. We can make datetimes timezone aware by calling the tz_localize() method:\n\n\nCode\ntemp_series_ny = temp_series.tz_localize(\"America/New_York\")\ntemp_series_ny\n\n\n2016-10-29 17:30:00-04:00    4.4\n2016-10-29 18:30:00-04:00    5.1\n2016-10-29 19:30:00-04:00    6.1\n2016-10-29 20:30:00-04:00    6.2\n2016-10-29 21:30:00-04:00    6.1\n2016-10-29 22:30:00-04:00    6.1\n2016-10-29 23:30:00-04:00    5.7\n2016-10-30 00:30:00-04:00    5.2\n2016-10-30 01:30:00-04:00    4.7\n2016-10-30 02:30:00-04:00    4.1\n2016-10-30 03:30:00-04:00    3.9\n2016-10-30 04:30:00-04:00    3.5\ndtype: float64\n\n\nNote that -04:00 is now appended to all the datetimes. This means that these datetimes refer to UTC - 4 hours.\nWe can convert these datetimes to Paris time like this:\n\n\nCode\ntemp_series_paris = temp_series_ny.tz_convert(\"Europe/Paris\")\ntemp_series_paris\n\n\n2016-10-29 23:30:00+02:00    4.4\n2016-10-30 00:30:00+02:00    5.1\n2016-10-30 01:30:00+02:00    6.1\n2016-10-30 02:30:00+02:00    6.2\n2016-10-30 02:30:00+01:00    6.1\n2016-10-30 03:30:00+01:00    6.1\n2016-10-30 04:30:00+01:00    5.7\n2016-10-30 05:30:00+01:00    5.2\n2016-10-30 06:30:00+01:00    4.7\n2016-10-30 07:30:00+01:00    4.1\n2016-10-30 08:30:00+01:00    3.9\n2016-10-30 09:30:00+01:00    3.5\ndtype: float64\n\n\nYou may have noticed that the UTC offset changes from +02:00 to +01:00: this is because France switches to winter time at 3am that particular night (time goes back to 2am). Notice that 2:30am occurs twice! Let‚Äôs go back to a naive representation (if you log some data hourly using local time, without storing the timezone, you might get something like this):\n\n\nCode\ntemp_series_paris_naive = temp_series_paris.tz_localize(None)\ntemp_series_paris_naive\n\n\n2016-10-29 23:30:00    4.4\n2016-10-30 00:30:00    5.1\n2016-10-30 01:30:00    6.1\n2016-10-30 02:30:00    6.2\n2016-10-30 02:30:00    6.1\n2016-10-30 03:30:00    6.1\n2016-10-30 04:30:00    5.7\n2016-10-30 05:30:00    5.2\n2016-10-30 06:30:00    4.7\n2016-10-30 07:30:00    4.1\n2016-10-30 08:30:00    3.9\n2016-10-30 09:30:00    3.5\ndtype: float64\n\n\nNow 02:30 is really ambiguous. If we try to localize these naive datetimes to the Paris timezone, we get an error:\n\n\nCode\ntry:\n    temp_series_paris_naive.tz_localize(\"Europe/Paris\")\nexcept Exception as e:\n    print(type(e))\n    print(e)\n\n\n&lt;class 'pytz.exceptions.AmbiguousTimeError'&gt;\nCannot infer dst time from 2016-10-30 02:30:00, try using the 'ambiguous' argument\n\n\nFortunately using the ambiguous argument we can tell pandas to infer the right DST (Daylight Saving Time) based on the order of the ambiguous timestamps:\n\n\nCode\ntemp_series_paris_naive.tz_localize(\"Europe/Paris\", ambiguous=\"infer\")\n\n\n2016-10-29 23:30:00+02:00    4.4\n2016-10-30 00:30:00+02:00    5.1\n2016-10-30 01:30:00+02:00    6.1\n2016-10-30 02:30:00+02:00    6.2\n2016-10-30 02:30:00+01:00    6.1\n2016-10-30 03:30:00+01:00    6.1\n2016-10-30 04:30:00+01:00    5.7\n2016-10-30 05:30:00+01:00    5.2\n2016-10-30 06:30:00+01:00    4.7\n2016-10-30 07:30:00+01:00    4.1\n2016-10-30 08:30:00+01:00    3.9\n2016-10-30 09:30:00+01:00    3.5\ndtype: float64"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#periods",
    "href": "posts/pandas/tools-pandas.html#periods",
    "title": "Tools - pandas",
    "section": "Periods",
    "text": "Periods\nThe pd.period_range() function returns a PeriodIndex instead of a DatetimeIndex. For example, let‚Äôs get all quarters in 2016 and 2017:\n\n\nCode\nquarters = pd.period_range('2016Q1', periods=8, freq='Q')\nquarters\n\n\nPeriodIndex(['2016Q1', '2016Q2', '2016Q3', '2016Q4', '2017Q1', '2017Q2',\n             '2017Q3', '2017Q4'],\n            dtype='period[Q-DEC]')\n\n\nAdding a number N to a PeriodIndex shifts the periods by N times the PeriodIndex‚Äôs frequency:\n\n\nCode\nquarters + 3\n\n\nPeriodIndex(['2016Q4', '2017Q1', '2017Q2', '2017Q3', '2017Q4', '2018Q1',\n             '2018Q2', '2018Q3'],\n            dtype='period[Q-DEC]')\n\n\nThe asfreq() method lets us change the frequency of the PeriodIndex. All periods are lengthened or shortened accordingly. For example, let‚Äôs convert all the quarterly periods to monthly periods (zooming in):\n\n\nCode\nquarters.asfreq(\"M\")\n\n\nPeriodIndex(['2016-03', '2016-06', '2016-09', '2016-12', '2017-03', '2017-06',\n             '2017-09', '2017-12'],\n            dtype='period[M]')\n\n\nBy default, the asfreq zooms on the end of each period. We can tell it to zoom on the start of each period instead:\n\n\nCode\nquarters.asfreq(\"M\", how=\"start\")\n\n\nPeriodIndex(['2016-01', '2016-04', '2016-07', '2016-10', '2017-01', '2017-04',\n             '2017-07', '2017-10'],\n            dtype='period[M]')\n\n\nAnd we can zoom out:\n\n\nCode\nquarters.asfreq(\"A\")\n\n\nPeriodIndex(['2016', '2016', '2016', '2016', '2017', '2017', '2017', '2017'], dtype='period[A-DEC]')\n\n\nOf course we can create a Series with a PeriodIndex:\n\n\nCode\nquarterly_revenue = pd.Series([300, 320, 290, 390, 320, 360, 310, 410], index = quarters)\nquarterly_revenue\n\n\n2016Q1    300\n2016Q2    320\n2016Q3    290\n2016Q4    390\n2017Q1    320\n2017Q2    360\n2017Q3    310\n2017Q4    410\nFreq: Q-DEC, dtype: int64\n\n\n\n\nCode\nquarterly_revenue.plot(kind=\"line\")\nplt.show()\n\n\n\n\n\nWe can convert periods to timestamps by calling to_timestamp. By default this will give us the first day of each period, but by setting how and freq, we can get the last hour of each period:\n\n\nCode\nlast_hours = quarterly_revenue.to_timestamp(how=\"end\", freq=\"H\")\nlast_hours\n\n\n2016-03-31 23:59:59.999999999    300\n2016-06-30 23:59:59.999999999    320\n2016-09-30 23:59:59.999999999    290\n2016-12-31 23:59:59.999999999    390\n2017-03-31 23:59:59.999999999    320\n2017-06-30 23:59:59.999999999    360\n2017-09-30 23:59:59.999999999    310\n2017-12-31 23:59:59.999999999    410\ndtype: int64\n\n\nAnd back to periods by calling to_period:\n\n\nCode\nlast_hours.to_period()\n\n\n2016Q1    300\n2016Q2    320\n2016Q3    290\n2016Q4    390\n2017Q1    320\n2017Q2    360\n2017Q3    310\n2017Q4    410\nFreq: Q-DEC, dtype: int64\n\n\nPandas also provides many other time-related functions that we recommend you check out in the documentation. To whet your appetite, here is one way to get the last business day of each month in 2016, at 9am:\n\n\nCode\nmonths_2016 = pd.period_range(\"2016\", periods=12, freq=\"M\")\none_day_after_last_days = months_2016.asfreq(\"D\") + 1\nlast_bdays = one_day_after_last_days.to_timestamp() - pd.tseries.offsets.BDay()\nlast_bdays.to_period(\"H\") + 9\n\n\nPeriodIndex(['2016-01-29 09:00', '2016-02-29 09:00', '2016-03-31 09:00',\n             '2016-04-29 09:00', '2016-05-31 09:00', '2016-06-30 09:00',\n             '2016-07-29 09:00', '2016-08-31 09:00', '2016-09-30 09:00',\n             '2016-10-31 09:00', '2016-11-30 09:00', '2016-12-30 09:00'],\n            dtype='period[H]')"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#creating-a-dataframe",
    "href": "posts/pandas/tools-pandas.html#creating-a-dataframe",
    "title": "Tools - pandas",
    "section": "Creating a DataFrame",
    "text": "Creating a DataFrame\nYou can create a DataFrame by passing a dictionary of Series objects:\n\n\nCode\npeople_dict = {\n    \"weight\": pd.Series([68, 83, 112], index=[\"alice\", \"bob\", \"charles\"]),\n    \"birthyear\": pd.Series([1984, 1985, 1992], index=[\"bob\", \"alice\", \"charles\"], name=\"year\"),\n    \"children\": pd.Series([0, 3], index=[\"charles\", \"bob\"]),\n    \"hobby\": pd.Series([\"Biking\", \"Dancing\"], index=[\"alice\", \"bob\"]),\n}\npeople = pd.DataFrame(people_dict)\npeople\n\n\n\n\n\n\n\n\n\nweight\nbirthyear\nchildren\nhobby\n\n\n\n\nalice\n68\n1985\nNaN\nBiking\n\n\nbob\n83\n1984\n3.0\nDancing\n\n\ncharles\n112\n1992\n0.0\nNaN\n\n\n\n\n\n\n\nA few things to note: * the Series were automatically aligned based on their index, * missing values are represented as NaN, * Series names are ignored (the name \"year\" was dropped), * DataFrames are displayed nicely in Jupyter notebooks, woohoo!\nYou can access columns pretty much as you would expect. They are returned as Series objects:\n\n\nCode\npeople[\"birthyear\"]\n\n\nalice      1985\nbob        1984\ncharles    1992\nName: birthyear, dtype: int64\n\n\nYou can also get multiple columns at once:\n\n\nCode\npeople[[\"birthyear\", \"hobby\"]]\n\n\n\n\n\n\n\n\n\nbirthyear\nhobby\n\n\n\n\nalice\n1985\nBiking\n\n\nbob\n1984\nDancing\n\n\ncharles\n1992\nNaN\n\n\n\n\n\n\n\nIf you pass a list of columns and/or index row labels to the DataFrame constructor, it will guarantee that these columns and/or rows will exist, in that order, and no other column/row will exist. For example:\n\n\nCode\nd2 = pd.DataFrame(\n        people_dict,\n        columns=[\"birthyear\", \"weight\", \"height\"],\n        index=[\"bob\", \"alice\", \"eugene\"]\n     )\nd2\n\n\n\n\n\n\n\n\n\nbirthyear\nweight\nheight\n\n\n\n\nbob\n1984.0\n83.0\nNaN\n\n\nalice\n1985.0\n68.0\nNaN\n\n\neugene\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\nAnother convenient way to create a DataFrame is to pass all the values to the constructor as an ndarray, or a list of lists, and specify the column names and row index labels separately:\n\n\nCode\nvalues = [\n            [1985, np.nan, \"Biking\",   68],\n            [1984, 3,      \"Dancing\",  83],\n            [1992, 0,      np.nan,    112]\n         ]\nd3 = pd.DataFrame(\n        values,\n        columns=[\"birthyear\", \"children\", \"hobby\", \"weight\"],\n        index=[\"alice\", \"bob\", \"charles\"]\n     )\nd3\n\n\n\n\n\n\n\n\n\nbirthyear\nchildren\nhobby\nweight\n\n\n\n\nalice\n1985\nNaN\nBiking\n68\n\n\nbob\n1984\n3.0\nDancing\n83\n\n\ncharles\n1992\n0.0\nNaN\n112\n\n\n\n\n\n\n\nTo specify missing values, you can either use np.nan or NumPy‚Äôs masked arrays:\n\n\nCode\nmasked_array = np.ma.asarray(values, dtype=np.object)\nmasked_array[(0, 2), (1, 2)] = np.ma.masked\nd3 = pd.DataFrame(\n        masked_array,\n        columns=[\"birthyear\", \"children\", \"hobby\", \"weight\"],\n        index=[\"alice\", \"bob\", \"charles\"]\n     )\nd3\n\n\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  \"\"\"Entry point for launching an IPython kernel.\n\n\n\n\n\n\n\n\n\nbirthyear\nchildren\nhobby\nweight\n\n\n\n\nalice\n1985\nNaN\nBiking\n68\n\n\nbob\n1984\n3\nDancing\n83\n\n\ncharles\n1992\n0\nNaN\n112\n\n\n\n\n\n\n\nInstead of an ndarray, you can also pass a DataFrame object:\n\n\nCode\nd4 = pd.DataFrame(\n         d3,\n         columns=[\"hobby\", \"children\"],\n         index=[\"alice\", \"bob\"]\n     )\nd4\n\n\n\n\n\n\n\n\n\nhobby\nchildren\n\n\n\n\nalice\nBiking\nNaN\n\n\nbob\nDancing\n3\n\n\n\n\n\n\n\nIt is also possible to create a DataFrame with a dictionary (or list) of dictionaries (or list):\n\n\nCode\npeople = pd.DataFrame({\n    \"birthyear\": {\"alice\":1985, \"bob\": 1984, \"charles\": 1992},\n    \"hobby\": {\"alice\":\"Biking\", \"bob\": \"Dancing\"},\n    \"weight\": {\"alice\":68, \"bob\": 83, \"charles\": 112},\n    \"children\": {\"bob\": 3, \"charles\": 0}\n})\npeople\n\n\n\n\n\n\n\n\n\nbirthyear\nhobby\nweight\nchildren\n\n\n\n\nalice\n1985\nBiking\n68\nNaN\n\n\nbob\n1984\nDancing\n83\n3.0\n\n\ncharles\n1992\nNaN\n112\n0.0"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#multi-indexing",
    "href": "posts/pandas/tools-pandas.html#multi-indexing",
    "title": "Tools - pandas",
    "section": "Multi-indexing",
    "text": "Multi-indexing\nIf all columns are tuples of the same size, then they are understood as a multi-index. The same goes for row index labels. For example:\n\n\nCode\nd5 = pd.DataFrame(\n  {\n    (\"public\", \"birthyear\"):\n        {(\"Paris\",\"alice\"):1985, (\"Paris\",\"bob\"): 1984, (\"London\",\"charles\"): 1992},\n    (\"public\", \"hobby\"):\n        {(\"Paris\",\"alice\"):\"Biking\", (\"Paris\",\"bob\"): \"Dancing\"},\n    (\"private\", \"weight\"):\n        {(\"Paris\",\"alice\"):68, (\"Paris\",\"bob\"): 83, (\"London\",\"charles\"): 112},\n    (\"private\", \"children\"):\n        {(\"Paris\", \"alice\"):np.nan, (\"Paris\",\"bob\"): 3, (\"London\",\"charles\"): 0}\n  }\n)\nd5\n\n\n\n\n\n\n\n\n\n\npublic\nprivate\n\n\n\n\nbirthyear\nhobby\nweight\nchildren\n\n\n\n\nParis\nalice\n1985\nBiking\n68\nNaN\n\n\nbob\n1984\nDancing\n83\n3.0\n\n\nLondon\ncharles\n1992\nNaN\n112\n0.0\n\n\n\n\n\n\n\nYou can now get a DataFrame containing all the \"public\" columns very simply:\n\n\nCode\nd5[\"public\"]\n\n\n\n\n\n\n\n\n\n\nbirthyear\nhobby\n\n\n\n\nParis\nalice\n1985\nBiking\n\n\nbob\n1984\nDancing\n\n\nLondon\ncharles\n1992\nNaN\n\n\n\n\n\n\n\n\n\nCode\nd5[\"public\", \"hobby\"]  # Same result as d5[\"public\"][\"hobby\"]\n\n\nParis   alice       Biking\n        bob        Dancing\nLondon  charles        NaN\nName: (public, hobby), dtype: object"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#dropping-a-level",
    "href": "posts/pandas/tools-pandas.html#dropping-a-level",
    "title": "Tools - pandas",
    "section": "Dropping a level",
    "text": "Dropping a level\nLet‚Äôs look at d5 again:\n\n\nCode\nd5\n\n\n\n\n\n\n\n\n\n\npublic\nprivate\n\n\n\n\nbirthyear\nhobby\nweight\nchildren\n\n\n\n\nParis\nalice\n1985\nBiking\n68\nNaN\n\n\nbob\n1984\nDancing\n83\n3.0\n\n\nLondon\ncharles\n1992\nNaN\n112\n0.0\n\n\n\n\n\n\n\nThere are two levels of columns, and two levels of indices. We can drop a column level by calling droplevel() (the same goes for indices):\n\n\nCode\nd5.columns = d5.columns.droplevel(level = 0)\nd5\n\n\n\n\n\n\n\n\n\n\nbirthyear\nhobby\nweight\nchildren\n\n\n\n\nParis\nalice\n1985\nBiking\n68\nNaN\n\n\nbob\n1984\nDancing\n83\n3.0\n\n\nLondon\ncharles\n1992\nNaN\n112\n0.0"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#transposing",
    "href": "posts/pandas/tools-pandas.html#transposing",
    "title": "Tools - pandas",
    "section": "Transposing",
    "text": "Transposing\nYou can swap columns and indices using the T attribute:\n\n\nCode\nd6 = d5.T\nd6\n\n\n\n\n\n\n\n\n\nParis\nLondon\n\n\n\nalice\nbob\ncharles\n\n\n\n\nbirthyear\n1985\n1984\n1992\n\n\nhobby\nBiking\nDancing\nNaN\n\n\nweight\n68\n83\n112\n\n\nchildren\nNaN\n3.0\n0.0"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#stacking-and-unstacking-levels",
    "href": "posts/pandas/tools-pandas.html#stacking-and-unstacking-levels",
    "title": "Tools - pandas",
    "section": "Stacking and unstacking levels",
    "text": "Stacking and unstacking levels\nCalling the stack() method will push the lowest column level after the lowest index:\n\n\nCode\nd7 = d6.stack()\nd7\n\n\n\n\n\n\n\n\n\n\nLondon\nParis\n\n\n\n\nbirthyear\nalice\nNaN\n1985\n\n\nbob\nNaN\n1984\n\n\ncharles\n1992\nNaN\n\n\nhobby\nalice\nNaN\nBiking\n\n\nbob\nNaN\nDancing\n\n\nweight\nalice\nNaN\n68\n\n\nbob\nNaN\n83\n\n\ncharles\n112\nNaN\n\n\nchildren\nbob\nNaN\n3.0\n\n\ncharles\n0.0\nNaN\n\n\n\n\n\n\n\nNote that many NaN values appeared. This makes sense because many new combinations did not exist before (eg. there was no bob in London).\nCalling unstack() will do the reverse, once again creating many NaN values.\n\n\nCode\nd8 = d7.unstack()\nd8\n\n\n\n\n\n\n\n\n\nLondon\nParis\n\n\n\nalice\nbob\ncharles\nalice\nbob\ncharles\n\n\n\n\nbirthyear\nNaN\nNaN\n1992\n1985\n1984\nNaN\n\n\nchildren\nNaN\nNaN\n0.0\nNaN\n3.0\nNaN\n\n\nhobby\nNaN\nNaN\nNaN\nBiking\nDancing\nNaN\n\n\nweight\nNaN\nNaN\n112\n68\n83\nNaN\n\n\n\n\n\n\n\nIf we call unstack again, we end up with a Series object:\n\n\nCode\nd9 = d8.unstack()\nd9\n\n\nLondon  alice    birthyear        NaN\n                 children         NaN\n                 hobby            NaN\n                 weight           NaN\n        bob      birthyear        NaN\n                 children         NaN\n                 hobby            NaN\n                 weight           NaN\n        charles  birthyear       1992\n                 children         0.0\n                 hobby            NaN\n                 weight           112\nParis   alice    birthyear       1985\n                 children         NaN\n                 hobby         Biking\n                 weight            68\n        bob      birthyear       1984\n                 children         3.0\n                 hobby        Dancing\n                 weight            83\n        charles  birthyear        NaN\n                 children         NaN\n                 hobby            NaN\n                 weight           NaN\ndtype: object\n\n\nThe stack() and unstack() methods let you select the level to stack/unstack. You can even stack/unstack multiple levels at once:\n\n\nCode\nd10 = d9.unstack(level = (0,1))\nd10\n\n\n\n\n\n\n\n\n\nLondon\nParis\n\n\n\nalice\nbob\ncharles\nalice\nbob\ncharles\n\n\n\n\nbirthyear\nNaN\nNaN\n1992\n1985\n1984\nNaN\n\n\nchildren\nNaN\nNaN\n0.0\nNaN\n3.0\nNaN\n\n\nhobby\nNaN\nNaN\nNaN\nBiking\nDancing\nNaN\n\n\nweight\nNaN\nNaN\n112\n68\n83\nNaN"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#most-methods-return-modified-copies",
    "href": "posts/pandas/tools-pandas.html#most-methods-return-modified-copies",
    "title": "Tools - pandas",
    "section": "Most methods return modified copies",
    "text": "Most methods return modified copies\nAs you may have noticed, the stack() and unstack() methods do not modify the object they apply to. Instead, they work on a copy and return that copy. This is true of most methods in pandas."
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#accessing-rows",
    "href": "posts/pandas/tools-pandas.html#accessing-rows",
    "title": "Tools - pandas",
    "section": "Accessing rows",
    "text": "Accessing rows\nLet‚Äôs go back to the people DataFrame:\n\n\nCode\npeople\n\n\n\n\n\n\n\n\n\nbirthyear\nhobby\nweight\nchildren\n\n\n\n\nalice\n1985\nBiking\n68\nNaN\n\n\nbob\n1984\nDancing\n83\n3.0\n\n\ncharles\n1992\nNaN\n112\n0.0\n\n\n\n\n\n\n\nThe loc attribute lets you access rows instead of columns. The result is a Series object in which the DataFrame‚Äôs column names are mapped to row index labels:\n\n\nCode\npeople.loc[\"charles\"]\n\n\nbirthyear    1992\nhobby         NaN\nweight        112\nchildren      0.0\nName: charles, dtype: object\n\n\nYou can also access rows by integer location using the iloc attribute:\n\n\nCode\npeople.iloc[2]\n\n\nbirthyear    1992\nhobby         NaN\nweight        112\nchildren      0.0\nName: charles, dtype: object\n\n\nYou can also get a slice of rows, and this returns a DataFrame object:\n\n\nCode\npeople.iloc[1:3]\n\n\n\n\n\n\n\n\n\nbirthyear\nhobby\nweight\nchildren\n\n\n\n\nbob\n1984\nDancing\n83\n3.0\n\n\ncharles\n1992\nNaN\n112\n0.0\n\n\n\n\n\n\n\nFinally, you can pass a boolean array to get the matching rows:\n\n\nCode\npeople[np.array([True, False, True])]\n\n\n\n\n\n\n\n\n\nbirthyear\nhobby\nweight\nchildren\n\n\n\n\nalice\n1985\nBiking\n68\nNaN\n\n\ncharles\n1992\nNaN\n112\n0.0\n\n\n\n\n\n\n\nThis is most useful when combined with boolean expressions:\n\n\nCode\npeople[people[\"birthyear\"] &lt; 1990]\n\n\n\n\n\n\n\n\n\nbirthyear\nhobby\nweight\nchildren\n\n\n\n\nalice\n1985\nBiking\n68\nNaN\n\n\nbob\n1984\nDancing\n83\n3.0"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#adding-and-removing-columns",
    "href": "posts/pandas/tools-pandas.html#adding-and-removing-columns",
    "title": "Tools - pandas",
    "section": "Adding and removing columns",
    "text": "Adding and removing columns\nYou can generally treat DataFrame objects like dictionaries of Series, so the following work fine:\n\n\nCode\npeople\n\n\n\n\n\n\n\n\n\nbirthyear\nhobby\nweight\nchildren\n\n\n\n\nalice\n1985\nBiking\n68\nNaN\n\n\nbob\n1984\nDancing\n83\n3.0\n\n\ncharles\n1992\nNaN\n112\n0.0\n\n\n\n\n\n\n\n\n\nCode\npeople[\"age\"] = 2018 - people[\"birthyear\"]  # adds a new column \"age\"\npeople[\"over 30\"] = people[\"age\"] &gt; 30      # adds another column \"over 30\"\nbirthyears = people.pop(\"birthyear\")\ndel people[\"children\"]\n\npeople\n\n\n\n\n\n\n\n\n\nhobby\nweight\nage\nover 30\n\n\n\n\nalice\nBiking\n68\n33\nTrue\n\n\nbob\nDancing\n83\n34\nTrue\n\n\ncharles\nNaN\n112\n26\nFalse\n\n\n\n\n\n\n\n\n\nCode\nbirthyears\n\n\nalice      1985\nbob        1984\ncharles    1992\nName: birthyear, dtype: int64\n\n\nWhen you add a new colum, it must have the same number of rows. Missing rows are filled with NaN, and extra rows are ignored:\n\n\nCode\npeople[\"pets\"] = pd.Series({\"bob\": 0, \"charles\": 5, \"eugene\":1})  # alice is missing, eugene is ignored\npeople\n\n\n\n\n\n\n\n\n\nhobby\nweight\nage\nover 30\npets\n\n\n\n\nalice\nBiking\n68\n33\nTrue\nNaN\n\n\nbob\nDancing\n83\n34\nTrue\n0.0\n\n\ncharles\nNaN\n112\n26\nFalse\n5.0\n\n\n\n\n\n\n\nWhen adding a new column, it is added at the end (on the right) by default. You can also insert a column anywhere else using the insert() method:\n\n\nCode\npeople.insert(1, \"height\", [172, 181, 185])\npeople\n\n\n\n\n\n\n\n\n\nhobby\nheight\nweight\nage\nover 30\npets\n\n\n\n\nalice\nBiking\n172\n68\n33\nTrue\nNaN\n\n\nbob\nDancing\n181\n83\n34\nTrue\n0.0\n\n\ncharles\nNaN\n185\n112\n26\nFalse\n5.0"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#assigning-new-columns",
    "href": "posts/pandas/tools-pandas.html#assigning-new-columns",
    "title": "Tools - pandas",
    "section": "Assigning new columns",
    "text": "Assigning new columns\nYou can also create new columns by calling the assign() method. Note that this returns a new DataFrame object, the original is not modified:\n\n\nCode\npeople.assign(\n    body_mass_index = people[\"weight\"] / (people[\"height\"] / 100) ** 2,\n    has_pets = people[\"pets\"] &gt; 0\n)\n\n\n\n\n\n\n\n\n\nhobby\nheight\nweight\nage\nover 30\npets\nbody_mass_index\nhas_pets\n\n\n\n\nalice\nBiking\n172\n68\n33\nTrue\nNaN\n22.985398\nFalse\n\n\nbob\nDancing\n181\n83\n34\nTrue\n0.0\n25.335002\nFalse\n\n\ncharles\nNaN\n185\n112\n26\nFalse\n5.0\n32.724617\nTrue\n\n\n\n\n\n\n\nNote that you cannot access columns created within the same assignment:\n\n\nCode\ntry:\n    people.assign(\n        body_mass_index = people[\"weight\"] / (people[\"height\"] / 100) ** 2,\n        overweight = people[\"body_mass_index\"] &gt; 25\n    )\nexcept KeyError as e:\n    print(\"Key error:\", e)\n\n\nKey error: 'body_mass_index'\n\n\nThe solution is to split this assignment in two consecutive assignments:\n\n\nCode\nd6 = people.assign(body_mass_index = people[\"weight\"] / (people[\"height\"] / 100) ** 2)\nd6.assign(overweight = d6[\"body_mass_index\"] &gt; 25)\n\n\n\n\n\n\n\n\n\nhobby\nheight\nweight\nage\nover 30\npets\nbody_mass_index\noverweight\n\n\n\n\nalice\nBiking\n172\n68\n33\nTrue\nNaN\n22.985398\nFalse\n\n\nbob\nDancing\n181\n83\n34\nTrue\n0.0\n25.335002\nTrue\n\n\ncharles\nNaN\n185\n112\n26\nFalse\n5.0\n32.724617\nTrue\n\n\n\n\n\n\n\nHaving to create a temporary variable d6 is not very convenient. You may want to just chain the assigment calls, but it does not work because the people object is not actually modified by the first assignment:\n\n\nCode\ntry:\n    (people\n         .assign(body_mass_index = people[\"weight\"] / (people[\"height\"] / 100) ** 2)\n         .assign(overweight = people[\"body_mass_index\"] &gt; 25)\n    )\nexcept KeyError as e:\n    print(\"Key error:\", e)\n\n\nKey error: 'body_mass_index'\n\n\nBut fear not, there is a simple solution. You can pass a function to the assign() method (typically a lambda function), and this function will be called with the DataFrame as a parameter:\n\n\nCode\n(people\n     .assign(body_mass_index = lambda df: df[\"weight\"] / (df[\"height\"] / 100) ** 2)\n     .assign(overweight = lambda df: df[\"body_mass_index\"] &gt; 25)\n)\n\n\n\n\n\n\n\n\n\nhobby\nheight\nweight\nage\nover 30\npets\nbody_mass_index\noverweight\n\n\n\n\nalice\nBiking\n172\n68\n33\nTrue\nNaN\n22.985398\nFalse\n\n\nbob\nDancing\n181\n83\n34\nTrue\n0.0\n25.335002\nTrue\n\n\ncharles\nNaN\n185\n112\n26\nFalse\n5.0\n32.724617\nTrue\n\n\n\n\n\n\n\nProblem solved!"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#evaluating-an-expression",
    "href": "posts/pandas/tools-pandas.html#evaluating-an-expression",
    "title": "Tools - pandas",
    "section": "Evaluating an expression",
    "text": "Evaluating an expression\nA great feature supported by pandas is expression evaluation. This relies on the numexpr library which must be installed.\n\n\nCode\npeople.eval(\"weight / (height/100) ** 2 &gt; 25\")\n\n\nalice      False\nbob         True\ncharles     True\ndtype: bool\n\n\nAssignment expressions are also supported. Let‚Äôs set inplace=True to directly modify the DataFrame rather than getting a modified copy:\n\n\nCode\npeople.eval(\"body_mass_index = weight / (height/100) ** 2\", inplace=True)\npeople\n\n\n\n\n\n\n\n\n\nhobby\nheight\nweight\nage\nover 30\npets\nbody_mass_index\n\n\n\n\nalice\nBiking\n172\n68\n33\nTrue\nNaN\n22.985398\n\n\nbob\nDancing\n181\n83\n34\nTrue\n0.0\n25.335002\n\n\ncharles\nNaN\n185\n112\n26\nFalse\n5.0\n32.724617\n\n\n\n\n\n\n\nYou can use a local or global variable in an expression by prefixing it with '@':\n\n\nCode\noverweight_threshold = 30\npeople.eval(\"overweight = body_mass_index &gt; @overweight_threshold\", inplace=True)\npeople\n\n\n\n\n\n\n\n\n\nhobby\nheight\nweight\nage\nover 30\npets\nbody_mass_index\noverweight\n\n\n\n\nalice\nBiking\n172\n68\n33\nTrue\nNaN\n22.985398\nFalse\n\n\nbob\nDancing\n181\n83\n34\nTrue\n0.0\n25.335002\nFalse\n\n\ncharles\nNaN\n185\n112\n26\nFalse\n5.0\n32.724617\nTrue"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#querying-a-dataframe",
    "href": "posts/pandas/tools-pandas.html#querying-a-dataframe",
    "title": "Tools - pandas",
    "section": "Querying a DataFrame",
    "text": "Querying a DataFrame\nThe query() method lets you filter a DataFrame based on a query expression:\n\n\nCode\npeople.query(\"age &gt; 30 and pets == 0\")\n\n\n\n\n\n\n\n\n\nhobby\nheight\nweight\nage\nover 30\npets\nbody_mass_index\noverweight\n\n\n\n\nbob\nDancing\n181\n83\n34\nTrue\n0.0\n25.335002\nFalse"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#sorting-a-dataframe",
    "href": "posts/pandas/tools-pandas.html#sorting-a-dataframe",
    "title": "Tools - pandas",
    "section": "Sorting a DataFrame",
    "text": "Sorting a DataFrame\nYou can sort a DataFrame by calling its sort_index method. By default it sorts the rows by their index label, in ascending order, but let‚Äôs reverse the order:\n\n\nCode\npeople.sort_index(ascending=False)\n\n\n\n\n\n\n\n\n\nhobby\nheight\nweight\nage\nover 30\npets\nbody_mass_index\noverweight\n\n\n\n\ncharles\nNaN\n185\n112\n26\nFalse\n5.0\n32.724617\nTrue\n\n\nbob\nDancing\n181\n83\n34\nTrue\n0.0\n25.335002\nFalse\n\n\nalice\nBiking\n172\n68\n33\nTrue\nNaN\n22.985398\nFalse\n\n\n\n\n\n\n\nNote that sort_index returned a sorted copy of the DataFrame. To modify people directly, we can set the inplace argument to True. Also, we can sort the columns instead of the rows by setting axis=1:\n\n\nCode\npeople.sort_index(axis=1, inplace=True)\npeople\n\n\n\n\n\n\n\n\n\nage\nbody_mass_index\nheight\nhobby\nover 30\noverweight\npets\nweight\n\n\n\n\nalice\n33\n22.985398\n172\nBiking\nTrue\nFalse\nNaN\n68\n\n\nbob\n34\n25.335002\n181\nDancing\nTrue\nFalse\n0.0\n83\n\n\ncharles\n26\n32.724617\n185\nNaN\nFalse\nTrue\n5.0\n112\n\n\n\n\n\n\n\nTo sort the DataFrame by the values instead of the labels, we can use sort_values and specify the column to sort by:\n\n\nCode\npeople.sort_values(by=\"age\", inplace=True)\npeople\n\n\n\n\n\n\n\n\n\nage\nbody_mass_index\nheight\nhobby\nover 30\noverweight\npets\nweight\n\n\n\n\ncharles\n26\n32.724617\n185\nNaN\nFalse\nTrue\n5.0\n112\n\n\nalice\n33\n22.985398\n172\nBiking\nTrue\nFalse\nNaN\n68\n\n\nbob\n34\n25.335002\n181\nDancing\nTrue\nFalse\n0.0\n83"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#plotting-a-dataframe",
    "href": "posts/pandas/tools-pandas.html#plotting-a-dataframe",
    "title": "Tools - pandas",
    "section": "Plotting a DataFrame",
    "text": "Plotting a DataFrame\nJust like for Series, pandas makes it easy to draw nice graphs based on a DataFrame.\nFor example, it is trivial to create a line plot from a DataFrame‚Äôs data by calling its plot method:\n\n\nCode\npeople.plot(kind = \"line\", x = \"body_mass_index\", y = [\"height\", \"weight\"])\nplt.show()\n\n\n\n\n\nYou can pass extra arguments supported by matplotlib‚Äôs functions. For example, we can create scatterplot and pass it a list of sizes using the s argument of matplotlib‚Äôs scatter() function:\n\n\nCode\npeople.plot(kind = \"scatter\", x = \"height\", y = \"weight\", s=[40, 120, 200])\nplt.show()\n\n\n\n\n\nAgain, there are way too many options to list here: the best option is to scroll through the Visualization page in pandas‚Äô documentation, find the plot you are interested in and look at the example code."
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#operations-on-dataframes",
    "href": "posts/pandas/tools-pandas.html#operations-on-dataframes",
    "title": "Tools - pandas",
    "section": "Operations on DataFrames",
    "text": "Operations on DataFrames\nAlthough DataFrames do not try to mimick NumPy arrays, there are a few similarities. Let‚Äôs create a DataFrame to demonstrate this:\n\n\nCode\ngrades_array = np.array([[8,8,9],[10,9,9],[4, 8, 2], [9, 10, 10]])\ngrades = pd.DataFrame(grades_array, columns=[\"sep\", \"oct\", \"nov\"], index=[\"alice\",\"bob\",\"charles\",\"darwin\"])\ngrades\n\n\n\n\n\n\n\n\n\nsep\noct\nnov\n\n\n\n\nalice\n8\n8\n9\n\n\nbob\n10\n9\n9\n\n\ncharles\n4\n8\n2\n\n\ndarwin\n9\n10\n10\n\n\n\n\n\n\n\nYou can apply NumPy mathematical functions on a DataFrame: the function is applied to all values:\n\n\nCode\nnp.sqrt(grades)\n\n\n\n\n\n\n\n\n\nsep\noct\nnov\n\n\n\n\nalice\n2.828427\n2.828427\n3.000000\n\n\nbob\n3.162278\n3.000000\n3.000000\n\n\ncharles\n2.000000\n2.828427\n1.414214\n\n\ndarwin\n3.000000\n3.162278\n3.162278\n\n\n\n\n\n\n\nSimilarly, adding a single value to a DataFrame will add that value to all elements in the DataFrame. This is called broadcasting:\n\n\nCode\ngrades + 1\n\n\n\n\n\n\n\n\n\nsep\noct\nnov\n\n\n\n\nalice\n9\n9\n10\n\n\nbob\n11\n10\n10\n\n\ncharles\n5\n9\n3\n\n\ndarwin\n10\n11\n11\n\n\n\n\n\n\n\nOf course, the same is true for all other binary operations, including arithmetic (*,/,**‚Ä¶) and conditional (&gt;, ==‚Ä¶) operations:\n\n\nCode\ngrades &gt;= 5\n\n\n\n\n\n\n\n\n\nsep\noct\nnov\n\n\n\n\nalice\nTrue\nTrue\nTrue\n\n\nbob\nTrue\nTrue\nTrue\n\n\ncharles\nFalse\nTrue\nFalse\n\n\ndarwin\nTrue\nTrue\nTrue\n\n\n\n\n\n\n\nAggregation operations, such as computing the max, the sum or the mean of a DataFrame, apply to each column, and you get back a Series object:\n\n\nCode\ngrades.mean()\n\n\nsep    7.75\noct    8.75\nnov    7.50\ndtype: float64\n\n\nThe all method is also an aggregation operation: it checks whether all values are True or not. Let‚Äôs see during which months all students got a grade greater than 5:\n\n\nCode\n(grades &gt; 5).all()\n\n\nsep    False\noct     True\nnov    False\ndtype: bool\n\n\nMost of these functions take an optional axis parameter which lets you specify along which axis of the DataFrame you want the operation executed. The default is axis=0, meaning that the operation is executed vertically (on each column). You can set axis=1 to execute the operation horizontally (on each row). For example, let‚Äôs find out which students had all grades greater than 5:\n\n\nCode\n(grades &gt; 5).all(axis = 1)\n\n\nalice       True\nbob         True\ncharles    False\ndarwin      True\ndtype: bool\n\n\nThe any method returns True if any value is True. Let‚Äôs see who got at least one grade 10:\n\n\nCode\n(grades == 10).any(axis = 1)\n\n\nalice      False\nbob         True\ncharles    False\ndarwin      True\ndtype: bool\n\n\nIf you add a Series object to a DataFrame (or execute any other binary operation), pandas attempts to broadcast the operation to all rows in the DataFrame. This only works if the Series has the same size as the DataFrames rows. For example, let‚Äôs subtract the mean of the DataFrame (a Series object) from the DataFrame:\n\n\nCode\ngrades - grades.mean()  # equivalent to: grades - [7.75, 8.75, 7.50]\n\n\n\n\n\n\n\n\n\nsep\noct\nnov\n\n\n\n\nalice\n0.25\n-0.75\n1.5\n\n\nbob\n2.25\n0.25\n1.5\n\n\ncharles\n-3.75\n-0.75\n-5.5\n\n\ndarwin\n1.25\n1.25\n2.5\n\n\n\n\n\n\n\nWe subtracted 7.75 from all September grades, 8.75 from October grades and 7.50 from November grades. It is equivalent to subtracting this DataFrame:\n\n\nCode\npd.DataFrame([[7.75, 8.75, 7.50]]*4, index=grades.index, columns=grades.columns)\n\n\n\n\n\n\n\n\n\nsep\noct\nnov\n\n\n\n\nalice\n7.75\n8.75\n7.5\n\n\nbob\n7.75\n8.75\n7.5\n\n\ncharles\n7.75\n8.75\n7.5\n\n\ndarwin\n7.75\n8.75\n7.5\n\n\n\n\n\n\n\nIf you want to subtract the global mean from every grade, here is one way to do it:\n\n\nCode\ngrades - grades.values.mean() # subtracts the global mean (8.00) from all grades\n\n\n\n\n\n\n\n\n\nsep\noct\nnov\n\n\n\n\nalice\n0.0\n0.0\n1.0\n\n\nbob\n2.0\n1.0\n1.0\n\n\ncharles\n-4.0\n0.0\n-6.0\n\n\ndarwin\n1.0\n2.0\n2.0"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#automatic-alignment-1",
    "href": "posts/pandas/tools-pandas.html#automatic-alignment-1",
    "title": "Tools - pandas",
    "section": "Automatic alignment",
    "text": "Automatic alignment\nSimilar to Series, when operating on multiple DataFrames, pandas automatically aligns them by row index label, but also by column names. Let‚Äôs create a DataFrame with bonus points for each person from October to December:\n\n\nCode\nbonus_array = np.array([[0,np.nan,2],[np.nan,1,0],[0, 1, 0], [3, 3, 0]])\nbonus_points = pd.DataFrame(bonus_array, columns=[\"oct\", \"nov\", \"dec\"], index=[\"bob\",\"colin\", \"darwin\", \"charles\"])\nbonus_points\n\n\n\n\n\n\n\n\n\noct\nnov\ndec\n\n\n\n\nbob\n0.0\nNaN\n2.0\n\n\ncolin\nNaN\n1.0\n0.0\n\n\ndarwin\n0.0\n1.0\n0.0\n\n\ncharles\n3.0\n3.0\n0.0\n\n\n\n\n\n\n\n\n\nCode\ngrades + bonus_points\n\n\n\n\n\n\n\n\n\ndec\nnov\noct\nsep\n\n\n\n\nalice\nNaN\nNaN\nNaN\nNaN\n\n\nbob\nNaN\nNaN\n9.0\nNaN\n\n\ncharles\nNaN\n5.0\n11.0\nNaN\n\n\ncolin\nNaN\nNaN\nNaN\nNaN\n\n\ndarwin\nNaN\n11.0\n10.0\nNaN\n\n\n\n\n\n\n\nLooks like the addition worked in some cases but way too many elements are now empty. That‚Äôs because when aligning the DataFrames, some columns and rows were only present on one side, and thus they were considered missing on the other side (NaN). Then adding NaN to a number results in NaN, hence the result."
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#handling-missing-data",
    "href": "posts/pandas/tools-pandas.html#handling-missing-data",
    "title": "Tools - pandas",
    "section": "Handling missing data",
    "text": "Handling missing data\nDealing with missing data is a frequent task when working with real life data. Pandas offers a few tools to handle missing data.\nLet‚Äôs try to fix the problem above. For example, we can decide that missing data should result in a zero, instead of NaN. We can replace all NaN values by a any value using the fillna() method:\n\n\nCode\n(grades + bonus_points).fillna(0)\n\n\n\n\n\n\n\n\n\ndec\nnov\noct\nsep\n\n\n\n\nalice\n0.0\n0.0\n0.0\n0.0\n\n\nbob\n0.0\n0.0\n9.0\n0.0\n\n\ncharles\n0.0\n5.0\n11.0\n0.0\n\n\ncolin\n0.0\n0.0\n0.0\n0.0\n\n\ndarwin\n0.0\n11.0\n10.0\n0.0\n\n\n\n\n\n\n\nIt‚Äôs a bit unfair that we‚Äôre setting grades to zero in September, though. Perhaps we should decide that missing grades are missing grades, but missing bonus points should be replaced by zeros:\n\n\nCode\nfixed_bonus_points = bonus_points.fillna(0)\nfixed_bonus_points.insert(0, \"sep\", 0)\nfixed_bonus_points.loc[\"alice\"] = 0\ngrades + fixed_bonus_points\n\n\n\n\n\n\n\n\n\ndec\nnov\noct\nsep\n\n\n\n\nalice\nNaN\n9.0\n8.0\n8.0\n\n\nbob\nNaN\n9.0\n9.0\n10.0\n\n\ncharles\nNaN\n5.0\n11.0\n4.0\n\n\ncolin\nNaN\nNaN\nNaN\nNaN\n\n\ndarwin\nNaN\n11.0\n10.0\n9.0\n\n\n\n\n\n\n\nThat‚Äôs much better: although we made up some data, we have not been too unfair.\nAnother way to handle missing data is to interpolate. Let‚Äôs look at the bonus_points DataFrame again:\n\n\nCode\nbonus_points\n\n\n\n\n\n\n\n\n\noct\nnov\ndec\n\n\n\n\nbob\n0.0\nNaN\n2.0\n\n\ncolin\nNaN\n1.0\n0.0\n\n\ndarwin\n0.0\n1.0\n0.0\n\n\ncharles\n3.0\n3.0\n0.0\n\n\n\n\n\n\n\nNow let‚Äôs call the interpolate method. By default, it interpolates vertically (axis=0), so let‚Äôs tell it to interpolate horizontally (axis=1).\n\n\nCode\nbonus_points.interpolate(axis=1)\n\n\n\n\n\n\n\n\n\noct\nnov\ndec\n\n\n\n\nbob\n0.0\n1.0\n2.0\n\n\ncolin\nNaN\n1.0\n0.0\n\n\ndarwin\n0.0\n1.0\n0.0\n\n\ncharles\n3.0\n3.0\n0.0\n\n\n\n\n\n\n\nBob had 0 bonus points in October, and 2 in December. When we interpolate for November, we get the mean: 1 bonus point. Colin had 1 bonus point in November, but we do not know how many bonus points he had in September, so we cannot interpolate, this is why there is still a missing value in October after interpolation. To fix this, we can set the September bonus points to 0 before interpolation.\n\n\nCode\nbetter_bonus_points = bonus_points.copy()\nbetter_bonus_points.insert(0, \"sep\", 0)\nbetter_bonus_points.loc[\"alice\"] = 0\nbetter_bonus_points = better_bonus_points.interpolate(axis=1)\nbetter_bonus_points\n\n\n\n\n\n\n\n\n\nsep\noct\nnov\ndec\n\n\n\n\nbob\n0.0\n0.0\n1.0\n2.0\n\n\ncolin\n0.0\n0.5\n1.0\n0.0\n\n\ndarwin\n0.0\n0.0\n1.0\n0.0\n\n\ncharles\n0.0\n3.0\n3.0\n0.0\n\n\nalice\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\nGreat, now we have reasonable bonus points everywhere. Let‚Äôs find out the final grades:\n\n\nCode\ngrades + better_bonus_points\n\n\n\n\n\n\n\n\n\ndec\nnov\noct\nsep\n\n\n\n\nalice\nNaN\n9.0\n8.0\n8.0\n\n\nbob\nNaN\n10.0\n9.0\n10.0\n\n\ncharles\nNaN\n5.0\n11.0\n4.0\n\n\ncolin\nNaN\nNaN\nNaN\nNaN\n\n\ndarwin\nNaN\n11.0\n10.0\n9.0\n\n\n\n\n\n\n\nIt is slightly annoying that the September column ends up on the right. This is because the DataFrames we are adding do not have the exact same columns (the grades DataFrame is missing the \"dec\" column), so to make things predictable, pandas orders the final columns alphabetically. To fix this, we can simply add the missing column before adding:\n\n\nCode\ngrades[\"dec\"] = np.nan\nfinal_grades = grades + better_bonus_points\nfinal_grades\n\n\n\n\n\n\n\n\n\nsep\noct\nnov\ndec\n\n\n\n\nalice\n8.0\n8.0\n9.0\nNaN\n\n\nbob\n10.0\n9.0\n10.0\nNaN\n\n\ncharles\n4.0\n11.0\n5.0\nNaN\n\n\ncolin\nNaN\nNaN\nNaN\nNaN\n\n\ndarwin\n9.0\n10.0\n11.0\nNaN\n\n\n\n\n\n\n\nThere‚Äôs not much we can do about December and Colin: it‚Äôs bad enough that we are making up bonus points, but we can‚Äôt reasonably make up grades (well I guess some teachers probably do). So let‚Äôs call the dropna() method to get rid of rows that are full of NaNs:\n\n\nCode\nfinal_grades_clean = final_grades.dropna(how=\"all\")\nfinal_grades_clean\n\n\n\n\n\n\n\n\n\nsep\noct\nnov\ndec\n\n\n\n\nalice\n8.0\n8.0\n9.0\nNaN\n\n\nbob\n10.0\n9.0\n10.0\nNaN\n\n\ncharles\n4.0\n11.0\n5.0\nNaN\n\n\ndarwin\n9.0\n10.0\n11.0\nNaN\n\n\n\n\n\n\n\nNow let‚Äôs remove columns that are full of NaNs by setting the axis argument to 1:\n\n\nCode\nfinal_grades_clean = final_grades_clean.dropna(axis=1, how=\"all\")\nfinal_grades_clean\n\n\n\n\n\n\n\n\n\nsep\noct\nnov\n\n\n\n\nalice\n8.0\n8.0\n9.0\n\n\nbob\n10.0\n9.0\n10.0\n\n\ncharles\n4.0\n11.0\n5.0\n\n\ndarwin\n9.0\n10.0\n11.0"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#aggregating-with-groupby",
    "href": "posts/pandas/tools-pandas.html#aggregating-with-groupby",
    "title": "Tools - pandas",
    "section": "Aggregating with groupby",
    "text": "Aggregating with groupby\nSimilar to the SQL language, pandas allows grouping your data into groups to run calculations over each group.\nFirst, let‚Äôs add some extra data about each person so we can group them, and let‚Äôs go back to the final_grades DataFrame so we can see how NaN values are handled:\n\n\nCode\nfinal_grades[\"hobby\"] = [\"Biking\", \"Dancing\", np.nan, \"Dancing\", \"Biking\"]\nfinal_grades\n\n\n\n\n\n\n\n\n\nsep\noct\nnov\ndec\nhobby\n\n\n\n\nalice\n8.0\n8.0\n9.0\nNaN\nBiking\n\n\nbob\n10.0\n9.0\n10.0\nNaN\nDancing\n\n\ncharles\n4.0\n11.0\n5.0\nNaN\nNaN\n\n\ncolin\nNaN\nNaN\nNaN\nNaN\nDancing\n\n\ndarwin\n9.0\n10.0\n11.0\nNaN\nBiking\n\n\n\n\n\n\n\nNow let‚Äôs group data in this DataFrame by hobby:\n\n\nCode\ngrouped_grades = final_grades.groupby(\"hobby\")\ngrouped_grades\n\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f697dab7310&gt;\n\n\nWe are ready to compute the average grade per hobby:\n\n\nCode\ngrouped_grades.mean()\n\n\n\n\n\n\n\n\n\nsep\noct\nnov\ndec\n\n\nhobby\n\n\n\n\n\n\n\n\nBiking\n8.5\n9.0\n10.0\nNaN\n\n\nDancing\n10.0\n9.0\n10.0\nNaN\n\n\n\n\n\n\n\nThat was easy! Note that the NaN values have simply been skipped when computing the means."
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#pivot-tables",
    "href": "posts/pandas/tools-pandas.html#pivot-tables",
    "title": "Tools - pandas",
    "section": "Pivot tables",
    "text": "Pivot tables\nPandas supports spreadsheet-like pivot tables that allow quick data summarization. To illustrate this, let‚Äôs create a simple DataFrame:\n\n\nCode\nbonus_points\n\n\n\n\n\n\n\n\n\noct\nnov\ndec\n\n\n\n\nbob\n0.0\nNaN\n2.0\n\n\ncolin\nNaN\n1.0\n0.0\n\n\ndarwin\n0.0\n1.0\n0.0\n\n\ncharles\n3.0\n3.0\n0.0\n\n\n\n\n\n\n\n\n\nCode\nmore_grades = final_grades_clean.stack().reset_index()\nmore_grades.columns = [\"name\", \"month\", \"grade\"]\nmore_grades[\"bonus\"] = [np.nan, np.nan, np.nan, 0, np.nan, 2, 3, 3, 0, 0, 1, 0]\nmore_grades\n\n\n\n\n\n\n\n\n\nname\nmonth\ngrade\nbonus\n\n\n\n\n0\nalice\nsep\n8.0\nNaN\n\n\n1\nalice\noct\n8.0\nNaN\n\n\n2\nalice\nnov\n9.0\nNaN\n\n\n3\nbob\nsep\n10.0\n0.0\n\n\n4\nbob\noct\n9.0\nNaN\n\n\n5\nbob\nnov\n10.0\n2.0\n\n\n6\ncharles\nsep\n4.0\n3.0\n\n\n7\ncharles\noct\n11.0\n3.0\n\n\n8\ncharles\nnov\n5.0\n0.0\n\n\n9\ndarwin\nsep\n9.0\n0.0\n\n\n10\ndarwin\noct\n10.0\n1.0\n\n\n11\ndarwin\nnov\n11.0\n0.0\n\n\n\n\n\n\n\nNow we can call the pd.pivot_table() function for this DataFrame, asking to group by the name column. By default, pivot_table() computes the mean of each numeric column:\n\n\nCode\npd.pivot_table(more_grades, index=\"name\")\n\n\n\n\n\n\n\n\n\nbonus\ngrade\n\n\nname\n\n\n\n\n\n\nalice\nNaN\n8.333333\n\n\nbob\n1.000000\n9.666667\n\n\ncharles\n2.000000\n6.666667\n\n\ndarwin\n0.333333\n10.000000\n\n\n\n\n\n\n\nWe can change the aggregation function by setting the aggfunc argument, and we can also specify the list of columns whose values will be aggregated:\n\n\nCode\npd.pivot_table(more_grades, index=\"name\", values=[\"grade\",\"bonus\"], aggfunc=np.max)\n\n\n\n\n\n\n\n\n\nbonus\ngrade\n\n\nname\n\n\n\n\n\n\nalice\nNaN\n9.0\n\n\nbob\n2.0\n10.0\n\n\ncharles\n3.0\n11.0\n\n\ndarwin\n1.0\n11.0\n\n\n\n\n\n\n\nWe can also specify the columns to aggregate over horizontally, and request the grand totals for each row and column by setting margins=True:\n\n\nCode\npd.pivot_table(more_grades, index=\"name\", values=\"grade\", columns=\"month\", margins=True)\n\n\n\n\n\n\n\n\nmonth\nnov\noct\nsep\nAll\n\n\nname\n\n\n\n\n\n\n\n\nalice\n9.00\n8.0\n8.00\n8.333333\n\n\nbob\n10.00\n9.0\n10.00\n9.666667\n\n\ncharles\n5.00\n11.0\n4.00\n6.666667\n\n\ndarwin\n11.00\n10.0\n9.00\n10.000000\n\n\nAll\n8.75\n9.5\n7.75\n8.666667\n\n\n\n\n\n\n\nFinally, we can specify multiple index or column names, and pandas will create multi-level indices:\n\n\nCode\npd.pivot_table(more_grades, index=(\"name\", \"month\"), margins=True)\n\n\n\n\n\n\n\n\n\n\nbonus\ngrade\n\n\nname\nmonth\n\n\n\n\n\n\nalice\nnov\nNaN\n9.00\n\n\noct\nNaN\n8.00\n\n\nsep\nNaN\n8.00\n\n\nbob\nnov\n2.000\n10.00\n\n\noct\nNaN\n9.00\n\n\nsep\n0.000\n10.00\n\n\ncharles\nnov\n0.000\n5.00\n\n\noct\n3.000\n11.00\n\n\nsep\n3.000\n4.00\n\n\ndarwin\nnov\n0.000\n11.00\n\n\noct\n1.000\n10.00\n\n\nsep\n0.000\n9.00\n\n\nAll\n\n1.125\n8.75"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#overview-functions",
    "href": "posts/pandas/tools-pandas.html#overview-functions",
    "title": "Tools - pandas",
    "section": "Overview functions",
    "text": "Overview functions\nWhen dealing with large DataFrames, it is useful to get a quick overview of its content. Pandas offers a few functions for this. First, let‚Äôs create a large DataFrame with a mix of numeric values, missing values and text values. Notice how Jupyter displays only the corners of the DataFrame:\n\n\nCode\nmuch_data = np.fromfunction(lambda x,y: (x+y*y)%17*11, (10000, 26))\nlarge_df = pd.DataFrame(much_data, columns=list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"))\nlarge_df[large_df % 16 == 0] = np.nan\nlarge_df.insert(3,\"some_text\", \"Blabla\")\nlarge_df\n\n\n\n\n\n\n\n\n\nA\nB\nC\nsome_text\nD\nE\nF\nG\nH\nI\n...\nQ\nR\nS\nT\nU\nV\nW\nX\nY\nZ\n\n\n\n\n0\nNaN\n11.0\n44.0\nBlabla\n99.0\nNaN\n88.0\n22.0\n165.0\n143.0\n...\n11.0\nNaN\n11.0\n44.0\n99.0\nNaN\n88.0\n22.0\n165.0\n143.0\n\n\n1\n11.0\n22.0\n55.0\nBlabla\n110.0\nNaN\n99.0\n33.0\nNaN\n154.0\n...\n22.0\n11.0\n22.0\n55.0\n110.0\nNaN\n99.0\n33.0\nNaN\n154.0\n\n\n2\n22.0\n33.0\n66.0\nBlabla\n121.0\n11.0\n110.0\n44.0\nNaN\n165.0\n...\n33.0\n22.0\n33.0\n66.0\n121.0\n11.0\n110.0\n44.0\nNaN\n165.0\n\n\n3\n33.0\n44.0\n77.0\nBlabla\n132.0\n22.0\n121.0\n55.0\n11.0\nNaN\n...\n44.0\n33.0\n44.0\n77.0\n132.0\n22.0\n121.0\n55.0\n11.0\nNaN\n\n\n4\n44.0\n55.0\n88.0\nBlabla\n143.0\n33.0\n132.0\n66.0\n22.0\nNaN\n...\n55.0\n44.0\n55.0\n88.0\n143.0\n33.0\n132.0\n66.0\n22.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9995\nNaN\nNaN\n33.0\nBlabla\n88.0\n165.0\n77.0\n11.0\n154.0\n132.0\n...\nNaN\nNaN\nNaN\n33.0\n88.0\n165.0\n77.0\n11.0\n154.0\n132.0\n\n\n9996\nNaN\n11.0\n44.0\nBlabla\n99.0\nNaN\n88.0\n22.0\n165.0\n143.0\n...\n11.0\nNaN\n11.0\n44.0\n99.0\nNaN\n88.0\n22.0\n165.0\n143.0\n\n\n9997\n11.0\n22.0\n55.0\nBlabla\n110.0\nNaN\n99.0\n33.0\nNaN\n154.0\n...\n22.0\n11.0\n22.0\n55.0\n110.0\nNaN\n99.0\n33.0\nNaN\n154.0\n\n\n9998\n22.0\n33.0\n66.0\nBlabla\n121.0\n11.0\n110.0\n44.0\nNaN\n165.0\n...\n33.0\n22.0\n33.0\n66.0\n121.0\n11.0\n110.0\n44.0\nNaN\n165.0\n\n\n9999\n33.0\n44.0\n77.0\nBlabla\n132.0\n22.0\n121.0\n55.0\n11.0\nNaN\n...\n44.0\n33.0\n44.0\n77.0\n132.0\n22.0\n121.0\n55.0\n11.0\nNaN\n\n\n\n\n10000 rows √ó 27 columns\n\n\n\nThe head() method returns the top 5 rows:\n\n\nCode\nlarge_df.head()\n\n\n\n\n\n\n\n\n\nA\nB\nC\nsome_text\nD\nE\nF\nG\nH\nI\n...\nQ\nR\nS\nT\nU\nV\nW\nX\nY\nZ\n\n\n\n\n0\nNaN\n11.0\n44.0\nBlabla\n99.0\nNaN\n88.0\n22.0\n165.0\n143.0\n...\n11.0\nNaN\n11.0\n44.0\n99.0\nNaN\n88.0\n22.0\n165.0\n143.0\n\n\n1\n11.0\n22.0\n55.0\nBlabla\n110.0\nNaN\n99.0\n33.0\nNaN\n154.0\n...\n22.0\n11.0\n22.0\n55.0\n110.0\nNaN\n99.0\n33.0\nNaN\n154.0\n\n\n2\n22.0\n33.0\n66.0\nBlabla\n121.0\n11.0\n110.0\n44.0\nNaN\n165.0\n...\n33.0\n22.0\n33.0\n66.0\n121.0\n11.0\n110.0\n44.0\nNaN\n165.0\n\n\n3\n33.0\n44.0\n77.0\nBlabla\n132.0\n22.0\n121.0\n55.0\n11.0\nNaN\n...\n44.0\n33.0\n44.0\n77.0\n132.0\n22.0\n121.0\n55.0\n11.0\nNaN\n\n\n4\n44.0\n55.0\n88.0\nBlabla\n143.0\n33.0\n132.0\n66.0\n22.0\nNaN\n...\n55.0\n44.0\n55.0\n88.0\n143.0\n33.0\n132.0\n66.0\n22.0\nNaN\n\n\n\n\n5 rows √ó 27 columns\n\n\n\nOf course there‚Äôs also a tail() function to view the bottom 5 rows. You can pass the number of rows you want:\n\n\nCode\nlarge_df.tail(n=2)\n\n\n\n\n\n\n\n\n\nA\nB\nC\nsome_text\nD\nE\nF\nG\nH\nI\n...\nQ\nR\nS\nT\nU\nV\nW\nX\nY\nZ\n\n\n\n\n9998\n22.0\n33.0\n66.0\nBlabla\n121.0\n11.0\n110.0\n44.0\nNaN\n165.0\n...\n33.0\n22.0\n33.0\n66.0\n121.0\n11.0\n110.0\n44.0\nNaN\n165.0\n\n\n9999\n33.0\n44.0\n77.0\nBlabla\n132.0\n22.0\n121.0\n55.0\n11.0\nNaN\n...\n44.0\n33.0\n44.0\n77.0\n132.0\n22.0\n121.0\n55.0\n11.0\nNaN\n\n\n\n\n2 rows √ó 27 columns\n\n\n\nThe info() method prints out a summary of each columns contents:\n\n\nCode\nlarge_df.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 10000 entries, 0 to 9999\nData columns (total 27 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   A          8823 non-null   float64\n 1   B          8824 non-null   float64\n 2   C          8824 non-null   float64\n 3   some_text  10000 non-null  object \n 4   D          8824 non-null   float64\n 5   E          8822 non-null   float64\n 6   F          8824 non-null   float64\n 7   G          8824 non-null   float64\n 8   H          8822 non-null   float64\n 9   I          8823 non-null   float64\n 10  J          8823 non-null   float64\n 11  K          8822 non-null   float64\n 12  L          8824 non-null   float64\n 13  M          8824 non-null   float64\n 14  N          8822 non-null   float64\n 15  O          8824 non-null   float64\n 16  P          8824 non-null   float64\n 17  Q          8824 non-null   float64\n 18  R          8823 non-null   float64\n 19  S          8824 non-null   float64\n 20  T          8824 non-null   float64\n 21  U          8824 non-null   float64\n 22  V          8822 non-null   float64\n 23  W          8824 non-null   float64\n 24  X          8824 non-null   float64\n 25  Y          8822 non-null   float64\n 26  Z          8823 non-null   float64\ndtypes: float64(26), object(1)\nmemory usage: 2.1+ MB\n\n\nFinally, the describe() method gives a nice overview of the main aggregated values over each column: * count: number of non-null (not NaN) values * mean: mean of non-null values * std: standard deviation of non-null values * min: minimum of non-null values * 25%, 50%, 75%: 25th, 50th and 75th percentile of non-null values * max: maximum of non-null values\n\n\nCode\nlarge_df.describe()\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\n...\nQ\nR\nS\nT\nU\nV\nW\nX\nY\nZ\n\n\n\n\ncount\n8823.000000\n8824.000000\n8824.000000\n8824.000000\n8822.000000\n8824.000000\n8824.000000\n8822.000000\n8823.000000\n8823.000000\n...\n8824.000000\n8823.000000\n8824.000000\n8824.000000\n8824.000000\n8822.000000\n8824.000000\n8824.000000\n8822.000000\n8823.000000\n\n\nmean\n87.977559\n87.972575\n87.987534\n88.012466\n87.983791\n88.007480\n87.977561\n88.000000\n88.022441\n88.022441\n...\n87.972575\n87.977559\n87.972575\n87.987534\n88.012466\n87.983791\n88.007480\n87.977561\n88.000000\n88.022441\n\n\nstd\n47.535911\n47.535523\n47.521679\n47.521679\n47.535001\n47.519371\n47.529755\n47.536879\n47.535911\n47.535911\n...\n47.535523\n47.535911\n47.535523\n47.521679\n47.521679\n47.535001\n47.519371\n47.529755\n47.536879\n47.535911\n\n\nmin\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n...\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n\n\n25%\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n...\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n\n\n50%\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n...\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n\n\n75%\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n...\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n\n\nmax\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n...\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n\n\n\n\n8 rows √ó 26 columns"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#saving",
    "href": "posts/pandas/tools-pandas.html#saving",
    "title": "Tools - pandas",
    "section": "Saving",
    "text": "Saving\nLet‚Äôs save it to CSV, HTML and JSON:\n\n\nCode\nmy_df.to_csv(\"my_df.csv\")\nmy_df.to_html(\"my_df.html\")\nmy_df.to_json(\"my_df.json\")\n\n\nDone! Let‚Äôs take a peek at what was saved:\n\n\nCode\nfor filename in (\"my_df.csv\", \"my_df.html\", \"my_df.json\"):\n    print(\"#\", filename)\n    with open(filename, \"rt\") as f:\n        print(f.read())\n        print()\n\n\n# my_df.csv\n,hobby,weight,birthyear,children\nalice,Biking,68.5,1985,\nbob,Dancing,83.1,1984,3.0\n\n\n# my_df.html\n&lt;table border=\"1\" class=\"dataframe\"&gt;\n  &lt;thead&gt;\n    &lt;tr style=\"text-align: right;\"&gt;\n      &lt;th&gt;&lt;/th&gt;\n      &lt;th&gt;hobby&lt;/th&gt;\n      &lt;th&gt;weight&lt;/th&gt;\n      &lt;th&gt;birthyear&lt;/th&gt;\n      &lt;th&gt;children&lt;/th&gt;\n    &lt;/tr&gt;\n  &lt;/thead&gt;\n  &lt;tbody&gt;\n    &lt;tr&gt;\n      &lt;th&gt;alice&lt;/th&gt;\n      &lt;td&gt;Biking&lt;/td&gt;\n      &lt;td&gt;68.5&lt;/td&gt;\n      &lt;td&gt;1985&lt;/td&gt;\n      &lt;td&gt;NaN&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;th&gt;bob&lt;/th&gt;\n      &lt;td&gt;Dancing&lt;/td&gt;\n      &lt;td&gt;83.1&lt;/td&gt;\n      &lt;td&gt;1984&lt;/td&gt;\n      &lt;td&gt;3.0&lt;/td&gt;\n    &lt;/tr&gt;\n  &lt;/tbody&gt;\n&lt;/table&gt;\n\n# my_df.json\n{\"hobby\":{\"alice\":\"Biking\",\"bob\":\"Dancing\"},\"weight\":{\"alice\":68.5,\"bob\":83.1},\"birthyear\":{\"alice\":1985,\"bob\":1984},\"children\":{\"alice\":null,\"bob\":3.0}}\n\n\n\nNote that the index is saved as the first column (with no name) in a CSV file, as &lt;th&gt; tags in HTML and as keys in JSON.\nSaving to other formats works very similarly, but some formats require extra libraries to be installed. For example, saving to Excel requires the openpyxl library:\n\n\nCode\ntry:\n    my_df.to_excel(\"my_df.xlsx\", sheet_name='People')\nexcept ImportError as e:\n    print(e)\n\n\nNo module named 'openpyxl'"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#loading",
    "href": "posts/pandas/tools-pandas.html#loading",
    "title": "Tools - pandas",
    "section": "Loading",
    "text": "Loading\nNow let‚Äôs load our CSV file back into a DataFrame:\n\n\nCode\nmy_df_loaded = pd.read_csv(\"my_df.csv\", index_col=0)\nmy_df_loaded\n\n\n\n\n\n\n\n\n\nhobby\nweight\nbirthyear\nchildren\n\n\n\n\nalice\nBiking\n68.5\n1985\nNaN\n\n\nbob\nDancing\n83.1\n1984\n3.0\n\n\n\n\n\n\n\nAs you might guess, there are similar read_json, read_html, read_excel functions as well. We can also read data straight from the Internet. For example, let‚Äôs load the top 1,000 U.S. cities from github:\n\n\nCode\nus_cities = None\ntry:\n    csv_url = \"https://raw.githubusercontent.com/plotly/datasets/master/us-cities-top-1k.csv\"\n    us_cities = pd.read_csv(csv_url, index_col=0)\n    us_cities = us_cities.head()\nexcept IOError as e:\n    print(e)\nus_cities\n\n\n\n\n\n\n\n\n\nState\nPopulation\nlat\nlon\n\n\nCity\n\n\n\n\n\n\n\n\nMarysville\nWashington\n63269\n48.051764\n-122.177082\n\n\nPerris\nCalifornia\n72326\n33.782519\n-117.228648\n\n\nCleveland\nOhio\n390113\n41.499320\n-81.694361\n\n\nWorcester\nMassachusetts\n182544\n42.262593\n-71.802293\n\n\nColumbia\nSouth Carolina\n133358\n34.000710\n-81.034814\n\n\n\n\n\n\n\nThere are more options available, in particular regarding datetime format. Check out the documentation for more details."
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#sql-like-joins",
    "href": "posts/pandas/tools-pandas.html#sql-like-joins",
    "title": "Tools - pandas",
    "section": "SQL-like joins",
    "text": "SQL-like joins\nOne powerful feature of pandas is it‚Äôs ability to perform SQL-like joins on DataFrames. Various types of joins are supported: inner joins, left/right outer joins and full joins. To illustrate this, let‚Äôs start by creating a couple simple DataFrames:\n\n\nCode\ncity_loc = pd.DataFrame(\n    [\n        [\"CA\", \"San Francisco\", 37.781334, -122.416728],\n        [\"NY\", \"New York\", 40.705649, -74.008344],\n        [\"FL\", \"Miami\", 25.791100, -80.320733],\n        [\"OH\", \"Cleveland\", 41.473508, -81.739791],\n        [\"UT\", \"Salt Lake City\", 40.755851, -111.896657]\n    ], columns=[\"state\", \"city\", \"lat\", \"lng\"])\ncity_loc\n\n\n\n\n\n\n\n\n\nstate\ncity\nlat\nlng\n\n\n\n\n0\nCA\nSan Francisco\n37.781334\n-122.416728\n\n\n1\nNY\nNew York\n40.705649\n-74.008344\n\n\n2\nFL\nMiami\n25.791100\n-80.320733\n\n\n3\nOH\nCleveland\n41.473508\n-81.739791\n\n\n4\nUT\nSalt Lake City\n40.755851\n-111.896657\n\n\n\n\n\n\n\n\n\nCode\ncity_pop = pd.DataFrame(\n    [\n        [808976, \"San Francisco\", \"California\"],\n        [8363710, \"New York\", \"New-York\"],\n        [413201, \"Miami\", \"Florida\"],\n        [2242193, \"Houston\", \"Texas\"]\n    ], index=[3,4,5,6], columns=[\"population\", \"city\", \"state\"])\ncity_pop\n\n\n\n\n\n\n\n\n\npopulation\ncity\nstate\n\n\n\n\n3\n808976\nSan Francisco\nCalifornia\n\n\n4\n8363710\nNew York\nNew-York\n\n\n5\n413201\nMiami\nFlorida\n\n\n6\n2242193\nHouston\nTexas\n\n\n\n\n\n\n\nNow let‚Äôs join these DataFrames using the merge() function:\n\n\nCode\npd.merge(left=city_loc, right=city_pop, on=\"city\")\n\n\n\n\n\n\n\n\n\nstate_x\ncity\nlat\nlng\npopulation\nstate_y\n\n\n\n\n0\nCA\nSan Francisco\n37.781334\n-122.416728\n808976\nCalifornia\n\n\n1\nNY\nNew York\n40.705649\n-74.008344\n8363710\nNew-York\n\n\n2\nFL\nMiami\n25.791100\n-80.320733\n413201\nFlorida\n\n\n\n\n\n\n\nNote that both DataFrames have a column named state, so in the result they got renamed to state_x and state_y.\nAlso, note that Cleveland, Salt Lake City and Houston were dropped because they don‚Äôt exist in both DataFrames. This is the equivalent of a SQL INNER JOIN. If you want a FULL OUTER JOIN, where no city gets dropped and NaN values are added, you must specify how=\"outer\":\n\n\nCode\nall_cities = pd.merge(left=city_loc, right=city_pop, on=\"city\", how=\"outer\")\nall_cities\n\n\n\n\n\n\n\n\n\nstate_x\ncity\nlat\nlng\npopulation\nstate_y\n\n\n\n\n0\nCA\nSan Francisco\n37.781334\n-122.416728\n808976.0\nCalifornia\n\n\n1\nNY\nNew York\n40.705649\n-74.008344\n8363710.0\nNew-York\n\n\n2\nFL\nMiami\n25.791100\n-80.320733\n413201.0\nFlorida\n\n\n3\nOH\nCleveland\n41.473508\n-81.739791\nNaN\nNaN\n\n\n4\nUT\nSalt Lake City\n40.755851\n-111.896657\nNaN\nNaN\n\n\n5\nNaN\nHouston\nNaN\nNaN\n2242193.0\nTexas\n\n\n\n\n\n\n\nOf course LEFT OUTER JOIN is also available by setting how=\"left\": only the cities present in the left DataFrame end up in the result. Similarly, with how=\"right\" only cities in the right DataFrame appear in the result. For example:\n\n\nCode\npd.merge(left=city_loc, right=city_pop, on=\"city\", how=\"right\")\n\n\n\n\n\n\n\n\n\nstate_x\ncity\nlat\nlng\npopulation\nstate_y\n\n\n\n\n0\nCA\nSan Francisco\n37.781334\n-122.416728\n808976\nCalifornia\n\n\n1\nNY\nNew York\n40.705649\n-74.008344\n8363710\nNew-York\n\n\n2\nFL\nMiami\n25.791100\n-80.320733\n413201\nFlorida\n\n\n3\nNaN\nHouston\nNaN\nNaN\n2242193\nTexas\n\n\n\n\n\n\n\nIf the key to join on is actually in one (or both) DataFrame‚Äôs index, you must use left_index=True and/or right_index=True. If the key column names differ, you must use left_on and right_on. For example:\n\n\nCode\ncity_pop2 = city_pop.copy()\ncity_pop2.columns = [\"population\", \"name\", \"state\"]\npd.merge(left=city_loc, right=city_pop2, left_on=\"city\", right_on=\"name\")\n\n\n\n\n\n\n\n\n\nstate_x\ncity\nlat\nlng\npopulation\nname\nstate_y\n\n\n\n\n0\nCA\nSan Francisco\n37.781334\n-122.416728\n808976\nSan Francisco\nCalifornia\n\n\n1\nNY\nNew York\n40.705649\n-74.008344\n8363710\nNew York\nNew-York\n\n\n2\nFL\nMiami\n25.791100\n-80.320733\n413201\nMiami\nFlorida"
  },
  {
    "objectID": "posts/pandas/tools-pandas.html#concatenation",
    "href": "posts/pandas/tools-pandas.html#concatenation",
    "title": "Tools - pandas",
    "section": "Concatenation",
    "text": "Concatenation\nRather than joining DataFrames, we may just want to concatenate them. That‚Äôs what concat() is for:\n\n\nCode\nresult_concat = pd.concat([city_loc, city_pop])\nresult_concat\n\n\n\n\n\n\n\n\n\nstate\ncity\nlat\nlng\npopulation\n\n\n\n\n0\nCA\nSan Francisco\n37.781334\n-122.416728\nNaN\n\n\n1\nNY\nNew York\n40.705649\n-74.008344\nNaN\n\n\n2\nFL\nMiami\n25.791100\n-80.320733\nNaN\n\n\n3\nOH\nCleveland\n41.473508\n-81.739791\nNaN\n\n\n4\nUT\nSalt Lake City\n40.755851\n-111.896657\nNaN\n\n\n3\nCalifornia\nSan Francisco\nNaN\nNaN\n808976.0\n\n\n4\nNew-York\nNew York\nNaN\nNaN\n8363710.0\n\n\n5\nFlorida\nMiami\nNaN\nNaN\n413201.0\n\n\n6\nTexas\nHouston\nNaN\nNaN\n2242193.0\n\n\n\n\n\n\n\nNote that this operation aligned the data horizontally (by columns) but not vertically (by rows). In this example, we end up with multiple rows having the same index (eg. 3). Pandas handles this rather gracefully:\n\n\nCode\nresult_concat.loc[3]\n\n\n\n\n\n\n\n\n\nstate\ncity\nlat\nlng\npopulation\n\n\n\n\n3\nOH\nCleveland\n41.473508\n-81.739791\nNaN\n\n\n3\nCalifornia\nSan Francisco\nNaN\nNaN\n808976.0\n\n\n\n\n\n\n\nOr you can tell pandas to just ignore the index:\n\n\nCode\npd.concat([city_loc, city_pop], ignore_index=True)\n\n\n\n\n\n\n\n\n\nstate\ncity\nlat\nlng\npopulation\n\n\n\n\n0\nCA\nSan Francisco\n37.781334\n-122.416728\nNaN\n\n\n1\nNY\nNew York\n40.705649\n-74.008344\nNaN\n\n\n2\nFL\nMiami\n25.791100\n-80.320733\nNaN\n\n\n3\nOH\nCleveland\n41.473508\n-81.739791\nNaN\n\n\n4\nUT\nSalt Lake City\n40.755851\n-111.896657\nNaN\n\n\n5\nCalifornia\nSan Francisco\nNaN\nNaN\n808976.0\n\n\n6\nNew-York\nNew York\nNaN\nNaN\n8363710.0\n\n\n7\nFlorida\nMiami\nNaN\nNaN\n413201.0\n\n\n8\nTexas\nHouston\nNaN\nNaN\n2242193.0\n\n\n\n\n\n\n\nNotice that when a column does not exist in a DataFrame, it acts as if it was filled with NaN values. If we set join=\"inner\", then only columns that exist in both DataFrames are returned:\n\n\nCode\npd.concat([city_loc, city_pop], join=\"inner\")\n\n\n\n\n\n\n\n\n\nstate\ncity\n\n\n\n\n0\nCA\nSan Francisco\n\n\n1\nNY\nNew York\n\n\n2\nFL\nMiami\n\n\n3\nOH\nCleveland\n\n\n4\nUT\nSalt Lake City\n\n\n3\nCalifornia\nSan Francisco\n\n\n4\nNew-York\nNew York\n\n\n5\nFlorida\nMiami\n\n\n6\nTexas\nHouston\n\n\n\n\n\n\n\nYou can concatenate DataFrames horizontally instead of vertically by setting axis=1:\n\n\nCode\npd.concat([city_loc, city_pop], axis=1)\n\n\n\n\n\n\n\n\n\nstate\ncity\nlat\nlng\npopulation\ncity\nstate\n\n\n\n\n0\nCA\nSan Francisco\n37.781334\n-122.416728\nNaN\nNaN\nNaN\n\n\n1\nNY\nNew York\n40.705649\n-74.008344\nNaN\nNaN\nNaN\n\n\n2\nFL\nMiami\n25.791100\n-80.320733\nNaN\nNaN\nNaN\n\n\n3\nOH\nCleveland\n41.473508\n-81.739791\n808976.0\nSan Francisco\nCalifornia\n\n\n4\nUT\nSalt Lake City\n40.755851\n-111.896657\n8363710.0\nNew York\nNew-York\n\n\n5\nNaN\nNaN\nNaN\nNaN\n413201.0\nMiami\nFlorida\n\n\n6\nNaN\nNaN\nNaN\nNaN\n2242193.0\nHouston\nTexas\n\n\n\n\n\n\n\nIn this case it really does not make much sense because the indices do not align well (eg. Cleveland and San Francisco end up on the same row, because they shared the index label 3). So let‚Äôs reindex the DataFrames by city name before concatenating:\n\n\nCode\npd.concat([city_loc.set_index(\"city\"), city_pop.set_index(\"city\")], axis=1)\n\n\n\n\n\n\n\n\n\nstate\nlat\nlng\npopulation\nstate\n\n\ncity\n\n\n\n\n\n\n\n\n\nSan Francisco\nCA\n37.781334\n-122.416728\n808976.0\nCalifornia\n\n\nNew York\nNY\n40.705649\n-74.008344\n8363710.0\nNew-York\n\n\nMiami\nFL\n25.791100\n-80.320733\n413201.0\nFlorida\n\n\nCleveland\nOH\n41.473508\n-81.739791\nNaN\nNaN\n\n\nSalt Lake City\nUT\n40.755851\n-111.896657\nNaN\nNaN\n\n\nHouston\nNaN\nNaN\nNaN\n2242193.0\nTexas\n\n\n\n\n\n\n\nThis looks a lot like a FULL OUTER JOIN, except that the state columns were not renamed to state_x and state_y, and the city column is now the index.\nThe append() method is a useful shorthand for concatenating DataFrames vertically:\n\n\nCode\ncity_loc.append(city_pop)\n\n\n\n\n\n\n\n\n\nstate\ncity\nlat\nlng\npopulation\n\n\n\n\n0\nCA\nSan Francisco\n37.781334\n-122.416728\nNaN\n\n\n1\nNY\nNew York\n40.705649\n-74.008344\nNaN\n\n\n2\nFL\nMiami\n25.791100\n-80.320733\nNaN\n\n\n3\nOH\nCleveland\n41.473508\n-81.739791\nNaN\n\n\n4\nUT\nSalt Lake City\n40.755851\n-111.896657\nNaN\n\n\n3\nCalifornia\nSan Francisco\nNaN\nNaN\n808976.0\n\n\n4\nNew-York\nNew York\nNaN\nNaN\n8363710.0\n\n\n5\nFlorida\nMiami\nNaN\nNaN\n413201.0\n\n\n6\nTexas\nHouston\nNaN\nNaN\n2242193.0\n\n\n\n\n\n\n\nAs always in pandas, the append() method does not actually modify city_loc: it works on a copy and returns the modified copy."
  },
  {
    "objectID": "posts/qmd_computations/computations.html",
    "href": "posts/qmd_computations/computations.html",
    "title": "Quarto Computations",
    "section": "",
    "text": "Code\nimport numpy as np\na = np.arange(15).reshape(3, 5)\na\n\n\narray([[ 0,  1,  2,  3,  4],\n       [ 5,  6,  7,  8,  9],\n       [10, 11, 12, 13, 14]])"
  },
  {
    "objectID": "posts/qmd_computations/computations.html#numpy",
    "href": "posts/qmd_computations/computations.html#numpy",
    "title": "Quarto Computations",
    "section": "",
    "text": "Code\nimport numpy as np\na = np.arange(15).reshape(3, 5)\na\n\n\narray([[ 0,  1,  2,  3,  4],\n       [ 5,  6,  7,  8,  9],\n       [10, 11, 12, 13, 14]])"
  },
  {
    "objectID": "posts/qmd_computations/computations.html#matplotlib",
    "href": "posts/qmd_computations/computations.html#matplotlib",
    "title": "Quarto Computations",
    "section": "Matplotlib",
    "text": "Matplotlib\n\n\nCode\nimport matplotlib.pyplot as plt\n\nfig = plt.figure()\nx = np.arange(10)\ny = 2.5 * np.sin(x / 20 * np.pi)\nyerr = np.linspace(0.05, 0.2, 10)\n\nplt.errorbar(x, y + 3, yerr=yerr, label='both limits (default)')\nplt.errorbar(x, y + 2, yerr=yerr, uplims=True, label='uplims=True')\nplt.errorbar(x, y + 1, yerr=yerr, uplims=True, lolims=True,\n             label='uplims=True, lolims=True')\n\nupperlimits = [True, False] * 5\nlowerlimits = [False, True] * 5\nplt.errorbar(x, y, yerr=yerr, uplims=upperlimits, lolims=lowerlimits,\n             label='subsets of uplims and lolims')\n\nplt.legend(loc='lower right')\nplt.show(fig)"
  },
  {
    "objectID": "posts/qmd_computations/computations.html#plotly",
    "href": "posts/qmd_computations/computations.html#plotly",
    "title": "Quarto Computations",
    "section": "Plotly",
    "text": "Plotly\n\n\nCode\nimport plotly.express as px\nimport plotly.io as pio\ngapminder = px.data.gapminder()\ngapminder2007 = gapminder.query(\"year == 2007\")\nfig = px.scatter(gapminder2007, \n                 x=\"gdpPercap\", y=\"lifeExp\", color=\"continent\", \n                 size=\"pop\", size_max=60,\n                 hover_name=\"country\")\nfig.show()\n\n\nModuleNotFoundError: No module named 'plotly'"
  },
  {
    "objectID": "posts/qmd_titanic/titanic.html",
    "href": "posts/qmd_titanic/titanic.html",
    "title": "Titanic dataset analysis using Pandas and Numpy",
    "section": "",
    "text": "This notebook follows the fastai style conventions."
  },
  {
    "objectID": "posts/qmd_titanic/titanic.html#importnig-packages",
    "href": "posts/qmd_titanic/titanic.html#importnig-packages",
    "title": "Titanic dataset analysis using Pandas and Numpy",
    "section": "Importnig packages",
    "text": "Importnig packages\n\nimport pandas as pd\n\n\nimport numpy as np\n\n\nfrom scipy import stats, integrate\nimport scipy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(color_codes=True)\n%pylab inline\n\nPopulating the interactive namespace from numpy and matplotlib"
  },
  {
    "objectID": "posts/qmd_titanic/titanic.html#problem-statement",
    "href": "posts/qmd_titanic/titanic.html#problem-statement",
    "title": "Titanic dataset analysis using Pandas and Numpy",
    "section": "Problem Statement",
    "text": "Problem Statement\n\nWhat is the dependent variable and what are the factors in this data? Who had more chances of survival, what are the factors?\nData exploration section will investigate the dependent variable ‚ÄòSurvived‚Äô and understand the relationship of factors such as being a female, or child, or being in a certain class, or having sibling/spouse, parent/child affect the survival rate. We will also come up with a hypothesis and test it.\n\ndata = pd.read_csv('Titanic.csv')\n\n\ndata.head()\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS"
  },
  {
    "objectID": "posts/qmd_titanic/titanic.html#data-wrangling",
    "href": "posts/qmd_titanic/titanic.html#data-wrangling",
    "title": "Titanic dataset analysis using Pandas and Numpy",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\nPassengerId    891 non-null int64\nSurvived       891 non-null int64\nPclass         891 non-null int64\nName           891 non-null object\nSex            891 non-null object\nAge            714 non-null float64\nSibSp          891 non-null int64\nParch          891 non-null int64\nTicket         891 non-null object\nFare           891 non-null float64\nCabin          204 non-null object\nEmbarked       889 non-null object\ndtypes: float64(2), int64(5), object(5)\nmemory usage: 66.2+ KB\n\n\nThe titanic data given has 891 rows, most of the columns have 891 rows except Age, Cabin and Embarked.\n\nprint(data['Cabin'].describe())\nprint(data['Embarked'].describe())\nprint(data['Age'].describe())\n\ncount         204\nunique        147\ntop       B96 B98\nfreq            4\nName: Cabin, dtype: object\ncount     889\nunique      3\ntop         S\nfreq      644\nName: Embarked, dtype: object\ncount    714.000000\nmean      29.699118\nstd       14.526497\nmin        0.420000\n25%       20.125000\n50%       28.000000\n75%       38.000000\nmax       80.000000\nName: Age, dtype: float64\n\n\n\ndata['Cabin'].value_counts().plot(kind ='bar', figsize= (15,3))\nsns.plt.title('Frequency/Counts by Cabin')\n\n&lt;matplotlib.text.Text&gt;\n\n\n\n\n\nCabin has 147 unique values for 204 rows, Max freq is 4. It is difficult to draw conclusion on this data and since it has just 22.8% of rows, I will be dropping this column from any further analysis. Also PassengerId does not give me any useful information, so I will drop that column as well\n\ndel data['Cabin']\ndel data['PassengerId']\n\nLet us also drop the rows with missing values for Age and Embarked now\n\ndata.dropna(subset = ['Embarked', 'Age'], inplace = True)\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 712 entries, 0 to 890\nData columns (total 10 columns):\nSurvived    712 non-null int64\nPclass      712 non-null int64\nName        712 non-null object\nSex         712 non-null object\nAge         712 non-null float64\nSibSp       712 non-null int64\nParch       712 non-null int64\nTicket      712 non-null object\nFare        712 non-null float64\nEmbarked    712 non-null object\ndtypes: float64(2), int64(4), object(4)\nmemory usage: 50.1+ KB\n\n\nPclass should not be numeric, so let us update it to upper, middle and lower class. For that, we need to look at its relationship with Fare\n\nsns.barplot(x=\"Pclass\", y=\"Fare\", data=data);\nsns.plt.title('Pclass by Mean Fare')\n\n&lt;matplotlib.text.Text&gt;\n\n\n\n\n\nMean Fare of Pclass 1 was 88 dollars, Pclass 2 was 21.47 dollars and Pclass 3 was 13.22 dollars, so let us update the values of Pclass to ‚ÄòUpper‚Äô for Class 1, ‚ÄòMiddle‚Äô for Class 2 and ‚ÄòLower‚Äô for Class 3\n\ndata.loc[data['Pclass'] == 1, 'Pclass'] = 'Upper'\ndata.loc[data['Pclass'] == 2, 'Pclass'] = 'Middle'\ndata.loc[data['Pclass'] == 3, 'Pclass'] = 'Lower'"
  },
  {
    "objectID": "posts/qmd_titanic/titanic.html#data-exploration",
    "href": "posts/qmd_titanic/titanic.html#data-exploration",
    "title": "Titanic dataset analysis using Pandas and Numpy",
    "section": "Data Exploration",
    "text": "Data Exploration\n\n# Distribution of numeric variables\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(ncols=4, figsize = (15,3))\ndata['Age'].plot(kind ='hist', bins = 25, ax=ax1)\nax1.set_title('Age')\ndata['Fare'].plot(kind = 'hist', bins= 25, ax=ax2)\nax2.set_title('Fare')\ndata['Parch'].plot(kind = 'hist', ax=ax3)\nax3.set_title('Parch')\ndata['SibSp'].plot(kind = 'hist', ax=ax4)\nax4.set_title('SibSp')\n\n&lt;matplotlib.text.Text&gt;\n\n\n\n\n\n\n# Distribution of categorical variables\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(ncols=4, figsize = (12,3))\ndata['Sex'].value_counts().plot(kind ='bar', ax=ax1)\nax1.set_title('Sex')\ndata['Survived'].value_counts().plot(kind = 'bar', ax=ax2)\nax2.set_title('Survived')\nax2.set_xticklabels(['Perished', 'Survived'])\ndata['Pclass'].value_counts().plot(kind = 'bar', ax=ax3)\nax3.set_title('Pclass')\ndata['Embarked'].value_counts().plot(kind = 'bar', ax=ax4)\nax4.set_title('Embarked')\n\n&lt;matplotlib.text.Text&gt;\n\n\n\n\n\nThe above plots show the distributions of numerical and categorical columns in our data. Age ranges from 0 to 80 years with mean and mode around 25-30 years, Fare ranges from 0 to over 500 dollars, Parch and SibSp has its mode at 0 meaning most people did not travel with any parent/child or sibling/spouse, There were around 453 males and 289 females onboard, 424 perished and 288 survived. Most of the passengers were in Lower Pclass and embarked at station S.\n\nUnderstanding the dependencies of dependent and independent variables\nSince for the given data, more than 50% of the passengers perished, We will investigate the factors that survival of the passengers depend on and would like to answer questions like did females have more chance of surviving, how does age or fare affect the survival, does having a parent or child, or sibling or spouse influence survival and how does Pclass affect survival. Dependent variable is ‚ÄòSurvived‚Äô which gives 0 for rows for passengers who perished and 1 for passengers that survived. Independent variables are Sex, Pclass, Embarked, Age, Fare etc.\nThere could be other factors or variables like location of cabins or location/state(sleep or awake) of passengers at the time of the accident etc which we had limited data for and hence ave been omitted from the analysis. We also omitted rows that had missing values for ‚ÄòAge‚Äô and ‚ÄòEmbarked‚Äô so that will also skew the statistical analysis a bit.\n\n#Function to create grouped data by factors\ndef grouped_by_factors(df,factor):\n    mean_by_factor = df.groupby(factor).describe()\n    return mean_by_factor\n\nSome understanding of mean/max/std/count would be helpful for our analysis so I created a function to display statistics using groupby function. We will also be creating plots to help visualize the data.\n\n\nUnderstanding Dependent variable ‚ÄòSurvived‚Äô by numerical columns\n\n‚ÄòSurvived‚Äô by Age and Fare\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize =(12,3))\nfig1 = sns.regplot(x=\"Age\", y=\"Survived\", data=data, ax = ax1)\nfig2 = sns.regplot(x=\"Fare\", y=\"Survived\", data=data, ax = ax2)\nplt.suptitle(\"Perished vs. Survived by Age and Fare\", size=12)\nfig1.set(ylabel='Survival Rate'), fig2.set(ylabel='Survival Rate')\n\n([&lt;matplotlib.text.Text&gt;], [&lt;matplotlib.text.Text&gt;])\n\n\n\n\n\n\n\n‚ÄòSurvived‚Äô by SibSp and Parch\n\ng = sns.PairGrid(data, y_vars=[\"Survived\"], x_vars=[\"SibSp\", \"Parch\"], size=4)\ng.map(sns.barplot, color=\".4\")\ng.set(ylabel='Survival Rate')\nplt.suptitle(\"Perished vs. Survived by SibSp and Parch\", size=12)\n\n&lt;matplotlib.text.Text&gt;\n\n\n\n\n\n\ngrouped_by_factors(data,'Survived')\n\n\n\n\n\n\n\n\nAge\nFare\nParch\nSibSp\n\n\nSurvived\n\n\n\n\n\n\n\n\n\n0\ncount\n424.000000\n424.000000\n424.000000\n424.000000\n\n\nmean\n30.626179\n22.965456\n0.365566\n0.525943\n\n\nstd\n14.172110\n31.448825\n0.878341\n1.044760\n\n\nmin\n1.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n21.000000\n7.895800\n0.000000\n0.000000\n\n\n50%\n28.000000\n11.887500\n0.000000\n0.000000\n\n\n75%\n39.000000\n26.550000\n0.000000\n1.000000\n\n\nmax\n74.000000\n263.000000\n6.000000\n5.000000\n\n\n1\ncount\n288.000000\n288.000000\n288.000000\n288.000000\n\n\nmean\n28.193299\n51.647672\n0.531250\n0.496528\n\n\nstd\n14.859146\n70.664499\n0.808747\n0.732512\n\n\nmin\n0.420000\n0.000000\n0.000000\n0.000000\n\n\n25%\n19.000000\n13.000000\n0.000000\n0.000000\n\n\n50%\n28.000000\n26.250000\n0.000000\n0.000000\n\n\n75%\n36.000000\n65.000000\n1.000000\n1.000000\n\n\nmax\n80.000000\n512.329200\n5.000000\n4.000000\n\n\n\n\n\n\n\nThe data shows 424 passengers did not survive and 288 did.\nAverage age of passengers that survived was 28.2(std=14.8) years as compared to 30.62(14.17) for those who did not survive. On average, passengers who survived paid higher fare(mean=51.6 dollars) as compared to who did not(mean=22.9 dollars).\nFrom the barchart, the survival rate for those travelling with 1/2 sibling or spouse and 1/2/3 parent or children was higher than the ones that did not. The relationship of survival is not linear with the number of sibsp/parch which could be due to lack of data.\nFrom the correlation plot, Survival rate is positively correlated to Fare and negatively correlated to Age which means younger people and those who paid more had higher chances of surviving\n\n\n\nUnderstanding Dependent variable ‚ÄòSurvived‚Äô by Categorical columns\n\n‚ÄòSurvived‚Äô by Pclass\n\ngrouped_by_factors(data,'Pclass')\n\n\n\n\n\n\n\n\nAge\nFare\nParch\nSibSp\nSurvived\n\n\nPclass\n\n\n\n\n\n\n\n\n\n\nLower\ncount\n355.000000\n355.000000\n355.000000\n355.000000\n355.000000\n\n\nmean\n25.140620\n13.229435\n0.456338\n0.585915\n0.239437\n\n\nstd\n12.495398\n10.043158\n0.971447\n1.157303\n0.427342\n\n\nmin\n0.420000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n18.000000\n7.775000\n0.000000\n0.000000\n0.000000\n\n\n50%\n24.000000\n8.050000\n0.000000\n0.000000\n0.000000\n\n\n75%\n32.000000\n15.741700\n1.000000\n1.000000\n0.000000\n\n\nmax\n74.000000\n56.495800\n6.000000\n5.000000\n1.000000\n\n\nMiddle\ncount\n173.000000\n173.000000\n173.000000\n173.000000\n173.000000\n\n\nmean\n29.877630\n21.471556\n0.404624\n0.427746\n0.479769\n\n\nstd\n14.001077\n13.187429\n0.705775\n0.611645\n0.501041\n\n\nmin\n0.670000\n10.500000\n0.000000\n0.000000\n0.000000\n\n\n25%\n23.000000\n13.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n29.000000\n15.045800\n0.000000\n0.000000\n0.000000\n\n\n75%\n36.000000\n26.000000\n1.000000\n1.000000\n1.000000\n\n\nmax\n70.000000\n73.500000\n3.000000\n3.000000\n1.000000\n\n\nUpper\ncount\n184.000000\n184.000000\n184.000000\n184.000000\n184.000000\n\n\nmean\n38.105543\n88.048121\n0.413043\n0.456522\n0.652174\n\n\nstd\n14.778904\n81.293524\n0.734061\n0.634406\n0.477580\n\n\nmin\n0.920000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n27.000000\n33.890600\n0.000000\n0.000000\n0.000000\n\n\n50%\n37.000000\n67.950000\n0.000000\n0.000000\n1.000000\n\n\n75%\n49.000000\n107.043750\n1.000000\n1.000000\n1.000000\n\n\nmax\n80.000000\n512.329200\n4.000000\n3.000000\n1.000000\n\n\n\n\n\n\n\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize =(12,3))\nfig1 = sns.countplot(x=\"Pclass\", data=data, hue='Survived', palette=\"Greens_d\", ax=ax1);\nplt.legend([\"Perished\", \"Survived\"])\nfig2 = sns.barplot(x=\"Pclass\", y=\"Survived\", data=data, ax=ax2);\nplt.suptitle(\"Survival rate by Pclass\", size=12)\n\n&lt;matplotlib.text.Text&gt;\n\n\n\n\n\nMean Fare of Upper Class was 88 dollars, Middle Class was 21.47 dollars and Lower Class was 13.22 dollars. Most survivors were from upper class(mean survival = 0.65), followed by middle(mean survival = 0.48) and then lower(mean survival = 0.24). Most of the passengers who did not survive belonged to the lower class Pclass shows linear relation to survival probability. There could be several reasons for that. People in upper classes could have boarded lifeboats before the lower classes, it also fits well with the correlation to fare in the prev plot.\n\n\n‚ÄòSurvived‚Äô by Embarked\n\nsns.countplot(x=\"Embarked\", data=data, hue='Survived', palette=\"Greens_d\");\nsns.countplot(x=\"Embarked\", data=data, hue='Pclass', palette=\"Reds_d\");\nplt.suptitle(\"Valuecounts of Survivors by Pclass\", size=12)\nplt.suptitle(\"Valuecounts of Female vs Male survivors\", size=12)\nlabel = [\"Perished\", \"Survived\", \"Lower\", \"Upper\", \"Middle\"]\nplt.legend(label, loc='upper center')\n\n&lt;matplotlib.legend.Legend&gt;\n\n\n\n\n\nMost of the passengers were in lower Pclass and embarked from ‚ÄòS‚Äô followed by ‚ÄòC‚Äô and ‚ÄòQ‚Äô. Does not show much relationship to survival rate\n\n\n‚ÄòSurvived‚Äô by Sex\n\ndata.groupby('Sex').describe()\n\n\n\n\n\n\n\n\nAge\nFare\nParch\nSibSp\nSurvived\n\n\nSex\n\n\n\n\n\n\n\n\n\n\nfemale\ncount\n259.000000\n259.000000\n259.000000\n259.000000\n259.000000\n\n\nmean\n27.745174\n47.332433\n0.714286\n0.644788\n0.752896\n\n\nstd\n13.989760\n61.517487\n1.069045\n0.930367\n0.432163\n\n\nmin\n0.750000\n6.750000\n0.000000\n0.000000\n0.000000\n\n\n25%\n18.000000\n13.000000\n0.000000\n0.000000\n1.000000\n\n\n50%\n27.000000\n26.000000\n0.000000\n0.000000\n1.000000\n\n\n75%\n36.000000\n56.964600\n1.000000\n1.000000\n1.000000\n\n\nmax\n63.000000\n512.329200\n6.000000\n5.000000\n1.000000\n\n\nmale\ncount\n453.000000\n453.000000\n453.000000\n453.000000\n453.000000\n\n\nmean\n30.726645\n27.268836\n0.271523\n0.439294\n0.205298\n\n\nstd\n14.678201\n45.841889\n0.651076\n0.923609\n0.404366\n\n\nmin\n0.420000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n21.000000\n7.895800\n0.000000\n0.000000\n0.000000\n\n\n50%\n29.000000\n13.000000\n0.000000\n0.000000\n0.000000\n\n\n75%\n39.000000\n28.500000\n0.000000\n1.000000\n0.000000\n\n\nmax\n80.000000\n512.329200\n5.000000\n5.000000\n1.000000\n\n\n\n\n\n\n\n\nsns.countplot(x=\"Sex\", data=data, hue='Survived', palette=\"Greens_d\");\nplt.suptitle(\"Valuecounts of Female vs Male survivors\", size=12)\nlabel = [\"Perished\", \"Survived\"]\nplt.legend(label, loc='upper center')\n\n&lt;matplotlib.legend.Legend&gt;\n\n\n\n\n\nMean age of females who boarded the ship was 27-28 years and males was 30-31 years. There were 259 females and 453 males, more number of females(mean survival = 0.75) survived than males(mean survival = 0.20)\nFor the purpose of this analysis, I will pick Sex, Pclass and Age as major factors and investigate them further. The reason why I am picking them is because they show correlation with survival rate. Survival showed correlation to Fare as well but since the fare is represented by Pclass, I picked Pclass over Fare. Although other factors also affect survival, but I will focus on these three for this exercise\n\n\n\nUnderstanding Pclass and Sex as a factor\n\nsns.barplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=data);\nplt.suptitle(\"Mean Survival rate of Female vs Male survivors by Pclass\", size=12)\n\n&lt;matplotlib.text.Text&gt;\n\n\n\n\n\nThere were 314 females and 577 males, mean for female survivors(mean=0.74,std= 0.44) is more than males(mean=0.19,std= 0.39) across all Pclasses, Survival has linear relationship with class. Females had high probability of survival in both Upper and Middle class. Only upper class males had high probability of survival, which was lower than low class female passengers however\n\n\nUnderstanding Age as a factor\n\nprint(grouped_by_factors(data,'Age').head())\nprint(grouped_by_factors(data,'Age').tail())\n\n              Fare  Parch  SibSp  Survived\nAge                                       \n0.42 count  1.0000    1.0    1.0       1.0\n     mean   8.5167    1.0    0.0       1.0\n     std       NaN    NaN    NaN       NaN\n     min    8.5167    1.0    0.0       1.0\n     25%    8.5167    1.0    0.0       1.0\n          Fare  Parch  SibSp  Survived\nAge                                   \n80.0 min  30.0    0.0    0.0       1.0\n     25%  30.0    0.0    0.0       1.0\n     50%  30.0    0.0    0.0       1.0\n     75%  30.0    0.0    0.0       1.0\n     max  30.0    0.0    0.0       1.0\n\n\n\nsurv_age = data[data['Survived'] == 1]\ng = surv_age['Age'].plot(kind='hist', figsize=[12,6], alpha=.8)\nnotsurv_age = data[data['Survived'] == 0]\nnotsurv_age['Age'].plot(kind='hist', figsize=[12,6], alpha=.4)\nplt.legend(label)\ng.set(xlabel='Age')\nplt.suptitle(\"Distribution of Age for Perished and Survived\", size=12)\n\n&lt;matplotlib.text.Text&gt;\n\n\n\n\n\nAge of the passengers ranged from 0 to 80 years. Green bar is for passengers who did not survive and the blue is for those who survived. The distribution is almost normal distribution with similar shape and mode around 20 years. Below is the correlation for Age vs mean survived, it shows slight negative correlation with pearson‚Äôr value of -0.082\n\nCorrelation of ‚ÄòSurvived‚Äô with Age\n\nsns.set(style=\"darkgrid\", color_codes=True)\ng = sns.jointplot(\"Age\", \"Survived\", data=data, kind=\"reg\",color=\"g\", size=7)\nplt.subplots_adjust(top=0.95)\nplt.suptitle(\"Distribution of Age for Perished and Survived\", size=12)\n\n&lt;matplotlib.text.Text&gt;\n\n\n\n\n\n\n\nCorrelation of ‚ÄòSurvived‚Äô with Age and Sex\n\ng = sns.lmplot(x=\"Age\", y=\"Survived\", col=\"Sex\", hue=\"Sex\", data=data, y_jitter=.02, logistic=True)\nplt.subplots_adjust(top=0.9)\nplt.suptitle(\"Correlation of Age with Survival Rate\", size=12)\n\n&lt;matplotlib.text.Text&gt;\n\n\n\n\n\nSurvival probability was higher for Younger Men and Older Women, Side by side comparison of males and females by age further supports that\n\ng = sns.factorplot(x=\"Survived\", y=\"Age\", hue=\"Sex\", data=data, size=6, kind=\"bar\", palette=\"muted\")\ng.despine(left=True)\nplt.subplots_adjust(top=0.95)\nplt.suptitle(\"Survived vs Age for males and females\", size=12)\ng.set_xticklabels(['Perished', 'Survived'])"
  },
  {
    "objectID": "posts/qmd_titanic/titanic.html#hypothesis-testing",
    "href": "posts/qmd_titanic/titanic.html#hypothesis-testing",
    "title": "Titanic dataset analysis using Pandas and Numpy",
    "section": "Hypothesis testing",
    "text": "Hypothesis testing\nI have a hypothesis that passengers that are lower in age(&lt;15 years) had greater chance of survival than females.\nNull Hypothesis would be that the difference in chances of survival of passengers greater or lower than 15 years is not significant and alternate would be that it is significant.\n                        H0: ¬µchild = ¬µfemale at Œ± = 0.05, \n                        HA: ¬µchild ‚â† ¬µfemale at Œ± = 0.05, where Œ± is the t-critical at which the probability is .05 and ¬µchild and ¬µfemale are population means for the two groups.\n\n#Children under 15yrs of age\ndata_children = data[data['Age'] &lt;= 15]\n\n\n#Females of age greater than 15 years\ndata_female = data[(data['Sex'] == 'female') & (data['Age'] &gt; 15)]\n\n\nscipy.stats.ttest_ind(data_children['Survived'], data_female['Survived'], axis=0, equal_var=False, nan_policy='propagate')\n\nTtest_indResult(statistic=-2.978953154108325, pvalue=0.0034528377861817636)\n\n\n\nSince p value is low, the difference in mean survival is significant for females vs.¬†children. Negative t-statistic shows that the mean survival of females is more than that of children"
  },
  {
    "objectID": "posts/qmd_titanic/titanic.html#conclusions",
    "href": "posts/qmd_titanic/titanic.html#conclusions",
    "title": "Titanic dataset analysis using Pandas and Numpy",
    "section": "Conclusions",
    "text": "Conclusions\nIn Conclusion with the given dataset, Most contributing factors are ‚ÄòSex‚Äô and Pclass. Women had the most probability of survival in general. Survival rate is positively correlated to Fare and negatively correlated to Age which means younger people and those who paid more had higher chances of surviving. Females had positive correlation of survival with age and Males had negative correlation. Most survivors were from Upper Pclass followed by medium and lower class passengers. Most of the passengers in lower class perished. Passengers with any parent/child/sibling or spouse had higher chance at survival than the ones that did not. The analysis has following limitations: Omitted rows with missing values for ‚ÄòAge‚Äô and ‚Äòemabarked‚Äô Did not draw conclusions based on ‚ÄòName‚Äô column dropped ‚ÄòCabin‚Äô and ‚ÄòPassengerId‚Äô during data wrangling phase The data set is limited, the complete dataset should contain data for 1500 passengers"
  },
  {
    "objectID": "posts/sample_ipynb/hello.html",
    "href": "posts/sample_ipynb/hello.html",
    "title": "Quarto Basics",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure¬†1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure¬†1: A line plot on a polar axis"
  },
  {
    "objectID": "posts/sample_ipynb/hello.html#polar-axis",
    "href": "posts/sample_ipynb/hello.html#polar-axis",
    "title": "Quarto Basics",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure¬†1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure¬†1: A line plot on a polar axis"
  },
  {
    "objectID": "posts/when_to_water/index.html",
    "href": "posts/when_to_water/index.html",
    "title": "Can AI decide when to water crops?",
    "section": "",
    "text": "In Texas, 2022 was one of the driest years on record. The article in general wishes to tackle the water scarcity problems in agriculture. For that, Texas A&M AgriLife Research and IBM are collaborating to streamline a new low-cost app, similar to Liquid prep, to help farmers determine the optimal time to water their crops. The app, called ‚ÄúWhen to Water,‚Äù combines data from various sources, including soil sensors, crop types, and weather forecasts, to provide customized irrigation recommendations to farmers. The collaboration aims to improve the accuracy and speed of data collection, analysis, and transmission, ultimately helping farmers make more informed decisions about irrigation management."
  },
  {
    "objectID": "series-gha.html",
    "href": "series-gha.html",
    "title": "GitHub Actions Series: Making awesome automations",
    "section": "",
    "text": "Here you can find a series of posts about GitHub Actions, which are complementary material for my lightning talk at the RStudio Conference - rstudio::conf 2022."
  },
  {
    "objectID": "series-gha.html#posts-in-english",
    "href": "series-gha.html#posts-in-english",
    "title": "GitHub Actions Series: Making awesome automations",
    "section": "Posts in English",
    "text": "Posts in English\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "series-gha.html#posts-en-fran√ßais",
    "href": "series-gha.html#posts-en-fran√ßais",
    "title": "GitHub Actions Series: Making awesome automations",
    "section": "Posts en fran√ßais",
    "text": "Posts en fran√ßais\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "Homepage",
    "section": "",
    "text": "See all/Voir tout"
  },
  {
    "objectID": "test.html#recent-posts-posts-recentes",
    "href": "test.html#recent-posts-posts-recentes",
    "title": "Homepage",
    "section": "",
    "text": "See all/Voir tout"
  },
  {
    "objectID": "test.html#posts-in-english",
    "href": "test.html#posts-in-english",
    "title": "Homepage",
    "section": "Posts in English",
    "text": "Posts in English\n\n\n\n See all posts in English"
  },
  {
    "objectID": "test.html#posts-en-fran√ßais",
    "href": "test.html#posts-en-fran√ßais",
    "title": "Homepage",
    "section": "Posts en fran√ßais",
    "text": "Posts en fran√ßais\n\n\n\n Voir posts en fran√ßais"
  },
  {
    "objectID": "posts/fertility_map_africa/afsis-soil-chem-EDA.html",
    "href": "posts/fertility_map_africa/afsis-soil-chem-EDA.html",
    "title": "Tools - Soil Chemistry Dataset",
    "section": "",
    "text": "To download data:\n\naws-cli\n\nTo parse and manage datasets:\n\nbrukeropusreader\npandas\ntqdm\n\n\nInstallation below:\n\n#! pip install awscli brukeropusreader tqdm pandas matplotlib folium seaborn pathlib \n\n#commented out after the installation is completed in case you run again the notebook         \n\n\n#import pyspark\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import Image\nimport folium\nfrom pathlib import Path\nfrom brukeropusreader import read_file\nfrom collections import Counter\n\nfrom tqdm import tqdm_notebook as tqdm"
  },
  {
    "objectID": "posts/fertility_map_africa/afsis-soil-chem-EDA.html#introduction",
    "href": "posts/fertility_map_africa/afsis-soil-chem-EDA.html#introduction",
    "title": "Tools - Soil Chemistry Dataset",
    "section": "",
    "text": "QED | https://qed.ai | info@qed.ai\n\n\n\nTo download data:\n\naws-cli\n\nTo parse and manage datasets:\n\nbrukeropusreader\npandas\ntqdm\n\n\nInstallation below:\n\n#! pip install awscli brukeropusreader tqdm pandas matplotlib folium seaborn pathlib \n\n#commented out after the installation is completed in case you run again the notebook         \n\n\n#import pyspark\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import Image\nimport folium\nfrom pathlib import Path\nfrom brukeropusreader import read_file\nfrom collections import Counter\n\nfrom tqdm import tqdm_notebook as tqdm\n\n\n\n\nDownload s3 bucket content with the aws-cli command line tool. Run aws configure beforehand to set your credentials.\n\n#! aws s3 sync s3://afsis afsis --no-sign-request \n\n# commented out after the download is completed in case you run again the notebook\n\n\n\n\nOPUS spectra are created using Bruker instruments Infrared spectrometers To be opened brukeropusreader package is needed.\nThe function brukeropusreader.read_file parses the binaries and returns a data structure containing information about the wave numbers, absorbance spectra, and file metadata.\nHere we plot a few of the spectra."
  },
  {
    "objectID": "posts/fertility_map_africa/afsis-soil-chem-EDA.html#geographical-references",
    "href": "posts/fertility_map_africa/afsis-soil-chem-EDA.html#geographical-references",
    "title": "Tools - Soil Chemistry Dataset",
    "section": "2. Geographical references",
    "text": "2. Geographical references\nThe AfSIS Soil Chemistry Dataset contains georeferences for each spectra. It is worth noting that it consists of two datasets, one for the subsaharian Africa and one with additional data recorded only in Tanzania.\n\n2.1 Dataframes exploration\n\nGEOREFS_FILE1 = 'afsis/2009-2013/Georeferences/georeferences.csv' \ndf_geo1 = pd.read_csv(GEOREFS_FILE1)# dataframe for several african countries\ndf_geo1 = df_geo1.sort_values(by=['Site', 'Cultivated'])\nprint(df_geo1.shape)\nprint(df_geo1.isna().sum())\ndf_geo1.head()\n\n(1843, 14)\nSSN                0\nPublic             0\nLatitude           0\nLongitude          0\nCluster            0\nPlot               0\nDepth              0\nSoil material     94\nScientist          0\nSite               0\nCountry            0\nRegion           232\nCultivated       798\nGid                0\ndtype: int64\n\n\n\n\n\n\n\n\n\nSSN\nPublic\nLatitude\nLongitude\nCluster\nPlot\nDepth\nSoil material\nScientist\nSite\nCountry\nRegion\nCultivated\nGid\n\n\n\n\n460\nicr016032\nTrue\n5.423182\n-0.709083\n15\n1\ntop\nAju.15.1.Topsoil.Std fine soil\nJerome Tondoh\nAjumako\nGhana\nWest Africa\nFalse\n345\n\n\n500\nicr016033\nTrue\n5.423182\n-0.709083\n15\n1\nsub\nAju.15.1.Subsoil.Std fine soil\nJerome Tondoh\nAjumako\nGhana\nWest Africa\nFalse\n346\n\n\n666\nicr015913\nTrue\n5.377035\n-0.726425\n9\n1\nsub\nAju.9.1.Subsoil.Std fine soil\nJerome Tondoh\nAjumako\nGhana\nWest Africa\nFalse\n334\n\n\n732\nicr016053\nTrue\n5.447667\n-0.717422\n16\n1\nsub\nAju.16.1.Subsoil.Std fine soil\nJerome Tondoh\nAjumako\nGhana\nWest Africa\nFalse\n348\n\n\n957\nicr015973\nTrue\n5.434675\n-0.728883\n12\n1\nsub\nAju.12.1.Subsoil.Std fine soil\nJerome Tondoh\nAjumako\nGhana\nWest Africa\nFalse\n340\n\n\n\n\n\n\n\n\nGEOREFS_FILE2 = 'afsis/tansis/Georeferences/georeferences.csv' \ndf_geo2 = pd.read_csv(GEOREFS_FILE2)# dataframe with additional data for Tanzania\ndf_geo2 = df_geo2.sort_values(by=['Site', 'Cultivated'])\nprint(df_geo2.shape)\n\ndf_geo2.head()\n\n(18819, 14)\n\n\n\n\n\n\n\n\n\nCluster\nCountry\nCultivated\nDepth\nGid\nLatitude\nLongitude\nPlot\nRegion\nSSN\nSampling date\nScientist\nSite\nSoil material\n\n\n\n\n29\n2.0\nTanzania\nFalse\ntop\n46.0\n-3.029698\n33.015202\n2.0\nEast Africa\nicr011843\nNaN\nLeigh Winoweicki\nBukwaya\nBuk.2.2.Topsoil.Std fine soil\n\n\n31\n4.0\nTanzania\nFalse\ntop\n48.0\n-2.992815\n33.016113\n1.0\nEast Africa\nicr011881\nNaN\nLeigh Winoweicki\nBukwaya\nBuk.4.1.Topsoil.Std fine soil\n\n\n32\n5.0\nTanzania\nFalse\ntop\n49.0\n-3.060982\n33.030663\n1.0\nEast Africa\nicr011901\nNaN\nLeigh Winoweicki\nBukwaya\nBuk.5.1.Topsoil.Std fine soil\n\n\n35\n8.0\nTanzania\nFalse\ntop\n52.0\n-2.987467\n33.033264\n1.0\nEast Africa\nicr011960\nNaN\nLeigh Winoweicki\nBukwaya\nBuk.8.1.Topsoil.Std fine soil\n\n\n36\n9.0\nTanzania\nFalse\ntop\n53.0\n-3.058480\n33.065014\n3.0\nEast Africa\nicr011978\nNaN\nLeigh Winoweicki\nBukwaya\nBuk.9.3.Topsoil.Std fine soil\n\n\n\n\n\n\n\nThe tansis folder contains measurements for Tanzania only. FTIR spectra in this folder are not readable and make it unusable\n\ntodrop = ['Soil material','Scientist', 'Site', 'Region', 'Gid','Plot','Public'] #irrelevant\ndf_geo = df_geo1.drop(todrop, axis = 1)"
  },
  {
    "objectID": "posts/fertility_map_africa/afsis-soil-chem-EDA.html#dry-chemistry",
    "href": "posts/fertility_map_africa/afsis-soil-chem-EDA.html#dry-chemistry",
    "title": "Tools - Soil Chemistry Dataset",
    "section": "3. Dry Chemistry",
    "text": "3. Dry Chemistry\nThe AfSIS Soil Chemistry dataset contains dry and wet chemistry data taken at each sampling location.\n\n3.1 X-ray fluorescence (XRF) elemental analysis\nwith XRF we get the concentration of various chemical elements in the sample\nUnits are parts per million (ppm)\nhttps://www.elementalanalysis.com/xrf.html\n\nfile1path = \"afsis/2009-2013/Dry_Chemistry/ICRAF/Bruker_TXRF/TXRF.csv\"\nfile2path = \"afsis/tansis/Dry_Chemistry/ICRAF/Bruker_TXRF/TXRF.csv\"\nxrf_africa = pd.read_csv(file1path)\nxrf_tanzania = pd.read_csv(file2path)\n\nprint('measurements 2009-13',xrf_africa.shape,'measurements  2014', xrf_tanzania.shape)\n\nprint(\"the table below shows the concentration in ppm for each element detected\")\nxrf_africa.head() \n\nmeasurements 2009-13 (1904, 42) measurements  2014 (224, 42)\nthe table below shows the concentration in ppm for each element detected\n\n\n\n\n\n\n\n\n\nSSN\nPublic\nNa\nMg\nAl\nP\nS\nCl\nK\nCa\n...\nPr\nNd\nSm\nHf\nTa\nW\nHg\nPb\nBi\nTh\n\n\n\n\n0\nicr005965\nTrue\n16023.3\n4433.5\n37618.6\n84.4\n45.7\n268.1\n12412.2\n30705.6\n...\n0.9\n14.7\n14.0\n0.9\n2.5\n0.2\n4.9\n3.9\n0.1\n13.4\n\n\n1\nicr005966\nTrue\n20524.6\n5832.2\n40248.2\n72.1\n45.7\n229.6\n12892.2\n23234.5\n...\n1.1\n15.8\n18.2\n0.5\n3.2\n0.2\n4.2\n3.3\n0.1\n19.9\n\n\n2\nicr005985\nTrue\n19350.4\n5085.8\n36766.3\n50.6\n45.7\n157.3\n16839.7\n16746.2\n...\n1.2\n19.2\n14.1\n0.8\n2.0\n1.2\n2.6\n12.0\n0.1\n17.9\n\n\n3\nicr005986\nTrue\n17410.2\n5271.2\n37912.2\n50.6\n45.7\n285.2\n16818.0\n31939.6\n...\n1.1\n16.7\n12.6\n0.3\n1.2\n0.5\n6.3\n10.2\n0.1\n16.5\n\n\n4\nicr005998\nTrue\n19092.5\n9169.8\n37359.8\n50.6\n45.7\n251.4\n17577.9\n25298.2\n...\n1.1\n16.7\n17.2\n0.5\n3.2\n0.4\n4.2\n5.6\n0.1\n18.4\n\n\n\n\n5 rows √ó 42 columns\n\n\n\n\nmask_diff_xrf = xrf_africa[xrf_africa['SSN'].isin(diff_list )]\ndf_xrf = pd.concat([xrf_tanzania, mask_diff_xrf]) \n\n\nprint(df_xrf.shape, len(df_xrf.SSN.unique()))\n\n(1838, 42) 1838\n\n\n\nprint('important elements for agriculture') \n# https://www.qld.gov.au/environment/land/management/soil/soil-properties/fertility\n\nimportant_elements = ['P','K', 'S','Ca','Mg','Cu','Cl','Zn','Fe', 'Mn','Mo' ]\ndf_xrf_reduced = df_xrf[important_elements]#/10000# express in %\ndf_xrf_reduced['SSN'] = df_xrf['SSN']\ndf_xrf_reduced.head()\n\nimportant elements for agriculture\n\n\nSettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_xrf_reduced['SSN'] = df_xrf['SSN']\n\n\n\n\n\n\n\n\n\nP\nK\nS\nCa\nMg\nCu\nCl\nZn\nFe\nMn\nMo\nSSN\n\n\n\n\n0\n84.4\n12412.2\n45.7\n30705.6\n4433.5\n10.1\n268.1\n25.3\n22263.2\n356.1\n184.5\nicr005965\n\n\n1\n72.1\n12892.2\n45.7\n23234.5\n5832.2\n11.8\n229.6\n29.4\n26269.6\n379.7\n184.5\nicr005966\n\n\n2\n50.6\n16839.7\n45.7\n16746.2\n5085.8\n22.1\n157.3\n32.4\n22790.2\n323.4\n184.5\nicr005985\n\n\n3\n50.6\n16818.0\n45.7\n31939.6\n5271.2\n24.9\n285.2\n34.4\n22939.1\n332.5\n184.5\nicr005986\n\n\n4\n50.6\n17577.9\n45.7\n25298.2\n9169.8\n13.1\n251.4\n35.4\n24985.2\n408.3\n184.5\nicr005998\n\n\n\n\n\n\n\n\n# how disperse are the XRF data?\nprint(df_xrf_reduced.shape)\ndf_xrf_reduced.describe()\n\n(1838, 12)\n\n\n\n\n\n\n\n\n\nP\nK\nS\nCa\nMg\nCu\nCl\nZn\nFe\nMn\nMo\n\n\n\n\ncount\n1838.000000\n1838.000000\n1838.000000\n1838.000000\n1838.000000\n1838.000000\n1838.000000\n1838.000000\n1838.000000\n1838.000000\n1838.000000\n\n\nmean\n0.011513\n1.126107\n0.007641\n0.758606\n0.730171\n0.001519\n0.026368\n0.002566\n2.633472\n0.040358\n0.019379\n\n\nstd\n0.046650\n1.268460\n0.040276\n2.284191\n0.733123\n0.001496\n0.222005\n0.002984\n2.673819\n0.052072\n0.012800\n\n\nmin\n0.002950\n0.009040\n0.003320\n0.004400\n0.402890\n0.000050\n0.000600\n0.000090\n0.007430\n0.000540\n0.017950\n\n\n25%\n0.005060\n0.240008\n0.004570\n0.046742\n0.557500\n0.000450\n0.005687\n0.000780\n0.779240\n0.010405\n0.018450\n\n\n50%\n0.005060\n0.696950\n0.004570\n0.141455\n0.557500\n0.001125\n0.010050\n0.001890\n1.947845\n0.022950\n0.018450\n\n\n75%\n0.005060\n1.624108\n0.004570\n0.518190\n0.557500\n0.002050\n0.016350\n0.003437\n3.452745\n0.052255\n0.018450\n\n\nmax\n1.246380\n10.627340\n0.970400\n42.643090\n13.689340\n0.012840\n7.438340\n0.073000\n22.908970\n0.660790\n0.444250\n\n\n\n\n\n\n\n\n\n3.2 Fourier transform infrared spectroscopy (FTIR)\nA word about units. Most spectra using electromagnetic radiation are presented with wavelength as the X-axis in nm or Œºm. Originally, IR spectra were presented in units of micrometers. Later (1953) a different measure, the wavenumber given the unit cm-1, was adopted.\nŒΩ (cm-1)= 10,000/Œª (Œºm)\nThe spectra may appear to be ‚Äúbackward‚Äù (large wavenumber values on the left, running to low values on the right); this is a consequence of the Œºm to cm-1 conversion\n\n3.2.1 NIR (near infrared range) FTIR\nspectral range: 12500 - 4000 cm-1 or 700 - 2500 nm for near infrared (NIR)\n\nImage(filename='img/NIR.jpeg') \n\n\n\n\n\nNIR_SPECTRA_DIR = 'Bruker_MPA/*'\nAFSIS_PATH = Path('afsis/2009-2013/Dry_Chemistry/ICRAF')\nnames = []\nspectra = []\n\nfor path in tqdm(AFSIS_PATH.glob(NIR_SPECTRA_DIR )):\n    if path.is_file():\n        spect_data = read_file(path)\n        spectra.append(spect_data[\"AB\"])\n        names.append(path.stem)\nwave_nums = spect_data.get_range()\n\ncolumn_names = ['{:.0f}'.format(x) for x in wave_nums]\nnear_infrared_df = pd.DataFrame(spectra, index=names, columns=column_names)\nnear_infrared_df.head()\n\nTqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  for path in tqdm(AFSIS_PATH.glob(NIR_SPECTRA_DIR )):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12493\n12489\n12485\n12482\n12478\n12474\n12470\n12466\n12462\n12458\n...\n3633\n3630\n3626\n3622\n3618\n3614\n3610\n3606\n3603\n3599\n\n\n\n\nicr033603\n0.744053\n0.744943\n0.737978\n0.734149\n0.738894\n0.741579\n0.738796\n0.740573\n0.745435\n0.745959\n...\n2.679350\n2.639192\n2.631729\n2.611440\n2.646616\n2.508814\n2.414326\n2.423105\n2.533958\n2.656340\n\n\nicr042897\n0.553651\n0.549628\n0.549979\n0.555073\n0.561452\n0.566700\n0.571925\n0.578335\n0.582339\n0.579222\n...\n2.455164\n2.484992\n2.565423\n2.588626\n2.550071\n2.503246\n2.361378\n2.318084\n2.421962\n2.429910\n\n\nicr049675\n0.533260\n0.531480\n0.535772\n0.545087\n0.551631\n0.550375\n0.546267\n0.545179\n0.545328\n0.542055\n...\n2.641663\n2.420862\n2.309041\n2.268263\n2.305246\n2.589037\n2.847070\n2.433076\n2.500584\n2.292974\n\n\nicr034693\n0.568058\n0.566100\n0.563312\n0.560136\n0.556757\n0.558751\n0.566576\n0.575022\n0.580073\n0.579828\n...\n2.410610\n2.373128\n2.324080\n2.145765\n2.025990\n2.089124\n2.294537\n2.543749\n2.517684\n2.474452\n\n\nicr033950\n0.555743\n0.550596\n0.548068\n0.554348\n0.559019\n0.554218\n0.551323\n0.556638\n0.561071\n0.562413\n...\n2.714783\n2.360171\n2.265779\n2.247130\n2.206304\n2.150379\n2.145735\n2.169409\n2.158581\n2.150478\n\n\n\n\n5 rows √ó 2307 columns\n\n\n\n\n# table with FTIR spectra for each sample\n\ndf_NIR_FTIRspectra = near_infrared_df.T.reset_index()\n\ndf_NIR_FTIRspectra = df_NIR_FTIRspectra.rename(columns={'index': 'labda'})\ndf_NIR_FTIRspectra.labda = pd.to_numeric(df_NIR_FTIRspectra.labda)\n\ndf_NIR_FTIRspectra.head()\n\n\n\n\n\n\n\n\nlabda\nicr033603\nicr042897\nicr049675\nicr034693\nicr033950\nicr034794\nicr015953\nicr050394\nicr048771\n...\nicr073540\nicr049437\nicr037699\nicr075004\nicr056181\nicr055563\nicr010159\nicr074792\nicr011321\nicr062275\n\n\n\n\n0\n12493\n0.744053\n0.553651\n0.533260\n0.568058\n0.555743\n0.643860\n0.431300\n0.684270\n0.569734\n...\n0.268871\n0.623325\n0.427489\n0.664087\n0.612543\n0.400455\n0.604818\n0.511006\n0.633803\n0.539674\n\n\n1\n12489\n0.744943\n0.549628\n0.531480\n0.566100\n0.550596\n0.648193\n0.434562\n0.684778\n0.567334\n...\n0.266468\n0.631448\n0.425584\n0.665025\n0.612217\n0.398957\n0.605491\n0.507771\n0.635440\n0.541978\n\n\n2\n12485\n0.737978\n0.549979\n0.535772\n0.563312\n0.548068\n0.648293\n0.435362\n0.677241\n0.563958\n...\n0.265169\n0.638345\n0.430387\n0.654435\n0.615592\n0.396526\n0.604928\n0.507411\n0.637939\n0.545053\n\n\n3\n12482\n0.734149\n0.555073\n0.545087\n0.560136\n0.554348\n0.642154\n0.436202\n0.676640\n0.561916\n...\n0.265009\n0.636849\n0.433966\n0.644498\n0.619221\n0.399731\n0.601859\n0.511584\n0.640378\n0.543280\n\n\n4\n12478\n0.738894\n0.561452\n0.551631\n0.556757\n0.559019\n0.641057\n0.436053\n0.683005\n0.566201\n...\n0.265186\n0.632287\n0.433450\n0.646780\n0.623118\n0.405075\n0.597800\n0.517670\n0.642512\n0.540592\n\n\n\n\n5 rows √ó 1908 columns\n\n\n\n\nplt.figure(figsize= (6,6))\n\nplt.plot(df_NIR_FTIRspectra['labda'],df_NIR_FTIRspectra.iloc[:, [4,5,6,7,8,9,10,11,12,13,14,15,16,17,18]]) \n\nplt.title('Example spectra')\n\n\nplt.xticks(rotation=90)\nplt.xlim(12000,3500)\nplt.ylabel('Absorbance (A.U.)', fontsize=14)\nplt.xlabel('Wavelengths (cm-1)', fontsize = 14)\n\nplt.axvline(x=7500 , ymin=0, ymax=1, color='r', linewidth = 1)\nplt.axvline(x=4000, ymin=0, ymax=1, color='r', linewidth = 1)\nplt.text(7400, 2.5, 'useful range between \\n red lines')\n\nText(7400, 2.5, 'useful range between \\n red lines')\n\n\n\n\n\nIn the region 12000 - 7500 cm-1 there are no peaks, this part of the spectrum can be ignored below 4000 cm-1 the signal is unrealistic. It is an experimental artifact and this part of the spectrum should be cut off.\nAbsorbance in this range is analized with mid-range FTIR spectrometers (see section 3.2.2 below)\n\ndf_NIR_FTIRspectra =  df_NIR_FTIRspectra[df_NIR_FTIRspectra['labda'] &lt; 7500]  \ndf_NIR_FTIRspectra = df_NIR_FTIRspectra[df_NIR_FTIRspectra['labda'] &gt; 4000]\n\n\nplt.figure(figsize= (6,6))\n\nplt.plot(df_NIR_FTIRspectra['labda'],df_NIR_FTIRspectra.iloc[:, [4,5,6,12,16,17,48]]) \n\nplt.title('Example reduced spectra')\n\nplt.xticks(rotation=90)\nplt.xlim(7500,4000)\nplt.ylabel('Absorbance (A.U.)', fontsize=14)\nplt.xlabel('Wavelengths (cm-1)', fontsize = 14)\n\nText(0.5, 0, 'Wavelengths (cm-1)')\n\n\n\n\n\n\n# change original dataset accordingly\n\ndf_NIR_reindexed = df_NIR_FTIRspectra.set_index('labda')\nnear_infrared_df = df_NIR_reindexed.T\nnear_infrared_df.head()\n\n\n\n\n\n\n\nlabda\n7498\n7494\n7491\n7487\n7483\n7479\n7475\n7471\n7467\n7464\n...\n4038\n4035\n4031\n4027\n4023\n4019\n4015\n4011\n4008\n4004\n\n\n\n\nicr033603\n0.469107\n0.469056\n0.469180\n0.469344\n0.469312\n0.469145\n0.469012\n0.469019\n0.469138\n0.469201\n...\n0.693520\n0.696103\n0.698850\n0.701674\n0.704414\n0.707121\n0.709787\n0.712360\n0.714864\n0.717183\n\n\nicr042897\n0.453903\n0.453874\n0.453821\n0.453781\n0.453818\n0.453874\n0.453957\n0.454082\n0.454211\n0.454340\n...\n0.679447\n0.681886\n0.684869\n0.687917\n0.690609\n0.692926\n0.694870\n0.696561\n0.698304\n0.700157\n\n\nicr049675\n0.311495\n0.311474\n0.311484\n0.311448\n0.311298\n0.311103\n0.310900\n0.310745\n0.310745\n0.310800\n...\n0.471828\n0.473717\n0.475707\n0.477803\n0.479886\n0.481793\n0.483479\n0.485172\n0.487041\n0.488976\n\n\nicr034693\n0.420559\n0.420480\n0.420528\n0.420549\n0.420534\n0.420425\n0.420276\n0.420284\n0.420467\n0.420655\n...\n0.695133\n0.698124\n0.701617\n0.705556\n0.709398\n0.712633\n0.715035\n0.716666\n0.718061\n0.719766\n\n\nicr033950\n0.327227\n0.327036\n0.326880\n0.326711\n0.326502\n0.326389\n0.326441\n0.326477\n0.326314\n0.325999\n...\n0.437366\n0.439201\n0.441031\n0.442873\n0.444723\n0.446564\n0.448291\n0.449815\n0.451223\n0.452619\n\n\n\n\n5 rows √ó 907 columns\n\n\n\n\n\n3.2.2 MIR (middle infrared range) FTIR\nspectral range 400 - 4000 cm-1\ndata recorded with three spectrometers that differ for the method for collecting data.\n\nALPHA Kbr spectrometer: transmission, using samples pressed into a Kbr pellet\nALPHA ZnSe spectrometer: reflection over a diamond window and ZnSe filter / beam splitter\nTensor27 Kbr spectrometer further reading about experimental details can be found at afsis/2009-2013/Dry_Chemistry/ICRAF/SOP/METH07V01 ALPHA.pdf\n\nand a general introduction can be found here: https://www.shimadzu.com/an/service-support/technical-support/analysis-basics/ftirtalk/talk8.html\n\n2014-2018 Most FTIR spectra could not be opened using the Bruker open files library, in this kernel only those obtained with the Tensor27 spectrometer are analyzed\n\n\n- ALPHA spectrometer - KBr window\n\nKBR_SPECTRA_DIR = 'Bruker_Alpha_KBr/*'\nAFSIS_PATH = Path('afsis/2009-2013/Dry_Chemistry/ICRAF')\nTANSIS_PATH = Path('afsis/tansis/Dry_Chemistry/ICRAF')\nnames = []\nspectra = []\n\nfor path in tqdm(AFSIS_PATH.glob(KBR_SPECTRA_DIR )):\n    if path.is_file():\n        spect_data = read_file(path)\n        spectra.append(spect_data[\"AB\"])\n        names.append(path.stem)\nwave_nums = spect_data.get_range()\n\ncolumn_names = ['{:.0f}'.format(x) for x in wave_nums]\nkbr_df = pd.DataFrame(spectra, index=names, columns=column_names)\nkbr_df.head()\n\nTqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  for path in tqdm(AFSIS_PATH.glob(KBR_SPECTRA_DIR )):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3998\n3997\n3996\n3994\n3993\n3991\n3990\n3988\n3987\n3986\n...\n412\n411\n409\n408\n406\n405\n404\n402\n401\n399\n\n\n\n\nicr033603\n0.908592\n0.908981\n0.909435\n0.909956\n0.910562\n0.911266\n0.912069\n0.912953\n0.913888\n0.914843\n...\n1.849416\n1.860089\n1.872713\n1.885405\n1.896269\n1.903926\n1.908109\n1.910104\n1.912738\n1.919517\n\n\nicr042897\n0.810133\n0.809940\n0.809745\n0.809616\n0.809566\n0.809563\n0.809558\n0.809516\n0.809437\n0.809359\n...\n2.069042\n2.098991\n2.124525\n2.143315\n2.153621\n2.154386\n2.145273\n2.126816\n2.101002\n2.071937\n\n\nicr049675\n0.711836\n0.712355\n0.713021\n0.713747\n0.714438\n0.715037\n0.715535\n0.715951\n0.716298\n0.716546\n...\n2.042677\n2.025819\n2.009498\n1.997672\n1.991367\n1.989609\n1.990601\n1.992822\n1.995704\n1.999495\n\n\nicr034693\n0.788686\n0.789201\n0.789494\n0.789611\n0.789634\n0.789656\n0.789744\n0.789912\n0.790109\n0.790249\n...\n2.083768\n2.084394\n2.084220\n2.081256\n2.072820\n2.055961\n2.028284\n1.989270\n1.941584\n1.891300\n\n\nicr033950\n0.752478\n0.753251\n0.753915\n0.754423\n0.754733\n0.754816\n0.754683\n0.754402\n0.754084\n0.753866\n...\n1.925999\n1.940593\n1.958213\n1.975761\n1.988359\n1.990657\n1.979214\n1.954502\n1.920762\n1.884145\n\n\n\n\n5 rows √ó 2542 columns\n\n\n\n\n# table with FTIR spectra for each sample\ndf_KBR_FTIRspectra = kbr_df.T.reset_index()\n\ndf_KBR_FTIRspectra = df_KBR_FTIRspectra.rename(columns={'index': 'labda'})\ndf_KBR_FTIRspectra.labda = pd.to_numeric(df_KBR_FTIRspectra.labda)\n\ndf_KBR_FTIRspectra.head()\n\n\n\n\n\n\n\n\nlabda\nicr033603\nicr042897\nicr049675\nicr034693\nicr033950\nicr034794\nicr015953\nicr050394\nicr048771\n...\nicr011182\nicr073540\nicr049437\nicr037699\nicr056181\nicr055563\nicr010159\nicr074792\nicr011321\nicr062275\n\n\n\n\n0\n3998\n0.908592\n0.810133\n0.711836\n0.788686\n0.752478\n0.799947\n0.819920\n0.701762\n0.854325\n...\n0.741123\n0.707601\n0.735885\n0.792272\n0.784010\n0.675416\n0.859956\n0.675066\n0.807336\n0.753118\n\n\n1\n3997\n0.908981\n0.809940\n0.712355\n0.789201\n0.753251\n0.801083\n0.819910\n0.702142\n0.854623\n...\n0.741535\n0.707462\n0.735881\n0.793356\n0.784840\n0.675505\n0.859586\n0.675193\n0.807802\n0.753273\n\n\n2\n3996\n0.909435\n0.809745\n0.713021\n0.789494\n0.753915\n0.802259\n0.820077\n0.702510\n0.854676\n...\n0.741678\n0.707289\n0.735951\n0.794496\n0.785365\n0.675546\n0.859557\n0.675159\n0.808202\n0.753584\n\n\n3\n3994\n0.909956\n0.809616\n0.713747\n0.789611\n0.754423\n0.803318\n0.820496\n0.702824\n0.854543\n...\n0.741524\n0.707115\n0.736090\n0.795609\n0.785554\n0.675588\n0.859890\n0.674942\n0.808552\n0.754089\n\n\n4\n3993\n0.910562\n0.809566\n0.714438\n0.789634\n0.754733\n0.804114\n0.821235\n0.703081\n0.854348\n...\n0.741103\n0.706966\n0.736295\n0.796670\n0.785448\n0.675682\n0.860528\n0.674554\n0.808865\n0.754796\n\n\n\n\n5 rows √ó 1889 columns\n\n\n\n\nplt.figure(figsize= (6,6))\n\nplt.plot(df_KBR_FTIRspectra['labda'],df_KBR_FTIRspectra.iloc[:, [4,257,100]]) \n\nplt.title('Example spectra MIR-FTIR KBr window')\n\nplt.xticks(rotation=90)\nplt.xlim(4000,400)\nplt.ylabel('Absorbance (A.U.)', fontsize=14)\nplt.xlabel('Wavelengths (cm-1)', fontsize = 14)\n\nText(0.5, 0, 'Wavelengths (cm-1)')\n\n\n\n\n\n\n\n- Alpha spectrometer - ZnSe window\n\nZnSe_SPECTRA_DIR = 'Bruker_Alpha_ZnSe/*'\nAFSIS_PATH = Path('afsis/2009-2013/Dry_Chemistry/ICRAF')\n\nnames = []\nspectra = []\n\nfor path in tqdm(AFSIS_PATH.glob(ZnSe_SPECTRA_DIR )):\n    if path.is_file():\n        spect_data = read_file(path)\n        spectra.append(spect_data[\"AB\"])\n        names.append(path.stem)\nwave_nums = spect_data.get_range()\n\ncolumn_names1 = ['{:.0f}'.format(x) for x in wave_nums]\nZnSe_df = pd.DataFrame(spectra, index=names, columns=column_names1)\n\nTqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  for path in tqdm(AFSIS_PATH.glob(ZnSe_SPECTRA_DIR )):\n\n\n\n\n\n\n\n\n\nZnSe_df.head()\n\n\n\n\n\n\n\n\n3996\n3994\n3992\n3990\n3988\n3986\n3984\n3982\n3980\n3978\n...\n518\n516\n514\n512\n510\n508\n506\n504\n502\n500\n\n\n\n\nicr033603\n1.336817\n1.337777\n1.338591\n1.339180\n1.339602\n1.339999\n1.340485\n1.341091\n1.341770\n1.342445\n...\n2.274969\n2.276752\n2.252275\n2.223028\n2.210677\n2.225878\n2.251971\n2.209258\n2.221467\n0.0\n\n\nicr042897\n1.217690\n1.218229\n1.218768\n1.219244\n1.219670\n1.220118\n1.220660\n1.221307\n1.221991\n1.222615\n...\n2.221671\n2.189322\n2.175592\n2.185125\n2.209400\n2.227240\n2.216305\n2.164982\n2.129341\n0.0\n\n\nicr049675\n1.155588\n1.155937\n1.156262\n1.156588\n1.156925\n1.157270\n1.157606\n1.157913\n1.158161\n1.158312\n...\n2.400493\n2.362777\n2.327363\n2.304740\n2.297083\n2.296757\n2.288914\n2.260693\n2.206604\n0.0\n\n\nicr034693\n1.215348\n1.215774\n1.216170\n1.216544\n1.216940\n1.217407\n1.217960\n1.218564\n1.219142\n1.219622\n...\n2.146795\n2.123838\n2.091934\n2.052702\n2.009385\n1.963674\n1.915830\n1.866890\n1.819085\n0.0\n\n\nicr033950\n1.163011\n1.163466\n1.163930\n1.164330\n1.164612\n1.164770\n1.164867\n1.165003\n1.165248\n1.165577\n...\n2.140887\n2.131997\n2.106665\n2.065614\n2.013958\n1.957800\n1.904114\n1.861121\n1.835173\n0.0\n\n\n\n\n5 rows √ó 1716 columns\n\n\n\n\n# - table with FTIR spectra for each sample\n\ndf_ZnSe_FTIRspectra = ZnSe_df.T.reset_index()\n\ndf_ZnSe_FTIRspectra = df_ZnSe_FTIRspectra.rename(columns={'index': 'labda'})\ndf_ZnSe_FTIRspectra.labda = pd.to_numeric(df_ZnSe_FTIRspectra.labda)\n\n\nprint(\"spectral range:\", df_ZnSe_FTIRspectra.labda.min(), \"cm-1 - \",df_ZnSe_FTIRspectra.labda.max(), \"cm-1\" )\n\nspectral range: 500 cm-1 -  3996 cm-1\n\n\n\n\n- Tensor 27 HTS-XT spectrometer\nKBr window and wider range: both MID and Near IR\n\nHTSXT_SPECTRA_DIR = 'Bruker_HTSXT/*'\nAFSIS_PATH = Path('afsis/2009-2013/Dry_Chemistry/ICRAF')\n\nnames = []\nspectra = []\n\nfor path in tqdm(AFSIS_PATH.glob(HTSXT_SPECTRA_DIR )):\n    if path.is_file():\n        spect_data = read_file(path)\n        spectra.append(spect_data[\"AB\"])\n        names.append(path.stem)\nwave_nums = spect_data.get_range()\n\ncolumn_names = ['{:.0f}'.format(x) for x in wave_nums]\nhtsxt_df = pd.DataFrame(spectra, index=names, columns=column_names)\n\nTqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  for path in tqdm(AFSIS_PATH.glob(HTSXT_SPECTRA_DIR )):\n\n\n\n\n\n\n\n\n\nprint('Total measurements',len(htsxt_df.index),', unique samples', len(htsxt_df.index.unique()) )\n\nTotal measurements 7346 , unique samples 1839\n\n\n\n# table with FTIR spectra for each sample\n\ndf_htsxt_FTIRspectra = htsxt_df.T.reset_index()\n\ndf_htsxt_FTIRspectra = df_htsxt_FTIRspectra.rename(columns={'index': 'labda'})\ndf_htsxt_FTIRspectra.labda = pd.to_numeric(df_htsxt_FTIRspectra.labda)\nprint(\"spectral range:\", df_htsxt_FTIRspectra.labda.min(), \"cm-1 - \",df_htsxt_FTIRspectra.labda.max(), \"cm-1\" )\n\nspectral range: 600 cm-1 -  7498 cm-1\n\n\nAre Tensor 27 HTS-XT spectrometer measurements reproducible?\n\nplt.plot(df_htsxt_FTIRspectra['labda'],df_htsxt_FTIRspectra['icr034794'], label = 'HTSXT') \n\nplt.title('Example same sample repetitions - HTSXT')\nplt.legend()\n\nplt.xticks(rotation=90)\nplt.xlim(7500,400)\nplt.ylabel('Absorbance (A.U.)', fontsize=14)\nplt.xlabel('Wavelengths (cm-1)', fontsize = 14)\n\nprint('4 repetitions, identical result')\n\n4 repetitions, identical result\n\n\n\n\n\nlet‚Äôs average the repetitions\n\nhtsxt_df = htsxt_df.reset_index()\nhtsxt_df.head()\n\n\n\n\n\n\n\n\nindex\n7498\n7496\n7494\n7492\n7490\n7488\n7486\n7485\n7483\n...\n617\n615\n613\n611\n609\n607\n606\n604\n602\n600\n\n\n\n\n0\nicr033603\n0.363767\n0.358597\n0.352962\n0.355229\n0.364906\n0.370382\n0.363909\n0.354324\n0.350436\n...\n1.903912\n1.892159\n1.883104\n1.875069\n1.863860\n1.846478\n1.827501\n1.811521\n1.798762\n1.785432\n\n\n1\nicr010356\n0.138930\n0.132892\n0.136494\n0.148280\n0.153970\n0.145607\n0.134926\n0.131624\n0.130996\n...\n1.799327\n1.789603\n1.779350\n1.767130\n1.751588\n1.736565\n1.723430\n1.712572\n1.705888\nNaN\n\n\n2\nicr055782\n0.198082\n0.193039\n0.187723\n0.190430\n0.200350\n0.205885\n0.199621\n0.190752\n0.188001\n...\n1.612597\n1.624695\n1.637514\n1.644494\n1.651400\n1.663048\n1.675583\n1.689286\n1.701757\n1.708080\n\n\n3\nicr011264\n0.327606\n0.321836\n0.325163\n0.336058\n0.341134\n0.332636\n0.321906\n0.318676\n0.317745\n...\n1.875749\n1.869287\n1.868335\n1.877906\n1.887795\n1.885020\n1.873055\n1.863219\n1.855847\nNaN\n\n\n4\nicr034402\n0.339334\n0.334438\n0.329383\n0.331227\n0.339271\n0.343485\n0.337471\n0.329251\n0.326680\n...\n1.491697\n1.492001\n1.491628\n1.489214\n1.486117\n1.482170\n1.478855\n1.479840\n1.483233\n1.483134\n\n\n\n\n5 rows √ó 3579 columns\n\n\n\n\nhtsxt_df = htsxt_df.rename(columns={'index': 'SSN'})\n\n\ngb_htsxt = htsxt_df.groupby(['SSN']).mean().reset_index()\nprint(gb_htsxt.shape)\n\n(1839, 3579)\n\n\n\ngb_htsxt = gb_htsxt.set_index('SSN')\n\n\ngb_htsxt_FTIRspectra = gb_htsxt.T.reset_index()\n\ngb_htsxt_FTIRspectra = gb_htsxt_FTIRspectra.rename(columns={'index': 'labda'})\ngb_htsxt_FTIRspectra.labda = pd.to_numeric(gb_htsxt_FTIRspectra.labda)\n\n\nplt.plot(df_htsxt_FTIRspectra['labda'],df_htsxt_FTIRspectra['icr034794'], label = 'repetition') \nplt.plot(gb_htsxt_FTIRspectra['labda'],gb_htsxt_FTIRspectra['icr034794'],color = 'k', label = 'average') \nplt.title('Same sample repetitions and average')\nplt.legend()\n\nplt.xticks(rotation=90)\nplt.xlim(7500,400)\nplt.ylabel('Absorbance (A.U.)', fontsize=14)\nplt.xlabel('Wavelengths (cm-1)', fontsize = 14)\n\nprint('4 repetitions, identical result')\n\n4 repetitions, identical result\n\n\n\n\n\n\nprint('what is the difference between different spectrometers measurements?')\nplt.figure(figsize= (6,6))\n\nplt.plot(df_KBR_FTIRspectra['labda'],df_KBR_FTIRspectra['icr042897'], label = 'Kbr') \nplt.plot(df_ZnSe_FTIRspectra['labda'],df_ZnSe_FTIRspectra['icr042897'], label = 'ZnSe') \nplt.plot(gb_htsxt_FTIRspectra['labda'],gb_htsxt_FTIRspectra['icr042897'], label = 'HTSXT') \n\nplt.title('Example different spectrometers')\nplt.legend()\n\nplt.xticks(rotation=90)\nplt.xlim(4000,400)\nplt.ylabel('Absorbance (A.U.)', fontsize=14)\nplt.xlabel('Wavelengths (cm-1)', fontsize = 14)\n\nwhat is the difference between different spectrometers measurements?\n\n\nText(0.5, 0, 'Wavelengths (cm-1)')\n\n\n\n\n\n\nprint(df_KBR_FTIRspectra.shape)\ndf_KBR_FTIRspectra.head()\n\n(2542, 1889)\n\n\n\n\n\n\n\n\n\nlabda\nicr033603\nicr042897\nicr049675\nicr034693\nicr033950\nicr034794\nicr015953\nicr050394\nicr048771\n...\nicr011182\nicr073540\nicr049437\nicr037699\nicr056181\nicr055563\nicr010159\nicr074792\nicr011321\nicr062275\n\n\n\n\n0\n3998\n0.908592\n0.810133\n0.711836\n0.788686\n0.752478\n0.799947\n0.819920\n0.701762\n0.854325\n...\n0.741123\n0.707601\n0.735885\n0.792272\n0.784010\n0.675416\n0.859956\n0.675066\n0.807336\n0.753118\n\n\n1\n3997\n0.908981\n0.809940\n0.712355\n0.789201\n0.753251\n0.801083\n0.819910\n0.702142\n0.854623\n...\n0.741535\n0.707462\n0.735881\n0.793356\n0.784840\n0.675505\n0.859586\n0.675193\n0.807802\n0.753273\n\n\n2\n3996\n0.909435\n0.809745\n0.713021\n0.789494\n0.753915\n0.802259\n0.820077\n0.702510\n0.854676\n...\n0.741678\n0.707289\n0.735951\n0.794496\n0.785365\n0.675546\n0.859557\n0.675159\n0.808202\n0.753584\n\n\n3\n3994\n0.909956\n0.809616\n0.713747\n0.789611\n0.754423\n0.803318\n0.820496\n0.702824\n0.854543\n...\n0.741524\n0.707115\n0.736090\n0.795609\n0.785554\n0.675588\n0.859890\n0.674942\n0.808552\n0.754089\n\n\n4\n3993\n0.910562\n0.809566\n0.714438\n0.789634\n0.754733\n0.804114\n0.821235\n0.703081\n0.854348\n...\n0.741103\n0.706966\n0.736295\n0.796670\n0.785448\n0.675682\n0.860528\n0.674554\n0.808865\n0.754796\n\n\n\n\n5 rows √ó 1889 columns\n\n\n\n\n\n- build unique dataset for FTIR\n\nKBr_list = kbr_df.index.tolist()\nZnSe_list = ZnSe_df.index.tolist()\nHTSXT_list = gb_htsxt.index.tolist()\nprint('samples tested with alpha-KBr', len(KBr_list))\nprint('samples tested with alpha-ZnSe', len(ZnSe_list))\nprint('samples tested with Tensor27', len(HTSXT_list))\n\nprint (\"difference samples alpha spectrometers:\",len(KBr_list) - len(ZnSe_list))\nprint (\"difference samples alpha_kbr to tensor27 spectrometers:\", len(KBr_list) - len(HTSXT_list))\n\nsamples tested with alpha-KBr 1888\nsamples tested with alpha-ZnSe 1835\nsamples tested with Tensor27 1839\ndifference samples alpha spectrometers: 53\ndifference samples alpha_kbr to tensor27 spectrometers: 49\n\n\n\ndiff_list1 = np.setdiff1d(KBr_list,ZnSe_list)\nprint(\"samples tested using the Alpha-KBr and not the Alpha-ZnSe spectrometer\")\nprint(len(diff_list1))\n\nsamples tested using the Alpha-KBr and not the Alpha-ZnSe spectrometer\n53\n\n\n\ndiff_list_KBr = np.setdiff1d(HTSXT_list, KBr_list)\nprint(\"samples tested using the Tensor27 and not the Alpha-KBr spectrometer\")\nprint(len(diff_list_KBr))\n\nsamples tested using the Tensor27 and not the Alpha-KBr spectrometer\n8\n\n\n\nmask_diff_kbr = kbr_df[kbr_df.index.isin(diff_list )]\nmask_diff_znse = ZnSe_df[ZnSe_df.index.isin(diff_list )]\nmask_diff_htsxt = gb_htsxt[gb_htsxt.index.isin(diff_list )]\nprint('KBr',len(mask_diff_kbr.index) ,'ZnSe', len(mask_diff_znse.index), 'HTSXT', len(mask_diff_htsxt.index))\n\nKBr 1616 ZnSe 1616 HTSXT 1615\n\n\n\nMost information from Alpha spectrometers and MPA is redundant. Tensor27 NIR peaks provide the same information for both MID and Near IR range"
  },
  {
    "objectID": "posts/fertility_map_africa/afsis-soil-chem-EDA.html#wet-chemistry",
    "href": "posts/fertility_map_africa/afsis-soil-chem-EDA.html#wet-chemistry",
    "title": "Tools - Soil Chemistry Dataset",
    "section": "4. Wet chemistry",
    "text": "4. Wet chemistry\n\nWET_CHEM_PATH1 = 'afsis/2009-2013/Wet_Chemistry/CROPNUTS/Wet_Chemistry_CROPNUTS.csv'\n\nwet_chem_df = pd.read_csv(WET_CHEM_PATH1)#, usecols=columns_to_load)\nelements = ['SSN','M3 Ca', 'M3 K', 'M3 Al', 'M3 P', 'M3 S', 'PH']\nwet_chem_df = wet_chem_df[elements]\n\nprint(wet_chem_df.shape)\nwet_chem_df.head()\n\n(1907, 7)\n\n\n\n\n\n\n\n\n\nSSN\nM3 Ca\nM3 K\nM3 Al\nM3 P\nM3 S\nPH\n\n\n\n\n0\nicr006475\n207.1\n306.30\n1095.0\n4.495\n18.960\n4.682\n\n\n1\nicr006586\n1665.0\n1186.00\n1165.0\n12.510\n13.600\n7.062\n\n\n2\nicr007929\n2518.0\n72.57\n727.6\n21.090\n14.810\n7.114\n\n\n3\nicr008008\n734.3\n274.60\n1458.0\n109.200\n11.400\n5.650\n\n\n4\nicr010198\n261.8\n91.76\n2166.0\n3.958\n5.281\n5.501\n\n\n\n\n\n\n\n\nWET_CHEM_PATH2 = 'afsis/2009-2013/Wet_Chemistry/RRES/Wet_Chemistry_RRES.csv'\n\n#columns_to_load = elements + ['SSN']\n\n\nwet_chem_df1 = pd.read_csv(WET_CHEM_PATH2)#, usecols=columns_to_load)\nelements = ['SSN','pH', '%N', 'C % Org', 'ICP OES K mg/kg ', 'ICP OES P mg/kg ']\nwet_chem_df1 = wet_chem_df1[elements]\nwet_chem_df1 = wet_chem_df1.rename(columns={\"%N\": \"Leco_N_ppm\"})\nwet_chem_df1['Leco_N_ppm'] = wet_chem_df1['Leco_N_ppm']*10000\nprint(wet_chem_df1.columns)\nwet_chem_df1.head()\n\nIndex(['SSN', 'pH', 'Leco_N_ppm', 'C % Org', 'ICP OES K mg/kg ',\n       'ICP OES P mg/kg '],\n      dtype='object')\n\n\n\n\n\n\n\n\n\nSSN\npH\nLeco_N_ppm\nC % Org\nICP OES K mg/kg\nICP OES P mg/kg\n\n\n\n\n0\nicr006454\n7.85\n800.0\n0.94\n8517.919223\n96.575131\n\n\n1\nicr006455\n8.03\n600.0\n0.70\n10859.303780\n117.423139\n\n\n2\nicr006474\n5.01\n500.0\n0.57\n1343.124117\n87.040073\n\n\n3\nicr006475\n4.57\n500.0\n0.47\n1487.768795\n83.555482\n\n\n4\nicr006492\n6.78\n900.0\n0.98\n2999.240760\n150.936463\n\n\n\n\n\n\n\n\nWET_CHEM_PATH3 = 'afsis/2009-2013/Wet_Chemistry/ICRAF/Wet_Chemistry_ICRAF.csv'\n#elements = ['M3 Ca', 'M3 K', 'M3 Al']\n#columns_to_load = elements + ['SSN']\n\n\nwet_chem_df2 = pd.read_csv(WET_CHEM_PATH3)#, usecols=columns_to_load)\n#print(wet_chem_df2.columns)\nelements = ['SSN','Psa asand', 'Psa asilt','Psa aclay', 'Volfr', 'Awc1','Lshrinkpct', 'Acidified nitrogen',\n       'Acidified carbon']\nwet_chem_df2 = wet_chem_df2[elements]\nwet_chem_df2['Acidified nitrogen'] = wet_chem_df2['Acidified nitrogen']*10000\nwet_chem_df2 = wet_chem_df2.rename(columns={\"Acidified nitrogen\": \"Flash2000_N_ppm\"})\n\nprint(wet_chem_df2.shape)\nwet_chem_df2.head()\n\n(1907, 9)\n\n\n\n\n\n\n\n\n\nSSN\nPsa asand\nPsa asilt\nPsa aclay\nVolfr\nAwc1\nLshrinkpct\nFlash2000_N_ppm\nAcidified carbon\n\n\n\n\n0\nicr005928\n90.993000\n8.111667\n0.896333\n1.190000\n0.070768\n0.000000\n296.21345\n0.425854\n\n\n1\nicr005929\n87.847000\n11.416000\n0.737000\n1.192000\n0.061710\n5.000000\n230.68986\n0.263235\n\n\n2\nicr005946\n94.408333\n5.335333\n0.256000\n1.171280\n0.115414\n5.714286\n313.39549\n0.392983\n\n\n3\nicr005947\n94.601333\n5.239333\n0.159000\n1.198744\n0.122856\n0.000000\n170.62043\n0.233496\n\n\n4\nicr005965\n90.015333\n9.195667\n0.789000\n1.081575\n0.100874\n5.000000\n831.45402\n0.860801"
  },
  {
    "objectID": "posts/fertility_map_africa/afsis-soil-chem-EDA.html#join-and-clean-datasets",
    "href": "posts/fertility_map_africa/afsis-soil-chem-EDA.html#join-and-clean-datasets",
    "title": "Tools - Soil Chemistry Dataset",
    "section": "5. Join and clean datasets",
    "text": "5. Join and clean datasets\n\n5.1 Elemental analysis\n\ndf_elements1 = pd.merge(left=wet_chem_df1, right=df_xrf_reduced, left_on='SSN', right_on='SSN')\nprint(df_elements1.shape)\n\n\ndf_elements2 = pd.merge(left=wet_chem_df2, right=df_elements1, left_on='SSN', right_on='SSN')\nprint(df_elements2.shape)\n\n\ndf_elements = pd.merge(left=wet_chem_df, right=df_elements2, left_on='SSN', right_on='SSN')\nprint(df_elements.shape)\nprint(df_elements.columns)\n\ndf_elements.head()\n\n(467, 17)\n(467, 25)\n(467, 31)\nIndex(['SSN', 'M3 Ca', 'M3 K', 'M3 Al', 'M3 P', 'M3 S', 'PH', 'Psa asand',\n       'Psa asilt', 'Psa aclay', 'Volfr', 'Awc1', 'Lshrinkpct',\n       'Flash2000_N_ppm', 'Acidified carbon', 'pH', 'Leco_N_ppm', 'C % Org',\n       'ICP OES K mg/kg ', 'ICP OES P mg/kg ', 'P', 'K', 'S', 'Ca', 'Mg', 'Cu',\n       'Cl', 'Zn', 'Fe', 'Mn', 'Mo'],\n      dtype='object')\n\n\n\n\n\n\n\n\n\nSSN\nM3 Ca\nM3 K\nM3 Al\nM3 P\nM3 S\nPH\nPsa asand\nPsa asilt\nPsa aclay\n...\nK\nS\nCa\nMg\nCu\nCl\nZn\nFe\nMn\nMo\n\n\n\n\n0\nicr006475\n207.1\n306.30\n1095.000\n4.495\n18.960\n4.682\n97.848667\n1.845333\n0.306000\n...\n12991.3\n45.7\n944.1\n5575.0\n13.0\n210.2\n22.0\n12501.3\n81.9\n184.5\n\n\n1\nicr006586\n1665.0\n1186.00\n1165.000\n12.510\n13.600\n7.062\n89.520000\n9.553667\n0.926333\n...\n15173.5\n45.7\n9301.0\n5519.1\n18.4\n152.6\n38.5\n24094.6\n422.4\n184.5\n\n\n2\nicr021104\n258.7\n35.25\n441.400\n4.424\n3.608\n5.522\n89.950000\n5.205000\n4.845000\n...\n6838.9\n45.7\n884.3\n5575.0\n2.8\n229.5\n2.3\n2213.4\n13.2\n184.5\n\n\n3\nicr033622\n11858.3\n1156.00\n108.286\n31.233\n25.460\n8.583\n91.445000\n6.310000\n2.245000\n...\n15845.8\n45.7\n46529.1\n33771.2\n13.0\n58.2\n29.8\n24135.0\n460.4\n184.5\n\n\n4\nicr006570\n896.2\n607.30\n1151.000\n5.986\n20.080\n6.661\n97.789667\n1.885000\n0.325667\n...\n12201.6\n45.7\n1790.1\n5575.0\n7.7\n122.5\n13.4\n9135.1\n132.6\n184.5\n\n\n\n\n5 rows √ó 31 columns\n\n\n\n\n#check for possible negative values\nfor col in df_elements.columns.tolist()[1:]:\n    if df_elements[col].dtype == np.float64:\n         df_elements[col][df_elements[col] &lt; 0] =np.nan\n\nprint(df_elements.isna().sum().sum())\n\n54\n\n\nSettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_elements[col][df_elements[col] &lt; 0] =np.nan\n\n\n\n\nmerge geographical and chemical data\n\ndf_geoelements = pd.merge(left=df_elements, right=df_geo, left_on='SSN', right_on='SSN')\n\nprint(df_geoelements.shape)\nprint(df_geoelements.columns)\ndf_geoelements.head()\n\n(467, 37)\nIndex(['SSN', 'M3 Ca', 'M3 K', 'M3 Al', 'M3 P', 'M3 S', 'PH', 'Psa asand',\n       'Psa asilt', 'Psa aclay', 'Volfr', 'Awc1', 'Lshrinkpct',\n       'Flash2000_N_ppm', 'Acidified carbon', 'pH', 'Leco_N_ppm', 'C % Org',\n       'ICP OES K mg/kg ', 'ICP OES P mg/kg ', 'P', 'K', 'S', 'Ca', 'Mg', 'Cu',\n       'Cl', 'Zn', 'Fe', 'Mn', 'Mo', 'Latitude', 'Longitude', 'Cluster',\n       'Depth', 'Country', 'Cultivated'],\n      dtype='object')\n\n\n\n\n\n\n\n\n\nSSN\nM3 Ca\nM3 K\nM3 Al\nM3 P\nM3 S\nPH\nPsa asand\nPsa asilt\nPsa aclay\n...\nZn\nFe\nMn\nMo\nLatitude\nLongitude\nCluster\nDepth\nCountry\nCultivated\n\n\n\n\n0\nicr006475\n207.1\n306.30\n1095.000\n4.495\n18.960\n4.682\n97.848667\n1.845333\n0.306000\n...\n22.0\n12501.3\n81.9\n184.5\n-6.088750\n36.435982\n2\nsub\nTanzania\nFalse\n\n\n1\nicr006586\n1665.0\n1186.00\n1165.000\n12.510\n13.600\n7.062\n89.520000\n9.553667\n0.926333\n...\n38.5\n24094.6\n422.4\n184.5\n-6.055750\n36.457722\n8\ntop\nTanzania\nFalse\n\n\n2\nicr021104\n258.7\n35.25\n441.400\n4.424\n3.608\n5.522\n89.950000\n5.205000\n4.845000\n...\n2.3\n2213.4\n13.2\n184.5\n-8.049305\n37.333698\n14\nsub\nTanzania\nFalse\n\n\n3\nicr033622\n11858.3\n1156.00\n108.286\n31.233\n25.460\n8.583\n91.445000\n6.310000\n2.245000\n...\n29.8\n24135.0\n460.4\n184.5\n4.178087\n38.261890\n2\nsub\nEthiopia\nNaN\n\n\n4\nicr006570\n896.2\n607.30\n1151.000\n5.986\n20.080\n6.661\n97.789667\n1.885000\n0.325667\n...\n13.4\n9135.1\n132.6\n184.5\n-6.069970\n36.464588\n7\ntop\nTanzania\nFalse\n\n\n\n\n5 rows √ó 37 columns\n\n\n\n\n\ngeographical distribution of the selected samples\n\npd.value_counts(df_geoelements['Country']).plot.bar(title='Measurements per country')\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd0c8193b20&gt;\n\n\n\n\n\n\n #Draw map\n\n\nm = folium.Map(location=[-3.5, 35.6], tiles=\"stamentoner\", zoom_start=5)\n \nfor _, row in df_geoelements.iterrows():\n    if row[['Latitude', 'Longitude']].notnull().all():\n        folium.Marker([row['Latitude'], \n                       row['Longitude']], \n                      popup=row['SSN']\n                     ).add_to(m)\n\n#m\n\nImage(filename='img/folium.png') \n\n\n\n\n\n\nimputation of missing values\n\ndef replace_missings(data):\n    # this replaces missings with medians\n    # NOTE: mixed string num columns it does not do anything with\n    for cols in data._get_numeric_data().columns:\n        data[cols].fillna(value=data[cols].median(), inplace=True)\n\n\nreplace_missings(df_geoelements)\n\nprint(df_geoelements['Cultivated'].unique())\ndf_geoelements['Cultivated'] = df_geoelements['Cultivated'].fillna('unknown')\nprint(df_geoelements['Cultivated'].unique())\nprint(df_geoelements.isna().sum().sum())\n\n[False 'unknown' True]\n[False 'unknown' True]\n0\n\n\n\n\noutliers\n\ndef detect_outliers(df, n, features):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than n outliers according\n    to the Tukey method.\n    \"\"\"\n    outlier_indices = []\n\n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col], 75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n\n        # outlier step\n        outlier_step = 3 * IQR\n\n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] &lt; Q1 - outlier_step) | (df[col] &gt; Q3 + outlier_step)].index\n\n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n\n    # select observations containing more than 1 outlier\n    outlier_indices = Counter(outlier_indices)        \n  \n\n    return outlier_indices\n\n# detect outliers from list of features\nlof = ['M3 Ca', 'M3 K', 'M3 Al', 'M3 P', 'M3 S', 'PH', 'Psa asand',\n       'Psa asilt', 'Psa aclay', 'Volfr', 'Awc1', 'Lshrinkpct', 'pH', 'Flash2000_N_ppm','Leco_N_ppm',\n       'C % Org', 'ICP OES K mg/kg ', 'ICP OES P mg/kg ', 'P', 'K', 'S', 'Ca',\n       'Mg', 'Cu', 'Cl', 'Zn', 'Fe', 'Mn', 'Mo']#, \nOutliers_to_drop = detect_outliers(df_geoelements, 1, lof)\n\nprint(len(Outliers_to_drop), 'outliers according to Tukey method')\nif len(Outliers_to_drop)&gt;50:\n    print('loss of information would be too high if Tukey method would be applied')\n\n209 outliers according to Tukey method\nloss of information would be too high if Tukey method would be applied\n\n\n\ndf_chem = df_geoelements[lof]\ndf_chem_scaled =((df_chem -df_chem.min())/(df_chem.max()-df_chem.min()))*10\n\n%matplotlib inline\nplt.figure(figsize= (22,6))\nbox_plot_scaled = sns.boxplot( data= df_chem_scaled)\nfig = box_plot_scaled.get_figure()\nplt.xticks(rotation=90)\nplt.ylabel(\"Scaled values\")\nfig.savefig(\"box.png\", dpi= 100)\n\n\n\n\n\ndf_chem.describe()\n\n\n\n\n\n\n\n\nM3 Ca\nM3 K\nM3 Al\nM3 P\nM3 S\nPH\nPsa asand\nPsa asilt\nPsa aclay\nVolfr\n...\nK\nS\nCa\nMg\nCu\nCl\nZn\nFe\nMn\nMo\n\n\n\n\ncount\n467.000000\n467.000000\n467.000000\n467.000000\n467.000000\n467.000000\n467.000000\n467.000000\n467.000000\n467.000000\n...\n467.000000\n467.000000\n467.000000\n467.000000\n467.000000\n467.000000\n467.000000\n467.000000\n467.000000\n467.000000\n\n\nmean\n1605.142957\n186.569448\n802.767263\n10.296321\n23.906516\n6.133593\n84.902406\n8.056587\n7.041254\n1.050843\n...\n11204.939829\n77.834261\n5092.092505\n7004.122270\n12.137473\n190.165096\n22.208351\n22659.693362\n316.650107\n187.781370\n\n\nstd\n2780.256854\n303.097846\n423.564349\n22.179751\n154.640351\n1.151291\n14.436237\n5.750215\n9.775127\n0.161490\n...\n13913.144061\n467.413144\n11339.284556\n5675.704395\n13.737260\n1025.070888\n25.877222\n26454.053657\n423.535962\n48.287469\n\n\nmin\n0.001000\n5.110000\n14.300000\n0.001000\n1.490000\n4.000000\n0.440000\n0.000000\n0.000000\n0.644500\n...\n90.400000\n43.400000\n69.700000\n4032.600000\n0.700000\n19.700000\n0.900000\n729.900000\n7.900000\n184.500000\n\n\n25%\n236.800000\n47.200000\n446.543000\n2.495000\n4.732000\n5.315000\n83.500000\n4.352500\n2.375000\n0.943500\n...\n1917.350000\n45.700000\n344.750000\n5575.000000\n3.900000\n52.550000\n6.600000\n6575.600000\n67.650000\n184.500000\n\n\n50%\n560.700000\n82.200000\n735.486000\n4.495000\n7.530000\n5.950000\n88.525000\n6.710000\n4.595000\n1.050500\n...\n5963.000000\n45.700000\n991.700000\n5575.000000\n8.000000\n84.400000\n15.200000\n15149.100000\n153.900000\n184.500000\n\n\n75%\n1375.400000\n181.900000\n1130.000000\n8.590500\n12.300000\n6.603000\n92.545000\n9.607500\n7.640000\n1.180000\n...\n15435.750000\n45.700000\n3597.000000\n5575.000000\n15.100000\n136.850000\n28.550000\n28039.050000\n377.800000\n184.500000\n\n\nmax\n18510.000000\n3432.000000\n2444.000000\n221.800000\n2728.860000\n9.860000\n100.005000\n35.555000\n81.400000\n1.429500\n...\n106273.400000\n9704.000000\n89497.800000\n51204.800000\n110.300000\n21557.400000\n259.900000\n222256.400000\n3314.100000\n1085.400000\n\n\n\n\n8 rows √ó 29 columns\n\n\n\n\n# export to csv\ndf_geoelements.to_csv( 'elemental_analysis_dataset.csv',index=False)\n\n\n\n5.2 FTIR\n\ndf_FTIR_reindexed = df_KBR_FTIRspectra.set_index('labda')\nmid_infrared_df = df_FTIR_reindexed.T.reset_index()\nmid_infrared_df = mid_infrared_df.rename(columns={'index': 'SSN'})\n\n\n\nI select only the sample that are in the compositional dataframe ‚Äúgeoelements‚Äù\n\n\nThe composition and FTIR dataframes have same sample numbers (SSN) column\n\n\nEach infrared spectrum corresponds to an elemental analysis\n\ncomplete_measurements_list = df_geoelements_reduced.SSN.tolist()\n\n\nmid infrared\n\n\ndf_FTIRdata = mid_infrared_df[mid_infrared_df.SSN.isin(complete_measurements_list)]\nprint(df_FTIRdata.shape)\n#print(df_FTIRdata.isna().sum())\n\n(467, 2543)\n\n\n\ndf_FTIRdata.to_csv( 'middle_infrared_spectra_dataset.csv',index=False)\n\n\nnear infrared\n\n\ndf_FTIR_reindexed1 = df_NIR_FTIRspectra.set_index('labda')\nnear_infrared_df = df_FTIR_reindexed1.T.reset_index()\nnear_infrared_df = near_infrared_df.rename(columns={'index': 'SSN'})\n# I select only the sample that are in the compositional dataframe \"geoelements\"\n\ncomplete_measurements_list = df_geoelements_reduced.SSN.tolist()\n\ndf_FTIRdataM = near_infrared_df[near_infrared_df.SSN.isin(complete_measurements_list)]\nprint(df_FTIRdata.shape)\n#print(df_FTIRdata.isna().sum())\ndf_FTIRdataM.to_csv( 'near_infrared_spectra_dataset.csv',index=False)\n\n(467, 2543)"
  }
]